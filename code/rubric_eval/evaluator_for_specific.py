#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ‰¹é‡è¯„ä¼°æŒ‡å®šé¢˜ç›®å’Œæ¨¡å‹çš„è„šæœ¬
æ ¹æ®é¢˜å·-æ¨¡å‹æ˜ å°„å…³ç³»è¿›è¡Œç²¾å‡†è¯„ä¼°
"""

import os
import sys
import json
import time
import argparse
from typing import Dict, List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

# Add project root to path for imports
import sys
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from code.utils import (
    load_rubrics, load_model_responses, save_json_file, 
    create_output_directory
)
from code.rubric_eval.evaluator import RubricEvaluator


class TargetedEvaluationCoordinator:
    """é’ˆå¯¹ç‰¹å®šé¢˜ç›®çš„ç²¾å‡†è¯„ä¼°åè°ƒå™¨"""
    
    def __init__(self, max_workers: int = 4):
        """
        åˆå§‹åŒ–è¯„ä¼°åè°ƒå™¨
        
        Parameters:
        max_workers (int): æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
        """
        self.max_workers = max_workers
        self.print_lock = Lock()
        
        # é¢˜å·-æ¨¡å‹æ˜ å°„å…³ç³»
        self.question_model_mapping = {
            1: "OpenAI",
            3: "Google", 
            5: "Grok3",
            12: "Grok3deeper",
            22: "Perplexity",
            24: "Zhihu",
            34: "Yiyan",
            45: "OpenAI",
            50: "Google",
            55: "Grok3"
        }
        
        # æ¨¡å‹æ–‡ä»¶æ˜ å°„
        self.model_files = {
            "OpenAI": "data/user_data/OpenAI.json",
            "Google": "data/user_data/Google.json",
            "Grok3": "data/user_data/Grok3.json",
            "Grok3deeper": "data/user_data/Grok3deeper.json",
            "Perplexity": "data/user_data/Perplexity.json",
            "Zhihu": "data/user_data/Zhihu.json",
            "Yiyan": "data/user_data/Yiyan.json"
        }
        
    def safe_print(self, message: str):
        """çº¿ç¨‹å®‰å…¨çš„æ‰“å°å‡½æ•°"""
        with self.print_lock:
            print(message)
    
    def load_all_model_responses(self) -> Dict[str, Dict]:
        """
        åŠ è½½æ‰€æœ‰æ¨¡å‹çš„å“åº”æ•°æ®
        
        Returns:
        Dict[str, Dict]: æ¨¡å‹åç§°åˆ°å“åº”æ•°æ®çš„æ˜ å°„
        """
        all_responses = {}
        
        for model_name, file_path in self.model_files.items():
            if os.path.exists(file_path):
                responses = load_model_responses(file_path)
                if responses:
                    all_responses[model_name] = responses
                    self.safe_print(f"æˆåŠŸåŠ è½½ {model_name} çš„å“åº”æ•°æ®: {len(responses)} ä¸ªé—®é¢˜")
                else:
                    self.safe_print(f"è­¦å‘Š: æ— æ³•åŠ è½½ {model_name} çš„å“åº”æ•°æ®")
            else:
                self.safe_print(f"è­¦å‘Š: æ–‡ä»¶ä¸å­˜åœ¨ - {file_path}")
        
        return all_responses
    
    def prepare_evaluation_tasks(self, all_responses: Dict[str, Dict], 
                               rubrics_dict: Dict) -> List[Tuple]:
        """
        å‡†å¤‡è¯„ä¼°ä»»åŠ¡åˆ—è¡¨
        
        Parameters:
        all_responses (Dict): æ‰€æœ‰æ¨¡å‹çš„å“åº”æ•°æ®
        rubrics_dict (Dict): è¯„åˆ†æ ‡å‡†æ•°æ®
        
        Returns:
        List[Tuple]: è¯„ä¼°ä»»åŠ¡åˆ—è¡¨
        """
        evaluation_tasks = []
        
        for question_id, model_name in self.question_model_mapping.items():
            # æ£€æŸ¥æ¨¡å‹å“åº”æ˜¯å¦å­˜åœ¨
            if model_name not in all_responses:
                self.safe_print(f"è·³è¿‡ Q{question_id}: æ¨¡å‹ {model_name} çš„å“åº”æ•°æ®ä¸å­˜åœ¨")
                continue
            
            # æ£€æŸ¥å…·ä½“é—®é¢˜çš„å“åº”æ˜¯å¦å­˜åœ¨
            model_responses = all_responses[model_name]
            if question_id not in model_responses:
                self.safe_print(f"è·³è¿‡ Q{question_id}: æ¨¡å‹ {model_name} æ²¡æœ‰è¯¥é—®é¢˜çš„å“åº”")
                continue
            
            # æ£€æŸ¥è¯„åˆ†æ ‡å‡†æ˜¯å¦å­˜åœ¨
            if question_id not in rubrics_dict:
                self.safe_print(f"è·³è¿‡ Q{question_id}: æ²¡æœ‰å¯¹åº”çš„è¯„åˆ†æ ‡å‡†")
                continue
            
            # å‡†å¤‡ä»»åŠ¡å‚æ•°
            response_data = model_responses[question_id]
            rubric_data = rubrics_dict[question_id]
            
            task_args = (question_id, model_name, response_data, rubric_data)
            evaluation_tasks.append(task_args)
            
            self.safe_print(f"å‡†å¤‡è¯„ä¼°ä»»åŠ¡: Q{question_id} - {model_name}")
        
        return evaluation_tasks
    
    def evaluate_single_task(self, args: Tuple, eval_model: str) -> Dict:
        """
        è¯„ä¼°å•ä¸ªä»»åŠ¡
        
        Parameters:
        args (Tuple): ä»»åŠ¡å‚æ•°
        eval_model (str): è¯„ä¼°æ¨¡å‹
        
        Returns:
        Dict: è¯„ä¼°ç»“æœ
        """
        question_id, model_name, response_data, rubric_data = args
        
        try:
            # åˆ›å»ºè¯„ä¼°å™¨å®ä¾‹
            evaluator = RubricEvaluator(eval_model=eval_model)
            
            # æå–æ•°æ®
            question = response_data["question"]
            ai_response = response_data["response"]
            reference_rubrics = rubric_data["rubric"]
            
            # æ‰§è¡Œè¯„ä¼°
            result = evaluator.evaluate_response(
                question_id=question_id,
                model_name=model_name,
                question=question,
                ai_response=ai_response,
                reference_rubrics=reference_rubrics
            )
            
            self.safe_print(f"å®Œæˆè¯„ä¼°: Q{question_id} - {model_name}")
            return result
            
        except Exception as e:
            error_result = {
                "question_id": question_id,
                "model_name": model_name,
                "error": str(e),
                "status": "failed"
            }
            self.safe_print(f"è¯„ä¼°å¤±è´¥: Q{question_id} - {model_name}: {e}")
            return error_result
    
    def run_targeted_evaluation(self, rubrics_file: str, output_dir: str, 
                               eval_model: str = "o3-mini") -> str:
        """
        è¿è¡Œé’ˆå¯¹æ€§è¯„ä¼°
        
        Parameters:
        rubrics_file (str): è¯„åˆ†æ ‡å‡†æ–‡ä»¶è·¯å¾„
        output_dir (str): è¾“å‡ºç›®å½•
        eval_model (str): è¯„ä¼°æ¨¡å‹
        
        Returns:
        str: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        print(f"\n{'='*60}")
        print("å¼€å§‹æ‰¹é‡ç²¾å‡†è¯„ä¼°")
        print(f"è¯„ä¼°æ¨¡å‹: {eval_model}")
        print(f"æœ€å¤§å¹¶è¡Œçº¿ç¨‹æ•°: {self.max_workers}")
        print(f"ç›®æ ‡é¢˜ç›®æ•°é‡: {len(self.question_model_mapping)}")
        print(f"{'='*60}")
        
        # åŠ è½½è¯„åˆ†æ ‡å‡†
        self.safe_print("åŠ è½½è¯„åˆ†æ ‡å‡†...")
        rubrics_dict = load_rubrics(rubrics_file)
        if not rubrics_dict:
            self.safe_print("åŠ è½½è¯„åˆ†æ ‡å‡†å¤±è´¥")
            return None
        
        # åŠ è½½æ‰€æœ‰æ¨¡å‹å“åº”
        self.safe_print("åŠ è½½æ¨¡å‹å“åº”æ•°æ®...")
        all_responses = self.load_all_model_responses()
        if not all_responses:
            self.safe_print("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ¨¡å‹å“åº”æ•°æ®")
            return None
        
        # å‡†å¤‡è¯„ä¼°ä»»åŠ¡
        self.safe_print("å‡†å¤‡è¯„ä¼°ä»»åŠ¡...")
        evaluation_tasks = self.prepare_evaluation_tasks(all_responses, rubrics_dict)
        
        if not evaluation_tasks:
            self.safe_print("æ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°ä»»åŠ¡")
            return None
        
        self.safe_print(f"å‡†å¤‡è¯„ä¼° {len(evaluation_tasks)} ä¸ªä»»åŠ¡...")
        
        # æ‰§è¡Œå¹¶è¡Œè¯„ä¼°
        all_results = []
        failed_tasks = []
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {
                executor.submit(self.evaluate_single_task, task_args, eval_model): task_args 
                for task_args in evaluation_tasks
            }
            
            # æ”¶é›†ç»“æœ
            completed_count = 0
            for future in as_completed(future_to_task):
                task_args = future_to_task[future]
                try:
                    result = future.result()
                    if "error" in result:
                        failed_tasks.append(result)
                    else:
                        all_results.append(result)
                    
                    completed_count += 1
                    question_id, model_name = task_args[0], task_args[1]
                    self.safe_print(f"è¿›åº¦: {completed_count}/{len(evaluation_tasks)} - Q{question_id}({model_name})")
                    
                except Exception as e:
                    question_id, model_name = task_args[0], task_args[1]
                    error_result = {
                        "question_id": question_id,
                        "model_name": model_name,
                        "error": str(e),
                        "status": "exception"
                    }
                    failed_tasks.append(error_result)
                    self.safe_print(f"ä»»åŠ¡å¼‚å¸¸ - Q{question_id}({model_name}): {e}")
        
        end_time = time.time()
        evaluation_time = end_time - start_time
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.print_evaluation_summary(evaluation_tasks, all_results, failed_tasks, evaluation_time)
        
        # ä¿å­˜ç»“æœ
        return self.save_results(all_results, output_dir, eval_model)
    
    def print_evaluation_summary(self, tasks: List, results: List, 
                               failed: List, duration: float):
        """æ‰“å°è¯„ä¼°æ±‡æ€»ä¿¡æ¯"""
        print(f"\n{'='*50}")
        print("è¯„ä¼°ç»Ÿè®¡æ±‡æ€»")
        print(f"{'='*50}")
        print(f"æ€»ä»»åŠ¡æ•°: {len(tasks)}")
        print(f"æˆåŠŸå®Œæˆ: {len(results)}")
        print(f"å¤±è´¥ä»»åŠ¡: {len(failed)}")
        print(f"æˆåŠŸç‡: {len(results)/len(tasks)*100:.1f}%")
        print(f"æ€»è€—æ—¶: {duration:.2f} ç§’")
        print(f"å¹³å‡æ¯é¢˜è€—æ—¶: {duration/len(tasks):.2f} ç§’")
        
        # æŒ‰æ¨¡å‹ç»Ÿè®¡
        model_stats = {}
        for result in results:
            model = result.get("model", "Unknown")
            if model not in model_stats:
                model_stats[model] = {"count": 0, "recall": []}
            
            model_stats[model]["count"] += 1
            model_stats[model]["recall"].append(result["result"]["recall"])
        
        print(f"\næŒ‰æ¨¡å‹ç»Ÿè®¡:")
        for model, stats in model_stats.items():
            if stats["count"] > 0:
                avg_recall = sum(stats["recall"]) / stats["count"]
                print(f"  {model}: {stats['count']}é¢˜ | è¦†ç›–ç‡:{avg_recall:.3f}")
        
        # å¤±è´¥ä»»åŠ¡è¯¦æƒ…
        if failed:
            print(f"\nå¤±è´¥ä»»åŠ¡è¯¦æƒ…:")
            for failed_task in failed:
                qid = failed_task.get("question_id", "Unknown")
                model = failed_task.get("model_name", "Unknown")
                error = failed_task.get("error", "Unknown error")
                print(f"  Q{qid}({model}): {error}")
        
        print(f"{'='*50}")
    
    def save_results(self, results: List, output_dir: str, eval_model: str) -> str:
        """ä¿å­˜è¯„ä¼°ç»“æœåˆ°æŒ‡å®šçš„ç»Ÿä¸€æ–‡ä»¶"""
        if not results:
            self.safe_print("æ²¡æœ‰ç»“æœéœ€è¦ä¿å­˜")
            return None
        
        # åˆ›å»ºæŒ‡å®šçš„è¾“å‡ºç›®å½•ç»“æ„: results_for_specific/targeted_evaluation/{eval_model}/
        targeted_output_dir = os.path.join(output_dir, "results_for_specific", "targeted_evaluation", eval_model.replace("/", "_"))
        if not create_output_directory(targeted_output_dir):
            self.safe_print(f"æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½•: {targeted_output_dir}")
            return None
        
        # ç»Ÿä¸€ä¿å­˜ä¸º meta_eval.json
        output_file = os.path.join(targeted_output_dir, "meta_eval.json")
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        total_questions = len(results)
        avg_recall = sum(r["result"]["recall"] for r in results) / total_questions if total_questions > 0 else 0
        
        # æŒ‰æ¨¡å‹åˆ†ç»„ç»Ÿè®¡
        model_stats = {}
        for result in results:
            model = result.get("model", "Unknown")
            if model not in model_stats:
                model_stats[model] = {
                    "question_count": 0,
                    "questions": [],
                    "avg_recall": 0,
                    "recalls": []
                }
            
            model_stats[model]["question_count"] += 1
            model_stats[model]["questions"].append(f"Q{result['id']}")
            model_stats[model]["recalls"].append(result["result"]["recall"])
        
        # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„å¹³å‡å€¼
        for model, stats in model_stats.items():
            if stats["question_count"] > 0:
                stats["avg_recall"] = sum(stats["recalls"]) / stats["question_count"]
                # ç§»é™¤ä¸´æ—¶çš„åˆ—è¡¨æ•°æ®ï¼Œä¿æŒç»“æœæ–‡ä»¶ç®€æ´
                del stats["recalls"]
        
        # æ„å»ºæœ€ç»ˆç»“æœç»“æ„
        final_results = {
            "metadata": {
                "evaluation_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "eval_model": eval_model,
                "total_questions": total_questions,
                "question_model_mapping": self.question_model_mapping,
                "summary_statistics": {
                    "overall_avg_recall": round(avg_recall, 4)
                },
                "model_statistics": model_stats
            },
            "detailed_results": results
        }
        
        if save_json_file(final_results, output_file):
            self.safe_print(f"\nâœ… æ‰€æœ‰ç»“æœå·²ç»Ÿä¸€ä¿å­˜åˆ°: {output_file}")
            self.safe_print(f"ğŸ“ ç›®å½•ç»“æ„: {targeted_output_dir}")
            return output_file
        else:
            self.safe_print("âŒ ä¿å­˜ç»“æœå¤±è´¥")
            return None
    



def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ‰¹é‡è¯„ä¼°æŒ‡å®šé¢˜ç›®å’Œæ¨¡å‹')
    
    parser.add_argument('--rubrics_file', type=str, default='data/eval_data/rubric.json',
                        help='è¯„åˆ†æ ‡å‡†JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='results',
                        help='è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--eval_model', type=str, default='o3-mini',
                        help='ç”¨äºè¯„ä¼°çš„æ¨¡å‹')
    parser.add_argument('--max_workers', type=int, default=4,
                        help='æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤: 4)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¯„åˆ†æ ‡å‡†æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.rubrics_file):
        print(f"é”™è¯¯: è¯„åˆ†æ ‡å‡†æ–‡ä»¶ä¸å­˜åœ¨ - {args.rubrics_file}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if not create_output_directory(args.output_dir):
        print("åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥ï¼Œé€€å‡ºç¨‹åºã€‚")
        return
    
    # åˆ›å»ºè¯„ä¼°åè°ƒå™¨å¹¶è¿è¡Œè¯„ä¼°
    coordinator = TargetedEvaluationCoordinator(max_workers=args.max_workers)
    
    result_file = coordinator.run_targeted_evaluation(
        rubrics_file=args.rubrics_file,
        output_dir=args.output_dir,
        eval_model=args.eval_model
    )
    
    # æœ€ç»ˆæ±‡æ€»
    print(f"\n{'='*60}")
    print("æ‰¹é‡ç²¾å‡†è¯„ä¼°å®Œæˆ")
    print(f"{'='*60}")
    
    if result_file:
        print(f"âœ… è¯„ä¼°æˆåŠŸå®Œæˆ!")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶: {result_file}")
        print(f"ğŸ“Š ç»Ÿä¸€ä¿å­˜: meta_eval.json")
        print(f"ğŸ§µ ä½¿ç”¨å¹¶è¡Œçº¿ç¨‹æ•°: {args.max_workers}")
        print(f"ğŸ¤– è¯„ä¼°æ¨¡å‹: {args.eval_model}")
        print(f"ğŸ“ˆ æ–‡ä»¶åŒ…å«è¯¦ç»†ç»“æœå’Œæ±‡æ€»ç»Ÿè®¡ä¿¡æ¯")
    else:
        print(f"âŒ è¯„ä¼°å¤±è´¥!")
        print(f"è¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚")


if __name__ == "__main__":
    main()