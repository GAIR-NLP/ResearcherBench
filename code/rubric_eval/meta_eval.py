#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ¨¡å‹è¯„ä¼°ç»“æœä¸äººç±»æ ‡æ³¨ç»“æœå¯¹æ¯”åˆ†æè„šæœ¬
è®¡ç®—F1åˆ†æ•°å’Œå‡†ç¡®åº¦ï¼ˆå¸¦æƒé‡å’Œä¸å¸¦æƒé‡ï¼‰
"""

import os
import sys
import json
import glob
import argparse
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import statistics

# Add project root to path for imports
import sys
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from code.utils import load_json_file, save_json_file, create_output_directory


class ModelHumanComparison:
    """æ¨¡å‹ä¸äººç±»æ ‡æ³¨ç»“æœå¯¹æ¯”åˆ†æå™¨"""
    
    def __init__(self):
        self.human_annotations = {}  # äººç±»æ ‡æ³¨æ•°æ®
        self.model_results = {}      # æ¨¡å‹è¯„ä¼°æ•°æ®
        self.rubric_weights = {}     # rubricæƒé‡æ•°æ®
        self.comparison_results = {} # å¯¹æ¯”ç»“æœ
        
    def load_human_annotations(self, human_data_dir: str) -> bool:
        """
        åŠ è½½äººç±»æ ‡æ³¨æ•°æ®
        
        Parameters:
        human_data_dir (str): äººç±»æ ‡æ³¨æ•°æ®ç›®å½•
        
        Returns:
        bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        print(f"æ­£åœ¨åŠ è½½äººç±»æ ‡æ³¨æ•°æ®ä»: {human_data_dir}")
        
        # æŸ¥æ‰¾æ‰€æœ‰çš„äººç±»æ ‡æ³¨æ–‡ä»¶ (Q*_result_*.json)
        pattern = os.path.join(human_data_dir, "Q*_result_*.json")
        annotation_files = glob.glob(pattern)
        
        if not annotation_files:
            print(f"é”™è¯¯: åœ¨ {human_data_dir} ä¸­æœªæ‰¾åˆ°äººç±»æ ‡æ³¨æ–‡ä»¶")
            return False
        
        for file_path in annotation_files:
            filename = os.path.basename(file_path)
            
            # è§£ææ–‡ä»¶åè·å–é¢˜å·å’Œæ ‡æ³¨è€…
            # æ ¼å¼: Q{id}_result_{annotator}.json
            try:
                parts = filename.replace('.json', '').split('_')
                question_id = int(parts[0][1:])  # Q1 -> 1
                annotator = parts[2]             # A, B, C, etc.
                
                # åŠ è½½æ ‡æ³¨æ•°æ®
                annotation_data = load_json_file(file_path)
                if annotation_data:
                    if question_id not in self.human_annotations:
                        self.human_annotations[question_id] = {}
                    
                    self.human_annotations[question_id][annotator] = annotation_data
                    print(f"  âœ… åŠ è½½: Q{question_id} - æ ‡æ³¨è€…{annotator}")
                else:
                    print(f"  âŒ åŠ è½½å¤±è´¥: {filename}")
                    
            except (IndexError, ValueError) as e:
                print(f"  âš ï¸  è·³è¿‡æ— æ•ˆæ–‡ä»¶å: {filename} ({e})")
                continue
        
        total_questions = len(self.human_annotations)
        total_annotations = sum(len(annotators) for annotators in self.human_annotations.values())
        print(f"æˆåŠŸåŠ è½½ {total_annotations} ä¸ªæ ‡æ³¨ï¼Œæ¶‰åŠ {total_questions} ä¸ªé¢˜ç›®")
        
        return total_annotations > 0
    
    def load_model_results(self, model_results_file: str) -> bool:
        """
        åŠ è½½æ¨¡å‹è¯„ä¼°ç»“æœ
        
        Parameters:
        model_results_file (str): æ¨¡å‹ç»“æœæ–‡ä»¶è·¯å¾„
        
        Returns:
        bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹è¯„ä¼°ç»“æœä»: {model_results_file}")
        
        model_data = load_json_file(model_results_file)
        if not model_data:
            print("é”™è¯¯: æ— æ³•åŠ è½½æ¨¡å‹è¯„ä¼°ç»“æœ")
            return False
        
        # è§£ææ¨¡å‹ç»“æœ
        if "detailed_results" in model_data:
            results = model_data["detailed_results"]
        else:
            results = model_data.get("results", [])
        
        for result in results:
            question_id = result.get("id")
            if question_id:
                self.model_results[question_id] = result
                print(f"  âœ… åŠ è½½æ¨¡å‹ç»“æœ: Q{question_id}")
        
        print(f"æˆåŠŸåŠ è½½ {len(self.model_results)} ä¸ªæ¨¡å‹è¯„ä¼°ç»“æœ")
        return len(self.model_results) > 0
    
    def load_rubric_weights(self, rubric_file: str) -> bool:
        """
        åŠ è½½rubricæƒé‡æ•°æ®
        
        Parameters:
        rubric_file (str): rubricæ–‡ä»¶è·¯å¾„
        
        Returns:
        bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        print(f"æ­£åœ¨åŠ è½½rubricæƒé‡æ•°æ®ä»: {rubric_file}")
        
        rubric_data = load_json_file(rubric_file)
        if not rubric_data:
            print("é”™è¯¯: æ— æ³•åŠ è½½rubricæƒé‡æ•°æ®")
            return False
        
        for item in rubric_data:
            question_id = item.get("id")
            if question_id and "rubric" in item:
                self.rubric_weights[question_id] = {}
                for rubric_item in item["rubric"]:
                    point = rubric_item.get("point", "")
                    weight = rubric_item.get("weight", 1)
                    # ä½¿ç”¨pointçš„å‰50ä¸ªå­—ç¬¦ä½œä¸ºkeyæ¥åŒ¹é…
                    key = point[:50] if point else ""
                    self.rubric_weights[question_id][key] = weight
                
                print(f"  âœ… åŠ è½½æƒé‡: Q{question_id} ({len(item['rubric'])} ä¸ªrubricé¡¹)")
        
        print(f"æˆåŠŸåŠ è½½ {len(self.rubric_weights)} ä¸ªé¢˜ç›®çš„æƒé‡æ•°æ®")
        return len(self.rubric_weights) > 0
    
    def find_rubric_weight(self, question_id: int, point_text: str) -> int:
        """
        æŸ¥æ‰¾rubricé¡¹çš„æƒé‡
        
        Parameters:
        question_id (int): é¢˜ç›®ID
        point_text (str): rubricç‚¹çš„æ–‡æœ¬
        
        Returns:
        int: æƒé‡å€¼ï¼Œé»˜è®¤ä¸º1
        """
        if question_id not in self.rubric_weights:
            return 1
        
        weights = self.rubric_weights[question_id]
        
        # ç²¾ç¡®åŒ¹é…
        point_key = point_text[:50] if point_text else ""
        if point_key in weights:
            return weights[point_key]
        
        # æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„æƒé‡
        for key, weight in weights.items():
            if key and point_text and (key in point_text or point_text in key):
                return weight
        
        return 1  # é»˜è®¤æƒé‡
    
    def compare_single_question(self, question_id: int, annotator: str) -> Optional[Dict]:
        """
        æ¯”è¾ƒå•ä¸ªé¢˜ç›®çš„æ¨¡å‹ç»“æœå’Œäººç±»æ ‡æ³¨
        
        Parameters:
        question_id (int): é¢˜ç›®ID
        annotator (str): æ ‡æ³¨è€…ID
        
        Returns:
        Optional[Dict]: æ¯”è¾ƒç»“æœï¼Œå¦‚æœæ— æ³•æ¯”è¾ƒåˆ™è¿”å›None
        """
        # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
        if question_id not in self.human_annotations:
            print(f"  âš ï¸  Q{question_id}: æ— äººç±»æ ‡æ³¨æ•°æ®")
            return None
        
        if annotator not in self.human_annotations[question_id]:
            print(f"  âš ï¸  Q{question_id}: æ— æ ‡æ³¨è€…{annotator}çš„æ•°æ®")
            return None
        
        if question_id not in self.model_results:
            print(f"  âš ï¸  Q{question_id}: æ— æ¨¡å‹è¯„ä¼°æ•°æ®")
            return None
        
        human_data = self.human_annotations[question_id][annotator]
        model_data = self.model_results[question_id]
        
        # è·å–äººç±»æ ‡æ³¨çš„rubricè¯„ä¼°
        human_rubric_eval = human_data.get("rubric_eval", [])
        
        # è·å–æ¨¡å‹çš„coverage_results
        model_coverage = model_data.get("result", {}).get("coverage_results", [])
        
        if not human_rubric_eval or not model_coverage:
            print(f"  âš ï¸  Q{question_id}: ç¼ºå°‘rubricè¯„ä¼°æ•°æ®")
            return None
        
        # å»ºç«‹æ˜ å°„å…³ç³»ï¼ˆé€šè¿‡pointæ–‡æœ¬åŒ¹é…ï¼‰
        comparisons = []
        matched_count = 0
        
        for human_item in human_rubric_eval:
            human_point = human_item.get("point", "")
            human_covered = human_item.get("covered", False)
            
            # åœ¨æ¨¡å‹ç»“æœä¸­æŸ¥æ‰¾åŒ¹é…çš„point
            model_covered = None
            model_weight = 1
            
            for model_item in model_coverage:
                model_point = model_item.get("point", "")
                
                # æ–‡æœ¬ç›¸ä¼¼æ€§åŒ¹é…ï¼ˆç®€å•ç‰ˆæœ¬ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸åŒçš„å…³é”®è¯ï¼‰
                if self.is_point_match(human_point, model_point):
                    model_covered = model_item.get("covered", False)
                    model_weight = model_item.get("weight", 1)
                    matched_count += 1
                    break
            
            if model_covered is not None:
                # è·å–æƒé‡
                weight = self.find_rubric_weight(question_id, human_point)
                
                comparisons.append({
                    "point": human_point[:100] + "..." if len(human_point) > 100 else human_point,
                    "human_covered": human_covered,
                    "model_covered": model_covered,
                    "weight": weight,
                    "match": human_covered == model_covered
                })
        
        if not comparisons:
            print(f"  âš ï¸  Q{question_id}: æ— æ³•åŒ¹é…ä»»ä½•rubricé¡¹")
            return None
        
        print(f"  âœ… Q{question_id}: åŒ¹é…äº† {matched_count}/{len(human_rubric_eval)} ä¸ªrubricé¡¹")
        
        return {
            "question_id": question_id,
            "annotator": annotator,
            "total_items": len(comparisons),
            "matched_items": matched_count,
            "comparisons": comparisons
        }
    
    def is_point_match(self, human_point: str, model_point: str) -> bool:
        """
        åˆ¤æ–­ä¸¤ä¸ªrubricç‚¹æ˜¯å¦åŒ¹é…
        
        Parameters:
        human_point (str): äººç±»æ ‡æ³¨çš„ç‚¹
        model_point (str): æ¨¡å‹è¯„ä¼°çš„ç‚¹
        
        Returns:
        bool: æ˜¯å¦åŒ¹é…
        """
        if not human_point or not model_point:
            return False
        
        # ç®€å•çš„æ–‡æœ¬åŒ¹é…ç­–ç•¥
        human_words = set(human_point.lower().split())
        model_words = set(model_point.lower().split())
        
        # è®¡ç®—äº¤é›†æ¯”ä¾‹
        intersection = human_words.intersection(model_words)
        union = human_words.union(model_words)
        
        if len(union) == 0:
            return False
        
        similarity = len(intersection) / len(union)
        
        # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯åŒ¹é…çš„
        return similarity >= 0.3
    
    def calculate_metrics(self, comparisons: List[Dict]) -> Dict:
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        
        Parameters:
        comparisons (List[Dict]): æ¯”è¾ƒç»“æœåˆ—è¡¨
        
        Returns:
        Dict: è®¡ç®—å‡ºçš„æŒ‡æ ‡
        """
        if not comparisons:
            return {
                "unweighted": {"accuracy": 0, "precision": 0, "recall": 0, "f1": 0},
                "weighted": {"accuracy": 0, "precision": 0, "recall": 0, "f1": 0}
            }
        
        # ä¸å¸¦æƒé‡çš„è®¡ç®—
        tp = sum(1 for c in comparisons if c["human_covered"] and c["model_covered"])
        tn = sum(1 for c in comparisons if not c["human_covered"] and not c["model_covered"])
        fp = sum(1 for c in comparisons if not c["human_covered"] and c["model_covered"])
        fn = sum(1 for c in comparisons if c["human_covered"] and not c["model_covered"])
        
        total = len(comparisons)
        accuracy_unweighted = (tp + tn) / total if total > 0 else 0
        precision_unweighted = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall_unweighted = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1_unweighted = 2 * (precision_unweighted * recall_unweighted) / (precision_unweighted + recall_unweighted) if (precision_unweighted + recall_unweighted) > 0 else 0
        
        # å¸¦æƒé‡çš„è®¡ç®—
        tp_weighted = sum(c["weight"] for c in comparisons if c["human_covered"] and c["model_covered"])
        tn_weighted = sum(c["weight"] for c in comparisons if not c["human_covered"] and not c["model_covered"])
        fp_weighted = sum(c["weight"] for c in comparisons if not c["human_covered"] and c["model_covered"])
        fn_weighted = sum(c["weight"] for c in comparisons if c["human_covered"] and not c["model_covered"])
        
        total_weight = sum(c["weight"] for c in comparisons)
        accuracy_weighted = (tp_weighted + tn_weighted) / total_weight if total_weight > 0 else 0
        precision_weighted = tp_weighted / (tp_weighted + fp_weighted) if (tp_weighted + fp_weighted) > 0 else 0
        recall_weighted = tp_weighted / (tp_weighted + fn_weighted) if (tp_weighted + fn_weighted) > 0 else 0
        f1_weighted = 2 * (precision_weighted * recall_weighted) / (precision_weighted + recall_weighted) if (precision_weighted + recall_weighted) > 0 else 0
        
        return {
            "unweighted": {
                "accuracy": round(accuracy_unweighted, 4),
                "precision": round(precision_unweighted, 4),
                "recall": round(recall_unweighted, 4),
                "f1": round(f1_unweighted, 4),
                "confusion_matrix": {"tp": tp, "tn": tn, "fp": fp, "fn": fn}
            },
            "weighted": {
                "accuracy": round(accuracy_weighted, 4),
                "precision": round(precision_weighted, 4),
                "recall": round(recall_weighted, 4),
                "f1": round(f1_weighted, 4),
                "confusion_matrix": {"tp": tp_weighted, "tn": tn_weighted, "fp": fp_weighted, "fn": fn_weighted}
            }
        }
    
    def run_comparison(self, target_annotator: str = "A") -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„æ¯”è¾ƒåˆ†æ
        
        Parameters:
        target_annotator (str): ç›®æ ‡æ ‡æ³¨è€…IDï¼Œé»˜è®¤ä¸º"A"
        
        Returns:
        Dict: å®Œæ•´çš„æ¯”è¾ƒç»“æœ
        """
        print("\n" + "="*60)
        print(f"å¼€å§‹æ¨¡å‹ä¸äººç±»æ ‡æ³¨ç»“æœå¯¹æ¯”åˆ†æ (æ ‡æ³¨è€…: {target_annotator})")
        print("="*60)
        
        all_comparisons = []
        detailed_results = {}
        processed_questions = 0
        
        # éå†æ‰€æœ‰é¢˜ç›®ï¼Œåªå¤„ç†æŒ‡å®šæ ‡æ³¨è€…çš„æ•°æ®
        for question_id in sorted(self.human_annotations.keys()):
            if target_annotator not in self.human_annotations[question_id]:
                print(f"\nâš ï¸  Q{question_id}: æ— æ ‡æ³¨è€…{target_annotator}çš„æ•°æ®ï¼Œè·³è¿‡")
                continue
            
            print(f"\nå¤„ç† Q{question_id} - æ ‡æ³¨è€…{target_annotator}:")
            
            result = self.compare_single_question(question_id, target_annotator)
            if result:
                detailed_results[question_id] = result
                all_comparisons.extend(result["comparisons"])
                processed_questions += 1
        
        if processed_questions == 0:
            print(f"\nâŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°æ ‡æ³¨è€…{target_annotator}çš„ä»»ä½•æœ‰æ•ˆæ•°æ®")
            return {}
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        print(f"\n" + "="*50)
        print("è®¡ç®—æ€»ä½“è¯„ä¼°æŒ‡æ ‡")
        print("="*50)
        
        overall_metrics = self.calculate_metrics(all_comparisons)
        
        # æŒ‰é¢˜ç›®è®¡ç®—æŒ‡æ ‡
        question_metrics = {}
        for question_id, question_data in detailed_results.items():
            if question_data and "comparisons" in question_data:
                question_metrics[question_id] = self.calculate_metrics(question_data["comparisons"])
        
        # æ±‡æ€»ç»“æœ
        comparison_results = {
            "summary": {
                "annotator": target_annotator,
                "total_questions": len(detailed_results),
                "processed_questions": processed_questions,
                "total_comparisons": len(all_comparisons),
                "overall_metrics": overall_metrics
            },
            "question_metrics": question_metrics,
            "detailed_results": detailed_results
        }
        
        return comparison_results
    
    def print_summary_report(self, results: Dict):
        """æ‰“å°æ±‡æ€»æŠ¥å‘Š"""
        if not results:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¯”è¾ƒç»“æœ")
            return
            
        print(f"\n" + "="*60)
        print("ğŸ“Š æ¨¡å‹ä¸äººç±»æ ‡æ³¨å¯¹æ¯”åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        summary = results["summary"]
        overall = summary["overall_metrics"]
        
        print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        print(f"  æ ‡æ³¨è€…: {summary['annotator']}")
        print(f"  å¤„ç†é¢˜ç›®æ•°: {summary['processed_questions']}")
        print(f"  æ¯”è¾ƒé¡¹ç›®æ•°: {summary['total_comparisons']}")
        
        print(f"\nğŸ¯ æ€»ä½“æ€§èƒ½æŒ‡æ ‡:")
        print(f"  {'æŒ‡æ ‡':<12} {'ä¸å¸¦æƒé‡':<12} {'å¸¦æƒé‡':<12}")
        print(f"  {'-'*36}")
        print(f"  {'å‡†ç¡®åº¦':<12} {overall['unweighted']['accuracy']:<12} {overall['weighted']['accuracy']:<12}")
        print(f"  {'ç²¾ç¡®ç‡':<12} {overall['unweighted']['precision']:<12} {overall['weighted']['precision']:<12}")
        print(f"  {'å¬å›ç‡':<12} {overall['unweighted']['recall']:<12} {overall['weighted']['recall']:<12}")
        print(f"  {'F1åˆ†æ•°':<12} {overall['unweighted']['f1']:<12} {overall['weighted']['f1']:<12}")
        
        # æ˜¾ç¤ºæ··æ·†çŸ©é˜µ
        unw_cm = overall['unweighted']['confusion_matrix']
        w_cm = overall['weighted']['confusion_matrix']
        print(f"\nğŸ“‹ æ··æ·†çŸ©é˜µ:")
        print(f"  {'ç±»å‹':<12} {'ä¸å¸¦æƒé‡':<12} {'å¸¦æƒé‡':<12}")
        print(f"  {'-'*36}")
        print(f"  {'TP (æ­£ç¡®è¦†ç›–)':<12} {unw_cm['tp']:<12} {w_cm['tp']:<12.1f}")
        print(f"  {'TN (æ­£ç¡®æœªè¦†ç›–)':<12} {unw_cm['tn']:<12} {w_cm['tn']:<12.1f}")
        print(f"  {'FP (é”™è¯¯è¦†ç›–)':<12} {unw_cm['fp']:<12} {w_cm['fp']:<12.1f}")
        print(f"  {'FN (é”™è¯¯æœªè¦†ç›–)':<12} {unw_cm['fn']:<12} {w_cm['fn']:<12.1f}")
        
        # æŒ‰é¢˜ç›®æ˜¾ç¤ºæŒ‡æ ‡
        print(f"\nğŸ“‹ åˆ†é¢˜ç›®æ€§èƒ½:")
        question_metrics = results["question_metrics"]
        if question_metrics:
            print(f"  {'é¢˜ç›®':<6} {'F1(æ— æƒ)':<10} {'F1(å¸¦æƒ)':<10} {'å‡†ç¡®åº¦(æ— æƒ)':<12} {'å‡†ç¡®åº¦(å¸¦æƒ)':<12}")
            print(f"  {'-'*50}")
            
            for question_id in sorted(question_metrics.keys()):
                metrics = question_metrics[question_id]
                unw_f1 = metrics["unweighted"]["f1"]
                w_f1 = metrics["weighted"]["f1"]
                unw_acc = metrics["unweighted"]["accuracy"]
                w_acc = metrics["weighted"]["accuracy"]
                print(f"  Q{question_id:<5} {unw_f1:<10} {w_f1:<10} {unw_acc:<12} {w_acc:<12}")
        else:
            print("  æ— å¯ç”¨çš„åˆ†é¢˜ç›®æ•°æ®")
        
        # æ€§èƒ½è¯„ä¼°
        f1_score = overall['weighted']['f1']
        print(f"\nğŸ† æ€§èƒ½è¯„ä¼°:")
        if f1_score >= 0.8:
            print(f"  âœ… æ¨¡å‹è¡¨ç°ä¼˜ç§€ (F1={f1_score:.3f})")
        elif f1_score >= 0.6:
            print(f"  ğŸŸ¡ æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œæœ‰æ”¹è¿›ç©ºé—´ (F1={f1_score:.3f})")
        else:
            print(f"  ğŸ”´ æ¨¡å‹è¡¨ç°éœ€è¦æ˜¾è‘—æ”¹è¿› (F1={f1_score:.3f})")
        
        # æä¾›æ”¹è¿›å»ºè®®
        precision = overall['weighted']['precision']
        recall = overall['weighted']['recall']
        
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        if precision > recall + 0.1:
            print(f"  â€¢ æ¨¡å‹è¿‡äºä¿å®ˆï¼Œå»ºè®®é™ä½åˆ¤æ–­é˜ˆå€¼ä»¥æé«˜å¬å›ç‡")
        elif recall > precision + 0.1:
            print(f"  â€¢ æ¨¡å‹è¿‡äºæ¿€è¿›ï¼Œå»ºè®®æé«˜åˆ¤æ–­é˜ˆå€¼ä»¥æé«˜ç²¾ç¡®ç‡")
        else:
            print(f"  â€¢ ç²¾ç¡®ç‡å’Œå¬å›ç‡è¾ƒä¸ºå‡è¡¡ï¼Œå¯è€ƒè™‘æ•´ä½“ä¼˜åŒ–ç­–ç•¥")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ¨¡å‹è¯„ä¼°ç»“æœä¸äººç±»æ ‡æ³¨ç»“æœå¯¹æ¯”åˆ†æ')
    
    parser.add_argument('--human_data_dir', type=str, 
                        default='data/meta_eval_data/meta_eval',
                        help='äººç±»æ ‡æ³¨æ•°æ®ç›®å½•')
    parser.add_argument('--model_results_file', type=str,
                        default='results/results_for_specific/targeted_evaluation/google_gemini-2.5-flash/meta_eval.json',
                        help='æ¨¡å‹è¯„ä¼°ç»“æœæ–‡ä»¶')
    parser.add_argument('--rubric_file', type=str,
                        default='data/eval_data/rubric.json',
                        help='rubricæƒé‡æ–‡ä»¶')
    parser.add_argument('--output_dir', type=str,
                        default='results/comparison_analysis',
                        help='å¯¹æ¯”ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--annotator', type=str,
                        default='A',
                        help='ç›®æ ‡æ ‡æ³¨è€…ID (é»˜è®¤: A)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.human_data_dir):
        print(f"é”™è¯¯: äººç±»æ ‡æ³¨æ•°æ®ç›®å½•ä¸å­˜åœ¨ - {args.human_data_dir}")
        return
    
    if not os.path.exists(args.model_results_file):
        print(f"é”™è¯¯: æ¨¡å‹ç»“æœæ–‡ä»¶ä¸å­˜åœ¨ - {args.model_results_file}")
        return
    
    if not os.path.exists(args.rubric_file):
        print(f"é”™è¯¯: rubricæ–‡ä»¶ä¸å­˜åœ¨ - {args.rubric_file}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    create_output_directory(args.output_dir)
    
    # åˆ›å»ºæ¯”è¾ƒåˆ†æå™¨
    comparator = ModelHumanComparison()
    
    # åŠ è½½æ•°æ®
    if not comparator.load_human_annotations(args.human_data_dir):
        print("åŠ è½½äººç±»æ ‡æ³¨æ•°æ®å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
        return
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æŒ‡å®šæ ‡æ³¨è€…çš„æ•°æ®
    annotator_found = False
    for question_data in comparator.human_annotations.values():
        if args.annotator in question_data:
            annotator_found = True
            break
    
    if not annotator_found:
        print(f"é”™è¯¯: æœªæ‰¾åˆ°æ ‡æ³¨è€… '{args.annotator}' çš„ä»»ä½•æ•°æ®")
        print(f"å¯ç”¨çš„æ ‡æ³¨è€…: {set(ann for q_data in comparator.human_annotations.values() for ann in q_data.keys())}")
        return
    
    if not comparator.load_model_results(args.model_results_file):
        print("åŠ è½½æ¨¡å‹è¯„ä¼°ç»“æœå¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
        return
    
    if not comparator.load_rubric_weights(args.rubric_file):
        print("åŠ è½½rubricæƒé‡å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
        return
    
    # è¿è¡Œæ¯”è¾ƒåˆ†æ
    results = comparator.run_comparison(target_annotator=args.annotator)
    
    if not results:
        print("æ¯”è¾ƒåˆ†æå¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆç»“æœ")
        return
    
    # æ‰“å°æŠ¥å‘Š
    comparator.print_summary_report(results)
    
    # ä¿å­˜ç»“æœ
    output_file = os.path.join(args.output_dir,"gpt-4.1", f"model_human_comparison_annotator_{args.annotator}.json")
    if save_json_file(results, output_file):
        print(f"\nâœ… è¯¦ç»†å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    else:
        print(f"\nâŒ ä¿å­˜å¯¹æ¯”ç»“æœå¤±è´¥")


if __name__ == "__main__":
    main()
