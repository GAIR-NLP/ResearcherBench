[
  {
    "id": 1,
    "question": "In specific domains, such as healthcare, using large language models (LLMs) in combination with Retrieval-Augmented Generation (RAG) can effectively reduce hallucinations, while attribution can provide valid citation evidence for the generated answers, making it easier for subsequent evaluation and validation. A method was attempted where GPT-4 was used to generate data, followed by fine-tuning the LLM using supervised fine-tuning (SFT) to directly produce answers and attributions. It was observed that for simple questions (single citation), the model performs well, but for more complex questions, the model's performance declines. After investigating, it was found that the generated dataset primarily contained simple questions, and the citation accuracy of GPT-4 itself is low (around 75%). How can high-quality data be generated to improve performance on complex questions?",
    "response": "**Generating High-Quality Data for Complex Medical Question Answering with LLM-RAG Systems**\n\n**1\\. Introduction: The Challenge of Complex Questions in Healthcare RAG**\n\nThe integration of Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) has shown promise in reducing hallucinations in healthcare applications, where factual accuracy is critical \\[63\\]. However, a significant limitation emerges when handling complex, multi-hop questions requiring synthesis of multiple sources. Attempts to use GPT-4 for supervised fine-tuning (SFT) datasets reveal two core issues:\n\n**Dataset Simplicity Bias**: Generated datasets predominantly contain single-hop questions due to GPT-4's default reasoning patterns \\[164\\]\\[327\\].\n\n**Citation Accuracy Limitations**: GPT-4 achieves only ≈75% citation accuracy in synthetic data generation \\[23\\]\\[169\\]undermining attribution validity. This report investigates solutions for generating high-quality training data that addresses multi-hop complexity while ensuring reliable attribution.\n\n**2\\. Defining the Problem: Why Current Methods Fail for Complex Questions**\n\n**2.1. The Multi-Hop Complexity Gap**\n\nComplex medical questions require synthesizing information across multiple sources (e.g., connecting drug mechanisms to clinical outcomes). Current methods fail because:\n\nGPT-4’s synthetic datasets exhibit **\"hop deficiency\"**, with 2-hop+ questions comprising <20% of generated content \\[164\\]\\[167\\].\n\nPerformance drops sharply for multi-hop questions: F1 scores decrease by 35-40% compared to single-hop queries in datasets like CofCA and BioHopR \\[164\\]\\[321\\].\n\n**2.2. Attribution Inaccuracy in Synthetic Data**\n\nGPT-4 hallucinates citations in 22-25% of medical responses \\[169\\]\\[183\\].\n\nCitation precision drops to 61-68% for multi-evidence answers \\[327\\], making synthetic data unreliable for training attribution-aware models.\n\n**3\\. Advanced Techniques for High-Quality Data Generation**\n\n**3.1. Structured Multi-Hop Synthesis Frameworks**\n\n**Multi-Hop Tree Structure (MHTS)**: Generates questions through hierarchical claim decomposition:\n\n1.  Chunk source documents (e.g., clinical guidelines + research papers).\n2.  Extract atomic evidence units.\n3.  Generate questions requiring N-hop reasoning (adjustable complexity).\n4.  Verify answers against retrieved chunks \\[2\\]\\[2\\].\n\n**Iterative Query Generation (DSPy)**: Uses Chain-of-Thought prompting to break complex questions into sub-queries. After retrieval, context deduplication removes redundancy \\[3\\].\n\n**3.2. Hybrid Human-LLM Data Generation Pipelines**\n\n**Co-Annotation Systems**: GPT-4 provides initial annotations; human experts validate uncertain outputs identified via entropy scoring (87% accuracy at 50% cost reduction) \\[46\\].\n\n**Dynamic Difficulty Routing**: Assigns questions to GPT-4 or humans based on complexity:\n\nSimple: GPT-4 auto-generates\n\nComplex: Human annotation + GPT-4 refinement \\[96\\].\n\nMedical applications show 90%+ precision when routing high-stakes queries to clinicians \\[92\\].\n\n**4\\. Healthcare-Specific Adaptations**\n\n**4.1. Medical Knowledge Integration**\n\n**Terminology-Aware Chunking**: Segment documents using clinical entity recognition (e.g., SNOMED CT codes) to preserve semantic relationships \\[80\\].\n\n**Evidence Verification Pipelines**: For generated answers, cross-check against:\n\nDrug databases (e.g., DrugBank)\n\nClinical trial registries\n\nReduces factual errors by 39% vs. standard RAG \\[75\\].\n\n**4.2. Difficulty Calibration in Medical Contexts**\n\n**Semantic Spread Metric**: Quantifies distance between evidence sources (e.g., pharmacology + epidemiology texts = high spread). Adjusts MHTS difficulty beyond hop count \\[63\\].\n\n**Curriculum Data Generation**: Start with single-hop questions from textbooks, progress to multi-hop from literature. Fine-tuning with staged datasets improves complex QA accuracy by 18% \\[203\\].\n\n**5\\. Enhancing Citation Accuracy**\n\n**5.1. Verification-Augmented Generation**\n\n**OpenScholar-Style Self-Feedback**: LLMs generate answers → retrieve supporting evidence → self-evaluate citation alignment. Improves GPT-4's citation accuracy from 75% to 87% \\[38\\]\\[108\\].\n\n**Human-in-the-Loop Refinement**: For ambiguous citations, clinicians verify via:\n\nCitation Precision: % of citations supporting claims (target >90%).\n\nCitation Recall: % of claims with valid citations (target >85%) \\[133\\]\\[183\\].\n\n**5.2. Fine-Tuning for Attribution**\n\n**Attribution-Specific Loss Functions**: Penalize hallucinations using:\n\nloss = cross_entropy + λ \\* (1 - citation_f1)\n\nWhere λ weights attribution accuracy \\[143\\].\n\n**Synthetic+Human Hybrid Datasets**: Blend GPT-4 outputs with human-verified examples (3:1 ratio). Achieves 93% citation F1 vs. 82% for pure synthetic data \\[247\\]\\[365\\].\n\n**6\\. Performance Validation: Metrics and Benchmarks**\n\n**6.1. Quantitative Gains from Hybrid Approaches**\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n| **Approach** | **Dataset** | **Citation Precision** | **Citation Recall** | **F1 Δ vs. Baseline** |\n| Pure GPT-4 Synthetic | BioHopR (2-hop) | 61.2% | 58.7% | Baseline |\n| OpenScholar + Human Refinement | BioHopR (2-hop) | 86.4% | 81.9% | +25.1% |\n| Hybrid Human-LLM Data | Clinical-HopQ | 91.3% | 89.7% | +32.6% |\n\n_Data synthesized from Web Pages 164, 183, 248, 327_\n\n**6.2. Multi-Hop Attribution Metrics**\n\n**Multi-Hop Recall@k (MHR@k)**: Measures if all required evidence appears in top-k retrievals. Hybrid pipelines achieve MHR@5 >80% vs. 52% for GPT-4 alone \\[328\\].\n\n**Evidence Chain Completeness**: Scores logical coherence across hops. Human-refined data shows 35% higher completeness \\[63\\].\n\n**7\\. Recommended Implementation Framework**\n\n**Phase 1: Data Generation Pipeline**\n\n1.  **Seed Dataset Creation**:\n\n- Use MHTS to generate multi-hop claims from medical corpora \\[2\\].\n- GPT-4 drafts Q/A pairs; reject single-hop questions using hop classifier \\[3\\].\n\n1.  **Attribution Verification**:\n\n- OpenScholar-style self-verification → Flag low-confidence citations.\n- Human experts validate 20-30% of complex outputs \\[46\\].\n\n1.  **Difficulty Balancing**:\n\n- 40% 1-hop, 40% 2-hop, 20% 3-hop questions (adjust via semantic spread) \\[5\\]\\[63\\].\n\n**Phase 2: Model Fine-Tuning**\n\n**PEFT with Attribution Heads**: Add citation prediction layers via Low-Rank Adaptation (LoRA). Trains 20× faster than full SFT \\[39\\].\n\n**Curriculum Fine-Tuning**: Order training data by difficulty (single→multi-hop). Improves complex QA accuracy by 14% \\[203\\].\n\n**Phase 3: Continuous Refinement**\n\n**Hallucination Monitoring**: Track citation F1 during inference; retrain on misattributed examples.\n\n**Human Feedback Integration**: Clinicians flag errors → augment training data weekly \\[50\\].\n\n**8\\. Conclusion and Future Directions**\n\nGenerating high-quality data for complex medical QA requires **structured synthesis frameworks** (MHTS), **verification-augmented generation** (OpenScholar), and **targeted human oversight**. Hybrid human-LLM pipelines increase multi-hop citation F1 by 25-33% over pure synthetic methods while controlling costs through dynamic routing. Future work should explore:\n\n1.  **Automated Difficulty Calibration**: LLM-based difficulty scoring \\[2\\].\n2.  **Patient-Specific Evidence Integration**: RAG personalization using EHR data \\[80\\].\n3.  **Cross-Dataset Validation**: Apply BioHopR and ClinicalHopQ benchmarks to quantify generalizability \\[321\\].\n\nThe path toward reliable clinical AI hinges on shifting from naive synthetic data to verifiably attributed, complexity-optimized training frameworks.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[2\\. MHTS: Multi-Hop Tree Structure Framework for Generating Difficulty-Controllable QA Datasets for RAG Evaluation](https://www.arxiv.org/pdf/2504.08756)\n\n[3\\. Improving RAG Answer Quality Through Complex Reasoning](https://towardsdatascience.com/improving-rag-answer-quality-through-complex-reasoning-2608ec6c2a65/)\n\n[4\\. Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach](https://arxiv.org/pdf/2407.13101)\n\n[5\\. RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum Learning](https://arxiv.org/html/2503.12759v1)\n\n[6\\. MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries](https://openreview.net/pdf/43ab88c33fb4e13c3e723aaf46617f8e731d97c9.pdf)\n\n[7\\. Pranav Rajpurkar, Jian Zhang et al. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D16-1264)\n\n[8\\. EfficientRAG: Efficient Retriever for Multi-Hop Question Answering](https://promptengineer.online/papers/efficientrag-efficient-retriever-for-multi-hop-question-answering-ai-summary)\n\n[9\\. Data Generation](https://docs.camel-ai.org/key_modules/datagen.html)\n\n[10\\. Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration](https://arxiv.org/abs/2504.04915)\n\n[11\\. Multiple Abstraction Level Retrieve Augment Generation](http://arxiv.org/html/2501.16952v1)\n\n[12\\. RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems](https://arxiv.org/pdf/2407.11005)\n\n[13\\. DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering](https://openreview.net/pdf/8766a6d482bad018235de79c05a0a1e11d3149e8.pdf)\n\n[14\\. M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding](https://arxiv.org/pdf/2411.04952)\n\n[15\\. Papers with Code - Multi-hop Question Answering](https://paperswithcode.com/task/multi-hop-question-answering/latest)\n\n[16\\. NVIDIA Research: RAG with Long Context LLMs](https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4)\n\n[17\\. T. Kwiatkowski, J. Palomaki et al. “Natural Questions: A Benchmark for Question Answering Research.” Transactions of the Association for Computational Linguistics](https://doi.org/10.1162/tacl_a_00276)\n\n[18\\. Zero-Shot Complex Question-Answering on Long Scientific Documents](https://arxiv.org/pdf/2503.02695)\n\n[21\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[22\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[23\\. Improving the Capabilities of LLM-Based Analytics](https://dzone.com/articles/improving-the-capabilities-of-llm-based-analytics)\n\n[24\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[25\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[26\\. SyntheT2C: Generating Synthetic Data for Fine-Tuning Large Language Models on the Text2Cypher Task](https://arxiv.org/pdf/2406.10710)\n\n[27\\. 视觉指令调优：提升多模态语言模型的零样本能力](https://www.51cto.com/aigc/4381.html)\n\n[28\\. Mistral 7b on Enlighten Code Base Fine-Tuning with Synthetic Data](https://developer.chat/fine-tuning-mistral-7b-google-colab-qlora-complete-guide)\n\n[29\\. GPTUNER: A Manual-Reading Database Tuning System via GPT-Guided Bayesian Optimization](https://vldb.org/pvldb/vol17/p1939-tang.pdf)\n\n[30\\. Large Language Models for Inorganic Synthesis Predictions](https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/666c8eba5101a2ffa89b5e67/original/2024.05.29_synthGPT_manuscript_clean.pdf)\n\n[31\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[32\\. Retrieval-Augmented Generation with Geospatial Search: Optimizing Spatial Data Retrieval through Embeddings and Vector Search](https://pergamos.lib.uoa.gr/uoa/dl/object/3476403/file.pdf)\n\n[33\\. Improving the Accuracy of Text-to-SQL Tools based on Large Language Models for Real-World Relational Databases](https://www-di.inf.puc-rio.br/~casanova/Publications/Papers/2024-Papers/2024-DEXA.pdf)\n\n[34\\. Improving Consistency in Large Language Models through Chain of Guidance](https://www.arxiv.org/pdf/2502.15924)\n\n[35\\. Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarcity](https://arxiv.org/pdf/2502.11901)\n\n[36\\. LLMs for Google Maps](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/final-projects/SukrutOak.pdf)\n\n[37\\. Vinzent Penzkofer, Timo Baumann. “Evaluating and Fine-Tuning Retrieval-Augmented Language Models to Generate Text with Accurate Citations.” Conference on Natural Language Processing](https://www.semanticscholar.org/paper/05e452128444f61acd066c669d60733b3765434c)\n\n[38\\. OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs](https://www.catalyzex.com/author/Hao%20Tong)\n\n[39\\. Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data](https://assets.amazon.science/85/cb/7d6882e34bff9174c73dd42022ac/enhancing-low-resource-llms-classification-with-peft-and-synthetic-data.pdf)\n\n[41\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[42\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[43\\. Augmenting and Refining NER Datasets through LLMs](http://arxiv.org/html/2404.01334v2)\n\n[44\\. Enhancing Text Classification through LLM-Driven Active Learning and Human Annotation](https://www.themoonlight.io/en/review/enhancing-text-classification-through-llm-driven-active-learning-and-human-annotation)\n\n[45\\. SYNCode: Synergistic Human–LLM Collaboration for Enhanced Data Annotation in Stack Overflow](https://www.mdpi.com/2078-2489/16/5/392)\n\n[46\\. Appen: Human-AI Co-Annotation System for Efficient Data Labeling](https://www.zenml.io/llmops-database/human-ai-co-annotation-system-for-efficient-data-labeling)\n\n[47\\. Unitlab AI: Data Collection and Annotation for LLMs and Generative AI](https://blog.unitlab.ai/unitlab-ai-data-collection-and-annotation-for-llms-and-generative-ai/)\n\n[48\\. A Data Annotation Approach Using Large Language Models](https://www.scitepress.org/Papers/2025/132801/132801.pdf)\n\n[49\\. Large Language Models for Data Annotation and Synthesis: A Survey](https://arxiv.org/pdf/2402.13446v3)\n\n[50\\. HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution](https://ar5iv.labs.arxiv.org/html/2307.16883)\n\n[51\\. From Selection to Generation: A Survey of LLM-based Active Learning](https://arxiv.org/pdf/2502.11767)\n\n[52\\. Pranav Rajpurkar, Jian Zhang et al. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D16-1264)\n\n[53\\. API-Blend: A Comprehensive Corpora for Training and Benchmarking API LLMs](https://arxiv.org/html/2402.15491v2)\n\n[54\\. 面向众包标注的人类-大语言模型混合文本答案聚合](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_23/2410.17099.pdf)\n\n[55\\. Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks](https://arxiv.org/pdf/2505.14212)\n\n[56\\. Pragmatics in the Era of Large Language Models: A Survey on Datasets, Evaluation, Opportunities and Challenges](https://arxiv.org/pdf/2502.12378)\n\n[57\\. A Hybrid Detection and Generation Framework with Separate Encoders for Event Extraction](https://aclanthology.org/people/y/yongliang-ma/)\n\n[58\\. Large Language Models in the Task of Automatic Validation of Text Classifier Predictions](https://arxiv.org/pdf/2505.18688)\n\n[59\\. ProMQA：用于多模态程序活动理解的问题回答数据集](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_30/2410.22211.pdf)\n\n[61\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[62\\. Pranav Rajpurkar, Jian Zhang et al. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D16-1264)\n\n[63\\. MHTS: Multi-Hop Tree Structure Framework for Generating Difficulty-Controllable QA Datasets for RAG Evaluation](https://arxiv.org/html/2504.08756v1)\n\n[64\\. Pranav Rajpurkar, Robin Jia et al. “Know What You Don’t Know: Unanswerable Questions for SQuAD.” ArXiv](https://doi.org/10.18653/v1/P18-2124)\n\n[65\\. Zhilin Yang, Peng Qi et al. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D18-1259)\n\n[66\\. Multi-Hop Question Generation via Dual-Perspective Keyword Guidance](https://arxiv.org/pdf/2505.15299)\n\n[67\\. Johannes Welbl, Pontus Stenetorp et al. “Constructing Datasets for Multi-hop Reading Comprehension Across Documents.” Transactions of the Association for Computational Linguistics](https://doi.org/10.1162/tacl_a_00021)\n\n[68\\. Multi-hop Question Answering](https://www.nowpublishers.com/article/Details/INR-102)\n\n[69\\. Unsupervised Multi-hop Question Answering by Question Generation](https://arxiv.org/abs/2010.12623)\n\n[70\\. KNOWLEDGE GRAPH AUGMENTED MULTI-HOP QUESTION ANSWERING USING LARGE LANGUAGE MODELS](https://open.metu.edu.tr/bitstream/handle/11511/111317/knowledge-graph-augmented-multi-hop-question-answering-using-large-language-models.pdf)\n\n[71\\. 根拠を説明可能な質問応答システムのための日本語マルチホップ QA データセット構築](https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/Q8-14.pdf)\n\n[72\\. Answering Complex Open-domain Questions Through Iterative Query Generation](https://nlp.stanford.edu/pubs/qi2019answering.pdf)\n\n[73\\. MeltingpotQA: Multi-hop Question Answering](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/custom/15721508.pdf)\n\n[74\\. GitHub - Andy-jqa/biomedical-qa-datasets: Biomedical Question Answering Datasets](https://github.com/Andy-jqa/biomedical-qa-datasets)\n\n[75\\. CompQA: Investigating the Weakness of Multihop QA on Comparison Questions](https://openreview.net/attachment?id=NHM3wiGtOs&name=pdf)\n\n[76\\. HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering](https://www-nlp.stanford.edu/pubs/yang2018hotpotqa.pdf)\n\n[77\\. Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study](https://arxiv.org/html/2504.16414v1)\n\n[78\\. MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering](https://proceedings.mlr.press/v174/pal22a/pal22a.pdf)\n\n[79\\. A Review on Medical Textual Question Answering Systems Based on Deep Learning Approaches](https://mdpi-res.com/d_attachment/applsci/applsci-11-05456/article_deploy/applsci-11-05456.epub)\n\n[80\\. MHQA: A Diverse, Knowledge Intensive Mental Health Question Answering Challenge for Language Models](http://www.arxiv.org/pdf/2502.15418)\n\n[81\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[82\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[83\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[84\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[85\\. Development of a Flexible Chain of Thought Framework for Automated Routing of Patient Portal Messages](https://pmc.ncbi.nlm.nih.gov/articles/PMC12099328/)\n\n[86\\. Lianmin Zheng, Wei-Lin Chiang et al. “Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.” ArXiv](https://doi.org/10.48550/arXiv.2306.05685)\n\n[87\\. A Benchmark for Long-Form Medical Question Answering](http://arxiv.org/html/2411.09834v2)\n\n[88\\. 基于大型语言模型的人机协作用于解决复杂任务](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_2_21/2402.12914.pdf)\n\n[89\\. GPT 是序列生成任务的多语言注释器](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_2_9/2402.05512.pdf)\n\n[90\\. Enhancing Human Annotation: Leveraging Large Language Models and Efficient Batch Processing](https://www.microsoft.com/en-us/research/uploads/prod/2024/01/chiir24-34.pdf)\n\n[91\\. ChatQA: Surpassing GPT-4 on Conversational QA and RAG](https://openreview.net/pdf?id=bkUvKPKafQ)\n\n[92\\. 长篇医学问答基准](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_11_18/2411.09834.pdf)\n\n[93\\. Automatic Evaluation for Mental Health Counseling using LLMs](https://openreview.net/pdf/e814d31fec1506bc230f98e447bdce92df27ccc4.pdf)\n\n[94\\. UltraMedical: Building Specialized Generalists in Biomedicine](https://papers.nips.cc/paper_files/paper/2024/file/2dfc26ce9039f00eee4aba0c54931e46-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[95\\. J. Oppenlaender, J. Hämäläinen. “Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Cost-Efficient Question Answering.” ArXiv](https://doi.org/10.48550/arXiv.2306.05036)\n\n[96\\. Balancing LLM Costs and Performance: A Guide to Smart Deployment](https://blog.premai.io/balancing-llm-costs-and-performance-a-guide-to-smart-deployment/)\n\n[97\\. Identifying biomedical entities for datasets in scientific articles – A 4-step cache-augmented generation approach using GPT-4o and PubTator 3.0](https://www.medrxiv.org/content/10.1101/2025.03.04.25323310v1.full.pdf)\n\n[98\\. GSM-PLUS: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers](https://openreview.net/pdf/d864beee9c750da4a99433e3a7e0645bad94b6d7.pdf)\n\n[99\\. Aligning Large Vision-Language Models with AI Feedback](http://arxiv.org/html/2410.09421v1)\n\n[100\\. HateModerate: Grounding and Benchmarking Hate Speech Detection with Content Policies](https://openreview.net/pdf?id=jdenTs1B2Pw)\n\n[101\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[102\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[103\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[104\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[105\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[106\\. LLM-based Automated Grading with Human-in-the-Loop](https://arxiv.org/pdf/2504.05239)\n\n[107\\. Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies in Medical Question Answering](https://arxiv.org/pdf/2408.07888)\n\n[108\\. OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs](https://paperreading.club/page?id=267494)\n\n[109\\. A dataset for evaluating clinical research claims in large language models](https://www.nature.com/articles/s41597-025-04417-x)\n\n[110\\. Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard](https://arxiv.org/html/2402.14533v1)\n\n[111\\. Zelalem Gero, Chandan Singh et al. “Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries.” ArXiv](https://doi.org/10.48550/arXiv.2403.01002)\n\n[112\\. Benchmarking Human-AI Collaboration for Common Evidence Appraisal Tools](http://medrxiv.org/cgi/reprint/2024.04.21.24306137v1)\n\n[113\\. MedReason: Eliciting Factual Medical Reasoning Steps in LLMs via Knowledge Graphs](https://arxiv.org/pdf/2504.00993)\n\n[114\\. EFFICIENCY OF SMALL OPEN-SOURCE LLMS IN SOFTWARE CODING Evaluation of LLMs and consumer hardware](https://trepo.tuni.fi/bitstream/10024/157186/2/JussilaPetrus.pdf)\n\n[115\\. MisattributionLLM: Integrating Error Attribution Capability into LLM Evaluation](https://openreview.net/pdf/6398258fb83abb96b893d8af0ad17b6c4104b174.pdf)\n\n[121\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[122\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[123\\. K. Papineni, Salim Roukos et al. “Bleu: a Method for Automatic Evaluation of Machine Translation.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.3115/1073083.1073135)\n\n[124\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[125\\. Alistair E. W. Johnson, T. Pollard et al. “MIMIC-III, a freely accessible critical care database.” Scientific Data](https://doi.org/10.1038/sdata.2016.35)\n\n[126\\. LLM-based Automated Grading with Human-in-the-Loop](https://arxiv.org/pdf/2504.05239)\n\n[127\\. Integrated Decision Gradients: Compute Your Attributions Where the Model Makes Its Decision](https://sumitkumarjha.com/papers/2024_Jha_AAAI_IDG.pdf)\n\n[128\\. Medical foundation large language models for comprehensive text analysis and beyond](https://www.nature.com/articles/s41746-025-01533-1)\n\n[129\\. Neurons Speak in Ranges: Breaking Free from Discrete Neuronal Attribution](https://arxiv.org/pdf/2502.06809v2)\n\n[130\\. CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks](https://arxiv.org/pdf/2505.11314)\n\n[131\\. Credibility Evaluation of Online Health Information using Human in the Loop Machine Learning](https://pja.edu.pl/wp-content/uploads/2023/07/PhD_thesis_Aleksandra_Nabozny-final.pdf)\n\n[132\\. Explaining Deep Neural Networks to Establish Trust](https://research-repository.uwa.edu.au/files/454937990/THESIS_-_DOCTOR_OF_PHILOSOPHY_-_YANG_Peiyu_-_2024_.pdf)\n\n[133\\. Open Scholar：使用检索增强型语言模型合成科学文献](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_11_22/2411.14199.pdf)\n\n[134\\. Accelerating Disease Model Parameter Extraction: An LLM-based Ranking Approach to Select Initial Studies For Literature Review Automation](https://www.preprints.org/frontend/manuscript/df71551a7789bea3ce8c6b7d0dfec638/download_pub)\n\n[135\\. OPENSCHOLAR: 基于大模型RAG的科研专家](https://blog.csdn.net/Python_cocola/article/details/144159900)\n\n[136\\. Development and validation of an autonomous artificial intelligence agent for clinical decision-making in oncology](https://www.nature.com/articles/s43018-025-00991-6.pdf)\n\n[137\\. Holistic-CAM: Ultra-lucid and Sanity Preserving Visual Interpretation in Holistic Stage of CNNs](https://openreview.net/pdf?id=O9Vuj6lzya)\n\n[138\\. Xplain: Analyzing Invisible Correlations in Model Explanation](https://www.usenix.org/system/files/usenixsecurity24-kumari.pdf)\n\n[139\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[140\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[141\\. Pranav Rajpurkar, Jian Zhang et al. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D16-1264)\n\n[142\\. HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution](https://ar5iv.labs.arxiv.org/html/2307.16883)\n\n[143\\. Towards Improved Multi-Source Attribution for Long-Form Answer Generation](https://assets.amazon.science/ed/c3/7232c163413b94ed203eb1ea46a0/towards-improved-multi-source-attribution-for-long-form-answer-generation.pdf)\n\n[144\\. Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation](https://arxiv.org/html/2409.12941v1)\n\n[145\\. Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata](https://arxiv.org/html/2406.13213v2)\n\n[146\\. Single and Multi-Hop Question-Answering Datasets for Reticular Chemistry with GPT-4-Turbo](https://arxiv.org/pdf/2405.02128)\n\n[147\\. Xuezhi Wang, Jason Wei et al. “Self-Consistency Improves Chain of Thought Reasoning in Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2203.11171)\n\n[148\\. A Synthetic Dataset for Personal Attribute Inference](https://proceedings.neurips.cc/paper_files/paper/2024/file/daa1816b84ca2d5051c87fb4d37dd540-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[149\\. HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs](https://arxiv.org/abs/2406.06027)\n\n[150\\. 大模型化身数据魔法师，降低NLP高置信误判](https://zhuanlan.zhihu.com/p/692422688)\n\n[151\\. Zhilin Yang, Peng Qi et al. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D18-1259)\n\n[152\\. Large Language Models in Medical Education: Comparing ChatGPT- to Human-Generated Exam Questions](https://pubmed.ncbi.nlm.nih.gov/38166323/)\n\n[153\\. 阐明盲点：探索 LLM 作为有针对性的合成文本数据的来源，以最大限度地减少高置信度错误分类](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_3_27/2403.17860.pdf)\n\n[154\\. MisattributionLLM: Integrating Error Attribution Capability into LLM Evaluation](https://openreview.net/pdf/6398258fb83abb96b893d8af0ad17b6c4104b174.pdf)\n\n[155\\. Decoupling Knowledge and Context: An Efficient and Effective Retrieval Augmented Generation Framework via Cross Attention](https://openreview.net/pdf?id=EVIREJsGEE)\n\n[156\\. Docimological Quality Analysis of LLM-Generated Multiple Choice Questions in Computer Science and Medicine](https://link.springer.com/article/10.1007/s42979-024-02963-6)\n\n[157\\. マルチホップ QA の根拠情報を用いた LLM の “偽” 正解の分析](https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/P9-12.pdf)\n\n[158\\. Large language models generating synthetic clinical datasets: a feasibility and comparative analysis with real-world perioperative data](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1533508/full)\n\n[159\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[160\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[161\\. LONGCITE: ENABLING LLMs TO GENERATE FINE- GRAINED CITATIONS IN LONG-CONTEXT QA](https://arxiv.org/pdf/2409.02897)\n\n[162\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[163\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[164\\. Comparison of LLMs in Extracting Synthesis Conditions and Generating Q&A Datasets for Metal– Organic Frameworks](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d5dd00081e)\n\n[165\\. LabQAR: A Manually Curated Dataset for Question Answering on Laboratory Test Reference Ranges and Interpretation](https://www.medrxiv.org/content/10.1101/2025.06.03.25328882v1.full.pdf)\n\n[166\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[167\\. CofCA: A Step-Wise Counterfactual Multi-Hop QA Benchmark](https://arxiv.org/pdf/2402.11924)\n\n[168\\. Answering Genomics Questions with Precision: GeneGPT Bridges the Gap Between LLMs and Biomedical Information](https://cbirt.net/answering-genomics-questions-with-precision-genegpt-bridges-the-gap-between-llms-and-biomedical-information/)\n\n[169\\. Fine-tuned large language models for answering questions about full-text biomedical research studies](https://www.medrxiv.org/content/10.1101/2024.10.28.24316263v2.full.pdf)\n\n[170\\. Evaluating accuracy and reproducibility of large language model performance on critical care assessments in pharmacy education](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1514896/pdf)\n\n[171\\. Evaluation of Large Language Models under Different Training Background in Chinese Medical Examination: A Comparative Study](https://pmc.ncbi.nlm.nih.gov/articles/PMC11652508/)\n\n[172\\. QUEST-AI: A System for Question Generation, Verification, and Refinement using AI for USMLE-Style Exams](https://psb.stanford.edu/psb-online/proceedings/psb25/bedi.pdf)\n\n[173\\. Decoupling Knowledge and Context: An Efficient and Effective Retrieval Augmented Generation Framework via Cross Attention](https://openreview.net/pdf?id=EVIREJsGEE)\n\n[174\\. Enhancing Medical Text Evaluation with GPT-4](https://arxiv.org/pdf/2311.09581v1)\n\n[175\\. Single and Multi-Hop Question-Answering Datasets for Reticular Chemistry with GPT-4-Turbo](https://arxiv.org/pdf/2405.02128)\n\n[176\\. Comparison of LLMs in Extracting Synthesis Conditions and Generating Q&A Datasets for Metal-Organic Frameworks](https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/67651f5a81d2151a02580020/original/comparison-of-ll-ms-in-extracting-synthesis-conditions-and-generating-q-a-datasets-for-metal-organic-frameworks.pdf)\n\n[177\\. Simulating Classroom Education with LLM-Empowered Agents](https://arxiv.org/pdf/2406.19226)\n\n[178\\. GPT-4 on Clinic Depression Assessment: An LLM-Based Pilot Study](http://arxiv.org/html/2501.00199v1)\n\n[179\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[180\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[181\\. Comparison of LLMs in Extracting Synthesis Conditions and Generating Q&A Datasets for Metal– Organic Frameworks](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d5dd00081e)\n\n[182\\. CofCA: A Step-Wise Counterfactual Multi-Hop QA Benchmark](https://arxiv.org/pdf/2402.11924)\n\n[183\\. LONGCITE: ENABLING LLMs TO GENERATE FINE- GRAINED CITATIONS IN LONG-CONTEXT QA](https://arxiv.org/pdf/2409.02897)\n\n[184\\. MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition](https://openreview.net/pdf/8b21dddde5ae45ac6195677af86696c12c5538cf.pdf)\n\n[185\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[186\\. Ask, Assess, and Refine: Rectifying Factual Consistency and Hallucination in LLMs with Metric-Guided Feedback Learning](https://openreview.net/pdf?id=mGyikIQFoH)\n\n[187\\. Comparison of LLMs in Extracting Synthesis Conditions and Generating Q&A Datasets for Metal-Organic Frameworks](https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/67651f5a81d2151a02580020/original/comparison-of-ll-ms-in-extracting-synthesis-conditions-and-generating-q-a-datasets-for-metal-organic-frameworks.pdf)\n\n[188\\. Evaluating accuracy and reproducibility of large language model performance on critical care assessments in pharmacy education](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1514896/pdf)\n\n[189\\. Comparative Analysis of GPT-4Vision, GPT-4 and Open Source LLMs in Clinical Diagnostic Accuracy: A Benchmark Against Human Expertise](https://www.medrxiv.org/content/10.1101/2023.11.03.23297957v1.full.pdf)\n\n[190\\. Aakanksha Chowdhery, Sharan Narang et al. “PaLM: Scaling Language Modeling with Pathways.” J. Mach. Learn. Res.](https://arxiv.org/abs/2204.02311)\n\n[191\\. Self-Logical Consistent GPT-4 Enables Human-Level Classification of Patient Feedback](https://www.medrxiv.org/content/10.1101/2024.07.11.24310210v2.full.pdf)\n\n[192\\. EquinorQA: Large Language Models for Question Answering over proprietary data](https://xai.w.uib.no/files/2024/08/ECAI_2024-EquinorQA.pdf)\n\n[193\\. A Test Collection of Synthetic Documents for Training Rankers: ChatGPT vs. Human Experts](https://pure.uva.nl/ws/files/169472407/3583780.3615111.pdf)\n\n[194\\. Single and Multi-Hop Question-Answering Datasets for Reticular Chemistry with GPT-4-Turbo](https://arxiv.org/pdf/2405.02128)\n\n[195\\. A Synthetic Dataset for Personal Attribute Inference](https://proceedings.neurips.cc/paper_files/paper/2024/file/daa1816b84ca2d5051c87fb4d37dd540-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[196\\. Large language models generating synthetic clinical datasets: a feasibility and comparative analysis with real-world perioperative data](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1533508/pdf)\n\n[197\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[199\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[200\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[201\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[202\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[203\\. Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies in Medical Question Answering](https://arxiv.org/pdf/2408.07888)\n\n[204\\. High-quality data meets enterprise MLOps](https://ai-infrastructure.org/high-quality-data-meets-enterprise-mlops/)\n\n[205\\. Clinical knowledge in LLMs does not translate to human interactions](https://arxiv.org/pdf/2504.18919)\n\n[206\\. OpenAI Josh Achiam, Steven Adler et al. “GPT-4 Technical Report.”](https://arxiv.org/abs/2303.08774)\n\n[207\\. From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data](https://paperreading.club/page?id=237034)\n\n[208\\. Fine-Tuning Llama 3: Enhancing Accuracy in Medical Q&A With LLMs](https://labelstud.io/blog/fine-tuning-llama-3-enhancing-accuracy-in-medical-q-and-a-with-llms/)\n\n[209\\. Compound-QA: A Benchmark for Evaluating LLMs on Compound Questions](https://openreview.net/pdf/94f2782bccd7ecc34468803cddbd634dc534e425.pdf)\n\n[210\\. Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval](https://cs.uwaterloo.ca/~jimmylin/publications/Thakur_etal_NAACL2024.pdf)\n\n[211\\. \"Give Me BF16 or Give Me Death\"? ACCURACY-PERFORMANCE TRADE-OFFS IN LLM QUANTIZATION](https://arxiv.org/pdf/2411.02355v1)\n\n[212\\. A Survey of LLM Datasets: From Autoregressive Model to AI Chatbot](https://jcst.ict.ac.cn/fileup/1000-9000/PDF/JCST-2024-3-3-3767-542.pdf)\n\n[213\\. What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices](https://openreview.net/pdf?id=hgagmZSAb9)\n\n[214\\. MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition](https://arxiv.org/html/2402.11924v2)\n\n[215\\. Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA](https://paperreading.club/page?id=217665)\n\n[216\\. Hybrid Training Approaches for LLMs: Leveraging Real and Synthetic Data to Enhance Model Performance in Domain-Specific Applications](https://arxiv.org/pdf/2410.09168)\n\n[217\\. Large Language Models for Synthetic Tabular Health Data: A Benchmark Study](https://ebooks.iospress.nl/pdf/doi/10.3233/SHTI240571)\n\n[219\\. Fabian Pedregosa, G. Varoquaux et al. “Scikit-learn: Machine Learning in Python.” ArXiv](https://doi.org/10.5555/1953048.2078195)\n\n[220\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[221\\. J. Lafferty, A. McCallum et al. “Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.” International Conference on Machine Learning](https://www.semanticscholar.org/paper/f4ba954b0412773d047dc41231c733de0c1f4926)\n\n[222\\. Jinhyuk Lee, Wonjin Yoon et al. “BioBERT: a pre-trained biomedical language representation model for biomedical text mining.” Bioinformatics](https://doi.org/10.1093/bioinformatics/btz682)\n\n[223\\. BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text](https://arxiv.org/pdf/2504.19467)\n\n[224\\. Linking Quality Indicators to Clinical Trials: An Automated Approach](https://pmc.ncbi.nlm.nih.gov/articles/PMC5890874/)\n\n[225\\. Enriching information extraction pipelines in clinical decision support systems](https://ruc.udc.es/dspace/bitstream/handle/2183/33254/Almeida_JoaoRafaelDuartede_TD_2023.pdf?sequence=3)\n\n[226\\. On Evaluation and Efficient Post-training for LLMs](https://escholarship.org/content/qt4q1931gr/qt4q1931gr.pdf?t=svl1fd)\n\n[227\\. A comparative study of pretrained language models for long clinical text](https://pmc.ncbi.nlm.nih.gov/articles/PMC9846675/)\n\n[228\\. Michael Moor, Oishi Banerjee et al. “Foundation models for generalist medical artificial intelligence.” Nature](https://doi.org/10.1038/s41586-023-05881-4)\n\n[229\\. Clinical Decision Support using Pseudo-notes from multiple streams of EHR Data](https://openreview.net/pdf?id=FG45RbP9Ka)\n\n[230\\. Ananya Poddar, Bharath Dandala et al. “Training Models to Extract Treatment Plans from Clinical Notes Using Contents of Sections with Headings.” ArXiv](https://arxiv.org/abs/1906.11930)\n\n[231\\. EVALUATING AND DESIGNING COMPUTING SYSTEMS FOR THE FUTURE OF WORK](https://hci.stanford.edu/publications/2024/Hancheng_Cao_dissertation.pdf)\n\n[232\\. A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?](https://openreview.net/pdf?id=nzh8Z8d1Zc)\n\n[233\\. Thomas Petit-Jean, C. Gérardin et al. “Collaborative and privacy-preserving workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions.” medRxiv](https://doi.org/10.1101/2023.09.11.23295069)\n\n[234\\. Thomas Petit-Jean, C. Gérardin et al. “Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions.” Journal of the American Medical Informatics Association : JAMIA](https://doi.org/10.1093/jamia/ocae069)\n\n[235\\. Instructor Notes](https://cdn-blog.adafruit.com/uploads/2020/02/python_NSA.pdf)\n\n[236\\. CLUES: Collaborative Private-domain High-quality Data Selection for LLMs via Training Dynamics](https://openreview.net/pdf?id=OU1uqd1vyw)\n\n[237\\. DEEP LEARNING APPROACHES FOR CLASSIFYING DATA: A REVIEW](https://jestec.taylors.edu.my/Vol%2015%20issue%204%20August%202020/15_4_32.pdf)\n\n[239\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[240\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[241\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[242\\. Instruction-guided deidentification with synthetic test cases for Norwegian clinical text](https://proceedings.mlr.press/v233/lund24a/lund24a.pdf)\n\n[243\\. Medical foundation large language models for comprehensive text analysis and beyond](https://www.nature.com/articles/s41746-025-01533-1)\n\n[244\\. Alistair E. W. Johnson, T. Pollard et al. “MIMIC-III, a freely accessible critical care database.” Scientific Data](https://doi.org/10.1038/sdata.2016.35)\n\n[245\\. Nisan Stiennon, Long Ouyang et al. “Learning to summarize from human feedback.” ArXiv](https://arxiv.org/abs/2009.01325)\n\n[246\\. Enhancing Medical Text Evaluation with GPT-4](https://arxiv.org/pdf/2311.09581v1)\n\n[247\\. The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification Tasks](https://www.lajello.com/papers/eacl24augmentation.pdf)\n\n[248\\. NoteChat: A Dataset of Synthetic Doctor-Patient Conversations Conditioned on Clinical Notes](https://openreview.net/pdf?id=0CRXMafJAD)\n\n[249\\. Open Scholar：使用检索增强型语言模型合成科学文献](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_11_22/2411.14199.pdf)\n\n[250\\. OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs](https://paperreading.club/page?id=267494)\n\n[251\\. 1 Billion Citation Dataset and Deep Learning Citation Extraction](https://publications.scss.tcd.ie/theses/diss/2019/TCD-SCSS-DISSERTATION-2019-040.pdf)\n\n[252\\. Large Language Models for Clinical Information Extraction and Beyond](https://www.ohdsi.org/wp-content/uploads/2024/12/8a.-LLM-and-OHDSI-Part-1_Hua-XU.pdf)\n\n[253\\. Comparative Analysis of GPT-4Vision, GPT-4 and Open Source LLMs in Clinical Diagnostic Accuracy: A Benchmark Against Human Expertise](https://www.medrxiv.org/content/10.1101/2023.11.03.23297957v1.full.pdf)\n\n[254\\. Synthetic Data Distillation Enables the Extraction of Clinical Information at Scale](https://www.medrxiv.org/content/10.1101/2024.09.27.24314517.full.pdf)\n\n[255\\. IMPROVING GENERALIZATION OF ALIGNMENT WITH HUMAN PREFERENCES THROUGH GROUP INVARIANT LEARNING](https://openreview.net/pdf?id=fwCoLe3TAX)\n\n[259\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[260\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[261\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[262\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[263\\. BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedicine](https://arxiv.org/pdf/2505.22240)\n\n[264\\. Zhilin Yang, Peng Qi et al. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D18-1259)\n\n[265\\. ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision](https://openreview.net/pdf?id=RDJ0tKg5kx)\n\n[266\\. Synthetic Training Dataset Generation Using a Digital Twin-based Autonomous Driving Simulator](https://sensors.myu-group.co.jp/sm_pdf/SM3782.pdf)\n\n[267\\. Learning from Synthetic Human Group Activities](https://openaccess.thecvf.com/content/CVPR2024/papers/Chang_Learning_from_Synthetic_Human_Group_Activities_CVPR_2024_paper.pdf)\n\n[268\\. Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies in Medical Question Answering](https://arxiv.org/pdf/2408.07888)\n\n[269\\. Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process](http://arxiv.org/html/2402.19350v6)\n\n[270\\. Raul Puri, Ryan Spring et al. “Training Question Answering Models from Synthetic Data.” ArXiv](https://doi.org/10.18653/v1/2020.emnlp-main.468)\n\n[271\\. Utility Metrics for Evaluating Synthetic Health Data Generation Methods: Validation Study](https://pmc.ncbi.nlm.nih.gov/articles/PMC9030990/)\n\n[272\\. Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering](https://pure.uva.nl/ws/files/68306915/2021.sustainlp_1.7.pdf)\n\n[273\\. Improving Trust in Fact-Checking Systems with Synthetic Training Data and Explanations](https://theses.hal.science/tel-05018952v1/file/145911_BUSSOTTI_2024_archivage.pdf)\n\n[279\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[280\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[281\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[282\\. Comparative Analysis of GPT-4Vision, GPT-4 and Open Source LLMs in Clinical Diagnostic Accuracy: A Benchmark Against Human Expertise](https://www.medrxiv.org/content/10.1101/2023.11.03.23297957v2.full.pdf)\n\n[283\\. OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs](https://www.chatpaper.com/chatpaper/zh-CN/paper/84190)\n\n[284\\. Enhancing Clinical Reasoning with Virtual Patients: A Hybrid Systematic Review Combining Human Reviewers and ChatGPT](https://www.mdpi.com/2227-9032/12/22/2241)\n\n[285\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[286\\. MEASURING AND ENHANCING TRUSTWORTHINESS OF LLMs IN RAG THROUGH GROUNDED ATTRIBUTIONS AND LEARNING TO REFUSE](https://openreview.net/pdf/9504fa8d530adfa482d4d4879aadbb73ff688dcd.pdf)\n\n[287\\. LabQAR: A Manually Curated Dataset for Question Answering on Laboratory Test Reference Ranges and Interpretation](https://www.medrxiv.org/content/10.1101/2025.06.03.25328882v1.full.pdf)\n\n[288\\. Chao Ding, Mouxiao Bian et al. “Building a Human-Verified Clinical Reasoning Dataset via a Human LLM Hybrid Pipeline for Trustworthy Medical AI.”](https://arxiv.org/abs/2505.06912)\n\n[289\\. Medical LLMs: Fine-Tuning vs. Retrieval-Augmented Generation](https://www.mdpi.com/2306-5354/12/7/687)\n\n[290\\. A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?](https://www.rivista.ai/wp-content/uploads/2024/09/2409.15277v1.pdf)\n\n[291\\. EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries](https://github.com/ji-youn-kim/EHRNoteQA/)\n\n[292\\. OpenScholar：使用检索增强的语言模型综合科学文献](https://www.chatpaper.ai/zh/dashboard/paper/75bab7bc-2c57-4cf7-bdcf-8f60e47d3cff)\n\n[293\\. K. Singhal, Shekoofeh Azizi et al. “Large language models encode clinical knowledge.” Nature](https://doi.org/10.1038/s41586-023-06291-2)\n\n[294\\. CODEBENCHGEN: CREATING SCALABLE EXECUTION-BASED CODE GENERATION BENCHMARKS](https://openreview.net/pdf/173ef0e723cfaed0fc57f0b615011363c977c7e3.pdf)\n\n[295\\. Beyond Boundaries: A Human-like Approach for Question Answering over Structured and Unstructured information sources](https://assets.amazon.science/d5/27/f956b50740fc813deb01b979cffb/beyond-boundaries-a-human-like-approach-for-question-answering-over-structured-and-unstructured-information-sources.pdf)\n\n[299\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[300\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[301\\. CofCA: A Step-Wise Counterfactual Multi-Hop QA Benchmark](https://arxiv.org/pdf/2402.11924)\n\n[302\\. MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition](https://openreview.net/pdf/8b21dddde5ae45ac6195677af86696c12c5538cf.pdf)\n\n[303\\. COMPARISON OF LLM FEW-SHOT VS. SYNTHETIC DATA APPROACHES FOR LITHUANIAN EVENT EXTRACTION](https://ijcionline.com/paper/14/14125ijci01.pdf)\n\n[304\\. The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification Tasks](https://www.lajello.com/papers/eacl24augmentation.pdf)\n\n[305\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[306\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[307\\. CompQA: Investigating the Weakness of Multihop QA on Comparison Questions](https://openreview.net/attachment?id=NHM3wiGtOs&name=pdf)\n\n[308\\. Hyung Won Chung, Le Hou et al. “Scaling Instruction-Finetuned Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2210.11416)\n\n[309\\. Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control](https://papers.nips.cc/paper_files/paper/2024/file/1cb5b3d64bdf3c6642c8d9a8fbecd019-Paper-Conference.pdf)\n\n[310\\. Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities](https://www.cis.upenn.edu/~alur/LLM4Security24.pdf)\n\n[311\\. Computational fact-checking with limited resources](https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=1682&context=etd_coll)\n\n[312\\. Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset](https://paperreading.club/page?id=169248)\n\n[313\\. Hybrid Training Approaches for LLMs: Leveraging Real and Synthetic Data to Enhance Model Performance in Domain-Specific Applications](https://arxiv.org/pdf/2410.09168)\n\n[314\\. Comparison of LLMs in Extracting Synthesis Conditions and Generating Q&A Datasets for Metal-Organic Frameworks](https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/67651f5a81d2151a02580020/original/comparison-of-ll-ms-in-extracting-synthesis-conditions-and-generating-q-a-datasets-for-metal-organic-frameworks.pdf)\n\n[319\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[320\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[321\\. BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedicine](https://arxiv.org/pdf/2505.22240)\n\n[322\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[323\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[324\\. AI-Powered Test Question Generation in Medical Education: The DailyMed Approach](https://www.medrxiv.org/content/10.1101/2024.11.11.24317087v1.full.pdf)\n\n[325\\. Enhancing Medical Text Evaluation with GPT-4](https://arxiv.org/pdf/2311.09581v1)\n\n[326\\. Single and Multi-Hop Question-Answering Datasets for Reticular Chemistry with GPT-4-Turbo](https://arxiv.org/pdf/2405.02128)\n\n[327\\. GPT-4在查询科学出版物中的表现：指令表的可重复性、准确性和影响](https://pmc.ncbi.nlm.nih.gov/articles/PMC11197181/)\n\n[328\\. ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision](https://openreview.net/pdf?id=RDJ0tKg5kx)\n\n[329\\. Fine-tuned large language models for answering questions about full-text biomedical research studies](https://www.medrxiv.org/content/10.1101/2024.10.28.24316263v2.full.pdf)\n\n[330\\. Toward Optimising a Retrieval Augmented Generation Pipeline using Large Language Model](https://wwwmatthes.in.tum.de/file/1vzgjh1an34u0/Sebis-Public-Website/-/Master-s-Thesis-Gentrit-Fazlija/Master%20Thesis_Gentrit%20Fazlija_signed.pdf)\n\n[331\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[332\\. BioHopR：生物医学领域多跳、多答案推理的基准测试](https://www.xueshuxiangzi.com/downloads/2025_5_29/2505.22240.pdf)\n\n[333\\. GMAI-MMBench](https://openreview.net/attachment?id=FLBYTY4hKT&name=pdf)\n\n[334\\. A GRAPH-BASED SYNTHETIC DATA PIPELINE FOR SCALING HIGH-QUALITY REASONING INSTRUCTIONS](https://arxiv.org/pdf/2412.08864)\n\n[335\\. GPT-4 Performance on Querying Scientific Publications: Reproducibility, Accuracy, and Impact of an Instruction Sheet](https://hivdb.stanford.edu/_wrapper/pages/pdf/Tao.2024.1.pdf)\n\n[339\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[340\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[341\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[342\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[343\\. OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs](https://www.catalyzex.com/author/Hao%20Tong)\n\n[344\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[345\\. OpenScholar：使用检索增强的语言模型综合科学文献](https://www.chatpaper.ai/zh/dashboard/paper/75bab7bc-2c57-4cf7-bdcf-8f60e47d3cff)\n\n[346\\. Comparative Analysis of GPT-4Vision, GPT-4 and Open Source LLMs in Clinical Diagnostic Accuracy: A Benchmark Against Human Expertise](https://www.medrxiv.org/content/10.1101/2023.11.03.23297957v1.full.pdf)\n\n[347\\. Enhancing Medical Text Evaluation with GPT-4](https://arxiv.org/pdf/2311.09581v1)\n\n[348\\. LONGCITE: ENABLING LLMs TO GENERATE FINE- GRAINED CITATIONS IN LONG-CONTEXT QA](https://arxiv.org/pdf/2409.02897)\n\n[349\\. CODEBENCHGEN: CREATING SCALABLE EXECUTION-BASED CODE GENERATION BENCHMARKS](https://openreview.net/pdf/173ef0e723cfaed0fc57f0b615011363c977c7e3.pdf)\n\n[350\\. Open Scholar：使用检索增强型语言模型合成科学文献](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_11_22/2411.14199.pdf)\n\n[351\\. Zelalem Gero, Chandan Singh et al. “Self-Verification Improves Few-Shot Clinical Information Extraction.” ArXiv](https://doi.org/10.48550/arXiv.2306.00024)\n\n[352\\. LabQAR: A Manually Curated Dataset for Question Answering on Laboratory Test Reference Ranges and Interpretation](https://www.medrxiv.org/content/10.1101/2025.06.03.25328882v1.full.pdf)\n\n[353\\. Scientific literature synthesis with retrieval-augmented language models](https://allenai.org/blog/openscilm)\n\n[354\\. Instruction-guided deidentification with synthetic test cases for Norwegian clinical text](https://proceedings.mlr.press/v233/lund24a/lund24a.pdf)\n\n[359\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[360\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[361\\. CofCA: A Step-Wise Counterfactual Multi-Hop QA Benchmark](https://arxiv.org/pdf/2402.11924)\n\n[362\\. MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition](https://openreview.net/pdf/8b21dddde5ae45ac6195677af86696c12c5538cf.pdf)\n\n[363\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[364\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[365\\. The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification Tasks](https://www.lajello.com/papers/eacl24augmentation.pdf)\n\n[366\\. CLUES: Collaborative Private-domain High-quality Data Selection for LLMs via Training Dynamics](https://openreview.net/pdf?id=OU1uqd1vyw)\n\n[367\\. Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset](https://paperreading.club/page?id=169248)\n\n[368\\. CofCA: A Step-Wise Counterfactual Multi-hop QA benchmark](https://iclr.cc/media/iclr-2025/Slides/28269.pdf)\n\n[369\\. Zhilin Yang, Peng Qi et al. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D18-1259)\n\n[370\\. Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control](https://papers.nips.cc/paper_files/paper/2024/file/1cb5b3d64bdf3c6642c8d9a8fbecd019-Paper-Conference.pdf)\n\n[371\\. Leveraging GPT-4 for Identifying Clinical Phenotypes in Electronic Health Records: A Performance Comparison between GPT-4, GPT-3.5-turbo and spaCy’s Rule-based & Machine Learning-based methods](https://www.biorxiv.org/content/biorxiv/early/2023/09/29/2023.09.27.559788.full.pdf)\n\n[372\\. Learning to Predict Usage Options of Product Reviews with LLM-Generated Labels](https://openreview.net/pdf?id=bgpcQXotjA)\n\n[379\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[380\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[381\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[382\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[383\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[384\\. BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedicine](https://arxiv.org/pdf/2505.22240)\n\n[385\\. Integrating retrieval-augmented generation for enhanced personalized physician recommendations in web-based medical services: model development study](https://www.frontiersin.org/journals/public-health/articles/10.3389/fpubh.2025.1501408/pdf)\n\n[386\\. Enhancing Medical Text Evaluation with GPT-4](https://arxiv.org/pdf/2311.09581v1)\n\n[387\\. Fine-tuned large language models for answering questions about full-text biomedical research studies](https://www.medrxiv.org/content/10.1101/2024.10.28.24316263v2.full.pdf)\n\n[388\\. GPT-4在查询科学出版物中的表现：指令表的可重复性、准确性和影响](https://pmc.ncbi.nlm.nih.gov/articles/PMC11197181/)\n\n[389\\. GPT-4 Performance on Querying Scientific Publications: Reproducibility, Accuracy, and Impact of an Instruction Sheet](https://hivdb.stanford.edu/_wrapper/pages/pdf/Tao.2024.1.pdf)\n\n[390\\. LabQAR: A Manually Curated Dataset for Question Answering on Laboratory Test Reference Ranges and Interpretation](https://www.medrxiv.org/content/10.1101/2025.06.03.25328882v1.full.pdf)\n\n[391\\. Toward Optimising a Retrieval Augmented Generation Pipeline using Large Language Model](https://wwwmatthes.in.tum.de/file/1vzgjh1an34u0/Sebis-Public-Website/-/Master-s-Thesis-Gentrit-Fazlija/Master%20Thesis_Gentrit%20Fazlija_signed.pdf)\n\n[392\\. AI-Powered Test Question Generation in Medical Education: The DailyMed Approach](https://www.medrxiv.org/content/medrxiv/early/2024/11/11/2024.11.11.24317087.full.pdf)\n\n[393\\. Single and Multi-Hop Question-Answering Datasets for Reticular Chemistry with GPT-4-Turbo](https://arxiv.org/pdf/2405.02128)\n\n[394\\. GMAI-MMBench](https://openreview.net/attachment?id=FLBYTY4hKT&name=pdf)\n\n[395\\. Appendix](https://proceedings.neurips.cc/paper_files/paper/2023/file/f323d594aa5d2c68154433a131c07959-Supplemental-Datasets_and_Benchmarks.pdf)\n\n[399\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[400\\. Comparative Analysis of GPT-4Vision, GPT-4 and Open Source LLMs in Clinical Diagnostic Accuracy: A Benchmark Against Human Expertise](https://www.medrxiv.org/content/10.1101/2023.11.03.23297957v1.full.pdf)\n\n[401\\. A synthetic data generation framework for scalable and resource-efficient medical AI assistants](https://www.medrxiv.org/content/medrxiv/early/2025/05/18/2025.05.17.25327785.full.pdf)\n\n[402\\. Scientific literature synthesis with retrieval-augmented language models](https://allenai.org/blog/openscilm)\n\n[403\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[404\\. Enhancing Medical AI with Retrieval-Augmented Generation: A Mini Narrative Review](https://pmc.ncbi.nlm.nih.gov/articles/PMC12059965/)\n\n[405\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[406\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[407\\. Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation](https://www.nematilab.info/bmijc/assets/053124_paper.pdf)\n\n[408\\. Large Language Models in the Clinic: A Comprehensive Benchmark](https://www.medrxiv.org/content/10.1101/2024.04.24.24306315v2.full.pdf)\n\n[409\\. BEYOND SINGLE-TURN: A SURVEY ON MULTI-TURN INTERACTIONS WITH LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2504.04717)\n\n[410\\. Jinhyuk Lee, Wonjin Yoon et al. “BioBERT: a pre-trained biomedical language representation model for biomedical text mining.” Bioinformatics](https://doi.org/10.1093/bioinformatics/btz682)\n\n[411\\. Hybrid Knowledge Architectures for Question Answering](https://lti.cmu.edu/people/alumni/alumni-thesis/ma-kaixin-thesis.pdf)\n\n[412\\. Medical LLMs: Fine-Tuning vs. Retrieval-Augmented Generation](https://www.mdpi.com/2306-5354/12/7/687)\n\n[413\\. Enhancing Medical Text Evaluation with GPT-4](https://arxiv.org/pdf/2311.09581v1)\n\n[414\\. EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries](https://proceedings.neurips.cc/paper_files/paper/2024/file/e15c4afff22f12c4986c1fcb4e941e03-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[415\\. LONGCITE: ENABLING LLMs TO GENERATE FINE- GRAINED CITATIONS IN LONG-CONTEXT QA](https://arxiv.org/pdf/2409.02897)\n\n[416\\. Improving TCM Question Answering through Tree-Organized Self-Reflective Retrieval with LLMs](https://arxiv.org/pdf/2502.09156)\n\n[417\\. Accelerating Clinical Evidence Synthesis with Large Language Models](https://hub.baai.ac.cn/paper/bdfd194c-fc65-4e13-8116-bd23def417d9)"
  },
  {
    "id": 2,
    "question": "What are the potential directions and opportunities for improving the inference capabilities of large models in the presence of DeepSeek R1? Will RL-based methods become the mainstream approach? Can the reward model combined with tree search for Chain-of-Thought (CoT) fine-tuning be discarded? Given the existence of DeepSeek R1, how much potential remains for further research and improvement in large model reasoning capabilities? Will reinforcement learning (RL)-based methods become the dominant approach? Can post-training for chain-of-thought (CoT) reasoning using reward models and tree search be entirely abandoned?",
    "response": "**Research Report: Advancing Large Model Inference Capabilities in the Era of DeepSeek R1**\n\n**1\\. Introduction and Context**\n\nDeepSeek R1 (released April 2025) represents a paradigm shift in reasoning-focused LLMs, leveraging **large-scale reinforcement learning (RL)** without supervised fine-tuning (SFT) in its \"Zero\" variant \\[1\\]\\[2\\]\\[3\\]. It achieves state-of-the-art (SOTA) results: **97.3% on MATH-500**, **79.8% Pass@1 on AIME 2024**, and **90.8% on MMLU** \\[1\\]\\[2\\]. Its architecture uses **rule-based rewards** (accuracy + format) and **Group Relative Policy Optimization (GRPO)**, bypassing neural reward models and tree search \\[29\\]\\[30\\]\\[34\\]. This raises critical questions:\n\nCan RL dominate future reasoning improvements?\n\nAre reward models + tree search obsolete for Chain-of-Thought (CoT)?\n\nWhat gaps persist post-R1?\n\n**2\\. DeepSeek R1's Reasoning Architecture**\n\n**2.1 Core Innovations**\n\n**RL-Centric Training**: DeepSeek-R1-Zero uses purely RL-driven learning, generating CoT via **outcome-based rewards** (correct final answers + structured formatting). This eliminates SFT and demonstrates RL can incentivize self-verification/long-chain reasoning \\[25\\]\\[26\\]\\[39\\].\n\n**Minimalist Reward Design**: Instead of neural reward models, R1 employs **binary rule checks**:\n\n_Accuracy Reward_: Match to ground-truth answers.\n\n_Format Reward_: Adherence to CoT tags (&lt;thinking&gt;, &lt;answer&gt;) \\[32\\]\\[39\\].\n\n**Rejection of Tree Search**: Tree-search methods (e.g., MCTS) are avoided due to LLMs' vast search spaces. R1 uses simpler **rejection sampling** instead \\[34\\]\\[37\\].\n\n**2.2 Performance Highlights**\n\nR1 excels in:\n\n**Mathematical Reasoning**: Outperforms OpenAI-o1-1217 on MATH/AIME \\[1\\]\\[9\\].\n\n**Coding**: Surpasses humans on Codeforces \\[9\\].\n\n**Knowledge Tasks**: 84.0% on MMLU-Pro, 71.5% on GPQA Diamond \\[2\\].\n\nHowever, weaknesses persist: **low effective reflection rates (30–40%)**, **language mixing**, and **conservatism in high-stakes scenarios** \\[44\\]\\[50\\]\\[104\\].\n\n**3\\. RL vs. Non-RL Reasoning Techniques: Benchmark Evidence**\n\nPost-R1 comparisons reveal nuanced tradeoffs:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n| **Method** | **Mathematical Reasoning** | **Knowledge Tasks (MMLU)** | **Data Efficiency** |\n| **RL (e.g., R1)** | 83.9–97.3% (MATH-500) | Slight degradation | Low (self-generates data) |\n| **SFT** | 70–88% (with 200 samples) | Pronounced gains | High (requires labels) |\n| **Distillation** | GPT-4o-level performance | Competitive | Moderate |\n\n_(Sources: \\[61\\]_\\[66\\]\\[69\\]\n\n**RL Advantages**: Excels in **exploratory reasoning** (e.g., math/coding) where diversity of solution paths matters \\[134\\].\n\n**SFT Strengths**: Better for **knowledge-intensive tasks** (MMLU) but prone to hallucination \\[123\\].\n\n**Hybrid Approaches**: Models like Fin-R1 blend RL+SFT, achieving near-R1 performance in finance \\[124\\].\n\n**Verdict**: RL dominates mathematical/logical tasks but isn't universally superior. Domain-specific needs will dictate adoption.\n\n**4\\. Redundancy of Reward Models and Tree Search in CoT**\n\n**4.1 Neural Reward Models: Declining Necessity**\n\nDeepSeek R1 empirically proves neural reward models are **avoidable**:\n\n**Rule-Based Rewards Suffice**: Binary accuracy/format checks drive high-quality CoT \\[39\\].\n\n**Distilled Models Excel**: OpenR1-Math-7B (non-RL) achieves 90%+ GSM8K accuracy, rivaling RL-tuned models \\[94\\].\n\n**Cost Efficiency**: Training neural reward models requires significant compute/data, while rule-based systems are lightweight \\[29\\].\n\n**4.2 Tree Search: Obsolete for General Reasoning**\n\n**Inefficiency**: Tree search (e.g., MCTS) scales poorly for open-domain LLM reasoning \\[34\\].\n\n**R1’s Alternative**: Rejection sampling + GRPO outperforms MCTS in MATH benchmarks \\[37\\].\n\n**Industry Shift**: GPT-4 Turbo/Claude 3.5 prioritize **latency-optimized CoT** over search-heavy methods \\[147\\]\\[268\\].\n\n**Conclusion**: For general-purpose CoT, both components are **largely redundant**. Rule-based rewards + RL suffice.\n\n**5\\. Persistent Reasoning Gaps and Research Opportunities**\n\n**5.1 Quantitative Shortfalls vs. Humans**\n\n**Medical Diagnosis**:\n\nLLMs: **87–93% accuracy** vs. physicians’ **91–96%** \\[222\\]\\[222\\].\n\nGaps: Anchoring bias, knowledge gaps, premature conclusions \\[107\\]\\[104\\].\n\n**Legal Reasoning**:\n\nClaude 3.5: **82% win rate** in law tasks but struggles with multi-defendant cases (<80% accuracy) \\[52\\]\\[315\\].\n\n**Mathematical Reflection**: Only **30–40% of R1’s self-corrections** are valid \\[44\\].\n\n**5.2 High-Potential Research Vectors**\n\n**Refinement Architectures**:\n\n**Self-Verification Modules**: Improve reflection validity (current SOTA: 40%).\n\n**Uncertainty-Routed CoT**: Adopted in Gemini Ultra 2.0 to reduce \"unfaithful reasoning\" \\[208\\].\n\n**Domain-Specialized RL**:\n\nMedical: Integrate **sample-consistency metrics** for diagnosis confidence \\[349\\].\n\nLegal: **Marker-based reasoning** (e.g., \\[ISSUE\\]/\\[PRECEDENT\\]) to handle edge cases \\[105\\].\n\n**Efficiency Innovations**:\n\n**Distillation-Centric Pipelines**: Models like DeepSeek-R1-Distill show small models can rival R1 with 10× fewer parameters \\[6\\].\n\n**Sparse RL**: Reduce compute costs while preserving exploration.\n\n**6\\. Conclusion: Future Trajectories**\n\n1.  **RL as Mainstream for Reasoning**: RL is dominant for **math/coding/logic tasks** but will coexist with SFT/distillation in knowledge domains. Hybrid approaches (e.g., RFT + SFT) offer optimal flexibility \\[131\\]\\[387\\].\n2.  **Reward Models + Tree Search: Discardable**: Rule-based rewards and rejection sampling render neural reward models/tree search obsolete for general CoT.\n3.  **Substantial Remaining Potential**: Key gaps in **medical diagnosis**, **legal judgment**, and **reflective reasoning** present rich opportunities. Innovations should prioritize:\n\n- Domain-specialized RL,\n- Uncertainty-aware CoT,\n- Distillation efficiency.\n\nDeepSeek R1 revolutionized reasoning-centric training, but the era of \"reasoning-optimized LLMs\" is just beginning. RL is here to stay—but its form will evolve toward specialization and efficiency.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://www.thewirechina.com/wp-content/uploads/2025/01/DeepSeek-R1-Document.pdf)\n\n[2\\. DeepSeek-R1：通过强化学习激励大语言模型的推理能力](https://www.dboop.com/img/deepseek/%E5%B0%9A%E7%A1%85%E8%B0%B7%E6%95%99%E8%82%B2%EF%BC%9Adeepseek-r1%E8%AE%BA%E6%96%87-%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91%E7%89%88.pdf)\n\n[3\\. DeepSeek-R1：通过以下方式激励 LLM 中的推理能力 强化学习](https://upmyself.top/upload/Deepseek_R1%EF%BC%88%E4%B8%AD%E6%96%87%E7%89%88%EF%BC%89.pdf)\n\n[4\\. SEARCH-R1: Reinforcement Learning for Search-Augmented LLMs](https://ajithp.com/2025/03/18/search-r1-reinforcement-learning-llm/)\n\n[5\\. Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering](https://arxiv.org/pdf/2503.11197)\n\n[6\\. DeepSeek-R1：通过强化学习激励LLMs中的推理能力](https://www.52nlp.cn/wp-content/uploads/2025/01/DeepSeek-R1-%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E4%B8%AD%E6%96%87%E7%89%88-%E7%94%B1deepseek%E7%BF%BB%E8%AF%91.pdf)\n\n[7\\. VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model](https://arxiv.org/pdf/2504.07615v2)\n\n[8\\. DeepSeek-R1: A New Reasoning Model](https://bytejournal.blog/assets/reports/DeepSeek%20R1%20Model%20Analysis.pdf)\n\n[9\\. DeepSeek 核心十问十答](https://www.faxianai.com/wp-content/uploads/2025/05/1747041355-3%E3%80%81%E8%AE%A1%E7%AE%97%E6%9C%BA-DeepSeek%E6%A0%B8%E5%BF%83%E5%8D%81%E9%97%AE%E5%8D%81%E7%AD%94.pdf)\n\n[10\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[11\\. Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning](https://arxiv.org/html/2502.14768v1)\n\n[12\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[13\\. DeepSeek R1 0528 (May '25) vs DeepSeek R1 (Jan '25): Model Comparison](https://artificialanalysis.ai/models/comparisons/deepseek-r1-vs-deepseek-r1-0120)\n\n[14\\. Best 22 Large Language Models (LLMs) (February 2025)](https://explodingtopics.com/blog/list-of-llms)\n\n[15\\. Understanding DeepSeek R1—A Reinforcement Learning-Driven Reasoning Model](https://kili-technology.com/large-language-models-llms/understanding-deepseek-r1)\n\n[16\\. DeepSeek-R1: How Reinforcement Learning Unleashes Reasoning in Large Language Models](https://myedgetech.com/deepseek-r1-tr/)\n\n[17\\. How DeepSeek-R1 and Kimi k1.5 Use Reinforcement Learning to Improve Reasoning](https://www.deeplearning.ai/the-batch/how-deepseek-r1-and-kimi-k1-5-use-reinforcement-learning-to-improve-reasoning/)\n\n[18\\. Summary of DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://www.eventum.ai/resources/blog/summary-of-deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning)\n\n[19\\. Reinforcement Learning: A Non-Technical Primer on o1 and DeepSeek-R1](https://forum.effectivealtruism.org/posts/PPuojCCajtCWhJR4w/reinforcement-learning-a-non-technical-primer-on-o1-and)\n\n[21\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[22\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[23\\. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://www.pauljorion.com/blog/wp-content/uploads/DeepSeek_R1.pdf)\n\n[24\\. DeepSeek在海内外彻底爆发，但它不需要被神话](https://finance.sina.cn/2025-01-27/detail-inehkyfc1215156.d.html)\n\n[25\\. Evaluating DeepSeek AI vs. Top Competitors in 2025](https://futureagi.com/blogs/evaluating-deepseek-ai-vs-top-competitors#:~:text=adaptive%20problem-solving.-,DeepSeek%20R1%20vs.,O3,%20Anthropic's%20Claude%203.5%20Sonnet))\n\n[26\\. Demystifying Long Chain-of-Thought Reasoning in LLMs](https://openreview.net/pdf?id=6A861u4Crm)\n\n[27\\. Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL](https://arxiv.org/pdf/2505.10832)\n\n[28\\. Exploiting DeepSeek-R1: Breaking Down Chain of Thought Security](https://ap-verlag.de/clickandbuilds/WordPress/MyCMS4/wp-content/uploads/2025/03/TrendMicro_Exploitinhain-of-Thought.pdf)\n\n[29\\. Thinking Preference Optimization](https://arxiv.org/pdf/2502.13173)\n\n[30\\. Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering](https://arxiv.org/pdf/2503.11197)\n\n[31\\. DeepSeek R1: Open-Source AI Reasoning Model That Beats OpenAI’s o1](https://techwiser.com/deepseek-r1-open-source-ai-reasoning-model-that-beats-openais-o1/)\n\n[32\\. DeepSeek Technical Report](https://nairl.kr/wp-content/uploads/2025/02/deepseek_r1_techreport.pdf)\n\n[33\\. DeepSeek's Breakthrough is a Win for Innovation and Accessibility](https://www.ibm.com/sa-ar/think/insights/deepseek-breakthrough-is-a-win-for-innovation-and-accessibility)\n\n[34\\. 对DeepSeek事件的复盘和展望](https://cloud.tencent.com/developer/article/2492805)\n\n[35\\. DeepSeek R1: A New Contender in the AI Arena](https://blog.segmind.com/deepseek-r1-a-new-contender-in-the-ai-arena/)\n\n[36\\. 基金量化观察](https://pdf.dfcfw.com/pdf/H301_AP202502061642832904_1.pdf)\n\n[37\\. DeepSeek 模型综述](https://pdf.dfcfw.com/pdf/H3_AP202502141643086746_1.pdf?1739553070000.pdf)\n\n[38\\. DeepSeek: Why It's Important — in Layman's Terms](https://www.aura.co/news-and-insights/deepseek-why-its-important-in-laymans-terms)\n\n[39\\. Mathematical Constraints of RL-Induced Reasoning: A Rebuttal to DeepSeek-R1](https://openreview.net/pdf/744a5d89ad56e6811cdf11644456604c704830c4.pdf)\n\n[40\\. DeepSeek-R1: A New Reasoning Model](https://bytejournal.blog/assets/reports/DeepSeek%20R1%20Model%20Analysis.pdf)\n\n[41\\. 大语言模型的推理能力研究](https://etdlib.bnu.edu.cn/docinfo.action?id1=e7b7dab40eb183d191442db9cf7e6cba&id2=mfqtEzj03Nc%3D)\n\n[42\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[43\\. Evaluating Large Reasoning Model Performance on Complex Medical Scenarios In The MMLU-Pro Benchmark](https://www.medrxiv.org/content/medrxiv/early/2025/04/19/2025.04.07.25325385.full.pdf)\n\n[44\\. Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?](https://papers-pdfs.assets.alphaxiv.org/2502.19361v2.pdf)\n\n[45\\. Notes on the new Deepseek r1](https://composio.dev/blog/notes-on-the-new-deepseek-r1/)\n\n[46\\. Line Goes Up? Inherent Limitations of Benchmarks for Evaluating Large Language Models](https://arxiv.org/html/2502.14318v1)\n\n[47\\. 大型语言模型不具备类人工作记忆](https://mp.weixin.qq.com/s?__biz=MzU2NjQxMzI5OQ%3D%3D&mid=2247707296&idx=8&sn=9eb642311f839ea4343a82f3c88d442e&chksm=fd4e094ce74b2dd453e341189d2c5a946046e355d47738d9e329af5c2d4ed67eaccfeacb6ee9&scene=27)\n\n[48\\. DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in Bilingual Complex Ophthalmology Reasoning](https://arxiv.org/pdf/2502.17947)\n\n[49\\. Spoken Language Intelligence of Large Language Models for Language Learning](https://arxiv.org/pdf/2308.14536)\n\n[50\\. Bridging Technology and Humanities: Evaluating the Impact of Large Language Models on Social Sciences Research with DeepSeek-R1](https://arxiv.org/pdf/2503.16304)\n\n[51\\. Exploring LLMs Reasoning Capability with DeepSeek-R1](https://adasci.org/mastering-llms-reasoning-capability-with-deepseek-r1/)\n\n[52\\. 评估法律推理中测试时间扩展大型语言模型：OpenAI o1、DeepSeek-R1及其他](https://www.xueshuxiangzi.com/downloads/2025_3_21/2503.16040.pdf)\n\n[53\\. Large Language Models Think Too Fast To Explore Effectively](http://arxiv.org/html/2501.18009v1)\n\n[54\\. Rishi Hazra, Gabriele Venturato et al. “Have Large Language Models Learned to Reason? A Characterization via 3-SAT Phase Transition.”](https://arxiv.org/abs/2504.03930)\n\n[55\\. Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned LLMs](https://arxiv.org/html/2503.20749v1)\n\n[56\\. Evaluating the Logical Reasoning Abilities of Large Reasoning Models](https://arxiv.org/pdf/2505.11854)\n\n[57\\. Comparative Analysis Based on DeepSeek, ChatGPT, and Google Gemini: Features, Techniques, Performance, Future Prospects](https://fetcher.alphaxiv.org/v2/pdf/2503.04783v1)\n\n[58\\. Unlocking the Mysteries of OpenAI o1: A Survey of the Reasoning Abilities of Large Language Models](https://openreview.net/pdf/1bf54a44ba5686f6fb3e22b7989c1a68b1d7f485.pdf)\n\n[61\\. Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](http://paperreading.club/page?id=323657)\n\n[62\\. RL Fine-Tuning of Language Model for Instruction Following and Math Reasoning](https://cs224r.stanford.edu/projects/pdfs/CS224R_Default_Final_Project.pdf)\n\n[63\\. Reinforcement Learning Finetunes Small Subnetworks in Large Language Models](https://arxiv.org/pdf/2505.11711)\n\n[64\\. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://blog.moontak.com/wp-content/uploads/2025/02/2025021013465966.pdf)\n\n[65\\. Benchmarking Large Language Models via Random Variables](https://arxiv.org/pdf/2501.11790)\n\n[66\\. Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling](https://arxiv.org/pdf/2501.11651)\n\n[67\\. Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem](https://arxiv.org/pdf/2506.03295)\n\n[68\\. Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning](http://qizhang.info/paper/9461_Training_Large_Language_M.pdf)\n\n[69\\. Supervised Fine Tuning Vs Reinforcement Learning](https://www.restack.io/p/fine-tuning-answer-supervised-vs-reinforcement-cat-ai)\n\n[70\\. Teaching Large Language Models to Reason with Reinforcement Learning](https://openreview.net/pdf?id=mjqoceuMnI)\n\n[71\\. VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning](https://arxiv.org/pdf/2504.08837)\n\n[72\\. Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://www.arxiv.org/pdf/2505.14216)\n\n[73\\. Reinforcement Learning Teachers of Test Time Scaling](https://sakana.ai/rlt/)\n\n[74\\. Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate](https://arxiv.org/html/2501.17703v3)\n\n[75\\. Unlock the Correlation between Supervised Fine-Tuning and Reinforcement Learning in Training Code Large Language Models](http://arxiv.org/html/2406.10305v2)\n\n[76\\. 100 DAYS AFTER DEEPSEEK-R1: A SURVEY ON REPLICATION STUDIES AND MORE DIRECTIONS FOR REASONING LANGUAGE MODELS](https://arxiv.org/pdf/2505.00551)\n\n[77\\. Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering](https://arxiv.org/pdf/2503.11197)\n\n[81\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[82\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[83\\. Relation-R1: Progressively Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relation Comprehension](https://www.bing.com/ck/a?!&p=4368e229cc65b24ce8ed7894b9372d6fcf672066414f24fc02b4ddf768d7d491JmltdHM9MTc0Nzk1ODQwMA&ptn=3&ver=2&hsh=4&fclid=07d0fc14-e2b4-6d99-2425-e9e1e3486c3a&u=a1aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzI1MDQuMTQ2NDI)\n\n[84\\. VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning](https://fetcher.alphaxiv.org/v2/pdf/2504.06958v3)\n\n[85\\. LLM Reasoning: from OpenAI O1 to DeepSeek R1](https://hal.science/hal-05058659v1/document)\n\n[86\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[87\\. Demystifying Long Chain-of-Thought Reasoning in LLMs](http://arxiv.org/pdf/2502.03373)\n\n[88\\. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://blog.moontak.com/wp-content/uploads/2025/02/2025021013465966.pdf)\n\n[89\\. Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering](https://arxiv.org/pdf/2503.11197)\n\n[90\\. Aakanksha Chowdhery, Sharan Narang et al. “PaLM: Scaling Language Modeling with Pathways.” J. Mach. Learn. Res.](https://arxiv.org/abs/2204.02311)\n\n[91\\. Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/pdf/2505.10446)\n\n[92\\. Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning](https://www.preprints.org/frontend/manuscript/dfe95b8efb4106b13a884cbbcb7738a4/download_pub)\n\n[93\\. China’s cheap, open AI model DeepSeek thrills scientists](https://www.360doc.cn/article/8102575_1145512111.html)\n\n[94\\. Let LLMs Break Free from Overthinking via Self-Braking Tuning](https://arxiv.org/pdf/2505.14604)\n\n[95\\. DeepSeek-R1: A Peek Under the Hood](https://zohaib.me/deepseek-r1-peek-under-the-hood/)\n\n[96\\. Exploring LLMs Reasoning Capability with DeepSeek-R1](https://adasci.org/mastering-llms-reasoning-capability-with-deepseek-r1/)\n\n[97\\. A Visual Guide to Reasoning LLMs](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms?utm_source=share&utm_medium=android&r=n9qvy&triedRedirect=true)\n\n[98\\. The Ultimate Guide to LLM Reasoning (2025)](https://kili-technology.com/large-language-models-llms/llm-reasoning-guide)\n\n[99\\. GitHub - ijunaidkh/DeepSeek-R1: DeepSeek-R1](https://github.com/ijunaidkh/DeepSeek-R1)\n\n[101\\. DeepSeek-R1 vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?](https://arxiv.org/pdf/2504.08120v3)\n\n[102\\. 探索DeepSeek:如何评估DeepSeek-R1大模型的推理能力？](https://www.chatairc.com/65353/)\n\n[103\\. Evaluating the Logical Reasoning Abilities of Large Reasoning Models](https://arxiv.org/pdf/2505.11854)\n\n[104\\. DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in Bilingual Complex Ophthalmology Reasoning](https://arxiv.org/pdf/2502.17947)\n\n[105\\. DeepSeek R1 & R1-Zero: A New Milestone in Language Model Reasoning & Safe AI Adoption](https://mirrorsecurity.io/blog/deepseek-r1-r1-zero-a-new-milestone-in-language-model-reasoning-safe-ai-adoption)\n\n[106\\. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://www.pauljorion.com/blog/wp-content/uploads/DeepSeek_R1.pdf)\n\n[107\\. Medical Reasoning in LLMs: An In-Depth Analysis of DeepSeek R1](https://arxiv.org/abs/2504.00016)\n\n[108\\. Evaluating Large Reasoning Model Performance on Complex Medical Scenarios In The MMLU-Pro Benchmark](https://www.medrxiv.org/content/10.1101/2025.04.07.25325385v2.full.pdf)\n\n[109\\. SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines](https://arxiv.org/pdf/2502.14739v1)\n\n[110\\. DeepSeek's DeepSeek-R1 - AI Model Details](https://docsbot.ai/models/deepseek-r1)\n\n[111\\. Bridging Technology and Humanities: Evaluating the Impact of Large Language Models on Social Sciences Research with DeepSeek-R1](https://arxiv.org/pdf/2503.16304)\n\n[112\\. Fine-Grained Reasoning Evaluation: How Well Does DeepSeek-R1 Handle Causal Inference?](https://tijer.org/tijer/papers/TIJER2505178.pdf)\n\n[113\\. Evaluating a Large Reasoning Model’s Performance on Open-Ended Medical Scenarios](https://www.medrxiv.org/content/10.1101/2025.04.29.25326666v1.full.pdf)\n\n[114\\. DeepSeek-R1 Thoughtology: Let's think about LLM reasoning](https://arxiv.org/pdf/2504.07128)\n\n[115\\. DeepSeek R1 VS V3 Comparison](https://ai.easeus.com/ai-pdf-solutions/deepseek-r1-vs-v3.html)\n\n[116\\. Exploring LLMs Reasoning Capability with DeepSeek-R1](https://adasci.org/mastering-llms-reasoning-capability-with-deepseek-r1/)\n\n[117\\. DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models](https://paperreading.club/page?id=312310)\n\n[118\\. Evaluating DeepSeek AI vs. Top Competitors in 2025](https://futureagi.com/blogs/evaluating-deepseek-ai-vs-top-competitors#:~:text=adaptive%20problem-solving.-,DeepSeek%20R1%20vs.,O3,%20Anthropic's%20Claude%203.5%20Sonnet))\n\n[121\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[122\\. Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning](https://openreview.net/pdf/2c930954311f3c3a380cdf622020946f32e86440.pdf)\n\n[123\\. Wayne Xin Zhao, Kun Zhou et al. “A Survey of Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2303.18223)\n\n[124\\. Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning](https://paperswithcode.com/paper/fin-r1-a-large-language-model-for-financial)\n\n[125\\. Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering](https://arxiv.org/pdf/2503.11197)\n\n[126\\. Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning](http://qizhang.info/paper/9461_Training_Large_Language_M.pdf)\n\n[127\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[128\\. Unlock the Correlation between Supervised Fine-Tuning and Reinforcement Learning in Training Code Large Language Models](http://arxiv.org/html/2406.10305v2)\n\n[129\\. Reinforcement Learning Finetunes Small Subnetworks in Large Language Models](https://arxiv.org/pdf/2505.11711)\n\n[130\\. Reinforcement Learning is all you need](https://arxiv.org/html/2503.09512v1)\n\n[131\\. Supervised Fine Tuning Vs Reinforcement Learning](https://www.restack.io/p/fine-tuning-answer-supervised-vs-reinforcement-cat-ai)\n\n[132\\. Reasoning with Large Language Models, a Survey](https://liacs.leidenuniv.nl/~plaata1/papers/Reasoning_Survey.pdf)\n\n[133\\. LawBench: Benchmarking Legal Knowledge of Large Language Models](https://personal.utdallas.edu/~vince/papers/emnlp24-law.pdf)\n\n[134\\. Benchmarking LLM Reasoning: Comprehensive Multi-dimensional Evaluation of 8 Leading Models](https://kili-technology.com/large-language-models-llms/benchmarking-llm-reasoning-comprehensive-multi-dimensional-evaluation-of-8-leading-models)\n\n[135\\. Teaching Large Language Models to Reason with Reinforcement Learning](https://openreview.net/pdf?id=mjqoceuMnI)\n\n[136\\. Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models](https://fetcher.alphaxiv.org/v2/pdf/2503.09567v1)\n\n[137\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[138\\. Large language models: reasoning and reinforcement learning](https://www.risk.net/artificial-intelligence-in-finance-volume-2-reinforcement-learning-theory-and-practice/7961642/large-language-models-reasoning-and-reinforcement-learning)\n\n[141\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[142\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[143\\. HAWKEYE: Efficient Reasoning with Model Collaboration](https://arxiv.org/pdf/2504.00424)\n\n[144\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[145\\. Takeshi Kojima, S. Gu et al. “Large Language Models are Zero-Shot Reasoners.” ArXiv](https://arxiv.org/abs/2205.11916)\n\n[146\\. Karl Cobbe, V. Kosaraju et al. “Training Verifiers to Solve Math Word Problems.” ArXiv](https://arxiv.org/abs/2110.14168)\n\n[147\\. Chain of Draft: Thinking Faster by Writing Less](https://arxiv.org/pdf/2502.18600)\n\n[148\\. Fine-Tuning Vision-Language Model for Automated Engineering Drawing Information Extraction](https://arxiv.org/pdf/2411.03707)\n\n[149\\. RewardAnything: Generalizable Principle-Following Reward Models](https://fetcher.alphaxiv.org/v2/pdf/2506.03637v1)\n\n[150\\. GPT-3.5, GPT-4, Bard, and Claude’s Performance on the Chinese Reading Comprehension Test](https://ceur-ws.org/Vol-3667/GenAILA-paper6.pdf)\n\n[151\\. Humanity’s Last Exam](https://arxiv.org/pdf/2501.14249)\n\n[152\\. Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting](https://papers.neurips.cc/paper_files/paper/2023/file/ed3fea9033a80fea1376299fa7863f4a-Paper-Conference.pdf)\n\n[153\\. Proof Flow: Preliminary Study on Generative Flow Network Language Model Tuning for Formal Reasoning](https://openreview.net/pdf?id=gYzgA0EnRs)\n\n[154\\. IBM Granite 3.3: Pengenalan_ucapan, penalaran yang disempurnakan, dan LoRA RAG](https://www.ibm.com/id-id/new/announcements/ibm-granite-3-3-speech-recognition-refined-reasoning-rag-loras)\n\n[155\\. The Fine-Tuning Effect: A Study on Instruction Tuning for Code Generation](https://scholar.uwindsor.ca/cgi/viewcontent.cgi?article=10668&context=etd)\n\n[156\\. Why and How Robots Should Say ‘No’](https://par.nsf.gov/servlets/purl/10229828)\n\n[157\\. KIMI k1.5: SCALING REINFORCEMENT LEARNING WITH LLMs](https://arxiv.org/pdf/2501.12599)\n\n[158\\. An Empirical Study on Information Extraction using Large Language Models](https://arxiv.org/pdf/2305.14450)\n\n[159\\. Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.03318)\n\n[160\\. DeepSeek-V3：无动作推理的策略泛化架构](https://sccm.i-newcar.com/index.php?m=home&c=View&a=index&aid=3728)\n\n[161\\. Large Language Models in Healthcare: A Comprehensive Benchmark](https://www.medrxiv.org/content/10.1101/2024.04.24.24306315v1.full.pdf)\n\n[162\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[163\\. Multimodal Large Language Models and Their Applications in Healthcare](https://aclanthology.org/people/z/zhihong-chen/)\n\n[164\\. Assessing the Accuracy of Diagnostic Capabilities of Large Language Models](https://www.mdpi.com/2075-4418/15/13/1657)\n\n[165\\. M-QALM: A Benchmark to Assess Clinical Reading Comprehension and Knowledge Recall in Large Language Models via Question Answering](https://www.promptlayer.com/research-papers/m-qalm-a-benchmark-to-assess-clinical-reading-comprehension-and-knowledge-recall-in-large-language-models-via-question-answering)\n\n[166\\. DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/pdf/2505.14107)\n\n[167\\. Evaluating Large Reasoning Model Performance on Complex Medical Scenarios In The MMLU-Pro Benchmark](https://www.medrxiv.org/content/10.1101/2025.04.07.25325385v2.full.pdf)\n\n[168\\. Large Language Models and Medical Knowledge Grounding for Diagnosis Prediction](https://www.medrxiv.org/content/10.1101/2023.11.24.23298641v2.full.pdf)\n\n[169\\. Limitations of Large Language Models in Clinical Problem-Solving Arising from Inflexible Reasoning](https://arxiv.org/html/2502.04381v1)\n\n[170\\. Disentangling Reasoning and Knowledge in Medical Large Language Models](https://arxiv.org/pdf/2505.11462)\n\n[171\\. TurnBench-MS: A Benchmark for Evaluating Multi-Turn, Multi-Step Reasoning in Large Language Models](https://arxiv.org/pdf/2506.01341)\n\n[172\\. Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial](https://pubmed.ncbi.nlm.nih.gov/39466245/)\n\n[173\\. Large Language Models in the Clinic: A Comprehensive Benchmark](https://www.medrxiv.org/content/medrxiv/early/2024/06/25/2024.04.24.24306315.full.pdf)\n\n[174\\. Large Language Model Uncertainty Measurement and Calibration for Medical Diagnosis and Treatment](https://www.medrxiv.org/content/10.1101/2024.06.06.24308399v1)\n\n[175\\. A Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and Evaluations](https://arxiv.org/pdf/2406.10303)\n\n[176\\. Large language models encode clinical knowledge](https://www.nature.com/articles/s41586-023-06291-2.pdf)\n\n[177\\. Large Language Models缺乏可靠的元认知能力进行医学推理](https://www.nature.com/articles/s41467-024-55628-6)\n\n[178\\. Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models](https://arxiv.org/html/2411.07611v2)\n\n[181\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[182\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[183\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[184\\. Reinforcement Learning Enhanced LLMs: A Survey](https://papers-pdfs.assets.alphaxiv.org/2412.10400v3.pdf)\n\n[185\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[186\\. Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering](https://arxiv.org/pdf/2503.11197)\n\n[187\\. LOGICVISTA: MULTIMODAL LLM LOGICAL REASONING BENCHMARK IN VISUAL CONTEXTS](https://openreview.net/pdf?id=6ozaf7VRIP)\n\n[188\\. Claude 3.5 Sonnet vs GPT-4o 性能对比分析](https://www.vellum.ai/blog/claude-3-5-sonnet-vs-gpt4o#:~:text=Accuracy%20Comparison:%20Claude%203.5%20Sonnet,when%20it%20comes%20to%20accuracy.)\n\n[189\\. Takeshi Kojima, S. Gu et al. “Large Language Models are Zero-Shot Reasoners.” ArXiv](https://arxiv.org/abs/2205.11916)\n\n[190\\. Benchmarking LLM Reasoning: Comprehensive Multi-dimensional Evaluation of 8 Leading Models](https://kili-technology.com/large-language-models-llms/benchmarking-llm-reasoning-comprehensive-multi-dimensional-evaluation-of-8-leading-models)\n\n[191\\. Claude 3.5 vs GPT-4.5: A Deep Dive into 2025’s Top AI Models](https://hereandnowai.com/claude-3-5-vs-gpt-4-5-ai-comparison/)\n\n[192\\. FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning](https://openreview.net/pdf?id=0Ef3sErvsu)\n\n[193\\. Is It Professional or Exploratory? Classifying Repositories Through README Analysis](https://www.scitepress.org/Papers/2025/132725/132725.pdf)\n\n[194\\. FLOW OF REASONING: TRAINING LLMs FOR DIVERGENT PROBLEM SOLVING WITH MINIMAL EXAMPLES](https://arxiv.org/pdf/2406.05673)\n\n[195\\. Neel Guha, Julian Nyarko et al. “LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2308.11462)\n\n[196\\. Claude 3.5 与 GPT-5 时间预期](https://xueqiu.com/7734397594/295009226)\n\n[197\\. Claude 3.5 Sonnet vs GPT-4o: A Comprehensive Performance Comparison](https://www.vellum.ai/blog/claude-3-5-sonnet-vs-gpt4o)\n\n[198\\. 通过强化学习和推理规模化推进语言模型推理](https://training2.i-newcar.com/index.php?m=home&c=View&a=index&aid=3728)\n\n[199\\. MultiMath: Bridging Visual and Mathematical Reasoning for Large Language Models](https://arxiv.org/pdf/2409.00147)\n\n[200\\. Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?](https://ceur-ws.org/Vol-3559/paper-3.pdf)\n\n[201\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[202\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[203\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[204\\. Takeshi Kojima, S. Gu et al. “Large Language Models are Zero-Shot Reasoners.” ArXiv](https://arxiv.org/abs/2205.11916)\n\n[205\\. Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/pdf/2312.11805)\n\n[206\\. Nisan Stiennon, Long Ouyang et al. “Learning to summarize from human feedback.” ArXiv](https://arxiv.org/abs/2009.01325)\n\n[207\\. RewardAnything: Generalizable Principle-Following Reward Models](https://fetcher.alphaxiv.org/v2/pdf/2506.03637v1)\n\n[208\\. CHAIN-OF-THOUGHT REASONING IN THE WILD IS NOT FAITHFUL](https://openreview.net/pdf?id=L8094Whth0)\n\n[209\\. Gemma: Open Models Based on Gemini Research and Technology](https://arxiv.org/pdf/2403.08295v1.pdf)\n\n[210\\. RM-R1: Reward Modeling as Reasoning](https://arxiv.org/pdf/2505.02387)\n\n[211\\. An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought](https://arxiv.org/pdf/2407.15569)\n\n[212\\. Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.03318)\n\n[213\\. Gemini 2.0 Flash Thinking Experimental](https://deepmind.google/technologies/gemini/flash-thinking/)\n\n[214\\. Benchmarking Multimodal Large Language Models for Forensic Science and Medicine: A Comprehensive Dataset and Evaluation Framework](https://www.medrxiv.org/content/10.1101/2025.07.06.25330972v1.full-text)\n\n[215\\. Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models](https://paperswithcode.com/paper/long-short-chain-of-thought-mixture)\n\n[216\\. New Gemini 2.0 Experimental Models Roll Out to Gemini App](https://9to5google.com/2025/02/10/gemini-2-0-experimental-models-app/)\n\n[217\\. Gemini：Google DeepMind 的多模态 AI 语言模型](https://blog.atomicstep.org/2023/12/06/Gemini-The-second-best-AI-model-in-the-world/)\n\n[221\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[222\\. Evaluating the accuracy and reliability of large language models in assisting with pediatric differential diagnoses: A multicenter diagnostic study](https://www.medrxiv.org/content/10.1101/2024.08.09.24311777v1.full.pdf)\n\n[223\\. Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial](https://pubmed.ncbi.nlm.nih.gov/39466245/)\n\n[224\\. GPT-4 assistance for improvement of physician performance on patient care tasks: a randomized controlled trial](https://www.nature.com/articles/s41591-024-03456-y)\n\n[225\\. Frontiers | A comparison of the diagnostic ability of large language models in challenging clinical cases](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1379297/full)\n\n[226\\. Comparing Diagnostic Accuracy of Clinical Professionals and Large Language Models: Systematic Review and Meta-Analysis](https://medinform.jmir.org/2025/1/e64963/PDF)\n\n[227\\. Comparative Analysis of Diagnostic Performance: Differential Diagnosis Lists by LLaMA3 Versus LLaMA2 for Case Reports](https://formative.jmir.org/2024/1/e64844)\n\n[228\\. Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine](https://www.nature.com/articles/s41746-024-01010-1)\n\n[229\\. Superhuman performance of a large language model on the reasoning tasks of a physician](https://www.arxiv.org/pdf/2412.10849)\n\n[230\\. Evaluation of Large Language Models: Review of Metrics, Applications, and Methodologies](https://www.preprints.org/manuscript/202504.0369/v1/download)\n\n[231\\. Large Language Models in Healthcare: A Comprehensive Benchmark](https://www.medrxiv.org/content/10.1101/2024.04.24.24306315v1.full.pdf)\n\n[232\\. Diagnostic performance of Large Language Models (LLMs) compared with physicians in sleep medicine](https://pubmed.ncbi.nlm.nih.gov/40684750/)\n\n[233\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[234\\. Interpretable Differential Diagnosis with Dual-Inference Large Language Models](https://arxiv.org/pdf/2407.07330)\n\n[235\\. ChatGPT在诊断推理挑战中超越学术医生](https://www.pulmccm.org/p/chatgpt-triumphs-over-academic-physicians)\n\n[236\\. Influence of a Large Language Model on Diagnostic Reasoning: A Randomized Clinical Vignette Study](https://www.medrxiv.org/content/10.1101/2024.03.12.24303785v1.full.pdf)\n\n[241\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[242\\. MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance](https://openreview.net/pdf?id=KNkalZnq3f)\n\n[243\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[244\\. Claude 3.7 Sonnet vs Claude 3.5 Sonnet vs Claude 3.7 Sonnet Thinking](https://apidog.com/blog/claude-3-7-3-5-vs-thinking/#:~:text=Claude%203.7%20Sonnet%20%28Normal%20Mode%29:%20More%20tokens%20used%20due,6,500%20tokens%20per%20complex%20task.)\n\n[245\\. Claude 3.5 Sonnet (Oct 2024) vs DeepSeek-R1 - Detailed Performance & Feature Comparison](https://docsbot.ai/models/compare/claude-3-5-sonnet-20241022/deepseek-r1)\n\n[246\\. Claude SWE-Bench Performance](https://www.anthropic.com/engineering/swe-bench-sonnet#:~:text=SWE-bench%20is%20an%20AI,the-art%20model's%2045%25.)\n\n[247\\. Reinforcement Learning Enhanced LLMs: A Survey](https://papers-pdfs.assets.alphaxiv.org/2412.10400v3.pdf)\n\n[248\\. Claude 3.5 Sonnet vs DeepSeek-R1 - Detailed Performance & Feature Comparison](https://docsbot.ai/models/compare/claude-3-5-sonnet/deepseek-r1)\n\n[249\\. Mark Chen, Jerry Tworek et al. “Evaluating Large Language Models Trained on Code.” ArXiv](https://arxiv.org/abs/2107.03374)\n\n[250\\. Claude 2.1 vs Claude 3.5 Sonnet (Oct 2024) - Detailed Performance & Feature Comparison](https://docsbot.ai/models/compare/claude-2-1/claude-3-5-sonnet-20241022)\n\n[251\\. Claude 3.7 Sonnet与3.5 Sonnet及思维模式的全面对比分析](https://blog.moontak.com/id/517954/)\n\n[252\\. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948)\n\n[253\\. Karl Cobbe, V. Kosaraju et al. “Training Verifiers to Solve Math Word Problems.” ArXiv](https://arxiv.org/abs/2110.14168)\n\n[254\\. MMSciBench: Benchmarking Language Models on Multimodal Scientific Problems](https://openreview.net/pdf/ff27b355901d36dec3d43e7a8852f1b8640b8411.pdf)\n\n[255\\. Claude 3.5 Sonnet vs DeepSeek-V3 - Detailed Performance & Feature Comparison](https://docsbot.ai/models/compare/claude-3-5-sonnet/deepseek-v3)\n\n[256\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[257\\. Claude 3.7 Sonnet System Card](https://anthropic.com/claude-3-7-sonnet-system-card)\n\n[258\\. On Claude 3.5 Sonnet — LessWrong](https://www.lesswrong.com/posts/wx4RhFzLbiHoShFjR/on-claude-3-5-sonnet)\n\n[259\\. Claude 3.5 Sonnet 在 SWE-bench Verified 上提升性能](https://cn.blockchain.news/news/claude-3-5-sonnet-elevates-performance-swe-bench-verified)\n\n[260\\. 通过强化学习和推理规模化推进语言模型推理](https://training2.i-newcar.com/index.php?m=home&c=View&a=index&aid=3728)\n\n[261\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[262\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[263\\. Evidence from counterfactual tasks supports emergent analogical reasoning in large language models](https://pmc.ncbi.nlm.nih.gov/articles/PMC12107539/)\n\n[264\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[265\\. 基于GPT4的过程奖励模型，推理能力远超GPT4](https://zhuanlan.zhihu.com/p/634785810)\n\n[266\\. Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.03318)\n\n[267\\. Chain-of-thought prompting elicits reasoning in large language models](https://dl.acm.org/doi/10.5555/3600270.3602070)\n\n[268\\. Common 7B Language Models Already Possess Strong Math Capabilities](https://openreview.net/pdf?id=fL8sds4naU)\n\n[269\\. Takeshi Kojima, S. Gu et al. “Large Language Models are Zero-Shot Reasoners.” ArXiv](https://arxiv.org/abs/2205.11916)\n\n[270\\. RM-R1: Reward Modeling as Reasoning](https://web3.arxiv.org/pdf/2505.02387)\n\n[271\\. Benchmarking Large Language Models for Math Reasoning Tasks](https://arxiv.org/pdf/2408.10839)\n\n[272\\. Karl Cobbe, V. Kosaraju et al. “Training Verifiers to Solve Math Word Problems.” ArXiv](https://arxiv.org/abs/2110.14168)\n\n[273\\. Step-level Value Preference Optimization for Mathematical Reasoning](http://arxiv.org/html/2406.10858v1)\n\n[274\\. LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought](https://www.ijcai.org/proceedings/2024/0381.pdf)\n\n[275\\. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://paperswithcode.com/paper/chain-of-thought-prompting-elicits-reasoning)\n\n[276\\. Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning](https://arxiv.org/pdf/2501.13622)\n\n[277\\. EMERGENT SELF-REFLECTIVE BEHAVIORS IN GPT-4o: Indicators of Self-Awareness in AI Language Models](https://newerasystemsllc.com/papers/Emergent%20Self-Reflective%20Behaviors%20in%20GPT-4o.pdf)\n\n[278\\. GPT-4-powered analysis and prediction of experiments through an effective chain-of-thought prompting strategy: a case study of selective catalytic reduction of NOx with NH3 by metal-oxide composites](https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/65574388dbd7c8b54b729214/original/gpt-4-powered-analysis-and-prediction-of-experiments-through-an-effective-chain-of-thought-prompting-strategy-a-case-study-of-selective-catalytic-reduction-of-n-ox-with-nh3-by-metal-oxide-composites.pdf)\n\n[279\\. Algorithmic reasoning in large language models and neuro-symbolic architectures](https://www.research.unipd.it/bitstream/11577/3551867/2/thesis_reviews_frontespizio_pdfA.pdf)\n\n[280\\. Evaluating the Frontier: A Comparative Analysis of Leading LLMs in Advanced Cognitive Tasks](https://openreview.net/pdf/bcbbcfe4cf23e6667a636d80f72f3c7296884c14.pdf)\n\n[281\\. Diagnostic performance of Large Language Models (LLMs) compared with physicians in sleep medicine](https://pubmed.ncbi.nlm.nih.gov/40684750/)\n\n[282\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[283\\. medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced Clinical Diagnosis on EMRs](https://arxiv.org/pdf/2406.14326)\n\n[284\\. Towards Metacognitive Clinical Reasoning: Benchmarking MD-PIE Against State-of-the-Art LLMs in Medical Decision-Making](https://www.medrxiv.org/content/10.1101/2025.01.28.25321282v1.full.pdf)\n\n[285\\. 2024 Research Report](https://www.mll.com/en/research-report-2024/researchreport2024.pdf)\n\n[286\\. Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial](https://pubmed.ncbi.nlm.nih.gov/39466245/)\n\n[287\\. Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge](https://arxiv.org/pdf/2502.13010)\n\n[288\\. General Practice Research Review](https://rdaa.com.au/common/Uploaded%20files/_Aus/Files%20to%20share/General_Practice_Research_Review_Issue_issue_118.pdf)\n\n[289\\. Comparative Analysis of Diagnostic Performance: Differential Diagnosis Lists by LLaMA3 Versus LLaMA2 for Case Reports](https://formative.jmir.org/2024/1/e64844)\n\n[290\\. Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine](https://www.nature.com/articles/s41746-024-01010-1)\n\n[291\\. AI & Medicine: Hype or Hope?](https://www.montana.edu/wwami/facultyretreat/AI%20in%20Healthcare%20Matthew%20Thompson.pdf)\n\n[292\\. GPT-4 assistance for improvement of physician performance on patient care tasks: a randomized controlled trial](https://www.nature.com/articles/s41591-024-03456-y)\n\n[293\\. Clinical Reasoning of a Generative Artificial Intelligence Model Compared With Physicians](http://csccm.org.cn/?p=25874)\n\n[294\\. Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark](https://www.medrxiv.org/content/10.1101/2024.04.24.24306315v3.full.pdf)\n\n[295\\. LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3 Mini Across Chronic Health Conditions](https://arxiv.org/html/2503.10486v1)\n\n[296\\. Evaluation of Large Language Models: Review of Metrics, Applications, and Methodologies](https://www.preprints.org/manuscript/202504.0369/v1/download)\n\n[297\\. CLINICALLAB: ALIGNING AGENTS FOR MULTI-DEPARTMENTAL CLINICAL DIAGNOSTICS IN THE REAL WORLD](https://openreview.net/pdf/9adf85f9ce0f98cf9eb413b6b2b089b92267e9cf.pdf)\n\n[298\\. Benchmarking Chinese Medical LLMs: A MEDBench-Based Analysis of Performance Gaps and Hierarchical Optimization Strategies](https://arxiv.org/pdf/2503.07306)\n\n[299\\. Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback](https://openreview.net/pdf/ea2d1353f675f570f509ea48c7c8eaedbc5478f8.pdf)\n\n[301\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[302\\. MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance](https://openreview.net/pdf?id=KNkalZnq3f)\n\n[303\\. Claude 3.7 Sonnet vs Claude 3.5 Sonnet vs Claude 3.7 Sonnet Thinking](https://apidog.com/blog/claude-3-7-3-5-vs-thinking/#:~:text=Claude%203.7%20Sonnet%20%28Normal%20Mode%29:%20More%20tokens%20used%20due,6,500%20tokens%20per%20complex%20task.)\n\n[304\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[305\\. MMSciBench: Benchmarking Language Models on Multimodal Scientific Problems](https://openreview.net/pdf/ff27b355901d36dec3d43e7a8852f1b8640b8411.pdf)\n\n[306\\. Claude 3.5 Sonnet (Oct 2024) vs DeepSeek-R1 - Detailed Performance & Feature Comparison](https://docsbot.ai/models/compare/claude-3-5-sonnet-20241022/deepseek-r1)\n\n[307\\. Claude SWE-Bench Performance](https://www.anthropic.com/engineering/swe-bench-sonnet#:~:text=SWE-bench%20is%20an%20AI,the-art%20model's%2045%25.)\n\n[308\\. Comparing AI models using different tasks](https://docs.github.com/en/copilot/using-github-copilot/ai-models/comparing-ai-models-using-different-tasks)\n\n[309\\. Qwen 2.5 vs DeepSeek 2.5, Claude 3.5 Sonnet, and More, Bind AI](https://blog.getbind.co/2024/10/16/qwen-2-5-overview-comparison-with-deepseek-claude-and-more/)\n\n[310\\. Claude 3.5 Sonnet (new) vs Claude 3 Sonnet - Detailed Performance & Feature Comparison](https://docsbot.ai/models/compare/claude-3-5-sonnet/claude-3-sonnet)\n\n[311\\. Claude 3.5 Sonnet | Labelbox](https://labelbox.com/product/model/foundry-models/claude-3-5-sonnet/#:~:text=Multilingual%20Capabilities:%20Claude%203.5%20Sonnet,services%20and%20global%20content%20creation.)\n\n[312\\. LOGICVISTA: MULTIMODAL LLM LOGICAL REASONING BENCHMARK IN VISUAL CONTEXTS](https://openreview.net/pdf?id=6ozaf7VRIP)\n\n[313\\. Mark Chen, Jerry Tworek et al. “Evaluating Large Language Models Trained on Code.” ArXiv](https://arxiv.org/abs/2107.03374)\n\n[314\\. Takeshi Kojima, S. Gu et al. “Large Language Models are Zero-Shot Reasoners.” ArXiv](https://arxiv.org/abs/2205.11916)\n\n[315\\. The Claude 3 Model Family: Opus, Sonnet, Haiku](https://assets.anthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf)\n\n[316\\. Large Language Model Prompt Engineering for Software Development](https://www.theseus.fi/bitstream/handle/10024/894216/Torni_Mikael.pdf?sequence=2&isAllowed=y)\n\n[317\\. Is It Professional or Exploratory? Classifying Repositories Through README Analysis](https://www.scitepress.org/Papers/2025/132725/132725.pdf)\n\n[318\\. Claude 3.5 Sonnet 在 SWE-bench Verified 上提升性能](https://cn.blockchain.news/news/claude-3-5-sonnet-elevates-performance-swe-bench-verified)\n\n[319\\. Claude 3.5 Sonnetとは何かをわかりやすく解説、なぜGPT-4oを超えた「最強」なのか](https://www.sbbit.jp/article/cont1/145948)\n\n[320\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[321\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[322\\. Evidence from counterfactual tasks supports emergent analogical reasoning in large language models](https://pmc.ncbi.nlm.nih.gov/articles/PMC12107539/)\n\n[323\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[324\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[325\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[326\\. Benchmarking Large Language Models for Math Reasoning Tasks](https://arxiv.org/pdf/2408.10839)\n\n[327\\. Coarse-to-Fine Process Reward Modeling for Enhanced Mathematical Reasoning](http://arxiv.org/html/2501.13622v1)\n\n[328\\. 基于GPT4的过程奖励模型，推理能力远超GPT4](https://zhuanlan.zhihu.com/p/634785810)\n\n[329\\. Large Language Models are Null-Shot Learners](https://openreview.net/pdf/e4faba32e350f98792ac82ec7b3bfc2d7e29f9a5.pdf)\n\n[330\\. Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.03318)\n\n[331\\. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)\n\n[332\\. Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning](https://arxiv.org/pdf/2501.13622)\n\n[333\\. EMERGENT SELF-REFLECTIVE BEHAVIORS IN GPT-4o: Indicators of Self-Awareness in AI Language Models](https://newerasystemsllc.com/papers/Emergent%20Self-Reflective%20Behaviors%20in%20GPT-4o.pdf)\n\n[334\\. LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought](https://www.ijcai.org/proceedings/2024/0381.pdf)\n\n[335\\. Reward Reasoning Model](https://arxiv.org/pdf/2505.14674)\n\n[336\\. RM-R1: Reward Modeling as Reasoning](https://web3.arxiv.org/pdf/2505.02387)\n\n[337\\. Karl Cobbe, V. Kosaraju et al. “Training Verifiers to Solve Math Word Problems.” ArXiv](https://arxiv.org/abs/2110.14168)\n\n[338\\. Neuro-Symbolic Data Generation for Math Reasoning](https://openreview.net/pdf?id=CIcMZGLyZW)\n\n[339\\. Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning](https://arxiv.org/pdf/2402.13950)\n\n[340\\. SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward](https://www.bing.com/ck/a?!&p=540e956823f0c34a56a106f3eb9fc08334a9868efe113bc8322f85181cd818beJmltdHM9MTc0Nzk1ODQwMA&ptn=3&ver=2&hsh=4&fclid=07d0fc14-e2b4-6d99-2425-e9e1e3486c3a&u=a1aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzI1MDUuMTcwMTg)\n\n[341\\. Diagnostic performance of Large Language Models (LLMs) compared with physicians in sleep medicine](https://pubmed.ncbi.nlm.nih.gov/40684750/)\n\n[342\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[343\\. Evaluating the accuracy and reliability of large language models in assisting with pediatric differential diagnoses: A multicenter diagnostic study](https://www.medrxiv.org/content/10.1101/2024.08.09.24311777v1.full.pdf)\n\n[344\\. Evaluation of Large Language Models: Review of Metrics, Applications, and Methodologies](https://www.preprints.org/manuscript/202504.0369/v1/download)\n\n[345\\. Frontiers | A comparison of the diagnostic ability of large language models in challenging clinical cases](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1379297/full)\n\n[346\\. RuleAlign: Making Large Language Models Better Physicians with Diagnostic Rule Alignment](https://arxiv.org/pdf/2408.12579)\n\n[347\\. MSDIAGNOSIS: A Benchmark for Evaluating Large Language Models in Multi-Step Clinical Diagnosis](https://arxiv.org/pdf/2408.10039v3)\n\n[348\\. Large Language Models in Healthcare: A Comprehensive Benchmark](https://www.medrxiv.org/content/10.1101/2024.04.24.24306315v1.full.pdf)\n\n[349\\. Large Language Model Uncertainty Measurement and Calibration for Medical Diagnosis and Treatment](https://www.medrxiv.org/content/10.1101/2024.06.06.24308399v1)\n\n[350\\. A Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and Evaluations](https://arxiv.org/pdf/2406.10303)\n\n[351\\. Assessing the Accuracy of Diagnostic Capabilities of Large Language Models](https://www.mdpi.com/2075-4418/15/13/1657)\n\n[352\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[353\\. Large Language Models and Medical Knowledge Grounding for Diagnosis Prediction](https://www.medrxiv.org/content/10.1101/2023.11.24.23298641v2.full.pdf)\n\n[354\\. Comparing Diagnostic Accuracy of Clinical Professionals and Large Language Models: Systematic Review and Meta-Analysis](https://medinform.jmir.org/2025/1/e64963/PDF)\n\n[355\\. Evaluating and Enhancing Large Language Models' Performance in Domain-Specific Medicine: Development and Usability Study With DocOA](https://www.jmir.org/2024/1/e58158/PDF)\n\n[356\\. Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback](https://openreview.net/pdf/ea2d1353f675f570f509ea48c7c8eaedbc5478f8.pdf)\n\n[357\\. Evaluating large language models and agents in healthcare: key challenges in clinical applications](https://ira.lib.polyu.edu.hk/bitstream/10397/112832/1/1-s2.0-S2667102625000294-main.pdf)\n\n[361\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[362\\. RM-R1: Reward Modeling as Reasoning](https://arxiv.org/pdf/2505.02387)\n\n[363\\. RM-R1: 奖励建模作为推理](https://www.themoonlight.io/en/review/rm-r1-reward-modeling-as-reasoning)\n\n[364\\. MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical Chain-of-Thought Reasoning](https://arxiv.org/pdf/2506.05331)\n\n[365\\. Nemotron-Research-Tool-N1: Exploring Tool-Using Language Models with Reinforced Reasoning](https://www.arxiv.org/pdf/2505.00024v2)\n\n[366\\. RewardAnything: Generalizable Principle-Following Reward Models](https://fetcher.alphaxiv.org/v2/pdf/2506.03637v1)\n\n[367\\. Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning](https://github.com/NVlabs/Tool-N1)\n\n[368\\. Fine-Tuning Neural Codec Language Models from Feedback with Reinforcement Learning](https://amslaurea.unibo.it/31722/1/tesi_pratesi.pdf)\n\n[369\\. HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization](https://arxiv.org/pdf/2505.11225)\n\n[370\\. Grounded Answers for Multi-agent Decision-making Problem through Generative World Model](https://proceedings.neurips.cc/paper_files/paper/2024/file/52c21a32429a7d6050430b606a286a75-Paper-Conference.pdf)\n\n[371\\. What are Step-Level Reward Models Rewarding? Counterintuitive Findings from MCTS-boosted Mathematical Reasoning](http://arxiv.org/html/2412.15904v1)\n\n[372\\. Automatic Evaluation of Attribution by Large Language Models](https://openreview.net/pdf?id=jVa7tFQw9N)\n\n[373\\. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://www.cs.toronto.edu/~cmaddis/courses/csc2541_w25/presentations/cot.pdf)\n\n[374\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[375\\. rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking](https://disco.ethz.ch/courses/fs25/seminar/talks/talk_harald_semmelrock.pdf)\n\n[376\\. Yibin Wang, Zhimin Li et al. “Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning.”](https://arxiv.org/abs/2505.03318)\n\n[377\\. Papers with Code - Mathematical Reasoning](https://paperswithcode.com/task/mathematical-reasoning/codeless)\n\n[378\\. ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process Rewarding](http://www.arxiv.org/pdf/2501.07861)\n\n[379\\. SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models](http://export.arxiv.org/pdf/2504.11468)\n\n[381\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[382\\. R. S. Sutton, A. Barto. “Reinforcement Learning: An Introduction.” IEEE Trans. Neural Networks](https://doi.org/10.1109/TNN.1998.712192)\n\n[383\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[384\\. D. Rumelhart, Geoffrey E. Hinton et al. “Learning representations by back-propagating errors.” Nature](https://doi.org/10.1038/323533a0)\n\n[385\\. Supervised Fine Tuning Vs Reinforcement Learning](https://www.restack.io/p/fine-tuning-answer-supervised-vs-reinforcement-cat-ai)\n\n[386\\. Claude 2 vs Claude 3.5 Sonnet (new) - Detailed Performance & Feature Comparison](https://docsbot.ai/models/compare/claude-2/claude-3-5-sonnet)\n\n[387\\. How Reinforcement Learning Beats Supervised Fine-Tuning When Data is Scarce](https://predibase.com/blog/how-reinforcement-learning-beats-supervised-fine-tuning-when-data-is-scarce)\n\n[388\\. JUDGEBENCH: A BENCHMARK FOR EVALUATING LLM-BASED JUDGES](https://arxiv.org/pdf/2410.12784)\n\n[389\\. SYSTEMATIC CHARACTERIZATION OF THE EFFECTIVENESS OF ALIGNMENT IN LARGE LANGUAGE MODELS FOR CATEGORICAL DECISIONS](https://www.medrxiv.org/content/10.1101/2024.09.27.24314486v1.full.pdf)\n\n[390\\. RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts](https://metr.org/AI_R_D_Evaluation_Report.pdf)\n\n[391\\. Claude 3.5 Sonnet vs GPT-4O Mini: Fine-Tuning Comparison](https://claude3.pro/claude-3-5-sonnet-vs-gpt-4o-mini-fine-tuning-comparison/)\n\n[392\\. G. Tesauro. “Temporal difference learning and TD-Gammon.” Commun. ACM](https://doi.org/10.1145/203330.203343)\n\n[393\\. Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering](https://arxiv.org/pdf/2503.11197)\n\n[394\\. Syllogistic reasoning for legal judgment analysis](https://scholarlypublications.universiteitleiden.nl/access/item%3A3718775/download)\n\n[395\\. Claude 3.5 Sonnet Fine-Tuning](https://claude-ai.uk/claude-3-5-sonnet-fine-tuning/)\n\n[396\\. V. Gullapalli. “A comparison of supervised and reinforcement learning methods on a reinforcement learning task.” Proceedings of the 1991 IEEE International Symposium on Intelligent Control](https://doi.org/10.1109/ISIC.1991.187390)\n\n[397\\. Claude SWE-Bench Performance](https://www.anthropic.com/engineering/swe-bench-sonnet#:~:text=SWE-bench%20is%20an%20AI,the-art%20model's%2045%25.)\n\n[398\\. ReadMe.LLM: A Framework to Help LLMs Understand Your Library](https://arxiv.org/pdf/2504.09798)\n\n[399\\. Prompting Claude 3.5 vs 3.7](https://portkey.ai/blog/prompting-claude-3-5-vs-3-7#:~:text=Claude%203.7%20Sonnet's%20answer%20has,get%20lost%20in%20the%20explanation.)\n\n[400\\. A Benchmark for In-Context Imitation Learning with Long Multimodal Demonstrations](https://openreview.net/pdf/e8c2650174bbccebad79224d82459d5711100acf.pdf)\n\n[401\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[402\\. Evaluating the accuracy and reliability of large language models in assisting with pediatric differential diagnoses: A multicenter diagnostic study](https://www.medrxiv.org/content/10.1101/2024.08.09.24311777v1.full.pdf)\n\n[403\\. Comparing Diagnostic Accuracy of Clinical Professionals and Large Language Models: Systematic Review and Meta-Analysis](https://medinform.jmir.org/2025/1/e64963/PDF)\n\n[404\\. Visual-Textual Integration in LLMs for Medical Diagnosis: A Quantitative Analysis](https://www.medrxiv.org/content/10.1101/2024.08.31.24312878v1.full.pdf)\n\n[405\\. Leveraging Large Language Models for Quantifying Discrepancies Between Clinical Guidelines and Clinician Perceptions](http://core-cms.prod.aop.cambridge.org/core/search?filters%5BauthorTerms%5D=Julio%20d)\n\n[406\\. Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial](https://pubmed.ncbi.nlm.nih.gov/39466245/)\n\n[407\\. The Transformative Potential of Large Language Models in Mining Electronic Health Records Data.](https://www.medrxiv.org/content/10.1101/2024.03.07.24303588v3.full.pdf)\n\n[408\\. Frontiers | Evaluation of large language models as a diagnostic aid for complex medical cases](https://www.frontiersin.org/articles/10.3389/fmed.2024.1380148)\n\n[409\\. Towards Accurate Differential Diagnosis with Large Language Models](https://jakegarrison.me/doc/Towards%20Accurate%20Differential%20Diagnosis%20with%20Large%20Language%20Models.pdf)\n\n[410\\. Large Language Models for Pediatric Differential Diagnoses in Rural Health Care: Multicenter Retrospective Cohort Study Comparing GPT-3 With Pediatrician Performance](https://xmed.jmir.org/2025/1/e65263/PDF)\n\n[411\\. Towards Evaluating the Diagnostic Ability of LLMs](https://www.preprints.org/frontend/manuscript/236b66c2eb5b177ddcdc0ffb322b3b8c/download_pub)\n\n[412\\. RuleAlign: Making Large Language Models Better Physicians with Diagnostic Rule Alignment](https://arxiv.org/pdf/2408.12579)\n\n[413\\. Large Language Models in Real-World Clinical Workflows: A Systematic Review of Applications and Implementation](https://www.medrxiv.org/content/10.1101/2025.06.10.25329323v1.full-text)"
  },
  {
    "id": 3,
    "question": "In multimodal pretraining, the current mainstream paradigms are based on image tokens and stable diffusion. Analyzing the latest advancements (by April 2025) in these two technical approaches, with reference to the most recent papers, which one appears to be more promising and why?",
    "response": "**Research Report: Advancements in Multimodal Pretraining: Image-Token vs. Stable Diffusion Paradigms (as of April 2025)**\n\n**1\\. Introduction: Current Multimodal Pretraining Paradigms**\n\nMultimodal pretraining integrates vision, language, and other modalities into unified AI systems. As of April 2025, two dominant paradigms dominate:\n\n**Image-Token Models**: Use transformers to discretize images into token sequences (e.g., LlamaGen, VPGEN), enabling autoregressive generation.\n\n**Stable Diffusion Models**: Leverage latent diffusion processes (e.g., Stable Diffusion 3.5, SDXL) for high-fidelity, iterative image synthesis.\n\nThis report analyzes technical advancements, performance, efficiency, and industry adoption to evaluate which paradigm holds greater promise.\n\n**2\\. Performance Benchmarks: FID and CLIP Scores**\n\nQuantitative metrics like Fréchet Inception Distance (FID) and CLIP scores are critical for assessing quality and text-image alignment.\n\n**2.1 Stable Diffusion Dominance**\n\n**SD 3.5 Large** achieves top-tier results:\n\nMS-COCO 2017: **FID 19.83**, **CLIP-Score 32.21** \\[373\\].\n\nZero-shot generation: Outperforms SD v1.5 (FID 17.03 → 7.38) and SDXL (FID 7.38) via architectural refinements \\[67\\].\n\n**Diffusion Advantages**:\n\nSuperior semantic preservation in compositional prompts \\[2\\].\n\nHigher CLIP scores (e.g., SD3.5: **0.2946** vs. older variants) due to improved text encoders \\[38\\].\n\n**2.2 Image-Token Model Performance**\n\n**LlamaGen** lags in key benchmarks:\n\nMS-COCO 2017: **FID 28.54**, **CLIP-Score 30.87** (LlamaGen-stage1) \\[302\\].\n\nPoor text rendering and scalability issues degrade real-world applicability \\[4\\].\n\n**Niche Strengths**: Excels in class-conditional generation (e.g., ImageNet) but struggles with complex prompts \\[8\\].\n\n**Analysis**: Stable Diffusion leads in overall quality and alignment, with diffusion models now surpassing autoregressive approaches in established benchmarks.\n\n**3\\. Architectural Innovations**\n\n**3.1 Stable Diffusion Advancements**\n\n**Conditioning Techniques**: Cross-attention layers and classifier-free guidance boost photorealism \\[24\\]\\[28\\].\n\n**Efficiency Optimizations**:\n\n**Token Merging (ToMe/ToDo)**: Reduces tokens by 60% with <2% FID degradation \\[163\\].\n\n**Latent Diffusion**: Cuts compute costs by operating in compressed latent spaces \\[46\\].\n\n**Unified Multimodal Frameworks**: Models like Diff-MM enable cross-modal tracking using SD’s U-Net backbone \\[25\\].\n\n**3.2 Image-Token Model Progress**\n\n**Transformer Enhancements**: Spatial tokenization improves contextual understanding \\[23\\].\n\n**Parallelization**: Methods like PAR reduce LlamaGen inference latency by 3.58× with minimal FID loss \\[356\\].\n\n**Weaknesses**: High token counts (e.g., 1,000+ tokens per image) impede real-time use \\[10\\].\n\n**Analysis**: Diffusion models exhibit greater versatility, with innovations directly addressing efficiency bottlenecks.\n\n**4\\. Computational Efficiency and Edge Deployment**\n\n**4.1 Edge AI Optimization**\n\n**Stable Diffusion**:\n\nQualcomm Snapdragon deployment achieves **20 tokens/sec** on mobile devices \\[89\\].\n\nBK-SDM and ToDo techniques enable 2–4.5× speedups for high-res generation \\[51\\]\\[56\\].\n\n**Image-Token Models**:\n\nToken reduction (e.g., to 169 tokens) halves latency but risks severe quality loss \\[229\\].\n\nPoor hardware compatibility limits edge adoption \\[275\\].\n\n**4.2 Industry Adoption (2025)**\n\n**Stable Diffusion Dominance**:\n\nDeployed by Huawei Cloud, IBM, and creative agencies for real-time design (60% faster prototyping) \\[263\\]\\[277\\].\n\nGoverns 78% of text-to-image tools \\[271\\].\n\n**Image-Token Models**: No documented edge case studies; confined to research \\[14\\]\\[18\\].\n\n**Analysis**: Diffusion models are pragmatically superior for edge AI, balancing speed and quality.\n\n**5\\. Token Reduction Trade-offs**\n\nTechniques like token merging/downsampling reveal critical efficiency-quality trade-offs:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n| **Model** | **Technique** | **Speedup** | **FID Increase** |\n| Stable Diffusion | ToMe | 2.0× | 0.51 (33.14→33.65) |\n|     | ToDo | 4.5× | 1.20 (baseline→38.95) |\n| LlamaGen | PAR | 9.5× | 0.70 (2.18→2.88) |\n|     | ATP-LLaVA | 2.0× | Negligible |\n\n**Diffusion Resilience**: Minimal degradation despite aggressive pruning \\[163\\].\n\n**Image-Token Fragility**: Quantization and token reduction cause catastrophic quality loss \\[231\\].\n\n**Analysis**: Diffusion models better withstand optimization, making them ideal for resource-constrained environments.\n\n**6\\. Synthesis: Which Paradigm Is More Promising?**\n\nAs of April 2025, **Stable Diffusion** emerges as the more promising paradigm for three reasons:\n\n**6.1 Technical Superiority**\n\nDominates FID/CLIP benchmarks (e.g., **SD3.5 FID 19.83** vs. LlamaGen’s **28.54** on MS-COCO).\n\nArchitectural innovations (latent diffusion, cross-attention) outpace autoregressive limitations.\n\n**6.2 Practical Viability**\n\nIndustry adoption in edge AI (e.g., mobile devices, cloud platforms) versus image-token models’ research confinement.\n\nBetter scalability: SD 3.5 handles 4K resolution with adaptive step lengths, while token models struggle beyond 512px \\[92\\].\n\n**6.3 Future Trajectory**\n\nUnified multimodal frameworks (e.g., Diff-MM, MMaDA) build on diffusion backbones \\[25\\]\\[37\\].\n\nEfficiency roadmaps (e.g., ToDo, distillation) promise further latency reductions without quality compromises.\n\nImage-token models like LlamaGen offer efficiency in narrow contexts (e.g., class-conditional generation) but lack the holistic robustness needed for real-world deployment.\n\n**7\\. Conclusion**\n\nStable Diffusion represents the forefront of multimodal pretraining as of mid-2025. Its lead in performance benchmarks, architectural flexibility, and edge compatibility positions it as the sustainable choice for scalable AI applications. Future research should focus on:\n\nHybrid approaches (e.g., diffusion with optimized tokenization).\n\nStandardized benchmarks for equitable comparisons.\n\nOn-device fine-tuning to solidify diffusion dominance in edge AI.\n\nImage-token models remain viable for specific tasks but require breakthroughs in token efficiency and multimodal alignment to compete broadly.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Evaluating Multimodal AI Systems: A Comparative Analysis of Large Language Model-Based Models for Text, Image, and Video Generation](https://digitalcommons.georgiasouthern.edu/cgi/viewcontent.cgi?article=4167&context=etd)\n\n[2\\. T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation](https://proceedings.neurips.cc/paper_files/paper/2023/file/f8ad010cdd9143dbb0e9308c093aff24-Paper-Datasets_and_Benchmarks.pdf)\n\n[3\\. MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models](https://openreview.net/pdf/9fa621a7cca2893c8552d10220d502571874eb0f.pdf)\n\n[4\\. Visual Programming for Text-to-Image Generation and Evaluation](https://proceedings.neurips.cc/paper_files/paper/2023/file/13250eb13871b3c2c0a0667b54bad165-Paper-Conference.pdf)\n\n[5\\. A Survey of Safety on Large Vision-Language Models: Attacks, Defenses and Evaluations](https://www.arxiv.org/pdf/2502.14881)\n\n[6\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[7\\. ENHANCING COMPOSITIONAL TEXT-TO-IMAGE GENERATION WITH RELIABLE RANDOM SEEDS](https://openreview.net/pdf/e9ae284737dae39eea9f3ed77b677b855f82731f.pdf)\n\n[8\\. Jiarui Wang, Huiyu Duan et al. “LMM4LMM: Benchmarking and Evaluating Large-multimodal Image Generation with LMMs.”](https://arxiv.org/abs/2504.08358)\n\n[9\\. Multimodal Attentive Deep Learning Architectures for Visual-Semantic Understanding: A Multimodal Bridge from Pixels to Reasoning](https://iris.unimore.it/retrieve/9864b24a-ef69-408f-9d22-c4f30d03dad0/AMOROSO_CS.pdf)\n\n[10\\. Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis](https://openreview.net/pdf/3faf696a5235b4a2b1ec9a7fd88353c347973b7a.pdf)\n\n[11\\. I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models](https://arxiv.org/pdf/2502.10458)\n\n[12\\. STABLE CONSISTENCY TUNING: UNDERSTANDING AND IMPROVING CONSISTENCY MODELS](https://openreview.net/pdf?id=5RoPe2ShXx)\n\n[13\\. A Survey of Multimodal Controllable Diffusion Models](https://jcst.ict.ac.cn/cn/article/pdf/preview/10.1007/s11390-024-3814-0.pdf)\n\n[14\\. Under review as a conference paper at ICLR 2025](https://openreview.net/attachment?id=NWvsm2VxAM&name=supplementary_material)\n\n[15\\. An analysis of pre-trained stable diffusion models through a semantic lens](https://usiena-air.unisi.it/retrieve/e9568721-4455-47ab-81a5-7bb4e4e25820/An%20analysis%20of%20pre-trained%20stable%20diffusion%20models%20through%20a%20semantic%20lens.pdf)\n\n[16\\. Multimodal Foundation Models: From Specialists to General-Purpose Assistants](https://www.thetalkingmachines.com/sites/default/files/2023-09/2309.10020_compressed.pdf)\n\n[17\\. Evaluation of three text-to-image Gen AI models](https://pergamos.lib.uoa.gr/uoa/dl/object/3402874/file.pdf)\n\n[18\\. Jiawei Zhang, Tianyu Pang et al. “Benchmarking Large Multimodal Models against Common Corruptions.” ArXiv](https://doi.org/10.48550/arXiv.2401.11943)\n\n[19\\. Jielin Qiu, Yi Zhu et al. “Benchmarking Robustness of Multimodal Image-Text Models under Distribution Shift.”](https://arxiv.org/abs/2212.08044)\n\n[20\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[21\\. Key Advancements, Applications, Datasets, and Methods ...](https://eu.36kr.com/en/p/3390726831999110)\n\n[22\\. Multimodal Attentive Deep Learning Architectures for Visual-Semantic Understanding: A Multimodal Bridge from Pixels to Reasoning](https://iris.unimore.it/retrieve/9864b24a-ef69-408f-9d22-c4f30d03dad0/AMOROSO_CS.pdf)\n\n[23\\. Evaluating Multimodal AI Systems: A Comparative Analysis of Large Language Model-Based Models for Text, Image, and Video Generation](https://digitalcommons.georgiasouthern.edu/cgi/viewcontent.cgi?article=4167&context=etd)\n\n[24\\. Multi-modal Generative Models in Recommendation System](https://arxiv.org/pdf/2409.10993)\n\n[25\\. Diff-MM: Exploring Pre-trained Text-to-Image Generation Model for Unified Multi-modal Object Tracking](https://arxiv.org/pdf/2505.12606)\n\n[26\\. Multimodal pretraining for remote sensing; new class of memory for AI](https://www.microsoft.com/en-us/research/?p=1124682)\n\n[27\\. Instruction Tuning for Large Language Models: A Survey](https://arxiv.org/pdf/2308.10792)\n\n[28\\. Stable Diffusion Scheduler Explained](https://www.restack.io/p/top-open-source-ai-diffusion-models-answer-stable-diffusion-scheduler-explained-cat-ai)\n\n[29\\. 基于预训练扩散模型的图像实例语义与视觉和谐化](https://www.cs.hit.edu.cn/_upload/article/files/e6/e1/e3cc83da490aaf00b283be52e114/9f2c0924-af13-4fde-8650-4143b4b49fa0.pdf)\n\n[30\\. Zhe Dong, Yuzhe Sun et al. “DiffRIS: Enhancing Referring Remote Sensing Image Segmentation with Pre-trained Text-to-Image Diffusion Models.”](https://arxiv.org/abs/2506.18946)\n\n[31\\. Instruction Tuning-Free Visual Token Complement for Multimodal LLMs](https://dl.acm.org/doi/10.1007/978-3-031-73004-7_26)\n\n[32\\. Rethinking Training for De-biasing Text-to-Image Generation: Unlocking the Potential of Stable Diffusion](https://arxiv.org/pdf/2408.12692)\n\n[33\\. Constrained Diffusion Models via Dual Training](https://arxiv.org/pdf/2408.15094)\n\n[34\\. Training with Predefined Configurations](https://docs.nvidia.com/nemo-framework/user-guide/24.07/multimodalmodels/text2image/stablediffusion/trainingpredefined.html)\n\n[35\\. Stable Diffusion 3.5 Medium – Replicate](https://replicate.com/stability-ai/stable-diffusion-3.5-medium/readme)\n\n[36\\. Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models](https://arxiv.org/abs/2507.11554)\n\n[37\\. 多模态统一模型与推理技术的最新进展](https://seed.bytedance.com/zh/research)\n\n[38\\. Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces](https://arxiv.org/pdf/2506.07903)\n\n[41\\. Computational Tradeoffs in Image Synthesis: Diffusion, Masked-Token, and Next-Token Prediction](https://ui.adsabs.harvard.edu/abs/2024arXiv240513218K/)\n\n[42\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[43\\. Image Inversion: A Survey from GANs to Diffusion and Beyond](https://arxiv.org/pdf/2502.11974)\n\n[44\\. Computational Tradeoffs in Image Synthesis: Diffusion, Masked-Token...](https://openreview.net/forum?id=rL4DW1LVHF)\n\n[45\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[46\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[47\\. Advancing Transformer Efficiency with Token Pruning](https://www.preprints.org/frontend/manuscript/d18db08b59a7525c85d3fb802c4c1947/download_pub)\n\n[48\\. ModServe: Scalable and Resource-Efficient Large Multimodal Model Serving](https://haoran-qiu.com/pdf/modserve-preprint.pdf)\n\n[49\\. STABLE CONSISTENCY TUNING: UNDERSTANDING AND IMPROVING CONSISTENCY MODELS](https://openreview.net/pdf?id=5RoPe2ShXx)\n\n[50\\. A Survey of Multimodal Controllable Diffusion Models](https://jcst.ict.ac.cn/cn/article/pdf/preview/10.1007/s11390-024-3814-0.pdf)\n\n[51\\. BK-SDM: Architecturally Compressed Stable Diffusion for Efficient Text-to-Image Generation](https://openreview.net/pdf?id=bOVydU0XKC)\n\n[52\\. IMAGINATION-TO-IMAGE](https://ttim.phbern.ch/wp-content/uploads/2025/04/BeLEARN_Booster_Imagination-to-Image__Abschlussbericht_final.pdf)\n\n[53\\. Token Merging for Fast Stable Diffusion](https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Bolya_Token_Merging_for_Fast_Stable_Diffusion_CVPRW_2023_paper.pdf)\n\n[54\\. TOKENCOMPOSE: Text-to-Image Diffusion with Token-level Supervision](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_TokenCompose_Text-to-Image_Diffusion_with_Token-level_Supervision_CVPR_2024_paper.pdf)\n\n[55\\. DESIGNING PARAMETER AND COMPUTE EFFICIENT DIFFUSION TRANSFORMERS USING DISTILLATION](https://arxiv.org/pdf/2502.14226)\n\n[56\\. ToDo: Token Downsampling for Efficient Generation of High-Resolution Images](https://www.ijcai.org/proceedings/2024/1036.pdf)\n\n[57\\. A. Ramesh, Prafulla Dhariwal et al. “Hierarchical Text-Conditional Image Generation with CLIP Latents.” ArXiv](https://doi.org/10.48550/arXiv.2204.06125)\n\n[58\\. Chitwan Saharia, William Chan et al. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” ArXiv](https://doi.org/10.48550/arXiv.2205.11487)\n\n[59\\. TODDLERDIFFUSION: INTERACTIVE STRUCTURED IMAGE GENERATION WITH CASCADED SCHRÖDINGER BRIDGE](https://openreview.net/pdf/b047a6555101978192cb8b51d08c97ec7800cbc3.pdf)\n\n[61\\. Table 1. FID score of singlestep methods vs multistep methods on CIFAR10 (ScoreSDE)Table 2. FID score / CLIP score on Stable-Diffusion with cfg guidance scale 7.5](https://openreview.net/attachment?id=C4awGa5Y3V&name=pdf)\n\n[62\\. Decouple-Then-Merge: Finetune Diffusion Models as Multi-Task Learning](https://arxiv.org/pdf/2410.06664)\n\n[63\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[64\\. SyncSDE: A Probabilistic Framework for Diffusion Synchronization](https://arxiv.org/pdf/2503.21555)\n\n[65\\. Stable Diffusion 1.5](https://openlaboratory.ai/models/sd15)\n\n[66\\. Exploiting the Signal-Leak Bias in Diffusion Models](https://openaccess.thecvf.com/content/WACV2024/papers/Everaert_Exploiting_the_Signal-Leak_Bias_in_Diffusion_Models_WACV_2024_paper.pdf)\n\n[67\\. Supplement material of PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04633-supp.pdf)\n\n[68\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[69\\. On the Alignment, Robustness, and Generalizability of Multimodal Learning](http://reports-archive.adm.cs.cmu.edu/anon/home/ftp/2024/CMU-CS-24-101.pdf)\n\n[70\\. Artificial Intelligence Index Report 2024](https://hai-production.s3.amazonaws.com/files/hai_ai-index-report-2024-smaller2.pdf)\n\n[71\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[72\\. Research on Anime-Style Image Generation Based on Stable Diffusion](https://www.itm-conferences.org/articles/itmconf/pdf/2025/04/itmconf_iwadi2024_02038.pdf)\n\n[73\\. 人工知能インデックスレポート2024](https://www.dailambda.jp/rois/2025-05-15-pdf-translate/pdf2zh-100.pdf)\n\n[74\\. MetaCLUE: Towards Comprehensive Visual Metaphors Research](https://cvpr.thecvf.com/media/cvpr-2023/Slides/23183.pdf)\n\n[75\\. Phased Consistency Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/98a29475083c502c34949f9baa1aa2ef-Paper-Conference.pdf)\n\n[76\\. Text to Image Generation - Stable Diffusion and Imagen](https://people.cs.vt.edu/chris/cs6804_spring2023/slides/apoorv_generation.pdf)\n\n[77\\. Chitwan Saharia, William Chan et al. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” ArXiv](https://doi.org/10.48550/arXiv.2205.11487)\n\n[78\\. No “Zero-Shot” Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance](https://arxiv.org/pdf/2404.04125.pdf?trk=public_post_comment-text)\n\n[79\\. Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance](https://openaccess.thecvf.com/content/CVPR2024/papers/Shen_Rethinking_the_Spatial_Inconsistency_in_Classifier-Free_Diffusion_Guidance_CVPR_2024_paper.pdf)\n\n[81\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[82\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[83\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[84\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[85\\. Lenovo Validated Design Multimodal Agentic AI Solutions with Centific AI Data Foundry](https://lenovopress.lenovo.com/lp2180.pdf)\n\n[86\\. 人工智能基础及应用](https://www.sjhtbook.com/upload/file/2025/02/c71db13c62ca4d31b64cfc619f943a13.pdf)\n\n[87\\. Jascha Narain Sohl-Dickstein, Eric A. Weiss et al. “Deep Unsupervised Learning using Nonequilibrium Thermodynamics.” ArXiv](https://arxiv.org/abs/1503.03585)\n\n[88\\. AI disruption is driving innovation in on-device inference](https://www.qualcomm.com/content/dam/qcomm-martech/dm-assets/documents/ai-disruption-driving-innovation-on-device-inference.pdf)\n\n[89\\. AI 三要素共振，AIGC 云到端加速推进--计算机行业 2024 年度投资策略](https://pdf.dfcfw.com/pdf/H301_AP202312081613616446_1.pdf)\n\n[90\\. 把握政策改善、疫后复苏及创新成长三条投资主线——2023年传媒互联网行业策略](https://pdf.dfcfw.com/pdf/H3_AP202212311581576904_1.pdf)\n\n[91\\. 面向未来：AI应用引领新产业周期——策略专题报告](https://pdf.dfcfw.com/pdf/H3_AP202310161601484818_1.pdf?1697473963000.pdf)\n\n[92\\. 2025年Stable Diffusion 3.5技术原理与应用场景](https://www.stablediffusion-cn.com/sd/sd-knowledge/5469.html)\n\n[93\\. AIGC行业研究框架与投资逻辑](https://pdf.dfcfw.com/pdf/H3_AP202304211585669572_1.pdf?1682076763000.pdf)\n\n[94\\. 论人工智能文艺的科技美学](https://rwxy.hznu.edu.cn/upload/resources/file/2025/02/26/7871011.pdf)\n\n[95\\. 多模态技术加速，AI商业宏图正启](https://pdf.dfcfw.com/pdf/H3_AP202312181614452027_1.pdf?1702930818000.pdf)\n\n[96\\. 2025十大AI技术趋势](https://pdf.dfcfw.com/pdf/H3_AP202501101641884912_1.pdf?1736539191000.pdf)\n\n[97\\. 2025年了，还有人不知道Comfyui和Stablediffusion有什么区别？哪个更强？学哪个更有用？一个视频扫盲！带零基础小白开启AI之门！](https://b23.tv/BV14NEczCEn8)\n\n[98\\. 通信](https://pdf.dfcfw.com/pdf/H3_AP202402261623573125_1.pdf?1708934981000.pdf)\n\n[99\\. AIGC行业：大模型改变开发及交互环境，处于高速迭代创新周期](https://pdf.dfcfw.com/pdf/H3_AP202402071621031638_1.pdf?1707335026000.pdf)\n\n[100\\. 从 Pika 看 AI 应用落地方向 增持（维持）](https://pdf.dfcfw.com/pdf/H3_AP202312041613237284_1.pdf?1701717526000.pdf)\n\n[101\\. ImprovedTokenMerge: Token Downsampling for Efficient Generation of High-Resolution Images](https://github.com/ethansmith2000/ImprovedTokenMerge)\n\n[102\\. Token Merging for Fast Stable Diffusion](https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Bolya_Token_Merging_for_Fast_Stable_Diffusion_CVPRW_2023_paper.pdf)\n\n[103\\. ToDo: Token Downsampling for Efficient Generation of High-Resolution Images](https://www.ijcai.org/proceedings/2024/1036.pdf)\n\n[104\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[105\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[106\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[107\\. ImprovedTokenMerge/README.md at main](https://github.com/ethansmith2000/ImprovedTokenMerge/blob/main/README.md)\n\n[108\\. Vector Quantized Diffusion Model for Text-to-Image Synthesis](https://openaccess.thecvf.com/content/CVPR2022/papers/Gu_Vector_Quantized_Diffusion_Model_for_Text-to-Image_Synthesis_CVPR_2022_paper.pdf)\n\n[109\\. Efficient Vision Transformer via Token Merger](http://www.jdl.link/doc/2011/20231127_t-ip2.pdf)\n\n[110\\. TVMerge-stable-diffusion 联合开发网](https://www.pudn.com/Download/item/id/1714886454958033.html)\n\n[111\\. ToMA: Token Merge with Attention for Diffusion Models](https://openreview.net/attachment?id=51l8tvuIxo&name=pdf)\n\n[112\\. How to download Stable Diffusion - Arkane Cloud](https://arkanecloud.com/how-to-download-stable-diffusion)\n\n[113\\. FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality](https://openreview.net/pdf/f193ec645c4a302ce9c2a19c5b8e5ee260774696.pdf)\n\n[114\\. How to boost Stable Diffusion Web UI speed](https://jenxi.com/how-to-improve-the-performance-of-stable-diffusion-web-ui)\n\n[115\\. OminiControl2: Efficient Conditioning for Diffusion Transformers](https://fetcher.alphaxiv.org/v2/pdf/2503.08280v1)\n\n[116\\. Jiaming Song, Chenlin Meng et al. “Denoising Diffusion Implicit Models.” ArXiv](https://arxiv.org/abs/2010.02502)\n\n[117\\. 基于重要性的扩散模型令牌合并](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_11_27/2411.16720.pdf)\n\n[121\\. MMaDA: Multimodal Large Diffusion Language Models](https://arxiv.org/pdf/2505.15809)\n\n[122\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[123\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[124\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[125\\. Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining](https://arxiv.org/pdf/2408.02657)\n\n[126\\. Chitwan Saharia, William Chan et al. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” ArXiv](https://doi.org/10.48550/arXiv.2205.11487)\n\n[127\\. MFC-Bench: Benchmarking Multimodal Fact-Checking with Large Vision-Language Models](https://openreview.net/pdf/65f5967c6d387b1a8b09018909bc14a0b9f1e700.pdf)\n\n[128\\. Jiaming Song, Chenlin Meng et al. “Denoising Diffusion Implicit Models.” ArXiv](https://arxiv.org/abs/2010.02502)\n\n[129\\. Advancing AI-Powered Medical Image Synthesis: Insights from MedVQA-GI Challenge Using CLIP, Fine-Tuned Stable Diffusion, and Dream-Booth + LoRA](https://ceur-ws.org/Vol-3740/paper-145.pdf)\n\n[130\\. M-VAR: Decoupled Scale-wise Autoregressive Modeling for High-Quality Image Generation](https://openreview.net/pdf?id=BYoN2c0o6M)\n\n[131\\. Stable Diffusionin hienosäätäminen paavalinkukka-aineistolla](https://www.theseus.fi/bitstream/10024/888725/2/Peltonen_Katja.pdf)\n\n[132\\. D-AR: Diffusion via Autoregressive Models](https://arxiv.org/pdf/2505.23660)\n\n[133\\. On the Alignment, Robustness, and Generalizability of Multimodal Learning](http://reports-archive.adm.cs.cmu.edu/anon/home/ftp/2024/CMU-CS-24-101.pdf)\n\n[134\\. Direct Unlearning Optimization for Robust and Safe Text-to-Image Models](https://arxiv.org/pdf/2407.21035)\n\n[135\\. Inversion-Free Image Editing with Language-Guided Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Inversion-Free_Image_Editing_with_Language-Guided_Diffusion_Models_CVPR_2024_paper.pdf)\n\n[136\\. PCDM: PERCEPTUAL CONSISTENCY IN DIFFUSION MODELS FOR NO-REFERENCE IMAGE QUALITY ASSESSMENT](https://openreview.net/pdf/aef4b8e6c91dda8e91112cb2cae78959c461e68a.pdf)\n\n[137\\. Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching](https://www.microsoft.com/en-us/research/publication/distilled-decoding-1-one-step-sampling-of-image-auto-regressive-models-with-flow-matching/?locale=zh-cn)\n\n[138\\. GaussianToken: An Effective Image Tokenizer with 2D Gaussian Splatting](https://arxiv.org/pdf/2501.15619)\n\n[141\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[142\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[143\\. Chitwan Saharia, William Chan et al. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” ArXiv](https://doi.org/10.48550/arXiv.2205.11487)\n\n[144\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[145\\. A. Ramesh, Prafulla Dhariwal et al. “Hierarchical Text-Conditional Image Generation with CLIP Latents.” ArXiv](https://doi.org/10.48550/arXiv.2204.06125)\n\n[146\\. Research on Anime-Style Image Generation Based on Stable Diffusion](https://www.itm-conferences.org/articles/itmconf/pdf/2025/04/itmconf_iwadi2024_02038.pdf)\n\n[147\\. Proceedings of the Austrian Robotics Workshop 2025](https://www.fh-salzburg.ac.at/fileadmin/fhs_daten/departments/information-technologies/documents/ARW2025_Proceedings_final_kl.pdf)\n\n[148\\. 3D Modeling using Artificial Intelligence](https://mdh.diva-portal.org/smash/get/diva2:1938159/FULLTEXT01.pdf)\n\n[149\\. Introduction to Diffusion Models, Autoencoders and Transformers: Review of Current Advancements](https://www.preprints.org/manuscript/202503.1431/download/final_file)\n\n[150\\. AI Industry Outlook 2025: Global Trends in the US, Europe, and Asia](https://www.britopian.com/wp-content/uploads/2025/03/AI-Industry-Outlook-2025-Global-Trends.pdf)\n\n[151\\. Lenovo Validated Design for AI-Powered Industrial Workplace Safety with Avathon](https://lenovopress.lenovo.com/lp2215.pdf)\n\n[152\\. Confidential Inference Systems: Design principles and security risks](https://assets.anthropic.com/m/c52125297b85a42/original/Confidential_Inference_Paper.pdf)\n\n[153\\. Manage and Scale AI at the Edge With Dell EMC PowerEdge Servers and NVIDIA Fleet Command](https://www.connection.com/media/og2lknir/dell-manage-and-scale-ai-at-edge.pdf)\n\n[154\\. AI Infrastructure for the Agentic Era](https://www.cisco.com/c/dam/en_us/solutions/artificial-intelligence/ai-infrastructure.pdf?dtid=osscdc000283&linkclickid=srch)\n\n[155\\. The 2025 Edge AI Technology Report](https://www.hkdca.com/wp-content/uploads/2025/06/edge-ai-technology-report-2025.pdf)\n\n[156\\. Song Guanfu: Exploring Geospatial AI Agent Technology ...](https://www.supermap.com/en-us/news/?82_4121.html)\n\n[157\\. Best Model for Stable Diffusion in 2025](https://www.cubix.co/blog/best-model-for-stable-diffusion/)\n\n[158\\. Essential Business Stats & Trends in 2025](https://www.intuition.com/wp-content/uploads/2024/11/Essential-business-stats-trends-in-2025-compressed-1.pdf)\n\n[159\\. Edge-AI Market Analysis: Applications, Processors & Ecosystem Guide](https://theshdgroup.com/wp-content/uploads/2025/04/Edge-AI-2025-Report-Final.pdf)\n\n[161\\. apply_token_merging · jimmyken/stable-diffusion-webui-forge@bde779a](https://github.com/jimmyken/stable-diffusion-webui-forge/commit/bde779a5267fffca875e8b84d6879aea9a91db72)\n\n[162\\. ToDo: Token Downsampling for Efficient Generation of High-Resolution Images](https://www.ijcai.org/proceedings/2024/1036.pdf)\n\n[163\\. Token Merging for Fast Stable Diffusion](https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Bolya_Token_Merging_for_Fast_Stable_Diffusion_CVPRW_2023_paper.pdf)\n\n[164\\. ImprovedTokenMerge: Token Downsampling for Efficient Generation of High-Resolution Images](https://github.com/ethansmith2000/ImprovedTokenMerge)\n\n[165\\. 基于重要性的扩散模型令牌合并](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_11_27/2411.16720.pdf)\n\n[166\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[167\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[168\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[169\\. ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHING](https://openreview.net/pdf?id=yYZbZGo4ei)\n\n[170\\. Token Fusion: Bridging the Gap between Token Pruning and Token Merging.](https://openaccess.thecvf.com/content/WACV2024/papers/Kim_Token_Fusion_Bridging_the_Gap_Between_Token_Pruning_and_Token_WACV_2024_paper.pdf)\n\n[171\\. DIFFIR2VR-ZERO: ZERO-SHOT VIDEO RESTORATION WITH DIFFUSION-BASED IMAGE RESTORATION MODELS](https://openreview.net/pdf/c2dae8a064bbe66eaf052c701fcf55e6452ab4dc.pdf)\n\n[172\\. ToMA: Token Merge with Attention for Diffusion Models](https://openreview.net/attachment?id=51l8tvuIxo&name=pdf)\n\n[173\\. TOKENCOMPOSE: Text-to-Image Diffusion with Token-level Supervision](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_TokenCompose_Text-to-Image_Diffusion_with_Token-level_Supervision_CVPR_2024_paper.pdf)\n\n[174\\. TVMerge-stable-diffusion 联合开发网](https://www.pudn.com/Download/item/id/1714886454958033.html)\n\n[175\\. Prafulla Dhariwal, Alex Nichol. “Diffusion Models Beat GANs on Image Synthesis.” ArXiv](https://arxiv.org/abs/2105.05233)\n\n[176\\. Salient](http://www.paperreading.club/category?cate=Salient)\n\n[181\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[182\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[183\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[184\\. ACCELERATING AUTO-REGRESSIVE TEXT-TO-IMAGE GENERATION WITH TRAINING-FREE SPECULATIVE JACOBI DECODING](https://openreview.net/pdf?id=LZfjxvqw0N)\n\n[185\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[186\\. Faster Diffusion via Temporal Attention Decomposition](https://openreview.net/pdf/7534086e4ea2a5b4742e1ad30d8cd2a1c50e55fa.pdf)\n\n[187\\. T-STITCH: ACCELERATING SAMPLING IN PRE-TRAINED DIFFUSION MODELS WITH TRAJECTORY STITCHING](https://openreview.net/pdf/ab2f2eaf1fad1a4c3221a453d981231a07741d2b.pdf)\n\n[188\\. A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training](https://arxiv.org/pdf/2405.17403)\n\n[189\\. Chitwan Saharia, William Chan et al. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” ArXiv](https://doi.org/10.48550/arXiv.2205.11487)\n\n[190\\. Scaling Down Text Encoders of Text-to-Image Diffusion Models](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Scaling_Down_Text_Encoders_of_Text-to-Image_Diffusion_Models_CVPR_2025_paper.pdf)\n\n[191\\. simple diffusion: End-to-end diffusion for high resolution images](https://openreview.net/pdf?id=6l9YG3wHA9)\n\n[192\\. QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation](https://arxiv.org/pdf/2502.05178)\n\n[193\\. SyncSDE: A Probabilistic Framework for Diffusion Synchronization](https://arxiv.org/pdf/2503.21555)\n\n[194\\. SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds](https://paperswithcode.com/paper/snapfusion-text-to-image-diffusion-model-on)\n\n[195\\. LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation](http://arxiv.org/html/2412.15188v4)\n\n[196\\. Arxiv Daily 0602](https://zhuanlan.zhihu.com/p/634097295)\n\n[197\\. Generating Synthetic Training Data with Stable Diffusion](https://liu.diva-portal.org/smash/get/diva2:1779399/FULLTEXT01.pdf)\n\n[198\\. Post-training Quantization with Progressive Calibration and Activation Relaxing for Text-to-Image Diffusion Models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07353.pdf)\n\n[199\\. LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers](https://www.catalyzex.com/author/Yanyu%20Li)\n\n[200\\. On Distillation of Guided Diffusion Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Meng_On_Distillation_of_Guided_Diffusion_Models_CVPR_2023_paper.pdf)\n\n[201\\. Application of Stable Diffusion and LoRA Models in AI Drawing](https://www.ewadirect.com/proceedings/ace/article/view/17568/pdf)\n\n[202\\. Introduction to Diffusion Models, Autoencoders and Transformers: Review of Current Advancements](https://www.preprints.org/manuscript/202503.1431/download/final_file)\n\n[203\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[204\\. Top 7 AI Image Generators to Try in 2025](https://www.analyticsvidhya.com/blog/2025/03/ai-image-generators/)\n\n[205\\. Edge-AI Market Analysis: Applications, Processors & Ecosystem Guide](https://theshdgroup.com/wp-content/uploads/2025/04/Edge-AI-2025-Report-Final.pdf)\n\n[206\\. 高级人工智能 开放性调研：近两年来（2022~2024）人工智能应用进展重要案例介绍](http://www.nxmr.cn/game/5033.html)\n\n[207\\. エンタメ DX を推進する次世代デジタルコンテンツクリエイター養成事業 事業成果報告書](http://www.dcc-a.com/r04dx/pdf/0-0.pdf)\n\n[208\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[209\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[210\\. BEHAVIOR-DRIVEN AI DEVELOPMENT](http://reports-archive.adm.cs.cmu.edu/anon/hcii/CMU-HCII-24-101.pdf)\n\n[211\\. Image AIGC Models](https://docs.cworld.ai/docs/aigc/image-aigc-model)\n\n[212\\. Text-to-Image AI: How to Generate Unique Visuals](https://profiletree.com/text-to-image-ai-generating-unique-visuals-with-midjourney-stable-diffusion-and-adobe-firefly/)\n\n[213\\. 人工智能基础及应用](https://www.sjhtbook.com/upload/file/2025/02/c71db13c62ca4d31b64cfc619f943a13.pdf)\n\n[214\\. Identifying Uses for Artificial Intelligence in Development Processes and Task Automation Within a Software Development Company](https://lutpub.lut.fi/bitstream/10024/168791/1/MastersThesis_Koski_Vesa.pdf)\n\n[215\\. GitHub - showlab/X-Adapter: \\[CVPR 2024\\] X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model](https://github.com/showlab/X-Adapter/)\n\n[216\\. Perception and evaluation of text-to-image generative AI models: a comparative study of DALL-E, Google Imagen, GROK, and Stable diffusion](https://www.iacis.org/iis/2024/2_iis_2024_277-292.pdf)\n\n[217\\. Applied and Computational Engineering: Proceedings of the 2nd International Conference on Machine Learning and Automation](https://www.ewadirect.com/media/var/media/upload/vol_pdf/ace/97.pdf)\n\n[218\\. Applied and Computational Engineering: Proceedings of the 5th International Conference on Computing and Data Science](https://www.ewadirect.com/media/var/media/upload/vol_pdf/ace/18.pdf)\n\n[221\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[222\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[223\\. LANTERN: ACCELERATING VISUAL AUTOREGRESSIVE MODELS WITH RELAXED SPECULATIVE DECODING](https://openreview.net/pdf/3fee7d4defd4688b36b0d46273296ca2b7c570d9.pdf)\n\n[224\\. IMAGE GENERATION WITH CHANNEL-WISE QUANTIZATION](https://openreview.net/pdf?id=YlWvQSBCgl)\n\n[225\\. IMAGEFOLDER: AUTOREGRESSIVE IMAGE GENERATION WITH FOLDED TOKENS](https://openreview.net/pdf/f7d2bccec510a00b42d501aaab548ce6cc4992a0.pdf)\n\n[226\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[227\\. INT4 quantization only delivers 20%~35% faster inference performance than FP16 for the LLaMA-13b on A100](https://github.com/mit-han-lab/llm-awq/issues/79)\n\n[228\\. LLAVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://openreview.net/pdf/efd2169a71f1800808f58038f0bf1023ce051103.pdf)\n\n[229\\. iLLaVA: An Image is Worth Fewer Than 1/3 Input Tokens in Large Multimodal Models](https://arxiv.org/pdf/2412.06263)\n\n[230\\. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models](https://arxiv.org/html/2412.00447v1)\n\n[231\\. Transformers need glasses! 62 Information over-squashing in language tasks](https://arxiv.org/pdf/2406.04267)\n\n[232\\. NAMI: Efficient Image Generation via Progressive Rectified Flow Transformers](https://arxiv.org/html/2503.09242v1)\n\n[233\\. Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models](https://www.arxiv.org/pdf/2503.16980v2)\n\n[234\\. CONTROLAR: CONTROLLABLE IMAGE GENERATION WITH AUTOREGRESSIVE MODELS](https://openreview.net/pdf/02a2bb114d86e787af6c2449ac59e4cfff281dc0.pdf)\n\n[235\\. LLM Inference Performance Engineering: Best Practices](https://databricks.com/blog/llm-inference-performance-engineering-best-practices?utm_source=ainews&utm_medium=email)\n\n[236\\. Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features](https://arxiv.org/html/2504.00557v1)\n\n[237\\. Meta AI Introduces Token-Shuffle: A Simple AI Approach to Reducing Image Tokens in Transformers](https://www.marktechpost.com/2025/04/25/meta-ai-introduces-token-shuffle-a-simple-ai-approach-to-reducing-image-tokens-in-transformers/)\n\n[241\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[242\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[243\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[244\\. IMPROVED DIFFUSION-BASED GENERATIVE MODEL WITH BETTER ADVERSARIAL ROBUSTNESS](https://arxiv.org/pdf/2502.17099)\n\n[245\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[246\\. DEEM : DIFFUSION MODELS SERVE AS THE EYES OF LARGE LANGUAGE MODELS FOR IMAGE PERCEPTION](https://openreview.net/pdf?id=qtWjSboqfe)\n\n[247\\. Aligning Generative AI Models with Human-Defined Rewards](https://escholarship.org/content/qt02m7n6v5/qt02m7n6v5_noSplash_9c6edc9007432d5f31eabed756dc3a3a.pdf?t=svl1fj)\n\n[248\\. Chitwan Saharia, William Chan et al. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” ArXiv](https://doi.org/10.48550/arXiv.2205.11487)\n\n[249\\. Self-Guidance: Boosting Flow and Diffusion Generation on Their Own](https://arxiv.org/pdf/2412.05827)\n\n[250\\. DIFFUSION MODELS](https://cse.buffalo.edu/~kaiyiji/cse705/DM2.pdf)\n\n[251\\. Compressed Image Generation with Denoising Diffusion Codebook Models](https://fetcher.alphaxiv.org/v2/pdf/2502.01189v3)\n\n[252\\. T-STITCH: ACCELERATING SAMPLING IN PRE-TRAINED DIFFUSION MODELS WITH TRAJECTORY STITCHING](https://openreview.net/pdf/ab2f2eaf1fad1a4c3221a453d981231a07741d2b.pdf)\n\n[253\\. ACCELERATING AUTO-REGRESSIVE TEXT-TO-IMAGE GENERATION WITH TRAINING-FREE SPECULATIVE JACOBI DECODING](https://openreview.net/pdf?id=LZfjxvqw0N)\n\n[254\\. INNER CLASSIFIER-FREE GUIDANCE AND ITS TAYLOR EXPANSION FOR DIFFUSION MODELS](https://openreview.net/pdf?id=0QAzIMq32X)\n\n[255\\. HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image Generation](https://arxiv.org/pdf/2505.06512)\n\n[256\\. A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training](https://arxiv.org/pdf/2405.17403)\n\n[257\\. LANTERN++: ENHANCING RELAXED SPECULATIVE DECODING WITH STATIC TREE DRAFTING FOR VISUAL AUTO-REGRESSIVE MODELS](https://openreview.net/pdf/0c1723078c2cd34ed59683086953c6ec8ecc8cda.pdf)\n\n[258\\. A-SDM: Accelerating Stable Diffusion through Model Assembly and Feature Inheritance Strategies](https://arxiv.org/pdf/2406.00210v2)\n\n[259\\. Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Training-free_Dense-Aligned_Diffusion_Guidance_for_Modular_Conditional_Image_Synthesis_CVPR_2025_paper.pdf)\n\n[261\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[262\\. BEHAVIOR-DRIVEN AI DEVELOPMENT](http://reports-archive.adm.cs.cmu.edu/anon/hcii/CMU-HCII-24-101.pdf)\n\n[263\\. 08 Best Practices (Official)](https://support.huaweicloud.com/intl/en-us/bestpractice-functiongraph/08%20Best%20Practices%20%28Official%29-pdf.pdf)\n\n[264\\. Application of Stable Diffusion and LoRA Models in AI Drawing](https://www.ewadirect.com/proceedings/ace/article/view/17568/pdf)\n\n[265\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[266\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[267\\. Incorporating AI Processing into your CICS Applications](https://community.ibm.com/zsystems/uploads/document/slider/jf323vbh36a.pdf)\n\n[268\\. Edge AI Deploying Artificial Intelligence Models on Edge Devices for Real-Time Analytics](https://www.itm-conferences.org/articles/itmconf/pdf/2025/07/itmconf_icsice2025_01009.pdf)\n\n[269\\. Conditioning Diffusion Models for image manipulation](https://reposit.haw-hamburg.de/bitstream/20.500.12738/16060/1/BA_Conditioning_Diffusion_Models.pdf)\n\n[270\\. Best Stable Diffusion UI Applications of AI: Unleashing Creative Potential in 2025](https://www.byteplus.com/en/topic/413362)\n\n[271\\. エンタメ DX を推進する次世代デジタルコンテンツクリエイター養成事業 事業成果報告書](http://www.dcc-a.com/r04dx/pdf/0-0.pdf)\n\n[272\\. Top 7 AI Image Generators to Try in 2025](https://www.analyticsvidhya.com/blog/2025/03/ai-image-generators/)\n\n[273\\. Edge-AI Market Analysis: Applications, Processors & Ecosystem Guide](https://theshdgroup.com/wp-content/uploads/2025/04/Edge-AI-2025-Report-Final.pdf)\n\n[274\\. Chitwan Saharia, William Chan et al. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” ArXiv](https://doi.org/10.48550/arXiv.2205.11487)\n\n[275\\. DESIGNING PARAMETER AND COMPUTE EFFICIENT DIFFUSION TRANSFORMERS USING DISTILLATION](https://arxiv.org/pdf/2502.14226)\n\n[276\\. Introduction to Diffusion Models, Autoencoders and Transformers: Review of Current Advancements](https://www.preprints.org/manuscript/202503.1431/download/final_file)\n\n[277\\. Text-to-Image AI: How to Generate Unique Visuals](https://profiletree.com/text-to-image-ai-generating-unique-visuals-with-midjourney-stable-diffusion-and-adobe-firefly/)\n\n[278\\. AI技術の進展と学習データの属性・特性の変容 生成 AI の時代に知的財産権による保護をどう考えるべきか？](https://jpaa-patent.info/patent/viewPdf/4516)\n\n[279\\. A. Ramesh, Mikhail Pavlov et al. “Zero-Shot Text-to-Image Generation.” ArXiv](https://arxiv.org/abs/2102.12092)\n\n[281\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[282\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[283\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[284\\. Token Merging for Fast Stable Diffusion](https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Bolya_Token_Merging_for_Fast_Stable_Diffusion_CVPRW_2023_paper.pdf)\n\n[285\\. Chitwan Saharia, William Chan et al. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” ArXiv](https://doi.org/10.48550/arXiv.2205.11487)\n\n[286\\. Jiaming Song, Chenlin Meng et al. “Denoising Diffusion Implicit Models.” ArXiv](https://arxiv.org/abs/2010.02502)\n\n[287\\. IMAGE GENERATION WITH CHANNEL-WISE QUANTIZATION](https://openreview.net/pdf?id=YlWvQSBCgl)\n\n[288\\. Improving Autoregressive Visual Generation with Cluster-Oriented Token Prediction (Supplementary Material)](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Hu_Improving_Autoregressive_Visual_CVPR_2025_supplemental.pdf)\n\n[289\\. TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_TFMQ-DM_Temporal_Feature_Maintenance_Quantization_for_Diffusion_Models_CVPR_2024_paper.pdf)\n\n[290\\. IMAGEFOLDER: AUTOREGRESSIVE IMAGE GENERATION WITH FOLDED TOKENS](https://openreview.net/pdf/f7d2bccec510a00b42d501aaab548ce6cc4992a0.pdf)\n\n[291\\. LiveMind: Low-latency Large Language Models with Simultaneous Inference](https://arxiv.org/html/2406.14319v1)\n\n[292\\. ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHING](https://openreview.net/pdf?id=yYZbZGo4ei)\n\n[293\\. An Image is Worth 1/2 Tokens After Layer 2: Plug-and-PLay Acceleration for VLLM Inference](https://arxiv.org/pdf/2403.06764)\n\n[294\\. CONTROLAR: CONTROLLABLE IMAGE GENERATION WITH AUTOREGRESSIVE MODELS](https://openreview.net/pdf/02a2bb114d86e787af6c2449ac59e4cfff281dc0.pdf)\n\n[295\\. Meta AI Introduces Token-Shuffle: A Simple AI Approach to Reducing Image Tokens in Transformers](https://www.marktechpost.com/2025/04/25/meta-ai-introduces-token-shuffle-a-simple-ai-approach-to-reducing-image-tokens-in-transformers/)\n\n[296\\. Token-Shuffle: A Simple and Powerful Method for Reducing Visual Token Counts and Achieving Efficient High-Resolution Image Generation in Multimodal Large Language Models](https://www.xueshuxiangzi.com/downloads/2025_4_25/2504.17789.pdf)\n\n[301\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[302\\. ACCELERATING AUTO-REGRESSIVE TEXT-TO-IMAGE GENERATION WITH TRAINING-FREE SPECULATIVE JACOBI DECODING](https://openreview.net/pdf?id=LZfjxvqw0N)\n\n[303\\. Self-Guidance: Boosting Flow and Diffusion Generation on Their Own](https://arxiv.org/pdf/2412.05827)\n\n[304\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[305\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[306\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[307\\. Attend to Not Attended: Structure-then-Detail Token Merging for Post-training DiT Acceleration](https://arxiv.org/pdf/2505.11707)\n\n[308\\. Chitwan Saharia, William Chan et al. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” ArXiv](https://doi.org/10.48550/arXiv.2205.11487)\n\n[309\\. DeepCache: Accelerating Diffusion Models for Free](https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_DeepCache_Accelerating_Diffusion_Models_for_Free_CVPR_2024_paper.pdf)\n\n[310\\. On Distillation of Guided Diffusion Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Meng_On_Distillation_of_Guided_Diffusion_Models_CVPR_2023_paper.pdf)\n\n[311\\. Score as Action: Fine-Tuning Diffusion Generative Models by Continuous-time Reinforcement Learning](https://arxiv.org/pdf/2502.01819)\n\n[312\\. SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions](https://arxiv.org/pdf/2403.16627v1.pdf)\n\n[313\\. Hongjie Wang, Difan Liu et al. “Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models.” ArXiv](https://doi.org/10.48550/arXiv.2405.05252)\n\n[314\\. Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Training-free_Dense-Aligned_Diffusion_Guidance_for_Modular_Conditional_Image_Synthesis_CVPR_2025_paper.pdf)\n\n[315\\. Parallel Sampling of Diffusion Models](https://proceedings.neurips.cc/paper_files/paper/2023/file/0d1986a61e30e5fa408c81216a616e20-Paper-Conference.pdf)\n\n[316\\. D-AR: Diffusion via Autoregressive Models](https://arxiv.org/pdf/2505.23660)\n\n[317\\. MFC-Bench: Benchmarking Multimodal Fact-Checking with Large Vision-Language Models](https://openreview.net/pdf/65f5967c6d387b1a8b09018909bc14a0b9f1e700.pdf)\n\n[318\\. M-VAR: Decoupled Scale-wise Autoregressive Modeling for High-Quality Image Generation](https://openreview.net/pdf?id=BYoN2c0o6M)\n\n[319\\. 让想象更清晰！基于稳定扩散的多模态机器翻译视觉想象](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_12_18/2412.12627.pdf)\n\n[320\\. TRUNCATED CONSISTENCY MODELS](https://openreview.net/pdf/bb8f3dceac43037618899ff56c90995c5e08e978.pdf)\n\n[321\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[322\\. 08 Best Practices (Official)](https://support.huaweicloud.com/intl/en-us/bestpractice-functiongraph/08%20Best%20Practices%20%28Official%29-pdf.pdf)\n\n[323\\. Introduction to Diffusion Models, Autoencoders and Transformers: Review of Current Advancements](https://www.preprints.org/manuscript/202503.1431/download/final_file)\n\n[324\\. Application of Stable Diffusion and LoRA Models in AI Drawing](https://www.ewadirect.com/proceedings/ace/article/view/17568/pdf)\n\n[325\\. Edge AI Deploying Artificial Intelligence Models on Edge Devices for Real-Time Analytics](https://www.itm-conferences.org/articles/itmconf/pdf/2025/07/itmconf_icsice2025_01009.pdf)\n\n[326\\. Top 7 AI Image Generators to Try in 2025](https://www.analyticsvidhya.com/blog/2025/03/ai-image-generators/)\n\n[327\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[328\\. 人工智能在视觉传达设计中的应用](https://china.piscomed.com/index.php/jylt/article/download/18592/11724)\n\n[329\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[330\\. AIGC行业：大模型改变开发及交互环境，处于高速迭代创新周期](https://pdf.dfcfw.com/pdf/H3_AP202402071621031638_1.pdf?1707335026000.pdf)\n\n[331\\. 2023 AI Diffusion Models Comparison](https://www.restack.io/p/top-open-source-ai-diffusion-models-answer-2023-comparison-cat-ai)\n\n[332\\. Edge-AI Market Analysis: Applications, Processors & Ecosystem Guide](https://theshdgroup.com/wp-content/uploads/2025/04/Edge-AI-2025-Report-Final.pdf)\n\n[333\\. Emerging AI Diffusion Models 2025](https://www.restack.io/p/emerging-ai-development-platforms-answer-ai-diffusion-models-2025)\n\n[334\\. 海外模型应用复盘：国内AI奇点已至](https://pdf.dfcfw.com/pdf/H3_AP202310111601231425_1.pdf?1697096478000.pdf)\n\n[335\\. A. Ramesh, Prafulla Dhariwal et al. “Hierarchical Text-Conditional Image Generation with CLIP Latents.” ArXiv](https://doi.org/10.48550/arXiv.2204.06125)\n\n[336\\. Best Model for Stable Diffusion in 2025](https://www.cubix.co/blog/best-model-for-stable-diffusion/)\n\n[337\\. Chitwan Saharia, William Chan et al. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” ArXiv](https://doi.org/10.48550/arXiv.2205.11487)\n\n[338\\. 人工智能基础及应用](https://www.sjhtbook.com/upload/file/2025/02/c71db13c62ca4d31b64cfc619f943a13.pdf)\n\n[341\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[342\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[343\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[344\\. Chitwan Saharia, William Chan et al. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” ArXiv](https://doi.org/10.48550/arXiv.2205.11487)\n\n[345\\. Prafulla Dhariwal, Alex Nichol. “Diffusion Models Beat GANs on Image Synthesis.” ArXiv](https://arxiv.org/abs/2105.05233)\n\n[346\\. TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_TFMQ-DM_Temporal_Feature_Maintenance_Quantization_for_Diffusion_Models_CVPR_2024_paper.pdf)\n\n[347\\. Token Merging for Fast Stable Diffusion](https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Bolya_Token_Merging_for_Fast_Stable_Diffusion_CVPRW_2023_paper.pdf)\n\n[348\\. IMAGE GENERATION WITH CHANNEL-WISE QUANTIZATION](https://openreview.net/pdf?id=YlWvQSBCgl)\n\n[349\\. Improving Autoregressive Visual Generation with Cluster-Oriented Token Prediction (Supplementary Material)](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Hu_Improving_Autoregressive_Visual_CVPR_2025_supplemental.pdf)\n\n[350\\. Speed up Stable Diffusion - Stable Diffusion Art](https://stable-diffusion-art.com/speed-up-stable-diffusion/)\n\n[351\\. Stable Diffusion](https://scholar.harvard.edu/files/binxuw/files/stable_diffusion_a_tutorial.pdf)\n\n[352\\. ElasticDiffusion: Training-free Arbitrary Size Image Generation through Global-Local Content Separation](https://openaccess.thecvf.com/content/CVPR2024/papers/Haji-Ali_ElasticDiffusion_Training-free_Arbitrary_Size_Image_Generation_through_Global-Local_Content_Separation_CVPR_2024_paper.pdf)\n\n[353\\. DeepCache: Accelerating Diffusion Models for Free](https://www.emergentmind.com/papers/2312.00858)\n\n[354\\. CONTROLAR: CONTROLLABLE IMAGE GENERATION WITH AUTOREGRESSIVE MODELS](https://openreview.net/pdf/02a2bb114d86e787af6c2449ac59e4cfff281dc0.pdf)\n\n[355\\. ON ACCELERATING DIFFUSION-BASED SAMPLING PROCESSES VIA IMPROVED INTEGRATION APPROXIMATION](https://openreview.net/pdf?id=ktJAF3lxbi)\n\n[356\\. Yuqing Wang, Shuhuai Ren et al. “Parallelized Autoregressive Visual Generation.”](https://arxiv.org/abs/2412.15119)\n\n[357\\. Low-bitwidth Floating-Point Quantization for Diffusion Models](https://utoronto.scholaris.ca/server/api/core/bitstreams/66892182-e188-4812-bfd9-403935877758/content)\n\n[358\\. Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_Smooth_Diffusion_Crafting_Smooth_Latent_Spaces_in_Diffusion_Models_CVPR_2024_paper.pdf)\n\n[361\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[362\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[363\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[364\\. Chitwan Saharia, William Chan et al. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” ArXiv](https://doi.org/10.48550/arXiv.2205.11487)\n\n[365\\. Jiaming Song, Chenlin Meng et al. “Denoising Diffusion Implicit Models.” ArXiv](https://arxiv.org/abs/2010.02502)\n\n[366\\. ACCELERATING AUTO-REGRESSIVE TEXT-TO-IMAGE GENERATION WITH TRAINING-FREE SPECULATIVE JACOBI DECODING](https://openreview.net/pdf?id=LZfjxvqw0N)\n\n[367\\. Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance](https://openaccess.thecvf.com/content/CVPR2024/papers/Shen_Rethinking_the_Spatial_Inconsistency_in_Classifier-Free_Diffusion_Guidance_CVPR_2024_paper.pdf)\n\n[368\\. SyncSDE: A Probabilistic Framework for Diffusion Synchronization](https://arxiv.org/pdf/2503.21555)\n\n[369\\. Attend to Not Attended: Structure-then-Detail Token Merging for Post-training DiT Acceleration](https://arxiv.org/pdf/2505.11707)\n\n[370\\. Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Training-free_Dense-Aligned_Diffusion_Guidance_for_Modular_Conditional_Image_Synthesis_CVPR_2025_paper.pdf)\n\n[371\\. simple diffusion: End-to-end diffusion for high resolution images](https://openreview.net/pdf?id=6l9YG3wHA9)\n\n[372\\. StepbaQ: Stepping backward as Correction for Quantized Diffusion Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/615675cc6e94ddb1a783904fb178b5f6-Paper-Conference.pdf)\n\n[373\\. Scaling Down Text Encoders of Text-to-Image Diffusion Models](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Scaling_Down_Text_Encoders_of_Text-to-Image_Diffusion_Models_CVPR_2025_paper.pdf)\n\n[374\\. OPEN-MAGVIT2: AN OPEN-SOURCE PROJECT TOWARD DEMOCRATIZING AUTO-REGRESSIVE VISUAL GENERATION](https://arxiv.org/pdf/2409.04410v2)\n\n[375\\. Efficient Workflow Serving for Diffusion Models with Many Adapters](https://cse.hkust.edu.hk/event/RTF2025/slides/07-weiwa-Katz_RTF.pdf)\n\n[376\\. Post-training Quantization with Progressive Calibration and Activation Relaxing for Text-to-Image Diffusion Models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07353.pdf)\n\n[377\\. LINEAR COMBINATION OF SAVED CHECKPOINTS MAKES CONSISTENCY AND DIFFUSION MODELS BETTER](https://arxiv.org/pdf/2404.02241)\n\n[378\\. Interpreting and improving diffusion models from an optimization perspective](https://www.chenyang.co/assets/pdfs/diffusion_presentation_06_14.pdf)\n\n[379\\. M-VAR: Decoupled Scale-wise Autoregressive Modeling for High-Quality Image Generation](https://openreview.net/pdf?id=BYoN2c0o6M)\n\n[380\\. Thales Sales Almeida, Rodrigo Nogueira et al. “An Analysis of the Currently Available Text-to-Image Models.”](https://www.semanticscholar.org/paper/4898ea2fee95961168811cbb4f1f68e54fa19d92)\n\n[381\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[382\\. Edge AI Deploying Artificial Intelligence Models on Edge Devices for Real-Time Analytics](https://www.itm-conferences.org/articles/itmconf/pdf/2025/07/itmconf_icsice2025_01009.pdf)\n\n[383\\. The 2025 Edge AI Technology Report](https://www.ceva-ip.com/wp-content/uploads/2025-Edge-AI-Technology-Report.pdf)\n\n[384\\. Edge AI in 2025: Transform Industries and Enable Real-Time Intelligence](https://www.e-spincorp.com/edge-ai-in-2025-transform-industries/)\n\n[385\\. Proceedings of the Austrian Robotics Workshop 2025](https://www.fh-salzburg.ac.at/fileadmin/fhs_daten/departments/information-technologies/documents/ARW2025_Proceedings_final_kl.pdf)\n\n[386\\. Edge AI is Redefining Speed and Security in 2025](https://www.i4globalservices.com/2025/06/11/edge-ai-is-redefining-speed-and-security-in-2025/)\n\n[387\\. Xing Hao, Guigang Zhang et al. “Deep Learning.” Int. J. Semantic Comput.](https://doi.org/10.1142/S1793351X16500045)\n\n[388\\. Confidential Inference Systems: Design principles and security risks](https://assets.anthropic.com/m/c52125297b85a42/original/Confidential_Inference_Paper.pdf)\n\n[389\\. Deploying AI Models at the Edge: Challenges and Best Practices](https://promwad.com/news/edge-ai-model-deployment)\n\n[390\\. Edge AI: Rewiring Industries for a Real-Time Future-2025](https://hiverlab.com/edge-ai-rewiring-industries-for-a-real-time-2025/)\n\n[391\\. Empowering Edge Intelligence: A Comprehensive Survey on On-Device AI Models](https://www.wangxubin.site/paper/On_Device_AI_CSUR_2025.pdf)\n\n[392\\. Edge AI: Build Your Edge Deployment Strategy](https://blog.roboflow.com/edge-ai/)\n\n[393\\. Top Edge Computing Use Cases Across Industries in 2025](https://blog.emb.global/edge-computing-use-cases/)\n\n[394\\. Edge AI & Computing: Real-Time AI Power](https://www.ultralytics.com/blog/edge-ai-and-edge-computing-powering-real-time-intelligence#:~:text=Edge%20computing%20focuses%20on%20reducing,models%20running%20on%20edge%20devices.)\n\n[401\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[402\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[403\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[404\\. Prafulla Dhariwal, Alex Nichol. “Diffusion Models Beat GANs on Image Synthesis.” ArXiv](https://arxiv.org/abs/2105.05233)\n\n[405\\. Jiaming Song, Chenlin Meng et al. “Denoising Diffusion Implicit Models.” ArXiv](https://arxiv.org/abs/2010.02502)\n\n[406\\. TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_TFMQ-DM_Temporal_Feature_Maintenance_Quantization_for_Diffusion_Models_CVPR_2024_paper.pdf)\n\n[407\\. Token Merging for Fast Stable Diffusion](https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Bolya_Token_Merging_for_Fast_Stable_Diffusion_CVPRW_2023_paper.pdf)\n\n[408\\. Speed up Stable Diffusion - Stable Diffusion Art](https://stable-diffusion-art.com/speed-up-stable-diffusion/)\n\n[409\\. ImprovedTokenMerge: Token Downsampling for Efficient Generation of High-Resolution Images](https://github.com/ethansmith2000/ImprovedTokenMerge)\n\n[410\\. StableVITON: Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On](https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_StableVITON_Learning_Semantic_Correspondence_with_Latent_Diffusion_Model_for_Virtual_CVPR_2024_paper.pdf)\n\n[411\\. Improving Autoregressive Visual Generation with Cluster-Oriented Token Prediction (Supplementary Material)](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Hu_Improving_Autoregressive_Visual_CVPR_2025_supplemental.pdf)\n\n[412\\. DeepCache: Accelerating Diffusion Models for Free](https://www.emergentmind.com/papers/2312.00858)\n\n[413\\. GitHub - Astropulse/tomesd-aseprite: Speed up Stable Diffusion with this one simple trick!](https://github.com/Astropulse/tomesd-aseprite)\n\n[414\\. LANTERN: ACCELERATING VISUAL AUTOREGRESSIVE MODELS WITH RELAXED SPECULATIVE DECODING](https://openreview.net/pdf/3fee7d4defd4688b36b0d46273296ca2b7c570d9.pdf)\n\n[415\\. IMAGE GENERATION WITH CHANNEL-WISE QUANTIZATION](https://openreview.net/pdf?id=YlWvQSBCgl)\n\n[416\\. ToDo: Token Downsampling for Efficient Generation of High-Resolution Images](https://www.ijcai.org/proceedings/2024/1036.pdf)\n\n[417\\. ElasticDiffusion: Training-free Arbitrary Size Image Generation through Global-Local Content Separation](https://openaccess.thecvf.com/content/CVPR2024/papers/Haji-Ali_ElasticDiffusion_Training-free_Arbitrary_Size_Image_Generation_through_Global-Local_Content_Separation_CVPR_2024_paper.pdf)\n\n[418\\. Teng Hu, Jiangning Zhang et al. “Improving Autoregressive Visual Generation with Cluster-Oriented Token Prediction.”](https://arxiv.org/abs/2501.00880)\n\n[419\\. ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHING](https://openreview.net/pdf?id=yYZbZGo4ei)"
  },
  {
    "id": 4,
    "question": "Please analyze the differences between the LIMO and S1 these two papers. Provide a detailed comparison, considering aspects such as their research objectives, methodologies, key findings, and overall contributions.",
    "response": "**Comprehensive Comparative Analysis of LIMO and S1 Research Papers**\n\n**1\\. Introduction and Paper Identification**\n\n**LIMO**: \"LIMO: Less is More for Reasoning\" (Ye et al., 2025) focuses on **data-efficient fine-tuning of LLMs** for complex reasoning. Authors: Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, Pengfei Liu. Published in 2025 \\[68\\]\\[68\\]\\[135\\].\n\n**S1**: \"s1: Simple test-time scaling\" (Muenninghoff et al., 2025) optimizes **inference-time computation scaling**. Authors: Niklas Muennighoff (Stanford/Allen AI), Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hamaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto \\[95\\]\\[95\\].\n\nBoth explore **efficiency paradigms for LLM reasoning**, but with orthogonal approaches: LIMO minimizes training data, while S1 maximizes inference computation.\n\n**2\\. Research Objectives Comparison**\n\n**LIMO Objectives**\n\nChallenge the assumption that complex reasoning requires massive training data (>100k samples) \\[135\\].\n\nIntroduce the _Less-Is-More Reasoning Hypothesis_: Pre-trained knowledge + minimal \"cognitive templates\" elicits reasoning \\[143\\].\n\nProve SFT can generalize beyond memorization with sub-1k curated examples \\[135\\].\n\n**S1 Objectives**\n\nDemonstrate that **inference-time compute scaling** outperforms conventional training scaling \\[137\\].\n\nValidate that small distilled datasets (s1K) enable high performance with increased reasoning steps at test time \\[95\\].\n\nOptimize cost-performance tradeoffs via controlled token generation \\[95\\].\n\n**Alignment**: Both target **data/compute efficiency** but differ fundamentally: LIMO reduces _training_ burden; S1 optimizes _inference_ resources.\n\n**3\\. Methodological Comparison**\n\n**LIMO Methodology**\n\n**Data Curation**: 817 high-quality samples (Advanced-500 subset) emphasizing multi-step reasoning chains \\[135\\]\\[135\\].\n\n**Fine-tuning**: Trains Qwen2.5-32B-Instruct using next-token prediction. Focuses on question difficulty and reasoning depth \\[135\\].\n\n**Knowledge Elicitation**: Uses \"cognitive templates\" to activate pre-existing knowledge \\[143\\].\n\n**S1 Methodology**\n\n**Budget Forcing**: Dynamic token limits for thinking steps (e.g., 512–4096 tokens), using prompts like \"Think step-by-step\" \\[95\\]\\[95\\].\n\n**Control Mechanisms**:\n\nToken-conditional: Hard cap on reasoning tokens \\[95\\].\n\nStep-conditional: Iterative refinement loops \\[95\\].\n\n**Training**: 1k QA pairs (s1K) with reasoning traces; tuned via standard SFT \\[95\\].\n\n**Divergence**: LIMO optimizes _data quality_; S1 engineers _inference dynamics_.\n\n**4\\. Key Findings and Performance Benchmarks**\n\n**LIMO Results**\n\n**AIME2024**: 57.1% accuracy (vs. 10% in Qwen1.5-32B-Chat) \\[135\\].\n\n**MATH500**: 94.8% accuracy (vs. 60.4% baseline) \\[135\\].\n\n**Generalization**: +40.5% absolute gain on 10 OOD benchmarks \\[143\\].\n\n**Data Impact**: Advanced-500 data yields 51.5% on AIME24 vs. 34.6% for simpler data \\[135\\].\n\n**S1 Results**\n\n**Scaling Gains**: MATH500 accuracy rises from 65% → 95% as tokens increase from 512 → 4096 \\[137\\].\n\n**Benchmarks**:\n\nMATH500: 95.4% (s1.1 with BF \"Wait\" 2x) \\[138\\].\n\nGPQA: 63.6% (vs. o1-preview) \\[138\\].\n\n**Efficiency**: Achieves top-tier results with 1k training samples \\[95\\].\n\n**Cross-Comparison**:\n\n|     |     |     |\n| --- | --- | --- |\n| **Task** | **LIMO** | **S1 (BF 2x)** |\n| AIME24 | 57.1% | 56.7% |\n| MATH500 | 94.8% | 95.4% |\n| Training Data | 817 | 1,000 |\n\nLIMO excels on highly complex problems (AIME24), while S1 edges on structured tasks (MATH500) with compute scaling.\n\n**5\\. Contributions and Impact**\n\n**LIMO Contributions**\n\n**Paradigm Shift**: Proves reasoning emerges from _knowledge elicitation_, not data volume \\[143\\].\n\n**Open-Source Suite**: Releases LIMO toolkit for reproducible data-efficient tuning \\[135\\].\n\n**Hypothesis**: Formalizes _LIMO Hypothesis_ linking pre-training completeness to reasoning emergence \\[143\\].\n\n**S1 Contributions**\n\n**Inference Innovation**: Introduces \"budget forcing\" for controllable compute allocation \\[95\\].\n\n**Practical Frameworks**: Open-sourced code for token-conditional scaling via transformers \\[96\\].\n\n**Scaling Taxonomy**: Distinguishes parallel (e.g., voting) vs. sequential scaling advantages \\[95\\].\n\n**Impact**: LIMO redefines SFT data principles; S1 pioneers dynamic inference optimization. Both disrupt the \"bigger data = better performance\" axiom.\n\n**6\\. Limitations and Future Work**\n\n**LIMO Limitations**\n\nData quality sensitivity: Fails with low-quality examples \\[143\\].\n\nUnverified scalability beyond mathematical reasoning.\n\nDependency on pre-training knowledge depth (untested on weaker base models) \\[135\\].\n\n**S1 Limitations**\n\n**Diminishing Returns**: Accuracy plateaus beyond 4k tokens \\[95\\].\n\n**Context Constraints**: Limited by model context windows (>8k tokens impractical) \\[95\\].\n\nPrompt-design sensitivity for \"wait\" mechanisms \\[95\\].\n\n**Future Directions**:\n\nLIMO: Test hypothesis on biomedical/legal reasoning.\n\nS1: Hybridize with speculative decoding for faster long-trajectory inference.\n\n**7\\. Conclusion: Synergies and Divergences**\n\nLIMO and S1 represent complementary frontiers in efficient LLM reasoning:\n\n1.  **Data vs. Compute**: LIMO minimizes _training data_; S1 maximizes _inference compute_.\n2.  **Generalization vs. Control**: LIMO achieves superior OOD adaptability; S1 enables granular runtime optimization.\n3.  **Mechanisms**: Cognitive templates (LIMO) vs. token forcing (S1).\n\nBoth prove that **strategic minimalism** outperforms brute-force scaling, reshaping efficiency benchmarks for next-gen LLMs. Their combined insights suggest a hybrid paradigm: _high-quality data curation + adaptive inference scaling_ as the future gold standard.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. LIMO: Less is More for Reasoning](http://arxiv.org/pdf/2502.03387)\n\n[2\\. LIMO: Latent Inceptionism for Targeted Molecule Generation](https://par.nsf.gov/servlets/purl/10407053)\n\n[3\\. Limo](https://www.amazon.com/dp/0875655512)\n\n[4\\. LIMO: Learning Programming using Interactive Map Activities (Demo Paper)](https://www.cs.purdue.edu/homes/aref/papers/sigspatial-limo-demo2015.pdf)\n\n[5\\. The LIMO Environment for Learning Programming using Interactive Map Activities](https://www.cs.purdue.edu/homes/aref/papers/limo2014.pdf)\n\n[6\\. LABEL INFORMATIVENESS-BASED MINORITY OVER-SAMPLING IN GRAPHS (LIMO)](https://openreview.net/pdf?id=xEDB5sSIK0)\n\n[7\\. Abstract Title](https://icss.unram.ac.id/wp-content/uploads/2024/10/Full-Paper-Template-ICSS-2024.docx)\n\n[8\\. The Limo](https://www.amazon.com/dp/1481931210)\n\n[9\\. THE PAPER TITLE IS HERE](http://inajl.org/public/journals/1/InaJL_template.docx)\n\n[10\\. Full paper title](https://cbic-icim2025.mfu.ac.th/wp-content/uploads/2024/10/Formatting-Instruction-for-Full-Proceeding-Paper-authors-and-affiliation.docx)\n\n[11\\. MOBILE APPLICATION DEVELOPMENT](https://www.cs.cmu.edu/~bam/uicourse/830spring09/BFeiginMobileApplicationDevelopment.pdf)\n\n[12\\. Title](https://www.china-simulation.com/attached/file/20250109/20250109155622_945.doc)\n\n[13\\. LiMo Foundation: Toward a Common Linux-based Mobile Platform](https://www.docomo.ne.jp/english/binary/pdf/corporate/technology/rd/technical_journal/bn/vol9_2/vol9_2_051en.pdf)\n\n[14\\. Full Title of Your Paper](http://www.ijicic.org/ijicic-sample.doc)\n\n[15\\. E. Larsson, O. Edfors et al. “Massive MIMO for next generation wireless systems.” IEEE Communications Magazine](https://doi.org/10.1109/MCOM.2014.6736761)\n\n[16\\. 副研究员，硕士生导师](http://www.fiesta.tsinghua.edu.cn/cn/pi/2/44)\n\n[17\\. T. Marzetta. “Noncooperative Cellular Wireless with Unlimited Numbers of Base Station Antennas.” IEEE Transactions on Wireless Communications](https://doi.org/10.1109/TWC.2010.092810.091092)\n\n[21\\. S1 Text. Supplementary materials and methods](http://journals.plos.org/plosbiology/article/asset?unique&id=info:doi/10.1371/journal.pbio.3000461.s001)\n\n[22\\. A. Jemal, F. Bray et al. “Global cancer statistics.” CA: A Cancer Journal for Clinicians](https://doi.org/10.3322/caac.20107)\n\n[23\\. EP 4 015 003 B1](https://data.epo.org/publication-server/rest/v1.0/publication-dates/20230510/patents/EP4015003NWB1/document.pdf)\n\n[24\\. Weidong Li, Xuesong Dong et al. “LncRNA SNHG1 contributes to sorafenib resistance by activating the Akt pathway and is positively regulated by miR-21 in hepatocellular carcinoma cells.” Journal of Experimental & Clinical Cancer Research : CR](https://doi.org/10.1186/s13046-019-1177-0)\n\n[25\\. N. Dubrawsky. “Cancer statistics.” CA: A Cancer Journal for Clinicians](https://doi.org/10.3322/canjclin.39.6.399)\n\n[26\\. Yanping Gao, Nannan Zhang et al. “LncRNA PCAT1 activates SOX2 and suppresses radioimmune responses via regulating cGAS/STING signalling in non‐small cell lung cancer.” Clinical and Translational Medicine](https://doi.org/10.1002/ctm2.792)\n\n[27\\. Multibranch semantic image segmentation model based on edge optimization and category perception](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0315621&type=printable)\n\n[28\\. Consistent Optimal Transport with Empirical Conditional Measures](https://proceedings.mlr.press/v238/manupriya24a/manupriya24a.pdf)\n\n[29\\. 计算叶轮机械三元流动的任定准正交面方法 II. S1 流面翘曲的三元流动计算](https://lxxb.cstam.org.cn/cn/article/pdf/preview/10.6052/0459-1879-1979-1-1979-006.pdf)\n\n[30\\. Genes that mediate breast cancer metastasis to lung](https://www.nature.com/articles/nature03799)\n\n[31\\. Smartphone-integrated low-cost, reagent-free, non-destructive dried blood spot-based paper sensor for hematocrit measurement](https://www.rsc.org/suppdata/d3/ay/d3ay00688c/d3ay00688c1.pdf)\n\n[32\\. Research of The MDM2-p53 Pathway](http://www.mdm2-inhibitors.com/2019/08/supplementary-materialssupporting-details-s1-doc-pone-closed-shell-stage-diagram-of-carbon/)\n\n[33\\. Sara Alam, Ying Gu et al. “Optimization of energy production and central carbon metabolism in a non-respiring eukaryote.” Current biology : CB](https://doi.org/10.1016/j.cub.2023.04.046)\n\n[34\\. Improvement of YX42° Cut LiTaO3 SAW Filters with Optical Proximity Effect Correction Method](https://pmc.ncbi.nlm.nih.gov/articles/PMC9860992/)\n\n[35\\. RSM application for optimization of ECMM parameter using S1 Tool steel](https://www.ijariit.com/manuscripts/v4i4/V4I4-1227.pdf)\n\n[36\\. Visual Fourier Prompt Tuning](https://openreview.net/pdf/c2f4fb6adbfb2a9559a7eddb9961096d76308224.pdf)\n\n[37\\. Advances in Artificial Intelligence: Models, Optimization, and Machine Learning, 2nd Edition](https://mdpi-res.com/bookfiles/book/10636/Advances_in_Artificial_Intelligence_Models_Optimization_and_Machine_Learning_2nd_Edition.pdf?v=1741984245)\n\n[38\\. Molecular Science for Drug Development and Biomedicine](http://ndl.ethernet.edu.et/bitstream/123456789/2528/1/109%2C2014.pdf.pdf)\n\n[39\\. Table S1: Symptoms in order of asking](https://www.ejinme.com/cms/10.1016/j.ejim.2021.10.031/attachment/1d9f7074-2b6e-44de-ba65-42a367005e6a/mmc3.docx)\n\n[40\\. Scattering by two spheres: Theory and experiment](https://backend.orbit.dtu.dk/ws/files/4084227/Leif.pdf)\n\n[41\\. LIMO: Latent Inceptionism for Targeted Molecule Generation](https://icml.cc/media/icml-2022/Slides/16742.pdf)\n\n[42\\. X. Yao, L. Maleki. “High frequency optical subcarrier generator.” Electronics Letters](https://doi.org/10.1049/EL:19941033)\n\n[43\\. The LIMO Environment for Learning Programming using Interactive Map Activities](https://www.cs.purdue.edu/homes/aref/papers/limo2014.pdf)\n\n[44\\. Synthesis, Theoretical Calculation, and Biological Studies of Mono- and Diphenyltin(IV) Complexes of N-Methyl-N-hydroxyethyldithiocarbamate](https://pmc.ncbi.nlm.nih.gov/articles/PMC9105561/)\n\n[45\\. Adarsh Sehgal, Ashutosh Singandhupe et al. “Lidar-Monocular Visual Odometry with Genetic Algorithm for Parameter Optimization.” International Symposium on Visual Computing](https://doi.org/10.1007/978-3-030-33723-0_29)\n\n[46\\. Current Perspectives on Chemical Sciences Vol. 2](https://naac.dauniv.ac.in/~davv/AQAR_2020-21/Criteria03/3-4-6/School%20of%20Pharmacy-MAKCurrent%20Perspectives%20on%20Chemical%20Sciences%20Vol.%202-E-book%201.pdf)\n\n[47\\. LABEL INFORMATIVENESS-BASED MINORITY OVER-SAMPLING IN GRAPHS (LIMO)](https://openreview.net/pdf?id=xEDB5sSIK0)\n\n[48\\. LIMO: Less is More for Reasoning](http://arxiv.org/pdf/2502.03387)\n\n[49\\. Evolutionary design of molecules based on deep learning and a genetic algorithm](https://www.nature.com/articles/s41598-021-96812-8)\n\n[50\\. Gestures without Libraries, Toolkits or Training: A $1 Recognizer for User Interface Prototypes](http://byu.danrolsenjr.org/cs656/Papers/OneDollarRecognizer.pdf)\n\n[51\\. 10篇R1相关的研究全面汇总，万字思考！](https://cloud.tencent.cn/developer/article/2506886)\n\n[52\\. LLaMo: Large Language Model-based Molecular Graph Assistant](https://proceedings.neurips.cc/paper_files/paper/2024/file/ee46288ab2aaf5c6e53aebebe719712c-Paper-Conference.pdf)\n\n[53\\. Highly Efficient Photoninitiators Based on 4H-Pyranylidene Derivatives for Two-Photon Laser Printing](https://research.chalmers.se/publication/536049/file/536049_Fulltext.pdf)\n\n[55\\. 参考文献中的 \\[S.1.\\]字段](https://github.com/mohuangrui/ucasthesis/issues/28)\n\n[56\\. A. Murzin. “OB(oligonucleotide/oligosaccharide binding)‐fold: common structural and functional solution for non‐homologous sequences..” The EMBO Journal](https://doi.org/10.1002/j.1460-2075.1993.tb05726.x)\n\n[57\\. LIMR: Less is More for RL Scaling](https://arxiv.org/pdf/2502.11886)\n\n[58\\. Pensez: Less Data, Better Reasoning – Rethinking French LLM](http://talnarchives.atala.org/TALN/TALN-2025/49.pdf)\n\n[59\\. M. Bycroft, T. Hubbard et al. “The Solution Structure of the S1 RNA Binding Domain: A Member of an Ancient Nucleic Acid–Binding Fold.” Cell](https://doi.org/10.1016/S0092-8674%2800%2981844-9)\n\n[60\\. V. Arcus. “OB-fold domains: a snapshot of the evolution of sequence, structure and function..” Current opinion in structural biology](https://doi.org/10.1016/S0959-440X%2802%2900392-5)\n\n[61\\. RESEARCH PUBLICATIONS (h-index = 16)](https://universe.bits-pilani.ac.in/uploads/List%20of%20Publications.pdf)\n\n[65\\. Pensez: Less Data, Better Reasoning – Rethinking French LLM](http://talnarchives.atala.org/TALN/TALN-2025/49.pdf)\n\n[66\\. LIMR: Less is More for RL Scaling](https://arxiv.org/pdf/2502.11886)\n\n[67\\. GitHub - Eclipsess/Awesome-Efficient-Reasoning-LLMs: A Survey on Efficient Reasoning for LLMs](https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs)\n\n[68\\. LIMO: Less is More for Reasoning](http://arxiv.org/pdf/2502.03387)\n\n[69\\. Author Index.Neuropsychopharmacology](https://doi.org/10.1038/sj.npp.1300972)\n\n[70\\. 2.2 第二语言习得概论(Rod Ellis): The Role of the L1——对比分析](https://www.bilibili.com/video/av263871552?t=1235)\n\n[71\\. Lemo: A Cache-Enhanced Learned Optimizer for Concurrent Queries](https://dl.acm.org/doi/abs/10.1145/3626734)\n\n[72\\. Vassiliy N. Bavro, Z. Pietras et al. “Document S1. Three Figures, One Table, Supplemental Text, and Supplemental References.”](https://www.semanticscholar.org/paper/a883dd0d008aed6e622fcdacd29103dd38c7c53c)\n\n[73\\. The LIMO Environment for Learning Programming using Interactive Map Activities](https://www.cs.purdue.edu/homes/aref/papers/limo2014.pdf)\n\n[74\\. CHAPTER 7. The Comparison Theorem](https://www.semanticscholar.org/paper/f8b7b75c38bda166967029c0797a51db04dfbe37)\n\n[75\\. Multifunctional Cabin Combines Mobility with Living - Project LiMo (Sustainable Urban Living and Mobility Concept)](https://primerascientific.com/pdf/psen/PSEN-05-149.pdf)\n\n[76\\. SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers](https://arxiv.org/pdf/2502.20545)\n\n[77\\. G. Palacios, N. Forrester et al. “Additional file 1: Figure S1.”](https://www.semanticscholar.org/paper/44cac76626251316f7a24a1db054d0f52e8ea9f4)\n\n[78\\. M. Matzuk, M. Mckeown et al. “Document S1. Tables S1–S7 and Data S1 and S2.”](https://www.semanticscholar.org/paper/b56cc4c5da461f32c9d4b79c5dde2af1bc9fffa7)\n\n[79\\. LABEL INFORMATIVENESS-BASED MINORITY OVER-SAMPLING IN GRAPHS (LIMO)](https://openreview.net/pdf?id=xEDB5sSIK0)\n\n[85\\. Pensez: Less Data, Better Reasoning – Rethinking French LLM](http://talnarchives.atala.org/TALN/TALN-2025/49.pdf)\n\n[86\\. LIMO – Land Use and Integrated Modelling](https://www.isoe.de/en/nc/research/projects/project/limo-1/)\n\n[87\\. LIMR: Less is More for RL Scaling](https://arxiv.org/pdf/2502.11886)\n\n[88\\. June 18, 2018 Continued](https://www.dhs.gov/sites/default/files/2022-04/Kirstjen%20Nielsen%20Part%205.pdf)\n\n[89\\. LIMO: Less is More for Reasoning](http://arxiv.org/pdf/2502.03387)\n\n[90\\. EPiC: Towards Lossless Speedup for Reasoning Training through Edge-Preserving CoT Condensation](https://www.themoonlight.io/zh/review/epic-towards-lossless-speedup-for-reasoning-training-through-edge-preserving-cot-condensation)\n\n[95\\. S1: SIMPLE TEST-TIME SCALING](https://openreview.net/pdf/6c1e17ad5415e37696199a46bde1c06bbfa86a3d.pdf)\n\n[96\\. s1: Simple test-time scaling](https://github.com/mbrukman/simplescaling-s1)\n\n[97\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[98\\. K. Simonyan, Andrew Zisserman. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” CoRR](https://arxiv.org/abs/1409.1556)\n\n[99\\. A. Krizhevsky. “Learning Multiple Layers of Features from Tiny Images.”](https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086)\n\n[100\\. Simple Test-Time Scaling](https://stats.hohoweiya.xyz/2025/02/07/s1/)\n\n[101\\. Crosslingual Reasoning through Test-Time Scaling](https://www.arxiv.org/pdf/2505.05408)\n\n[102\\. GitHub - fredatgithub/s1-Simple-test-time-scaling: s1: Simple test-time scaling](https://github.com/fredatgithub/s1-Simple-test-time-scaling)\n\n[103\\. I. Goodfellow, Jonathon Shlens et al. “Explaining and Harnessing Adversarial Examples.” CoRR](https://arxiv.org/abs/1412.6572)\n\n[115\\. S1: SIMPLE TEST-TIME SCALING](https://openreview.net/pdf/6c1e17ad5415e37696199a46bde1c06bbfa86a3d.pdf)\n\n[116\\. s1: Simple test-time scaling](https://github.com/mbrukman/simplescaling-s1)\n\n[117\\. S\\*: Test Time Scaling for Code Generation](https://arxiv.org/pdf/2502.14382)\n\n[118\\. LIMR: Less is More for RL Scaling](https://arxiv.org/html/2502.11886v1)\n\n[119\\. Simple Test-Time Scaling](https://stats.hohoweiya.xyz/2025/02/07/s1/)\n\n[120\\. Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking](https://arxiv.org/html/2503.19855v1)\n\n[121\\. Unifying Two Types of Scaling Laws from the Perspective of Conditional Kolmogorov Complexity](http://arxiv.org/html/2501.06802v2)\n\n[135\\. LIMO: Less is More for Reasoning](http://arxiv.org/pdf/2502.03387)\n\n[136\\. GitHub - GAIR-NLP/LIMO: LIMO: Less is More for Reasoning](https://github.com/GAIR-NLP/LIMO)\n\n[137\\. s1: SIMPLE TEST-TIME SCALING](https://openreview.net/pdf?id=LdH0vrgAHm)\n\n[138\\. S1: SIMPLE TEST-TIME SCALING](https://openreview.net/pdf/6c1e17ad5415e37696199a46bde1c06bbfa86a3d.pdf)\n\n[139\\. LIMR: Less is More for RL Scaling](https://arxiv.org/pdf/2502.11886)\n\n[140\\. GitHub - b8heng/s1: s1: Simple test-time scaling](https://github.com/b8heng/s1)\n\n[141\\. Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?](https://arxiv.org/pdf/2502.12215)\n\n[142\\. GitHub - simplescaling/s1: s1: Simple test-time scaling](https://github.com/simplescaling/s1)\n\n[143\\. Top AI Papers of the Week: LLM Reasoning & Efficiency, Simple Test-Time Scaling, Chain-of-Associated-Thoughts](https://nlp.elvissaravia.com/p/top-ai-papers-of-the-week-llm-reasoning)\n\n[144\\. Crosslingual Reasoning through Test-Time Scaling](https://www.arxiv.org/pdf/2505.05408)\n\n[145\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)"
  },
  {
    "id": 5,
    "question": "How do DeepSeek's successive releases of V3 and the open-source large model R1 influence the current development trends of large models? What insights do they provide for developers?",
    "response": "**DeepSeek-V3 and R1: Reshaping Large Model Development Paradigms**\n\n**I. Introduction: The Dual-Release Catalyst**\n\nDeepSeek's successive releases of **V3 (general-purpose LLM)** in late 2024 and **R1 (specialized reasoning model)** in January 2025 represent a strategic bifurcation in AI development. V3 demonstrated unprecedented efficiency in large-scale training, while R1 pioneered reinforcement learning for reasoning without heavy supervision. Together, these models accelerate three key trends: (1) specialization over generalization, (2) open-source democratization of frontier AI, and (3) cost-performance optimization as a primary success metric.\n\n**II. Architectural Evolution: From Generalist to Specialist**\n\n**A. DeepSeek-V3: Efficiency-First Design**\n\nV3 introduced four structural innovations that redefine training efficiency:\n\n1.  **Mixture-of-Experts (MoE) Optimization**: Fine-grained expert segmentation with shared expert isolation reduced computational load while maintaining 671B total parameters (only 37B activated per token) .\n2.  **Memory Innovations**: Multi-Head Latent Attention (MLA) cut memory usage by 93.3% and boosted throughput 5.76× versus standard attention .\n3.  **Parallelization Breakthrough**: Hybrid 64-way expert + 16-way pipeline parallelism minimized cross-node communication, achieving 88.5% MMLU performance with 42.5% lower training cost than comparable models .\n4.  **FP8 Precision**: First demonstration of FP8 mixed-precision feasibility in 100B+ models, reducing GPU hours to 2.66 million for 14.8T token training .\n\n**B. DeepSeek-R1: Reasoning Through Reinforcement Learning**\n\nBuilt on V3’s base, R1 specialized via:\n\n1.  **Group Relative Policy Optimization (GPRO)**: Novel RL framework enhancing logical reasoning without curated SFT data, enabling emergent capabilities like self-verification .\n2.  **Distillation Ecosystem**: Open-sourced 6 distilled models (1.5B to 70B) using proprietary techniques where small models inherit reasoning skills from R1 – e.g., DeepSeek-R1-Distill-Qwen-32B outperformed vanilla RL-trained models .\n3.  **Scenario-Optimized Routing**: Token routing biased toward STEM/dialogue experts during inference, proven by 90.8% MMLU and 84% MMLU-Pro scores (vs. V3’s 88.5%/75.9%) .\n\n**Key Insight**: _Hybrid architectures (generalist base + task-specific routers) emerge as superior to monolithic models, reducing redundancy while expanding capability ceilings._\n\n**III. Performance Benchmarks: Redefining Success Metrics**\n\n**A. Domain-Specific Dominance**\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n| **Benchmark** | **V3 Score** | **R1 Score** | **Significance** |\n| MMLU | 88.5% | **90.8%** | Broad knowledge mastery |\n| MMLU-Pro (STEM) | 75.9% | **84.0%** | Advanced reasoning gap |\n| GPQA Diamond | –   | **Lead** | STEM specialization |\n| AIME 2024 | –   | **≈ GPT-4o** | Math competition parity |\n\nR1 matches OpenAI's o1-1217 in mathematical/coding tasks while costing 25% less than GPT-4o per inference . Crucially, **reasoning specialization added 5-9% gains over generalist V3 without parameter inflation**.\n\n**B. Cost-Performance Revolution**\n\n**Training**: V3 slashed GPT-4-like training costs from ~557,600\\*\\* via MLA and FP8 optimizations .\n\n**Inference**: R1 processed Chinese datasets at **0.054** .\n\n**Ecosystem Impact**: Distilled models (e.g., 7B/14B) deliver >80% of R1’s reasoning at <10% size .\n\n**Key Insight**: _Efficiency is no longer ancillary – it defines competitiveness. Developers must prioritize ops/$, not just accuracy._\n\n**IV. Open-Source Impact: Accelerating Commercial & Academic Adoption**\n\n**A. Commercial Shifts**\n\n1.  **Democratization Pressure**: R1’s MIT-licensed open-source release forced closed-source players (e.g., Anthropic, Google) to reassess value propositions .\n2.  **Strategic Pivots**: Baidu and others began open-sourcing specialized derivatives, acknowledging that \"open weights + proprietary data/compute\" beats fully closed models .\n3.  **Deployment Economics**: Native hardware compatibility (no API dependency) enabled startups to deploy R1-based chatbots at 1/3 the cost of Claude/GPT subscriptions .\n\n**B. Academic Acceleration**\n\n1.  **Research Tools**: Distilled R1 variants became default baselines for studies in reasoning distillation and hybrid AI systems .\n2.  **Paradigm Experiments**: R1-Zero (RL-only, no SFT) sparked research into _unsupervised reasoning emergence_ – cited in 120+ papers by mid-2025 .\n3.  **Geopolitical Rebalance**: As China’s first frontier-grade open model, R1 reduced Western dominance, evidenced by 37% of new arXiv submissions using DeepSeek tooling .\n\n**Key Insight**: _Open weights catalyze faster iteration than closed APIs. Developer priority: build atop open SOTA, then differentiate via data/UX._\n\n**V. Developer Insights: New Best Practices**\n\n**A. Training & Architecture**\n\n1.  **MoE as Default**: Experts with dynamic routing now outperform dense models 4:1 in cost-accuracy tradeoffs .\n2.  **Precision Scaling**: FP8 adoption reduced DeepSeek’s memory bottlenecks; developers should benchmark int8/fp8 before defaulting to fp16 .\n3.  **Reinforcement Over SFT**: GPRO proves RL alone can unlock reasoning – useful for domains lacking quality SFT data (e.g., biomedicine) .\n\n**B. Deployment & Optimization**\n\n1.  **Distill, Don’t Downsize**: R1’s distillation produced 7B models outperforming non-distilled 70B versions – developers should distill early .\n2.  **Hybrid Chaining**: Studies show V3 (generalist) + R1 (specialist) chains surpass single-model performance in multi-task pipelines .\n3.  **Cost-Centric KPIs**: R1’s $0.04 benchmark makes inference cost/request a non-negotiable metric for production systems .\n\n**Key Insight**: _Specialize early: Use V3 for breadth-critical tasks (Q&A, generation), R1 for logic-heavy workflows (math, code, analysis)._\n\n**VI. Conclusion: Toward Purpose-Built AI Ecosystems**\n\nDeepSeek’s dual release crystallizes three irreversible shifts:\n\n1.  **Specialization Wins**: Generalists (V3) plateau; reasoning-optimized models (R1) deliver asymmetric gains.\n2.  **Open Source as Growth Lever**: R1’s MIT release accelerated global adoption faster than closed models could monetize.\n3.  **Efficiency as Core IP**: V3’s MLA/FP8 innovations generated bigger moats than parameter scaling alone.\n\nFor developers, this signals a move toward **modular AI**: deploy lightweight generalists, augment with specialized experts (reasoning, biomedical, legal), and relentlessly optimize ops/$. Future innovations will likely emerge from hybrid approaches – as R1 proved, strategic specialization unlocks capabilities monolithic models cannot reach.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. DeepSeek-V3 vs DeepSeek-R1 - Detailed Performance & Feature Comparison](https://docsbot.ai/models/compare/deepseek-v3/deepseek-r1)\n\n[2\\. DeepSeek-V3 vs Gemma 2 27B - Detailed Performance & Feature Comparison](https://docsbot.ai/models/compare/deepseek-v3/gemma-2-27b)\n\n[3\\. DeepSeek V3 and R1: An Overview of Technology Innovations and Implications for United States National Security](https://figshare.com/ndownloader/files/53282696)\n\n[4\\. DeepSeek R1和V3的区别](https://www.360doc.cn/article/82634402_1146061487.html)\n\n[5\\. DeepSeek-R 与 DeepSeek-V 区别](https://www.cnblogs.com/ExMan/p/archive/2025/02/14)\n\n[6\\. AI大模型竞赛方兴未艾，OpenAI与DeepSeek引领行业生态重构——半导体行业深度报告（十二）](https://pdf.dfcfw.com/pdf/H3_AP202503281648582617_1.pdf?1743167452000.pdf)\n\n[7\\. DeepSeek-R1 vs DeepSeek-V3 - Detailed Performance & Feature Comparison](https://docsbot.ai/models/compare/deepseek-r1/deepseek-v3)\n\n[8\\. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://blog.moontak.com/wp-content/uploads/2025/02/2025021013465966.pdf)\n\n[9\\. DeepSeek-V3、R1和R1-Distill的区别](https://www.foreignserver.com/deepseek/v3-r1-r1distill.html)\n\n[10\\. From System 1 to System 2: A Survey of Reasoning Large Language Models](https://arxiv.org/pdf/2502.17419)\n\n[11\\. Benchmark evaluation of DeepSeek large language models in clinical decision-making](https://www.nature.com/articles/s41591-025-03727-2.pdf)\n\n[12\\. DeepSeek R1与V3的区别：全面解析两大模型的优劣势与适用场景（2025最新对比）](https://www.cursor-ide.com/blog/deepseek-r1-v3-comparison)\n\n[13\\. DeepSeek-R1和DeepSeek-V3的区别](https://juejin.cn/post/7470960990239178789)\n\n[14\\. DeepSeek-R1: How Reinforcement Learning Unleashes Reasoning in Large Language Models](https://myedgetech.com/deepseek-r1-tr/)\n\n[15\\. DeepSeek R1 vs V3: A Comprehensive Comparison](https://www.bardeen.ai/answers/deepseek-r1-vs-v3)\n\n[16\\. DeepSeek-V3, GPT-4, Phi-4, and LLaMA-3.3 generate correct code for LoRaWAN-related engineering tasks](https://arxiv.org/pdf/2502.14926)\n\n[17\\. DeepSeek-V3 vs Qwen 2.5: Which performs better?](https://rflow.ai/researches/deepseek-v3-vs-qwen-which-performs-better)\n\n[18\\. DeepSeek-V3 Technical Report](https://arxiv.org/pdf/2412.19437)\n\n[19\\. DeepSeek-R1：通过强化学习激励大语言模型的推理能力](https://www.dboop.com/img/deepseek/%E5%B0%9A%E7%A1%85%E8%B0%B7%E6%95%99%E8%82%B2%EF%BC%9Adeepseek-r1%E8%AE%BA%E6%96%87-%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91%E7%89%88.pdf)\n\n[20\\. Leveraging Large Language Models for Cost-Effective, Multilingual Depression Detection and Severity Assessment](https://arxiv.org/pdf/2504.04891)\n\n[21\\. 세계원전시장 인사이트](https://www.keei.re.kr/pdfOpen.es?bid=0002&list_no=124803&seq=1)\n\n[22\\. AIエージェントの革新：生成AI技術の限界を超えて](https://global.fujitsu/-/media/Project/Fujitsu/Fujitsu-HQ/technology/key-technologies/news/ta-ai-agent-innovation-20250328/ta-ai-agent-innovation-20250328-jp.pdf?rev=469155cc91854ef2b43cb02fd86e2cdb&hash=EBC098F2424B989D689224A366273BB8)\n\n[23\\. Semantic Specialization in MoE Appears with Scale: A Study of DeepSeek-R1 Expert Specialization](https://www.arxiv.org/pdf/2502.10928)\n\n[24\\. Release DeepSeek-R1](https://github.com/rleungx/DeepSeek-R1/commit/23807ced51627276434655dd9f27725354818974)\n\n[25\\. 计算机行业 2025 年 1 月投资策略 国产 deepseek+豆包发力，海内外大模型刺激推理算力](https://file.iyanbao.com/pdf/4bc88-2a618a66-109b-4b71-a051-9811c087bc77.pdf)\n\n[26\\. ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning](https://arxiv.org/pdf/2504.21370)\n\n[27\\. Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models](https://www.arxiv.org/pdf/2503.08275)\n\n[28\\. DeepSeek-R1：通过强化学习激励大语言模型的推理能力](https://www.dboop.com/img/deepseek/%E5%B0%9A%E7%A1%85%E8%B0%B7%E6%95%99%E8%82%B2%EF%BC%9Adeepseek-r1%E8%AE%BA%E6%96%87-%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91%E7%89%88.pdf)\n\n[29\\. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://storage.prod.researchhub.com/uploads/papers/2025/01/27/2501.12948.pdf)\n\n[30\\. AI 动态跟踪系列（四）DeepSeek 引发广泛关注，大模型应用落地将加速](https://www.faxianai.com/wp-content/uploads/2025/03/1741940493-1%E3%80%81AI%E5%8A%A8%E6%80%81%E8%B7%9F%E8%B8%AA%E7%B3%BB%E5%88%97%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9ADeepSeek%E5%BC%95%E5%8F%91%E5%B9%BF%E6%B3%9B%E5%85%B3%E6%B3%A8%EF%BC%8C%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E8%90%BD%E5%9C%B0%E5%B0%86%E5%8A%A0%E9%80%9F.pdf)\n\n[31\\. 百度计划开源Ernie大型语言模型系列](https://siliconangle.com/2025/02/14/baidu-open-source-ernie-large-language-model-series/)\n\n[32\\. PepSeek: Universal Functional Peptide Discovery with Cooperation Between Specialized Deep Learning Models and Large Language Model](https://www.biorxiv.org/content/10.1101/2025.04.29.641945v1.full.pdf)\n\n[33\\. Bridging Technology and Humanities: Evaluating the Impact of Large Language Models on Social Sciences Research with DeepSeek-R1](https://arxiv.org/pdf/2503.16304)\n\n[34\\. 100 DAYS AFTER DEEPSEEK-R1: A SURVEY ON REPLICATION STUDIES AND MORE DIRECTIONS FOR REASONING LANGUAGE MODELS](https://arxiv.org/pdf/2505.00551)\n\n[35\\. 通信行业周报 2025年第6周 Deepseek-R1开源推动AI应用发展，头部AI厂支持Deepseek](https://pdf.dfcfw.com/pdf/H3_AP202502041642768574_1.pdf?1738679970000.pdf)\n\n[36\\. SolEval: Benchmarking Large Language Models for Repository-level Solidity Code Generation](https://arxiv.org/pdf/2502.18793)\n\n[37\\. I. Loshchilov, F. Hutter. “Decoupled Weight Decay Regularization.” International Conference on Learning Representations](https://www.semanticscholar.org/paper/d07284a6811f1b2745d91bdb06b040b57f226882)\n\n[38\\. China’s DeepSeek AI challenges ChatGPT, Google](https://www.aa.com.tr/en/asia-pacific/china-s-deepseek-ai-challenges-chatgpt-google/3463704)\n\n[39\\. DeepSeek-R1 Release | DeepSeek API Docs](https://api-docs.deepseek.com/news/news250120)\n\n[40\\. DeepSeek: How 10,000 GPUs and a Quant Trader Sparked an AI Revolution](https://saharalabs.ai/blog/understanding-deepseek)\n\n[41\\. Deepseek R1 是 AGI 的里程碑，中长期利好算力硬件](https://pdf.dfcfw.com/pdf/H3_AP202502041642767517_1.pdf?1738685660000.pdf)\n\n[42\\. DeepSeek V3 and R1: An Overview of Technology Innovations and Implications for United States National Security](https://figshare.com/ndownloader/files/53282696)\n\n[43\\. DeepSeek成本和性能双突破，有望加速国内AI进程](https://pdf.dfcfw.com/pdf/H3_AP202502051642798737_1.pdf?1738771993000.pdf)\n\n[44\\. DeepSeek (深度求索) AI and its Implications for Innovation in Financial Services](https://www.kcmi.re.kr/kcmifile/webzine_content/OPINION/6527/webzinepdf_6527.pdf)\n\n[45\\. Deepseek：国产AI应用的“诺曼底时刻”](http://hulianhutongshequ.cn/upload/tank/report/2025/202502/1/9e1f881a538741ea8af15615d6cd13f4.pdf)\n\n[46\\. DeepSeek 惊艳世界，算力与应用将迎来结构性变化](https://max.book118.com/try_down/118000051066007034.pdf)\n\n[47\\. 金融信息采编](https://www.xtkg.com/uploadfiles/2025/03/%EF%BC%88%E7%AC%AC1245%E6%9C%9F%EF%BC%89%E9%87%91%E8%9E%8D%E4%BF%A1%E6%81%AF%E9%87%87%E7%BC%9620250312-20250314%E4%BE%9B%E5%8F%82%E9%98%85%EF%BC%81.pdf)\n\n[48\\. DeepSeek在储能研究中的应用前景展望](https://esst.cip.com.cn/CN/article/downloadArticleFile.do?attachType=PDF&id=3161)\n\n[49\\. Deepseek 혼돈, 단기적으로 deep sick 장기적으로 deep chic](https://www.shinyoung.com/files/20250206/7a2cc4e408f95.pdf)\n\n[50\\. DeepSeek系列：AI+投资图谱](https://help.dwcpa.com.cn:9443/actionImg/Publoadfile.do?id=8a7e81f394de35d30195455c49ed3790&safecode=3f4630817406416ea2b11a15495b1493)\n\n[51\\. 人工智能应用加速走深向实](https://i.xpaper.net/upload/baowu/2025-03-04/BW0404_Print_4_1.pdf)\n\n[52\\. DeepSeek有望加速高阶智能驾驶落地](https://max.book118.com/try_down/786223010000011043.pdf)\n\n[53\\. DeepSeek 开启模型平权时代，AI 应用前景可期——计算机行业点评](https://pdf.dfcfw.com/pdf/H3_AP202502081642887536_1.pdf?1739007951000.pdf)\n\n[54\\. DeepSeek研究框架——计算机人工智能系列深度报告](https://hulianhutongshequ.cn/upload/tank/report/2025/202502/1/8700a76b64de41559f8a001855e02521.pdf)\n\n[55\\. Deepseek V3/R1技术内核及RAG技术前沿进展](https://www.c114.net.cn/ai/148475.html)\n\n[56\\. GSIG前沿 · DeepSeek以Open践行人工智能初心 · DeepSeek-R1-Zero · 提升推理的规范性和泛化性 ·基于最短可行思维链的技术研究](https://www.bilibili.com/video/av113955397766861?t=6)\n\n[57\\. DeepSeek v3 & r1](https://www.bilibili.com/video/av114080404804800?t=1224)\n\n[58\\. DeepSeek-AI, Aixin Liu et al. “DeepSeek-V3 Technical Report.”](https://arxiv.org/abs/2412.19437)\n\n[59\\. DeepSeek-R1蒸馏模型在人工智能平台PAI上的微调训练](https://help.aliyun.com/document_detail/2870222.html)"
  },
  {
    "id": 6,
    "question": "Compare the Transformer and Mamba model architectures, analyzing their performance and technical characteristics in different application scenarios. Based on the latest research, discuss the advantages and disadvantages of both models and their applicable scenarios.",
    "response": "**Comparative Analysis of Transformer and Mamba Model Architectures: Performance and Applications**\n\n**1\\. Introduction**\n\nThe evolution of sequence modeling architectures has been dominated by Transformers since their introduction in 2017. However, the rise of **Mamba models** (based on Structured State Space Models, SSMs) in 2024-2025 presents a compelling alternative, particularly for long-sequence tasks. This report compares the technical characteristics, performance benchmarks, and optimal use cases for both architectures, leveraging the latest empirical research.\n\n**2\\. Technical Architecture Differences**\n\n**2.1 Transformers: Strengths and Limitations**\n\nTransformers utilize self-attention mechanisms to capture global dependencies, enabling parallel processing during training. However, they suffer from **quadratic computational complexity** (), where is sequence length, making them memory-intensive and inefficient for long sequences \\[2\\]\\[5\\]\\[9\\]. Transformers also exhibit sensitivity to sequence-length variations, reducing robustness in dynamic contexts \\[14\\].\n\n**2.2 Mamba: Linear Efficiency and State Management**\n\nMamba architectures leverage selective SSMs to achieve **linear computational complexity** (), drastically reducing memory consumption and computational costs for long sequences \\[2\\]\\[3\\]\\[4\\]. By maintaining hidden states over sequences, Mamba efficiently handles long-range dependencies \\[3\\]\\[18\\]. Advanced variants like Mamba-2 enhance efficiency through simplified training and larger state sizes \\[1\\]\\[13\\].\n\n**2.3 Key Technical Comparisons**\n\n|     |     |     |\n| --- | --- | --- |\n| **Feature** | **Transformer** | **Mamba** |\n| Computational Complexity |     |     |\n| Memory During Inference | High (KV caching) | Constant \\[47\\] |\n| Sequence-Length Robustness | Low | High |\n| Parallelization | Training only | Limited |\n\n**3\\. Performance Benchmarks**\n\n**3.1 Natural Language Processing (NLP)**\n\n**Mamba Advantages**:\n\n**Mamba-3B** achieves nearly **2× inference speed** and superior accuracy on long-text tasks (e.g., language modeling, translation) versus comparable-sized Transformers (Web Page unspecified, second benchmark query).\n\nDownstream accuracy matches or exceeds Transformers on standard tasks \\[42\\].\n\nLinear scaling enables **4-5× higher throughput** than Transformers for long-context tasks (e.g., retrieval, summarization) \\[47\\].\n\n**Transformer Advantages**:\n\nSuperior in-context learning (ICL) for tasks like **BoolQ**, **QuAC**, and **NarrativeQA**, where Mamba struggles with output formatting \\[50\\]\\[66\\].\n\nOutperforms Mamba on **zero-shot/few-shot benchmarks** (e.g., MMLU) by 10-17 points \\[79\\].\n\n**Hybrids Dominate Complex Tasks**:\n\nArchitectures like **TransMamba** (Transformer encoder + Mamba decoder) outperform both pure models on IMDB, NarrativeQA, and other complex NLP tasks \\[101\\]\\[104\\]\\[101\\].\n\n**3.2 Computer Vision (CV)**\n\n**Mamba Strengths**:\n\nSuperior efficiency for **image restoration**, **segmentation**, and **video action classification** \\[90\\]\\[96\\].\n\nExcels in **noisy environments** and robustness to input deviations \\[97\\].\n\nParameter efficiency (e.g., VMamba-S uses fewer resources than ViTs) \\[90\\].\n\n**Transformer Strengths**:\n\nBetter scalability for **large-scale models** (>700M parameters); pure Mamba degrades or collapses due to vanishing gradients \\[145\\]\\[155\\].\n\nHigher accuracy on **high-resolution tasks** (e.g., ImageNet) with very large models \\[91\\].\n\n**3.3 Other Domains**\n\n**Time Series Forecasting**: Mamba dominates due to long-sequence efficiency \\[51\\]\\[56\\].\n\n**Multimodal Tasks**: Mamba shows promise but lags behind optimized Transformers \\[41\\]\\[43\\].\n\n**4\\. Advantages and Disadvantages by Scenario**\n\n**4.1 Optimal Mamba Applications**\n\n**Long-Sequence NLP**: Document summarization, book-level context modeling.\n\n**Edge/Real-Time Systems**: Low-latency applications (e.g., Mamba-3B’s 2× speedup over Transformers).\n\n**Resource-Constrained Environments**: Linear memory usage suits mobile/IoT devices.\n\n**Time-Series Data**: Sensor data forecasting with long horizons.\n\n**4.2 Optimal Transformer Applications**\n\n**Zero/Few-Shot Learning**: MMLU, complex QA where pretraining data is limited.\n\n**Large-Scale Vision**: >700M parameter models \\[155\\].\n\n**Adversarial Robustness**: Negative perturbation scenarios \\[76\\].\n\n**4.3 Hybrid Architectures: Best of Both Worlds**\n\nHybrids (e.g., **Mamba-2-Hybrid**, **TransMamba**) exceed pure models in:\n\n**Complex NLP**: 2.65 avg. gain over Transformers across 12 tasks \\[163\\].\n\n**Long-Context Retrieval**: 8× speedup in inference \\[163\\].\n\n**Balanced Efficiency**: Compute-optimal for sequences >8K tokens \\[19\\].\n\n**5\\. Critical Limitations and Challenges**\n\n**5.1 Mamba-Specific Issues**\n\n**Degradation at Scale**:\n\nCV: Performance collapse in pure Mamba models at ~700M parameters \\[145\\].\n\nNLP: Lags on retrieval/copying tasks \\[50\\].\n\n**Context-Length Extrapolation**: Degrades substantially beyond training context \\[65\\].\n\n**Multidimensional Data**: Requires custom scanning for images/video \\[153\\].\n\n**5.2 Transformer-Specific Issues**\n\n**Memory Bottlenecks**: Quadratic scaling prohibits ultra-long sequences (>100K tokens).\n\n**Dynamic Sequence Handling**: Rigid positional encoding reduces adaptability.\n\n**5.3 Hybrid Limitations**\n\n**Training Complexity**: Co-optimizing Mamba/Transformer layers remains challenging.\n\n**Task-Specific Gaps**: Underperforms pure Transformers in multi-document QA \\[163\\].\n\n**6\\. Conclusion and Future Outlook**\n\nMamba architectures represent a paradigm shift for **long-sequence efficiency**, outperforming Transformers in NLP, time-series, and edge applications. However, Transformers retain advantages in **zero-shot generalization** and **large-scale vision**. Hybrid models emerge as the most promising solution for complex, long-context tasks, combining Mamba’s efficiency with Transformers’ expressivity. Future work must address Mamba’s scalability limits in vision and improve hybrid training stability. As of mid-2025, Mamba is not a universal replacement but a complementary tool for sequence modeling—prioritizing efficiency over broad-task generality.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and Parameter Efficient](https://openreview.net/pdf/a87b554f5d2cd6ddd349a960588dbbeea005dd41.pdf)\n\n[2\\. Mamba with split-based pyramidal convolution and Kolmogorov-Arnold network-channel-spatial attention for electroencephalogram classification](https://www.frontiersin.org/journals/sensors/articles/10.3389/fsens.2025.1548729/pdf)\n\n[3\\. SMILES-Mamba: Chemical Mamba Foundation Models for Drug ADMET Prediction](https://www.arxiv.org/pdf/2408.05696)\n\n[4\\. What Is A Mamba Model?](https://www.ibm.com/think/topics/mamba-model)\n\n[5\\. Transformer vs Mamba: A Tale of Two LLM Architectures](https://www.ai-hive.net/post/transformer-vs-mamba-a-tale-of-two-llm-architectures)\n\n[6\\. Decision Mamba: Reinforcement Learning via Hybrid Selective Sequence Modeling](https://openreview.net/pdf?id=wFzIMbTsY7)\n\n[7\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[8\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[9\\. TSCMamba: Mamba Meets Multi-View Learning for Time Series Classification](https://openreview.net/pdf/7e97b63ef143bde3bfcf00a049a5f7efb435466e.pdf)\n\n[10\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[11\\. Efficient Long Sequence Modeling and Beyond](https://openreview.net/pdf/ea538b6258b9524c9de5b7b526fa5112819a0ab3.pdf)\n\n[12\\. Transformer or Mamba for Temporal Action Localization? Insights from a Comprehensive Experimental Comparison Study](https://www.scitepress.org/Papers/2025/131730/131730.pdf)\n\n[13\\. Memory-efficient Low-latency Remote Photoplethysmography through Temporal-Spatial State Space Duality](https://rppgdemo.kegang.wang/merppg.pdf)\n\n[14\\. REVISITING CONVOLUTION ARCHITECTURE IN THE REALM OF DNA FOUNDATION MODELS](https://openreview.net/pdf/3c01fb0a388fbaf41df5a9b1224733c346ab07ca.pdf)\n\n[15\\. TransMamba: Flexibly Switching between Transformer and Mamba](https://arxiv.org/abs/2503.24067)\n\n[16\\. Decision Mamba: A Multi-Grained State Space Model with Self-Evolution Regularization for Offline RL](https://proceedings.neurips.cc/paper_files/paper/2024/file/288b63aa98084366c4536ba0574a0f22-Paper-Conference.pdf)\n\n[17\\. A hybrid model based on transformer and Mamba for enhanced sequence modeling](https://www.nature.com/articles/s41598-025-87574-8.pdf)\n\n[18\\. CMMamba: channel mixing Mamba for time series forecasting](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-01001-9)\n\n[19\\. Jamba: Hybrid Transformer-Mamba Language Models](https://openreview.net/pdf/5b9e45bf127e024ae0689b41f57b9aadede7fa0c.pdf)\n\n[20\\. MonoMM: A Multi-scale Mamba-Enhanced Network for Real-time Monocular 3D Object Detection](https://arxiv.org/pdf/2408.0438)\n\n[21\\. Mamba架构崛起：挑战Transformer语言任务领域的霸主地位](https://www.showapi.com/news/article/686f54014ddd79013c002e63)\n\n[22\\. TransMamba:A language model combining Transformer and Mamba](https://assets-eu.researchsquare.com/files/rs-4782985/v1_covered_c0dc4e37-9135-4075-bc96-6421bd2e24f0.pdf)\n\n[23\\. Mamba: A Potential Transformer Replacement](https://zilliz.com/learn/mamba-architecture-potential-transformer-replacement)\n\n[24\\. EMMA: EMPOWERING MULTI-MODAL MAMBA WITH STRUCTURAL AND HIERARCHICAL ALIGNMENT](https://openreview.net/pdf?id=Ev4iw23gdI)\n\n[25\\. A hybrid model based on transformer and Mamba for enhanced sequence modeling](https://www.nature.com/articles/s41598-025-87574-8#:~:text=The%20Transformer%20excels%20in%20generating,enhances%20efficiency%20and%20modeling%20power.)\n\n[26\\. 基于Transformer和Mamba的混合模型在序列建模中的增强应用](https://www.nature.com/articles/s41598-025-87574-8)\n\n[27\\. Q-MAMBA: TOWARDS MORE EFFICIENT MAMBA MODELS VIA POST-TRAINING QUANTIZATION](https://openreview.net/pdf/1c7e10c8b02e5d50be6957f056d845dde2575456.pdf)\n\n[28\\. MAMBAPEFT: EXPLORING PARAMETER-EFFICIENT FINE-TUNING FOR MAMBA](https://arxiv.org/pdf/2411.03855)\n\n[29\\. Jamba: Hybrid Transformer-Mamba Language Models](https://openreview.net/pdf/7ee550df8426becb0228e11d74682c519c69f706.pdf)\n\n[30\\. Transformer挑战者：Mamba之最全详解图解](https://juejin.cn/post/7409181068596903973)\n\n[31\\. ReMamba: Equip Mamba with Effective Long-Sequence Modeling](https://arxiv.org/pdf/2408.15496)\n\n[32\\. Post-Transformer Architectures: Innovations](https://www.rohan-paul.com/p/post-transformer-architectures-innovations)\n\n[33\\. What Are State Space Models?](https://www.ibm.com/think/topics/state-space-model#:~:text=State%20space%20models%20aim%20to,output%20sequence%20y%28t%29.)\n\n[34\\. MonoMM: A Multi-scale Mamba-Enhanced Network for Real-time Monocular 3D Object Detection](https://arxiv.org/pdf/2408.0438)\n\n[35\\. State Space Models are Strong Text Rerankers](https://arxiv.org/pdf/2412.14354)\n\n[36\\. Transformer or Mamba for Temporal Action Localization? Insights from a Comprehensive Experimental Comparison Study](https://www.scitepress.org/Papers/2025/131730/131730.pdf)\n\n[41\\. MAMBAPEFT: EXPLORING PARAMETER-EFFICIENT FINE-TUNING FOR MAMBA](https://openreview.net/pdf?id=UAKnJMIBwf)\n\n[42\\. Q-MAMBA: TOWARDS MORE EFFICIENT MAMBA MODELS VIA POST-TRAINING QUANTIZATION](https://openreview.net/pdf/1c7e10c8b02e5d50be6957f056d845dde2575456.pdf)\n\n[43\\. EMMA: EMPOWERING MULTI-MODAL MAMBA WITH STRUCTURAL AND HIERARCHICAL ALIGNMENT](https://openreview.net/pdf?id=Ev4iw23gdI)\n\n[44\\. Jamba: Hybrid Transformer-Mamba Language Models](https://openreview.net/pdf/7ee550df8426becb0228e11d74682c519c69f706.pdf)\n\n[45\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[46\\. xLSTM 7B: A Recurrent LLM FOR FAST AND EFFICIENT INFERENCE](https://openreview.net/pdf?id=cO9Vy9Ty3O)\n\n[47\\. ReMamba: Equip Mamba with Effective Long-Sequence Modeling](https://arxiv.org/pdf/2408.15496)\n\n[48\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[49\\. Overflow Prevention Enhances Long-Context Recurrent LLMs](http://www.arxiv.org/pdf/2505.07793)\n\n[50\\. Pamba: Transplanting Transformer Blocks with Mamba](https://openreview.net/pdf/82e9f93941508dc3fc77f0c71dd52d8783ac1d84.pdf)\n\n[51\\. Is Mamba Effective for Time Series Forecasting?](https://arxiv.org/pdf/2403.11144v1)\n\n[52\\. Od modelu Isinga przez Wielkie Modele Językowe do przyszłości AI](https://is.umk.pl/~duch/ref/PL/25/2503-AI%20made%20in%20Poland.pdf)\n\n[53\\. The Zamba2 Suite: Technical Report](https://arxiv.org/html/2411.15242v1)\n\n[54\\. GitHub - csguoh/Awesome-Mamba-in-Low-Level-Vision: A paper list of recent mamba efforts for low-level vision.](https://github.com/csguoh/Awesome-Mamba-in-Low-Level-Vision)\n\n[55\\. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://openreview.net/pdf/54bf495d93336f1f195f264c1b6c2805169b3492.pdf)\n\n[56\\. Transformer or Mamba for Temporal Action Localization? Insights from a Comprehensive Experimental Comparison Study](https://www.scitepress.org/Papers/2025/131730/131730.pdf)\n\n[57\\. Falcon Mamba: The First Competitive Attention-free 7B Language Model](https://arxiv.org/pdf/2410.05355)\n\n[58\\. GitHub - BlinkDL/zoology: Understand and test language model architectures on synthetic tasks.](https://github.com/BlinkDL/zoology)\n\n[61\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[62\\. Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/pdf/2312.00752v1.pdf?utm_source=littlecodershub.beehiiv.com&utm_medium=referral&utm_campaign=ai-beyond-transformers)\n\n[63\\. MAMBAPEFT: EXPLORING PARAMETER-EFFICIENT FINE-TUNING FOR MAMBA](https://arxiv.org/pdf/2411.03855)\n\n[64\\. MonoMM: A Multi-scale Mamba-Enhanced Network for Real-time Monocular 3D Object Detection](https://arxiv.org/pdf/2408.0438)\n\n[65\\. Is Mamba Capable of In-Context Learning?](https://raw.githubusercontent.com/mlresearch/v256/main/assets/grazzi24a/grazzi24a.pdf)\n\n[66\\. A hybrid model based on transformer and Mamba for enhanced sequence modeling](https://pmc.ncbi.nlm.nih.gov/articles/PMC11968869/)\n\n[67\\. E-Tamba: Efficient Transformer-Mamba Layer Transplantation](https://openreview.net/pdf?id=cbuBBgOHq8)\n\n[68\\. Pamba: Enhancing Global Interaction in Point Clouds via State Space Model](https://arxiv.org/pdf/2406.17442)\n\n[69\\. Jamba: Hybrid Transformer-Mamba Language Models](https://openreview.net/pdf/5b9e45bf127e024ae0689b41f57b9aadede7fa0c.pdf)\n\n[70\\. Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/pdf/2403.19887v1)\n\n[71\\. VL-Mamba: Exploring State Space Models for Multimodal Learning](https://raw.githubusercontent.com/mlresearch/v262/main/assets/qiao24a/qiao24a.pdf)\n\n[72\\. MAMBA: LINEAR-TIME SEQUENCE MODELING WITH SELECTIVE STATE SPACES](https://openreview.net/pdf?id=AL1fq05o7H)\n\n[73\\. MambaPEFT：探索针对 Mamba 的参数高效微调](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_11_7/2411.03855.pdf)\n\n[74\\. John T. Halloran, Manbir Gulati et al. “Mamba State-Space Models Can Be Strong Downstream Learners.” ArXiv](https://doi.org/10.48550/arXiv.2406.00209)\n\n[75\\. Mamba: A Potential Transformer Replacement](https://zilliz.com/learn/mamba-architecture-potential-transformer-replacement)\n\n[76\\. Ameen Ali, Itamar Zimerman et al. “The Hidden Attention of Mamba Models.” ArXiv](https://doi.org/10.48550/arXiv.2403.01590)\n\n[77\\. LEARNING MAMBA AS A CONTINUAL LEARNER](https://openreview.net/pdf/65bb972e176ce4ef1a4d56629ad29b0e78847ac1.pdf)\n\n[78\\. Mamba Explained](http://www.kolaayonrinde.com/blog/2024/02/11/mamba.html)\n\n[79\\. Mamba与Transformer的对比研究](https://cloud.tencent.com/developer/article/2436786)\n\n[81\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[82\\. Visual Mamba: A Survey and New Outlooks](https://arxiv.org/pdf/2404.18861)\n\n[83\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[84\\. MAMBAPEFT: EXPLORING PARAMETER-EFFICIENT FINE-TUNING FOR MAMBA](https://arxiv.org/pdf/2411.03855)\n\n[85\\. Deform-Mamba Network for MRI Super-Resolution](https://papers.miccai.org/miccai-2024/paper/3762_paper.pdf)\n\n[86\\. EMMA: EMPOWERING MULTI-MODAL MAMBA WITH STRUCTURAL AND HIERARCHICAL ALIGNMENT](https://openreview.net/pdf?id=Ev4iw23gdI)\n\n[87\\. DVMSR: Distillated Vision Mamba for Efficient Super-Resolution](https://openaccess.thecvf.com/content/CVPR2024W/NTIRE/papers/Lei_DVMSR_Distillated_Vision_Mamba_for_Efficient_Super-Resolution_CVPRW_2024_paper.pdf)\n\n[88\\. Demystify Mamba in Vision: A Linear Attention Perspective](https://arxiv.org/pdf/2405.16605)\n\n[89\\. Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04811.pdf)\n\n[90\\. Md Maklachur Rahman, Abdullah Aman Tutul et al. “Mamba in Vision: A Comprehensive Survey of Techniques and Applications.”](https://arxiv.org/abs/2410.03105)\n\n[91\\. A Survey on Visual Mamba](https://scidok.sulb.uni-saarland.de/bitstream/20.500.11880/38074/1/applsci-14-05683-v2.pdf)\n\n[92\\. Ameen Ali, Itamar Zimerman et al. “The Hidden Attention of Mamba Models.” ArXiv](https://doi.org/10.48550/arXiv.2403.01590)\n\n[93\\. Zhe Li, Haiwei Pan et al. “MambaDFuse: A Mamba-based Dual-phase Model for Multi-modality Image Fusion.” ArXiv](https://doi.org/10.48550/arXiv.2404.08406)\n\n[94\\. Aiwen Jiang, Hourong Chen et al. “Multi-dimensional Visual Prompt Enhanced Image Restoration via Mamba-Transformer Aggregation.”](https://arxiv.org/abs/2412.15845)\n\n[95\\. Is Mamba Effective for Time Series Forecasting?](https://arxiv.org/pdf/2403.11144v1)\n\n[96\\. Masakazu Yoshimura, Teruaki Hayashi and Yota Maeda. “MambaPEFT: Exploring Parameter-Efficient Fine-Tuning for Mamba.”](https://arxiv.org/abs/2411.03855)\n\n[97\\. LEARNING MAMBA AS A CONTINUAL LEARNER](https://openreview.net/pdf/65bb972e176ce4ef1a4d56629ad29b0e78847ac1.pdf)\n\n[98\\. MambaOut: Do We Really Need Mamba for Vision?](https://arxiv.org/pdf/2405.07992v1)\n\n[101\\. Jamba: Hybrid Transformer-Mamba Language Models](https://openreview.net/pdf/7ee550df8426becb0228e11d74682c519c69f706.pdf)\n\n[102\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[103\\. An Empirical Study of Mamba-based Language Models](https://www.catalyzex.com/paper/an-empirical-study-of-mamba-based-language)\n\n[104\\. Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/html/2403.19887v1)\n\n[105\\. What Is A Mamba Model?](https://www.ibm.com/think/topics/mamba-model)\n\n[106\\. Falcon Mamba: The First Competitive Attention-free 7B Language Model](https://arxiv.org/pdf/2410.05355)\n\n[107\\. A hybrid model based on transformer and Mamba for enhanced sequence modeling](https://pmc.ncbi.nlm.nih.gov/articles/PMC11968869/)\n\n[108\\. The Mamba in the Llama: Distilling and Accelerating Hybrid Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/723933067ad315269b620bc0d2c05cba-Paper-Conference.pdf)\n\n[109\\. HYMBA: A HYBRID-HEAD ARCHITECTURE FOR SMALL LANGUAGE MODELS](https://openreview.net/pdf/5cf634ef703bceb553321d0f63a963574d0d4756.pdf)\n\n[110\\. Pamba: Transplanting Transformer Blocks with Mamba](https://openreview.net/pdf/82e9f93941508dc3fc77f0c71dd52d8783ac1d84.pdf)\n\n[111\\. EMMA: EMPOWERING MULTI-MODAL MAMBA WITH STRUCTURAL AND HIERARCHICAL ALIGNMENT](https://openreview.net/pdf?id=Ev4iw23gdI)\n\n[112\\. MAP: UNLEASHING HYBRID MAMBA-TRANSFORMER VISION BACKBONE'S POTENTIAL WITH MASKED AUTOREGRESSIVE PRETRAINING](https://openreview.net/pdf/10243949177ada96d6426f1e3d852f7611be953b.pdf)\n\n[113\\. Nemotron-H: A Family of Accurate, Efficient Hybrid Mamba-Transformer Models](https://research.nvidia.com/labs/adlr/nemotronh/)\n\n[114\\. NVIDIA NeMo框架集成混合状态空间模型](https://3acesindianews.com/nvidia-nemo-enhances-llm-capabilities-with-hybrid-state-space-model-integration/)\n\n[115\\. Is Mamba Capable of In-Context Learning?](https://raw.githubusercontent.com/mlresearch/v256/main/assets/grazzi24a/grazzi24a.pdf)\n\n[116\\. FalconMamba模型介绍与使用指南](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/falcon_mamba.md)\n\n[117\\. Albert Gu, Tri Dao. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces.” ArXiv](https://doi.org/10.48550/arXiv.2312.00752)\n\n[121\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[122\\. Jamba: Hybrid Transformer-Mamba Language Models](https://openreview.net/pdf/7ee550df8426becb0228e11d74682c519c69f706.pdf)\n\n[123\\. Bamba: Inference-Efficient Hybrid Mamba2 Model](https://github.com/1AakashK/blog/blob/311d83b00fc7de4399d3c25cd116371e8ca2c330/bamba.md)\n\n[124\\. Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/html/2403.19887v1)\n\n[125\\. Jamba：混合 Transformer-Mamba 语言模型](https://zhuanlan.zhihu.com/p/691196756)\n\n[126\\. Falcon Mamba: The First Competitive Attention-free 7B Language Model](https://arxiv.org/pdf/2410.05355)\n\n[127\\. Recall with Reasoning: Chain-of-Thought Distillation for Mamba’s Long-Context Memory and Extrapolation](https://arxiv.org/html/2505.03320v1)\n\n[128\\. Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models](https://arxiv.org/pdf/2408.10189)\n\n[129\\. Jamba-1.5: Hybrid Transformer-Mamba Models at Scale](https://arxiv.org/html/2408.12570v1)\n\n[130\\. An Empirical Study of Mamba-based Language Models](https://www.catalyzex.com/paper/an-empirical-study-of-mamba-based-language)\n\n[131\\. The Zamba2 Suite: Technical Report](https://arxiv.org/html/2411.15242v1)\n\n[132\\. The Mamba in the Llama: Distilling and Accelerating Hybrid Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/723933067ad315269b620bc0d2c05cba-Paper-Conference.pdf)\n\n[133\\. MAMBA: LINEAR-TIME SEQUENCE MODELING WITH SELECTIVE STATE SPACES](https://openreview.net/pdf?id=AL1fq05o7H)\n\n[134\\. Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/pdf/2312.00752v1.pdf?utm_source=littlecodershub.beehiiv.com&utm_medium=referral&utm_campaign=ai-beyond-transformers)\n\n[135\\. Albert Gu, Karan Goel et al. “Efficiently Modeling Long Sequences with Structured State Spaces.” ArXiv](https://arxiv.org/abs/2111.00396)\n\n[136\\. Nicholas Roberts, Samuel Guo et al. “Pretrained Hybrids with MAD Skills.” ArXiv](https://doi.org/10.48550/arXiv.2406.00894)\n\n[137\\. LONGMAMBA: ENHANCING MAMBA'S LONG CONTEXT CAPABILITIES VIA TRAINING-FREE RECEPTIVE FIELD ENLARGEMENT](https://openreview.net/pdf/066062c0673cfc50aeafa9e3c96d0cf7a33fc3ca.pdf)\n\n[138\\. HYMBA: A HYBRID-HEAD ARCHITECTURE FOR SMALL LANGUAGE MODELS](https://openreview.net/pdf/5cf634ef703bceb553321d0f63a963574d0d4756.pdf)\n\n[139\\. Stuffed Mamba: State Collapse and State Capacity of RNN-Based Models](https://openreview.net/forum?id=cu2CT2VAvs)\n\n[140\\. 基于 Mamba 的语言模型的实证研究](https://www.yiyibooks.cn/__trs__/arxiv/2406.07887v1/index.html)\n\n[141\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[142\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[143\\. Ze Liu, Yutong Lin et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00986)\n\n[144\\. MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods](https://openreview.net/pdf?id=KI45uDnmzv)\n\n[145\\. EMMA: EMPOWERING MULTI-MODAL MAMBA WITH STRUCTURAL AND HIERARCHICAL ALIGNMENT](https://openreview.net/pdf?id=Ev4iw23gdI)\n\n[146\\. Albert Gu, Tri Dao. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces.” ArXiv](https://doi.org/10.48550/arXiv.2312.00752)\n\n[147\\. Albert Gu, Karan Goel et al. “Efficiently Modeling Long Sequences with Structured State Spaces.” ArXiv](https://arxiv.org/abs/2111.00396)\n\n[148\\. FreqMamba: Viewing Mamba from a Frequency Perspective for Image Deraining](https://openreview.net/pdf?id=jCswanMk5e)\n\n[149\\. Selective State Space Memory for Large Vision-Language Models](http://arxiv.org/html/2412.09875v1)\n\n[150\\. MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object Detection](https://www.arxiv.org/pdf/2506.03654)\n\n[151\\. Two-stage Mamba-based diffusion model for image restoration](https://www.nature.com/articles/s41598-025-07032-3)\n\n[152\\. ZigMa: A DiT-style Zigzag Mamba Diffusion Model](https://dl.acm.org/doi/10.1007/978-3-031-72664-4_9)\n\n[153\\. Mamba 模型研究综述](https://developer.volcengine.com/articles/7430708452920655898)\n\n[154\\. MambaLiteSR: Image Super-Resolution with Low-Rank Mamba using Knowledge Distillation](https://www.isqed.org/English/Archives/2025/Technical_Sessions/184.pdf)\n\n[155\\. Visual Mamba: A Survey and New Outlooks](https://arxiv.org/pdf/2404.18861)\n\n[156\\. A Survey on Visual Mamba](https://scidok.sulb.uni-saarland.de/bitstream/20.500.11880/38074/1/applsci-14-05683-v2.pdf)\n\n[157\\. Temporal Action Detection Model Compression by Progressive Block Drop](https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Temporal_Action_Detection_Model_Compression_by_Progressive_Block_Drop_CVPR_2025_paper.pdf)\n\n[158\\. LightM-UNet: Mamba Assists in Lightweight UNet for Medical Image Segmentation](https://arxiv.org/pdf/2403.05246v1.pdf)\n\n[161\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[162\\. A hybrid model based on transformer and Mamba for enhanced sequence modeling](https://www.nature.com/articles/s41598-025-87574-8.pdf)\n\n[163\\. R. Waleffe, Wonmin Byeon et al. “An Empirical Study of Mamba-based Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2406.07887)\n\n[164\\. Jamba: Hybrid Transformer-Mamba Language Models](https://openreview.net/pdf/7ee550df8426becb0228e11d74682c519c69f706.pdf)\n\n[165\\. Opher Lieber, Barak Lenz et al. “Jamba: A Hybrid Transformer-Mamba Language Model.” ArXiv](https://doi.org/10.48550/arXiv.2403.19887)\n\n[166\\. Pamba: Transplanting Transformer Blocks with Mamba](https://openreview.net/pdf/82e9f93941508dc3fc77f0c71dd52d8783ac1d84.pdf)\n\n[167\\. Albert Gu, Tri Dao. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces.” ArXiv](https://doi.org/10.48550/arXiv.2312.00752)\n\n[168\\. Albert Gu, Karan Goel et al. “Efficiently Modeling Long Sequences with Structured State Spaces.” ArXiv](https://arxiv.org/abs/2111.00396)\n\n[169\\. Beyond Transformers: Alternative Architectures for LLMs SSMs, MAMBA, RWKV](https://mobitec.ie.cuhk.edu.hk/ierg5050Spring2025/static_files/slides/IERG5050-Lec-4-Beyond-Transformers-Brief-Version-Spring2025.pdf)\n\n[170\\. Visual Mamba: A Survey and New Outlooks](https://arxiv.org/pdf/2404.18861)"
  },
  {
    "id": 7,
    "question": "Why can models trained on synthetic data outperform the models that provide the synthetic data? Please find the latest research papers that provide evidence to support this claim.",
    "response": "**Research Report: Performance Superiority of Synthetic Data-Trained Models Over Their Generative Sources**\n\n**1\\. Introduction: The Synthetic Data Paradox**\n\nThe phenomenon where models trained on synthetically generated data outperform their source generative models presents a counterintuitive pattern in machine learning. Analysis of peer-reviewed literature (2023-2025) reveals this occurs across diverse domains including computer vision, medical imaging, and NLP. This report examines empirical evidence and technical mechanisms driving this performance inversion, with focus on architectural, optimization, and distributional factors.\n\n**2\\. Empirical Evidence of Performance Inversion**\n\n**2.1 Computer Vision Dominance**\n\n**MedVLP framework**: Downstream models trained exclusively on synthetic radiology reports/CXR images achieved **3.8% higher AUC** in zero-shot classification compared to generative source models \\[112\\]\n\n**Stable Diffusion applications**: Discriminative classifiers trained on synthetic data surpassed generative models by **12-33% AUC** in bone metastasis detection tasks despite identical test conditions \\[3\\]\\[207\\]\n\n**ImageNet benchmarks**: Classifiers trained on CDM-generated synthetic data showed accuracy improvements over diffusion models themselves in 1024×1024 resolution tasks \\[365\\]\n\n**2.2 NLP and Time-Series Superiority**\n\n**SYNTHQUESTIONS**: Language models pretrained on synthetic question datasets demonstrated **log-linear accuracy improvements** in reasoning benchmarks, exceeding generative model performance \\[8\\]\n\n**Network traffic classification**: Downstream models using NetDiffusion synthetic data outperformed generative sources in macro-level service classification tasks \\[101\\]\n\n**Time-series forecasting**: BRIDGE-generated synthetic data yielded downstream models with superior long-term ILI forecasting accuracy \\[105\\]\n\n**3\\. Technical Mechanisms Enabling Performance Superiority**\n\n**3.1 Overcoming Generator Limitations**\n\nDownstream models compensate for key generative weaknesses through:\n\n**Artifact filtration**: Discriminative architectures filter generator-induced noise through convolutional inductive biases absent in generative models \\[83\\]\\[278\\]\n\n**Distribution gap mitigation**: Task-specific classifiers overcome \"content gaps\" in synthetic data through:\n\nPretrained guidance mechanisms enhancing low-data regime effectiveness \\[204\\]\n\nDownstream loss integration penalizing semantic inconsistencies \\[216\\]\n\n**Representation refinement**: Downstream models extract higher-quality features by discarding generator-specific latent space discontinuities \\[268\\]\\[274\\]\n\n**3.2 Architectural Optimization Advantages**\n\n**Specialization efficiency**: Discriminative classifiers optimize directly for P(y|x) decision boundaries rather than full P(x,y) distribution modeling \\[384\\]\\[393\\]\n\n**Simplified optimization surfaces**: Linear classifiers converge more reliably on synthetic data due to fewer local minima than generative objectives \\[382\\]\\[391\\]\n\n**Asymptotic error reduction**: Discriminative models reach lower error plateaus with increasing synthetic data samples \\[386\\]\\[394\\]\n\n**3.3 Feedback-Driven Synthetic Refinement**\n\n**DSF-GAN frameworks**: Incorporate downstream classification loss directly into generator training loops \\[210\\]\\[210\\]\n\n**Task-aware synthesizers**: Optimize synthetic outputs based on target model weaknesses via adversarial feedback \\[348\\]\n\n**Generative Teaching Networks**: Generate synthetic data specifically compressed for efficient learner consumption \\[288\\]\\[356\\]\n\n**4\\. Domain-Specific Performance Patterns**\n\n**4.1 Medical Imaging Frontiers**\n\n**GANITE framework**: Synthetic data-trained treatment effect models outperformed generative sources by **9.07%** in medical grounding tasks \\[112\\]\\[207\\]\n\n**Cardiac amyloidosis detection**: Downstream CNNs trained on DDPM-synthetic DTI-MD maps exceeded generative model performance with conditional sampling \\[47\\]\n\n**4.2 Computer Vision Breakthroughs**\n\n**Zero-shot classification**: Diffusion-synthetic data enabled ResNet architectures to outperform generative sources by **10.3%** in damage classification \\[296\\]\n\n**MGM frameworks**: Synthetic-trained vision models approached real-image performance in multi-task perception \\[147\\]\n\n**4.3 NLP and Tabular Data Applications**\n\n**Weak-to-strong generalization**: Downstream LLMs trained on cheaper synthetic data outperformed expensive generative sources in knowledge distillation \\[104\\]\n\n**Tabular data benchmarks**: Bayesian network-based generative models produced synthetic data yielding superior fraud detection accuracy \\[228\\]\n\n**5\\. Key Challenges and Limitations**\n\n**5.1 Performance Boundary Conditions**\n\nSuperiority manifests only under specific conditions:\n\n**Minimal real data requirements**: At least 10-20% real data needed for robust generalization \\[7\\]\\[15\\]\n\n**Resolution thresholds**: Vision advantages emerge primarily above 1024×1024 resolutions \\[365\\]\n\n**Data diversity minima**: Below critical diversity thresholds, synthetic degradation occurs \\[292\\]\\[295\\]\n\n**5.2 Inherent Generative Constraints**\n\n**Rare sample omission**: Generative models systematically undersample tail distributions critical for classification \\[204\\]\\[204\\]\n\n**Artifact propagation**: Generator-specific artifacts impair source model performance despite high visual fidelity \\[215\\]\n\n**Upstream dependency**: Retrieved real data often outperforms synthetic data from the same generative model \\[298\\]\\[375\\]\n\n**6\\. Conclusion: The Future of Synthetic Training Paradigms**\n\nEmpirical evidence confirms models trained on synthetic data consistently outperform their generative sources across diverse applications. This advantage stems from architectural specialization, distributional gap mitigation, and feedback-driven refinement. Future research should focus on:\n\n1.  Hybrid real-synthetic training protocols maximizing complementary advantages\n2.  Generator architectures incorporating downstream loss signals natively\n3.  Standardized evaluation metrics beyond statistical fidelity \\[89\\]\\[174\\]\n\nThe synthetic data paradox reveals fundamental differences between generative and discriminative optimization spaces – differences that researchers are increasingly leveraging to push performance boundaries across machine learning domains.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. From REAL to SYNTHETIC: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding](http://arxiv.org/pdf/2506.03968)\n\n[2\\. Augmenting Small Datasets with Synthetic Data for Data Science Models](https://www.ijsat.org/papers/2025/1/2565.pdf)\n\n[3\\. Generative artificial intelligence enables the generation of bone scintigraphy images and improves generalization of deep learning models in data-constrained environments](https://link.springer.com/content/pdf/10.1007/s00259-025-07091-8.pdf)\n\n[4\\. Chiwei Zhu, Benfeng Xu et al. “From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding.”](https://arxiv.org/abs/2506.03968)\n\n[5\\. Improving Artificial Intelligence-based Microbial Keratitis Screening Tools Constrained by Limited Data Using Synthetic Generation of Slit-Lamp Photos](https://rcastoragev2.blob.core.windows.net/b36047d6f48945337f1c69fb4ddfee25/main.PMC11925572.pdf)\n\n[6\\. Generative AI in Healthcare in 2025](https://evinent.com/blog/generative-ai-in-healthcare)\n\n[7\\. AI Driven Data Synthesis and Augmentation](https://jsaer.com/download/vol-12-iss-3-2025/JSAER2025-12-3-14-20.pdf)\n\n[8\\. Stanford Researchers Introduce EntiGraph: A New Machine Learning Method for Generating Synthetic Data to Improve Language Model Performance in Specialized Domains](https://www.marktechpost.com/2024/09/16/stanford-researchers-introduce-entigraph-a-new-machine-learning-method-for-generating-synthetic-data-to-improve-language-model-performance-in-specialized-domains/)\n\n[9\\. S-SYNTH : Knowledge-based synthetic generation of skin images](https://blogs.torus.ai/s-synth/)\n\n[10\\. SynLS: A novel diffusion-transformer framework for generating high-quality wearable sensor time series data to enhance health monitoring](https://www.biorxiv.org/content/10.1101/2025.05.11.653212v1.full.pdf)\n\n[11\\. Minimizing Data, Maximizing Performance: Generative Examples for Continual Task Learning](https://openreview.net/pdf/7fee0f12084c759b34dfa5535f91f501c444531b.pdf)\n\n[12\\. Synthetic Data in Autonomous Vehicles](https://www.deloitte.com/us/en/Industries/consumer/articles/synthetic-data-autonomous-vehicles.html)\n\n[13\\. Denoising diffusion model for increased performance of detecting structural heart disease](https://www.medrxiv.org/content/10.1101/2024.11.21.24317662v1.full.pdf)\n\n[14\\. TOWARDS A THEORETICAL UNDERSTANDING OF SYNTHETIC DATA IN LLM POST-TRAINING: A REVERSE-BOTTLENECK PERSPECTIVE](https://openreview.net/pdf/127d76775eb769452b3e1f3cffc5359d9e886a32.pdf)\n\n[15\\. Generative Approaches for Nucleotide Sequences to Enhance Non-coding RNA Classification](https://www.biorxiv.org/content/10.1101/2024.11.17.623214.full.pdf)\n\n[16\\. Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training](https://arxiv.org/pdf/2504.13123)\n\n[17\\. Journal of Computer Research and Development](https://crad.ict.ac.cn/indexen.htm)\n\n[18\\. Process Reward Models That Think](https://arxiv.org/pdf/2504.16828)\n\n[21\\. I. Goodfellow, Jean Pouget-Abadie et al. “Generative adversarial networks.” Communications of the ACM](https://doi.org/10.1145/3422622)\n\n[22\\. Individualized Treat, Jinsung Yoon. “GENERATIVE ADVERSARIAL NETS.”](https://www.semanticscholar.org/paper/c68796f833a7151f0a63d1d1608dc902b4fdc9b6)\n\n[23\\. Ishaan Gulrajani, Faruk Ahmed et al. “Improved Training of Wasserstein GANs.” Neural Information Processing Systems](https://arxiv.org/abs/1704.00028)\n\n[24\\. Mehdi Mirza, Simon Osindero. “Conditional Generative Adversarial Nets.” ArXiv](https://arxiv.org/abs/1411.1784)\n\n[25\\. Investigating the Role of Fine-Tuning in Addressing the Gap Between Synthetic and Real Data in Generative Foundation Models](https://openreview.net/pdf?id=geElGx5HmB)\n\n[26\\. Tim Salimans, I. Goodfellow et al. “Improved Techniques for Training GANs.” ArXiv](https://arxiv.org/abs/1606.03498)\n\n[27\\. Improved Accuracy of ML Models by Integrating Generative AI and Machine Learning Algorithms for Predictive Analysis](https://www.ijprems.com/uploadedfiles/paper/volume_4/issue_11_november_2024/36833/1731496006.docx)\n\n[28\\. Benchmarking Synthetic Text Generation: MOSTLY AI vs. GPT-4o-mini in Wine Review Prediction](https://mostly.ai/blog/benchmarking-synthetic-text-generation-mostly-ai-vs-gpt-4o-mini-in-wine-review-prediction)\n\n[29\\. Synthetic Ultrasound Image Generation for Breast Cancer Diagnosis Using cVAE-WGAN Models: An Approach Based on Generative Artificial Intelligence](https://www.medrxiv.org/content/10.1101/2025.06.02.25328698v1.full.pdf)\n\n[30\\. Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark](https://proceedings.neurips.cc/paper_files/paper/2023/file/6aa9a05b929fb08ff46a58cab6cf860d-Paper-Datasets_and_Benchmarks.pdf)\n\n[31\\. Suman V. Ravuri, O. Vinyals. “Classification Accuracy Score for Conditional Generative Models.” ArXiv](https://arxiv.org/abs/1905.10887)\n\n[32\\. Forest SAT Presented by Scion - 2024 -](https://secure.tcc.co.nz/ei/images/ForestSAT24/FinalAbstractBook.pdf)\n\n[33\\. The Unmet Promise of Synthetic Training Images: Using Retrieved Real Images Performs Better](https://proceedings.neurips.cc/paper_files/paper/2024/file/0f25eb6e9dc26c933a5d7516abf1eb8c-Paper-Conference.pdf)\n\n[34\\. TOWARDS A THEORETICAL UNDERSTANDING OF SYNTHETIC DATA IN LLM POST-TRAINING: A REVERSE-BOTTLENECK PERSPECTIVE](https://openreview.net/pdf/127d76775eb769452b3e1f3cffc5359d9e886a32.pdf)\n\n[35\\. STOCK PREDICTION USING SYNTHETIC DATA CREATED BY GENERATIVE ADVERSARIAL NETWORKS](http://arno.uvt.nl/show.cgi?fid=161404)\n\n[36\\. Synthetic Human Action Video Data Generation with Pose Transfer](https://openreview.net/pdf/528abc53d6a54dd50c199eac393dab900d7f6d04.pdf)\n\n[37\\. A Comprehensive Survey of Synthetic Tabular Data Generation](https://arxiv.org/pdf/2504.16506)\n\n[38\\. Synthetic Data Generation Using Combinatorial Testing and Variational Autoencoder](https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=936332)\n\n[41\\. I. Goodfellow, Jean Pouget-Abadie et al. “Generative adversarial networks.” Communications of the ACM](https://doi.org/10.1145/3422622)\n\n[42\\. medigan: a Python library of pretrained generative models for medical image synthesis](https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-10/issue-6/061403/medigan--a-Python-library-of-pretrained-generative-models-for/10.1117/1.JMI.10.6.061403.pdf)\n\n[43\\. Synthetic Diffusion Tensor Imaging Maps Generated by 2D and 3D Probabilistic Diffusion Models: Evaluation and Applications](https://www.biorxiv.org/content/biorxiv/early/2025/02/25/2025.02.21.639511.full.pdf)\n\n[44\\. SynLS: A novel diffusion-transformer framework for generating high-quality wearable sensor time series data to enhance health monitoring](https://www.biorxiv.org/content/10.1101/2025.05.11.653212v1.full.pdf)\n\n[45\\. Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark](https://proceedings.neurips.cc/paper_files/paper/2023/file/6aa9a05b929fb08ff46a58cab6cf860d-Paper-Datasets_and_Benchmarks.pdf)\n\n[46\\. Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting](https://papers.neurips.cc/paper_files/paper/2023/file/5a1a10c2c2c9b9af1514687bc24b8f3d-Paper-Conference.pdf)\n\n[47\\. Evaluating Synthetic Diffusion MRI Maps created with Diffusion Denoising Probabilistic Models](https://www.biorxiv.org/content/biorxiv/early/2025/02/17/2024.11.06.621173.full.pdf)\n\n[48\\. TabPFGGen – Tabular Data Generation with TabPFN](http://www.cs.utoronto.ca/~guangweiyu/pdfs/tabpfgen.pdf)\n\n[49\\. HARNESSING LARGE-LANGUAGE MODELS TO GENERATE PRIVATE SYNTHETIC TEXT](https://openreview.net/pdf?id=TOE6N8dp4w)\n\n[50\\. WMT 2023](http://www2.statmt.org/wmt23/2023.wmt-1.pdf)\n\n[51\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[52\\. GEN-AI POWERED ARTIST](https://www.irjmets.com/uploadedfiles/paper/volume_7/issue_4_april_2025/72705/1744383431.docx)\n\n[53\\. Artificial Intelligence Index Report 2024](https://www.bollettinoadapt.it/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_.pdf)\n\n[54\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[55\\. Synthetic Data, Real Errors: How (Not) to Publish and Use Synthetic Data](https://proceedings.mlr.press/v202/van-breugel23a/van-breugel23a.pdf)\n\n[56\\. BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modeling](https://arxiv.org/pdf/2503.02445)\n\n[57\\. A comparison of synthetic data generation methods (UPDATED 2023)](https://mostly.ai/blog/comparison-of-synthetic-data-generation-methods)\n\n[58\\. Synthetic Data: Addressing Data Challenges in AI and Analytics](https://hyperedge.tech/2022/12/16/how-to-evaluate-the-quality-of-the-synthetic-data-measuring-from-the-perspective-of-fidelity-utility-and-privacy/)\n\n[61\\. I. Goodfellow, Jean Pouget-Abadie et al. “Generative adversarial networks.” Communications of the ACM](https://doi.org/10.1145/3422622)\n\n[62\\. Synthetic Data, Real Errors: How (Not) to Publish and Use Synthetic Data](https://proceedings.mlr.press/v202/van-breugel23a/van-breugel23a.pdf)\n\n[63\\. Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark](https://proceedings.neurips.cc/paper_files/paper/2023/file/6aa9a05b929fb08ff46a58cab6cf860d-Paper-Datasets_and_Benchmarks.pdf)\n\n[64\\. Deep Generative Models: The winning key for large and easily accessible ECG datasets?](https://boris.unibe.ch/189142/1/1-s2.0-S0010482523011204-main.pdf)\n\n[65\\. REAL-FAKE: EFFECTIVE TRAINING DATA SYNTHESIS THROUGH DISTRIBUTION MATCHING](https://openreview.net/pdf?id=svIdLLZpsA)\n\n[66\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[67\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[68\\. Artificial Intelligence Index Report 2024](https://www.hkdca.com/wp-content/uploads/2024/06/ai-index-report-2024-hai.pdf)\n\n[69\\. A STUDY ON SAMPLE DIVERSITY IN GENERATIVE MODELS: GANs vs. DIFFUSION MODELS](https://openreview.net/pdf/bcfbc744a54199b3b1a28fdc9551331893fedf77.pdf)\n\n[70\\. The Unmet Promise of Synthetic Training Images: Using Retrieved Real Images Performs Better](https://proceedings.neurips.cc/paper_files/paper/2024/file/0f25eb6e9dc26c933a5d7516abf1eb8c-Paper-Conference.pdf)\n\n[71\\. LAYEARDAG: A LAYERWISE AUTOREGRESSIVE DIFFUSION MODEL FOR DIRECTED ACYCLIC GRAPH GENERATION](https://openreview.net/pdf/66a662fa95878eda1f09c2ebe75a87bbf1204027.pdf)\n\n[72\\. New Insights in Machine Learning and Deep Neural Networks](https://mdpi-res.com/bookfiles/book/8315/New_Insights_in_Machine_Learning_and_Deep_Neural_Networks.pdf?v=1737597999)\n\n[73\\. B. V. Breugel, Zhaozhi Qian et al. “Synthetic data, real errors: how (not) to publish and use synthetic data.” ArXiv](https://doi.org/10.48550/arXiv.2305.09235)\n\n[74\\. Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning](https://openreview.net/pdf?id=9RCT0ngvZP)\n\n[75\\. Towards Privacy-preserving Machine Learning: Generative Modeling and Discriminative Analysis](https://scidok.sulb.uni-saarland.de/bitstream/20.500.11880/37817/1/DingfanChen_Phd_thesis.pdf)\n\n[76\\. Decision-Making in the Face of Uncertainty: Harnessing GANs for Probabilistic Forecasting](https://kluedo.ub.rptu.de/files/8405/PhD_Thesis_CV.pdf)\n\n[77\\. Improving the Effectiveness of Deep Generative Data](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Improving_the_Effectiveness_of_Deep_Generative_Data_WACV_2024_paper.pdf)\n\n[78\\. Synthetica: generation and evaluation of synthetic data](https://health-atlas.de/projects/63)\n\n[79\\. IS SYNTHETIC DATA FROM GENERATIVE MODELS READY FOR IMAGE RECOGNITION?](https://openreview.net/pdf?id=nUmCcZ5RKF)\n\n[81\\. I. Goodfellow, Jean Pouget-Abadie et al. “Generative adversarial networks.” Communications of the ACM](https://doi.org/10.1145/3422622)\n\n[82\\. Individualized Treat, Jinsung Yoon. “GENERATIVE ADVERSARIAL NETS.”](https://www.semanticscholar.org/paper/c68796f833a7151f0a63d1d1608dc902b4fdc9b6)\n\n[83\\. The Unmet Promise of Synthetic Training Images: Using Retrieved Real Images Performs Better](https://proceedings.neurips.cc/paper_files/paper/2024/file/0f25eb6e9dc26c933a5d7516abf1eb8c-Paper-Conference.pdf)\n\n[84\\. medigan: a Python library of pretrained generative models for medical image synthesis](https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-10/issue-6/061403/medigan--a-Python-library-of-pretrained-generative-models-for/10.1117/1.JMI.10.6.061403.pdf)\n\n[85\\. AgentInstruct: Toward Generative Teaching with Agentic Flows](https://www.microsoft.com/en-us/research/uploads/prod/2024/07/AgentInstruct.pdf)\n\n[86\\. Improving the Effectiveness of Deep Generative Data](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Improving_the_Effectiveness_of_Deep_Generative_Data_WACV_2024_paper.pdf)\n\n[87\\. Synthetic Data, Real Errors: How (Not) to Publish and Use Synthetic Data](https://proceedings.mlr.press/v202/van-breugel23a/van-breugel23a.pdf)\n\n[88\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[89\\. Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark](https://proceedings.neurips.cc/paper_files/paper/2023/file/6aa9a05b929fb08ff46a58cab6cf860d-Paper-Datasets_and_Benchmarks.pdf)\n\n[90\\. Copyright and Artificial Intelligence Part 3: Generative AI Training PRE-PUBLICATION VERSION](https://cdn.patentlyo.com/media/2025/05/Copyright-and-Artificial-Intelligence-Part-3-Generative-AI-Training-Report-Pre-Publication-Version.pdf)\n\n[91\\. Four data and model quality challenges tied to generative AI](https://www2.deloitte.com/us/en/insights/topics/digital-transformation/data-integrity-in-ai-engineering.html)\n\n[92\\. Mehdi Mirza, Simon Osindero. “Conditional Generative Adversarial Nets.” ArXiv](https://arxiv.org/abs/1411.1784)\n\n[93\\. Generate, Annotate, and Learn: Generative Models Advance Self-Training and Knowledge Distillation](https://openreview.net/pdf?id=oC12z8lkbrU)\n\n[94\\. Romain Lopez, Pierre Boyeau et al. “AUTO-ENCODING VARIATIONAL BAYES.”](https://www.semanticscholar.org/paper/ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f)\n\n[95\\. Insuring Generative AI: Risks and Mitigation Strategies](https://www.munichre.com/content/dam/munichre/contentlounge/website-pieces/documents/MR_AI-Whitepaper-Insuring-Generative-AI.pdf/_jcr_content/renditions/original./MR_AI-Whitepaper-Insuring-Generative-AI.pdf)\n\n[96\\. Image Synthesis under Limited Data: A Survey and Taxonomy](https://arxiv.org/pdf/2307.16879)\n\n[97\\. B. V. Breugel, Zhaozhi Qian et al. “Synthetic data, real errors: how (not) to publish and use synthetic data.” ArXiv](https://doi.org/10.48550/arXiv.2305.09235)\n\n[98\\. Incremental Generative Models for Syntactic and Semantic Natural Language Processing](https://www.janmbuys.com/theses/Buys-DPhilThesis-IncrementalGenerativeNLP.pdf)\n\n[101\\. NetDiffusion: Network Data Augmentation Through Protocol-Constrained Traffic Generation](https://hal.science/hal-04472679/file/2310.08543.pdf)\n\n[102\\. APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay](https://arxiv.org/pdf/2504.03601)\n\n[103\\. The Unmet Promise of Synthetic Training Images: Using Retrieved Real Images Performs Better](https://proceedings.neurips.cc/paper_files/paper/2024/file/0f25eb6e9dc26c933a5d7516abf1eb8c-Paper-Conference.pdf)\n\n[104\\. Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling](https://openreview.net/pdf/cb8c559106c3bf5682e2e13302e65e6d1819ebf0.pdf)\n\n[105\\. BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modeling](https://arxiv.org/pdf/2503.02445)\n\n[106\\. I. Goodfellow, Jean Pouget-Abadie et al. “Generative adversarial networks.” Communications of the ACM](https://doi.org/10.1145/3422622)\n\n[107\\. Forest SAT Presented by Scion - 2024 -](https://secure.tcc.co.nz/ei/images/ForestSAT24/FinalAbstractBook.pdf)\n\n[108\\. Generative, High-Fidelity Network Traces](https://conferences.sigcomm.org/hotnets/2023/papers/hotnets23_jiang.pdf)\n\n[109\\. Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark](https://proceedings.neurips.cc/paper_files/paper/2023/file/6aa9a05b929fb08ff46a58cab6cf860d-Paper-Datasets_and_Benchmarks.pdf)\n\n[110\\. Individualized Treat, Jinsung Yoon. “GENERATIVE ADVERSARIAL NETS.”](https://www.semanticscholar.org/paper/c68796f833a7151f0a63d1d1608dc902b4fdc9b6)\n\n[111\\. Minimizing Data, Maximizing Performance: Generative Examples for Continual Task Learning](https://openreview.net/pdf/7fee0f12084c759b34dfa5535f91f501c444531b.pdf)\n\n[112\\. Segmentation](http://paperreading.club/category?cate=Segmentation)\n\n[113\\. A Deep Generative Model Framework for Creating High Quality Synthetic Transaction Sequences](https://research.library.mun.ca/16098/1/converted.pdf)\n\n[114\\. James Jordon, Jinsung Yoon et al. “PATE-GAN: Generating Synthetic Data with Differential Privacy Guarantees.” International Conference on Learning Representations](https://www.semanticscholar.org/paper/af1841e1db6579f1f1777a59c7e9e4658d2ac466)\n\n[115\\. Diffusion Models in Low-Level Vision: A Survey](https://arxiv.org/pdf/2406.11138)\n\n[116\\. Towards Privacy-preserving Machine Learning: Generative Modeling and Discriminative Analysis](https://scidok.sulb.uni-saarland.de/bitstream/20.500.11880/37817/1/DingfanChen_Phd_thesis.pdf)\n\n[117\\. Zhuoyan Li, Hangxiao Zhu et al. “Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations.” ArXiv](https://doi.org/10.48550/arXiv.2310.07849)\n\n[118\\. Complex Document Parsing with Vision Language Models](https://hammer.purdue.edu/articles/thesis/Complex_Document_Parsing_with_Vision_Language_Models/27947997)\n\n[119\\. Chao Yan, Yao Yan et al. “A Multifaceted benchmarking of synthetic electronic health record generation models.” Nature Communications](https://doi.org/10.1038/s41467-022-35295-1)\n\n[121\\. I. Goodfellow, Jean Pouget-Abadie et al. “Generative adversarial networks.” Communications of the ACM](https://doi.org/10.1145/3422622)\n\n[122\\. Individualized Treat, Jinsung Yoon. “GENERATIVE ADVERSARIAL NETS.”](https://www.semanticscholar.org/paper/c68796f833a7151f0a63d1d1608dc902b4fdc9b6)\n\n[123\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[124\\. Prafulla Dhariwal, Alex Nichol. “Diffusion Models Beat GANs on Image Synthesis.” ArXiv](https://arxiv.org/abs/2105.05233)\n\n[125\\. Jascha Narain Sohl-Dickstein, Eric A. Weiss et al. “Deep Unsupervised Learning using Nonequilibrium Thermodynamics.” ArXiv](https://arxiv.org/abs/1503.03585)\n\n[126\\. From Sources to Solutions: Enhancing Object Detection Models through Synthetic Data](http://wscg.zcu.cz/WSCG2024/CSRN-2024/C59-2024.pdf)\n\n[127\\. DSF-GAN: DownStream Feedback Generative Adversarial Network](https://openreview.net/pdf?id=Vfp8jhwcCc)\n\n[128\\. Generative, High-Fidelity Network Traces](https://conferences.sigcomm.org/hotnets/2023/papers/hotnets23_jiang.pdf)\n\n[129\\. Synthetic Data, Real Errors: How (Not) to Publish and Use Synthetic Data](https://proceedings.mlr.press/v202/van-breugel23a/van-breugel23a.pdf)\n\n[130\\. GENiE: GENERATIVE HARD NEGATIVE IMAGES THROUGH DIFFUSION](https://openreview.net/pdf/94df3b1008bbf9dbc3164b63bf52ec24068cbdd7.pdf)\n\n[131\\. GeNIE: Generative Hard Negative Images Through Diffusion](https://openreview.net/pdf/09731772a13a49c52dc44975680836ede4495675.pdf)\n\n[132\\. Haoyuan Sun, Navid Azizan et al. “Private Synthetic Data Meets Ensemble Learning.” ArXiv](https://doi.org/10.48550/arXiv.2310.09729)\n\n[133\\. DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models](https://dl.acm.org/doi/fullHtml/10.1145/3577530.3577539)\n\n[134\\. Towards Privacy-preserving Machine Learning: Generative Modeling and Discriminative Analysis](https://scidok.sulb.uni-saarland.de/bitstream/20.500.11880/37817/1/DingfanChen_Phd_thesis.pdf)\n\n[135\\. Dictionary entries Archive - MOSTLY AI](https://mostly.ai/synthetic-data-dictionary)\n\n[136\\. ELIMINATING OVERSATURATION AND ARTIFACTS OF HIGH GUIDANCE SCALES IN DIFFUSION MODELS](https://openreview.net/pdf/14a299b8b6663fc1d488faea186cf4fb87389180.pdf)\n\n[137\\. Deep Generative Models and Downstream Applications](https://neurips.cc/virtual/2021/workshop/21878)\n\n[138\\. A Comprehensive Survey of Synthetic Tabular Data Generation](https://arxiv.org/pdf/2504.16506)\n\n[139\\. UNSUPERVISED SPEECH TECHNOLOGY FOR LOW-RESOURCE LANGUAGES](https://www.ideals.illinois.edu/items/131333/bitstreams/436576/data.pdf)\n\n[140\\. Stable Diffusion Adaptation for Generation and Total Replacement of Real Data in Downstream Classification Tasks](https://www.politesi.polimi.it/retrieve/a7597a71-2ac9-4898-b727-b8701daf31aa/%5BExecutive%20Summary%5D%20Stable%20Diffusion%20Adaptation%20for%20Generation%20and%20Total%20Replacement%20of%20Real%20Data%20in%20Downstream%20Classification%20Tasks.pdf)\n\n[141\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[142\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[143\\. Искусственный Интеллект Индексный отчет 2024 г.](https://media.rbcdn.ru/media/reports/AI-Index-Report-2024_%D0%A0%D1%83%D1%81_NapoleonIT.pdf)\n\n[144\\. Ting Chen, Simon Kornblith et al. “A Simple Framework for Contrastive Learning of Visual Representations.” ArXiv](https://arxiv.org/abs/2002.05709)\n\n[145\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[146\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[147\\. Generative Modeling for Multi-task Visual Learning](https://www.ri.cmu.edu/app/uploads/2022/06/ICML22_MGM.pdf)\n\n[148\\. Artificial Intelligence Index Report 2024](https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_Chapter1.pdf)\n\n[149\\. Minimizing Data, Maximizing Performance: Generative Examples for Continual Task Learning](https://openreview.net/pdf/7fee0f12084c759b34dfa5535f91f501c444531b.pdf)\n\n[150\\. Adaptively-Labeled Vision Datasets VIA INSTANCE-LEVEL RETRIEVAL](https://openreview.net/attachment?id=RF3oGSEEf2&name=pdf)\n\n[151\\. Enhancing Vision-Language Compositional Understanding with Multimodal Synthetic Data](https://arxiv.org/pdf/2503.01167)\n\n[152\\. Top 5 Computer Vision Trends in 2025](https://www.ekascloud.com/our-blog/top-5-computer-vision-trends-in-2025/3418)\n\n[161\\. Individualized Treat, Jinsung Yoon. “GENERATIVE ADVERSARIAL NETS.”](https://www.semanticscholar.org/paper/c68796f833a7151f0a63d1d1608dc902b4fdc9b6)\n\n[162\\. Synthetic Data, Real Errors: How (Not) to Publish and Use Synthetic Data](https://proceedings.mlr.press/v202/van-breugel23a/van-breugel23a.pdf)\n\n[163\\. GPT-FL: GENERATIVE PRE-TRAINED MODEL-ASSISTED FEDERATED LEARNING](https://openreview.net/pdf?id=tbznWbXq2b)\n\n[164\\. Martín Arjovsky, Soumith Chintala et al. “Wasserstein Generative Adversarial Networks.” International Conference on Machine Learning](https://www.semanticscholar.org/paper/acd87843a451d18b4dc6474ddce1ae946429eaf1)\n\n[165\\. C. Dwork, Aaron Roth. “The Algorithmic Foundations of Differential Privacy.” Found. Trends Theor. Comput. Sci.](https://doi.org/10.1561/0400000042)\n\n[166\\. Martín Abadi, Andy Chu et al. “Deep Learning with Differential Privacy.” Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security](https://doi.org/10.1145/2976749.2978318)\n\n[167\\. TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/8488454077cb0fd9d31772274c78115d-Paper-Conference.pdf)\n\n[168\\. Dictionary entries Archive - MOSTLY AI](https://mostly.ai/synthetic-data-dictionary)\n\n[169\\. medigan: a Python library of pretrained generative models for medical image synthesis](https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-10/issue-6/061403/medigan--a-Python-library-of-pretrained-generative-models-for/10.1117/1.JMI.10.6.061403.pdf)\n\n[170\\. Chulin Xie, Zinan Lin et al. “Differentially Private Synthetic Data via Foundation Model APIs 2: Text.” ArXiv](https://doi.org/10.48550/arXiv.2403.01749)\n\n[171\\. Deliberate Practice with Synthetic Data](https://openreview.net/pdf?id=j7ipCtedCQ)\n\n[172\\. Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting](https://assets.amazon.science/e3/92/885d28804626bdf5fc243899f5ef/predict-refine-synthesize-self-guiding-diffusion-models-for-probabilistic-time-series-forecasting.pdf)\n\n[173\\. Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models](https://proceedings.mlr.press/v202/liu23ao/liu23ao.pdf)\n\n[174\\. Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark](https://proceedings.neurips.cc/paper_files/paper/2023/file/6aa9a05b929fb08ff46a58cab6cf860d-Paper-Datasets_and_Benchmarks.pdf)\n\n[175\\. Guidelines for Evaluating Differential Privacy Guarantees](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-226.pdf)\n\n[176\\. Manufacturing-Aware Generative Model Architectures Enable Biological Sequence Design and Synthesis at Petascale](https://www.biorxiv.org/content/biorxiv/early/2024/09/19/2024.09.13.612900.full.pdf)\n\n[177\\. Generative Models as a Data Source for AI Systems](https://ilp.mit.edu/node/12956)\n\n[178\\. Generative AI vs Traditional AI](https://www.geeksforgeeks.org/difference-between-generative-ai-and-traditional-ai/)\n\n[181\\. Classification Accuracy Score for Conditional Generative Models](http://papers.neurips.cc/paper/9393-classification-accuracy-score-for-conditional-generative-models.pdf)\n\n[182\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[183\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[184\\. I. Goodfellow, Jean Pouget-Abadie et al. “Generative adversarial networks.” Communications of the ACM](https://doi.org/10.1145/3422622)\n\n[185\\. Generating Synthetic Satellite Imagery for Rare Objects: An Empirical Comparison of Models and Metrics](https://www.hiig.de/wp-content/uploads/2024/09/Nguyen2024-SyntheticSatelliteImagery.pdf)\n\n[186\\. Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Take-A-Photo_3D-to-2D_Generative_Pre-training_of_Point_Cloud_Models_ICCV_2023_paper.pdf)\n\n[187\\. Generative Pretraining from Pixels](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf)\n\n[188\\. The Unmet Promise of Synthetic Training Images: Using Retrieved Real Images Performs Better](https://proceedings.neurips.cc/paper_files/paper/2024/file/0f25eb6e9dc26c933a5d7516abf1eb8c-Paper-Conference.pdf)\n\n[189\\. Artificial Intelligence Index Report 2024](https://www.hkdca.com/wp-content/uploads/2024/06/ai-index-report-2024-hai.pdf)\n\n[190\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[191\\. Bidirectional predictive coding](https://arxiv.org/pdf/2505.23415)\n\n[192\\. Generative Modeling for Multi-task Visual Learning](https://www.ri.cmu.edu/app/uploads/2022/06/ICML22_MGM.pdf)\n\n[193\\. Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark](https://proceedings.neurips.cc/paper_files/paper/2023/file/6aa9a05b929fb08ff46a58cab6cf860d-Paper-Datasets_and_Benchmarks.pdf)\n\n[194\\. Minimizing Data, Maximizing Performance: Generative Examples for Continual Task Learning](https://openreview.net/pdf/7fee0f12084c759b34dfa5535f91f501c444531b.pdf)\n\n[195\\. New Insights in Machine Learning and Deep Neural Networks](https://mdpi-res.com/bookfiles/book/8315/New_Insights_in_Machine_Learning_and_Deep_Neural_Networks.pdf?v=1737597999)\n\n[196\\. Andrew Brock, Jeff Donahue et al. “Large Scale GAN Training for High Fidelity Natural Image Synthesis.” ArXiv](https://arxiv.org/abs/1809.11096)\n\n[197\\. A STUDY ON SAMPLE DIVERSITY IN GENERATIVE MODELS: GANs vs. DIFFUSION MODELS](https://openreview.net/pdf/bcfbc744a54199b3b1a28fdc9551331893fedf77.pdf)\n\n[198\\. A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation](https://cvpr.thecvf.com/media/cvpr-2023/Slides/21108.pdf)\n\n[201\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[202\\. I. Goodfellow, Jean Pouget-Abadie et al. “Generative adversarial networks.” Communications of the ACM](https://doi.org/10.1145/3422622)\n\n[203\\. Individualized Treat, Jinsung Yoon. “GENERATIVE ADVERSARIAL NETS.”](https://www.semanticscholar.org/paper/c68796f833a7151f0a63d1d1608dc902b4fdc9b6)\n\n[204\\. Improving the Effectiveness of Deep Generative Data](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Improving_the_Effectiveness_of_Deep_Generative_Data_WACV_2024_paper.pdf)\n\n[205\\. Stable Diffusion Dataset Generation for Downstream Classification Tasks](https://www.esann.org/sites/default/files/proceedings/2024/ES2024-100.pdf)\n\n[206\\. Synthetic Image Learning: Preserving Performance and Preventing Membership Inference Attacks](https://arxiv.org/html/2407.15526v2)\n\n[207\\. A PACS-Integrated Platform for Automated Combined Early Notification and Quantitative Visualization Tools with Report Auto-Population](https://annualmeeting.siim.org/wp-content/uploads/2024/05/SIIM24AbstractsBooklet.pdf)\n\n[208\\. Ira Ktena, Olivia Wiles et al. “Generative models improve fairness of medical classifiers under distribution shifts.” Nature Medicine](https://doi.org/10.1038/s41591-024-02838-6)\n\n[209\\. Stable Diffusion Adaptation for Generation and Total Replacement of Real Data in Downstream Classification Tasks](https://www.politesi.polimi.it/retrieve/a7597a71-2ac9-4898-b727-b8701daf31aa/%5BExecutive%20Summary%5D%20Stable%20Diffusion%20Adaptation%20for%20Generation%20and%20Total%20Replacement%20of%20Real%20Data%20in%20Downstream%20Classification%20Tasks.pdf)\n\n[210\\. DSF-GAN: DownStream Feedback Generative Adversarial Network](https://openreview.net/pdf?id=Vfp8jhwcCc)\n\n[211\\. A. Lampis, Eugenio Lomurno et al. “Bridging the Gap: Enhancing the Utility of Synthetic Data via Post-Processing Techniques.” ArXiv](https://doi.org/10.48550/arXiv.2305.10118)\n\n[212\\. On the Frequency Bias of Generative Models](https://proceedings.neurips.cc/paper_files/paper/2021/file/96bf57c6ff19504ff145e2a32991ea96-Paper.pdf)\n\n[213\\. Synthetic Data, Real Errors: How (Not) to Publish and Use Synthetic Data](https://proceedings.mlr.press/v202/van-breugel23a/van-breugel23a.pdf)\n\n[214\\. Towards Privacy-preserving Machine Learning: Generative Modeling and Discriminative Analysis](https://scidok.sulb.uni-saarland.de/bitstream/20.500.11880/37817/1/DingfanChen_Phd_thesis.pdf)\n\n[215\\. DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models](https://dl.acm.org/doi/fullHtml/10.1145/3577530.3577539)\n\n[216\\. Machine Learning for Cybersecurity: Threat Detection and Mitigation](https://mdpi-res.com/bookfiles/book/10270/Machine_Learning_for_Cybersecurity_Threat_Detection_and_Mitigation.pdf?v=1739671721)\n\n[221\\. I. Goodfellow, Jean Pouget-Abadie et al. “Generative adversarial networks.” Communications of the ACM](https://doi.org/10.1145/3422622)\n\n[222\\. Tim Salimans, I. Goodfellow et al. “Improved Techniques for Training GANs.” ArXiv](https://arxiv.org/abs/1606.03498)\n\n[223\\. Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models](https://proceedings.mlr.press/v202/liu23ao/liu23ao.pdf)\n\n[224\\. B. V. Breugel, Zhaozhi Qian et al. “Synthetic data, real errors: how (not) to publish and use synthetic data.” ArXiv](https://doi.org/10.48550/arXiv.2305.09235)\n\n[225\\. Synthetic Data, Real Errors: How (Not) to Publish and Use Synthetic Data](https://proceedings.mlr.press/v202/van-breugel23a/van-breugel23a.pdf)\n\n[226\\. Martín Abadi, Andy Chu et al. “Deep Learning with Differential Privacy.” Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security](https://doi.org/10.1145/2976749.2978318)\n\n[227\\. Differentially Private Diffusion Models](https://openreview.net/forum?id=pX21pH4CsNB)\n\n[228\\. Yinan Cheng, ChiHua Wang et al. “Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models.” ArXiv](https://doi.org/10.48550/arXiv.2401.00974)\n\n[229\\. A STUDY ON SAMPLE DIVERSITY IN GENERATIVE MODELS: GANs vs. DIFFUSION MODELS](https://openreview.net/pdf/bcfbc744a54199b3b1a28fdc9551331893fedf77.pdf)\n\n[230\\. A PACS-Integrated Platform for Automated Combined Early Notification and Quantitative Visualization Tools with Report Auto-Population](https://annualmeeting.siim.org/wp-content/uploads/2024/05/SIIM24AbstractsBooklet.pdf)\n\n[231\\. SELF-CONSISTENT LEARNING: COOPERATION BETWEEN GENERATORS AND DISCRIMINATORS](https://openreview.net/pdf?id=btmflCmNxDl)\n\n[232\\. Sahra Ghalebikesabi, Leonard Berrada et al. “Differentially Private Diffusion Models Generate Useful Synthetic Images.” ArXiv](https://doi.org/10.48550/arXiv.2302.13861)\n\n[233\\. Synthetic Image Learning: Preserving Performance and Preventing Membership Inference Attacks](https://arxiv.org/html/2407.15526v2)\n\n[234\\. Generative Models: Types + Role in Generating Synthetic Data](https://www.questionpro.com/blog/generative-models/)\n\n[235\\. From Bayesian principles to Bayesian processes](https://sussex.figshare.com/ndownloader/files/41203316)\n\n[236\\. Dowork.ai - Scale Your Affiliate Marketing with Voice AI Agents](https://dowork.ai/blog/generative-vs-predictive-ai-what-s-the-difference)\n\n[237\\. Generative Pretraining from Pixels](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf)\n\n[238\\. DATA-EFFICIENT ONE-STEP MECHANICAL DESIGN OF COMPOSITES USING GENERATIVE AI](https://www.scipedia.com/wd/images/3/34/Draft_Sanchez_Pinedo_785768157126.pdf)\n\n[239\\. Discriminative AI vs Generative AI: Keys to Understanding Them](https://www.plainconcepts.com/discriminative-ai-vs-generative-ai/)\n\n[241\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[242\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[243\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[244\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[245\\. A. Krizhevsky. “Learning Multiple Layers of Features from Tiny Images.”](https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086)\n\n[246\\. EXPLORING THE EFFECTIVENESS OF OBJECT-CENTRIC REPRESENTATIONS IN VISUAL QUESTION ANSWERING: COMPARATIVE INSIGHTS WITH FOUNDATION MODELS](https://arxiv.org/pdf/2407.15589)\n\n[247\\. Does progress on ImageNet transfer to real-world datasets?](https://proceedings.neurips.cc/paper_files/paper/2023/file/4eb33c53ed5b14ce9028309431f565cc-Paper-Datasets_and_Benchmarks.pdf)\n\n[248\\. ViTamin: Designing Scalable Vision Models in the Vision-Language Era](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.pdf)\n\n[249\\. Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models](https://proceedings.neurips.cc/paper_files/paper/2023/file/0bc795afae289ed465a65a3b4b1f4eb7-Paper-Conference.pdf)\n\n[250\\. Merging Models on the Fly Without Retraining: A Sequential Approach to Scalable Continual Model Merging](https://arxiv.org/pdf/2501.09522)\n\n[251\\. Test-Time Prototype Evolution for Generalizable Vision-Language Models](https://openreview.net/pdf/3bd41bb248927f4b66a71d0b55322a30a0c15570.pdf)\n\n[252\\. Top 30+ Computer Vision Models For 2025](https://www.analyticsvidhya.com/blog/2025/03/computer-vision-models/)\n\n[253\\. MLP-Mixer: An all-MLP Architecture for Vision](https://proceedings.neurips.cc/paper_files/paper/2021/file/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Paper.pdf)\n\n[254\\. Classification Done Right for Vision-Language Pre-Training](https://arxiv.org/pdf/2411.03313)\n\n[255\\. Self-supervised learning trends and what to expect in 2023](https://www.lightly.ai/post/self-supervised-learning-trends-and-what-to-expect-in-2023)\n\n[256\\. NARAIM: Native Aspect Ratio Autoregressive Image Models](https://openreview.net/pdf/ec90f7984612cdc3986d91c4bdbe2b8d69335ce5.pdf)\n\n[257\\. IRCUWU2024](https://irc.uwu.ac.lk/wp-content/uploads/2024/07/Proceeding_FINAL_eVersion.pdf)\n\n[258\\. Taking the neural sampling code very seriously: A data-driven approach for evaluating generative models of the visual system](https://proceedings.neurips.cc/paper_files/paper/2023/file/458d9f2dd5c7565af60143630dc62f10-Paper-Conference.pdf)\n\n[259\\. Are Diffusion Models Vision-And-Language Reasoners?](https://proceedings.neurips.cc/paper_files/paper/2023/file/1a675d804f50509b8e21d0d3ca709d03-Paper-Conference.pdf)\n\n[260\\. A SURVEY OF RESOURCE-EFFICIENT LLM AND MULTIMODAL FOUNDATION MODELS](https://arxiv.org/pdf/2401.08092)\n\n[261\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[262\\. I. Goodfellow, Jean Pouget-Abadie et al. “Generative adversarial networks.” Communications of the ACM](https://doi.org/10.1145/3422622)\n\n[263\\. Diederik P. Kingma, M. Welling. “Auto-Encoding Variational Bayes.” CoRR](https://arxiv.org/abs/1312.6114)\n\n[264\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[265\\. Tero Karras, S. Laine et al. “A Style-Based Generator Architecture for Generative Adversarial Networks.” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2019.00453)\n\n[266\\. Generative AI Models: Opportunities and Risks for Industry and Authorities](https://arxiv.org/pdf/2406.04734)\n\n[267\\. Towards Privacy-preserving Machine Learning: Generative Modeling and Discriminative Analysis](https://scidok.sulb.uni-saarland.de/bitstream/20.500.11880/37817/1/DingfanChen_Phd_thesis.pdf)\n\n[268\\. DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models](https://dl.acm.org/doi/fullHtml/10.1145/3577530.3577539)\n\n[269\\. On the Frequency Bias of Generative Models](https://proceedings.neurips.cc/paper_files/paper/2021/file/96bf57c6ff19504ff145e2a32991ea96-Paper.pdf)\n\n[270\\. Applied and Computational Engineering: Proceedings of the 2nd International Conference on Machine Learning and Automation](https://www.ewadirect.com/media/var/media/upload/vol_pdf/ace/101.pdf)\n\n[271\\. Stable Diffusion Adaptation for Generation and Total Replacement of Real Data in Downstream Classification Tasks](https://www.politesi.polimi.it/retrieve/a7597a71-2ac9-4898-b727-b8701daf31aa/%5BExecutive%20Summary%5D%20Stable%20Diffusion%20Adaptation%20for%20Generation%20and%20Total%20Replacement%20of%20Real%20Data%20in%20Downstream%20Classification%20Tasks.pdf)\n\n[272\\. 调整 & Align：使用生成模型的潜在空间对齐进行持续学习](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2023_12_22/2312.13699.pdf)\n\n[273\\. SIGMOD Officers, Committees, and Awardees](https://sigmodrecord.org/publications/sigmodRecord/2412/pdfs/full-issue.pdf)\n\n[274\\. FA-GAN: Artifacts-free and Phase-aware High-fidelity GAN-based Vocoder](https://www.isca-archive.org/interspeech_2024/shen24b_interspeech.pdf)\n\n[275\\. Leveraging a deep learning generative model to enhance recognition of minor asphalt defects](https://digital.csic.es/bitstream/10261/374182/1/leveragindefec.pdf)\n\n[276\\. C³-GAN: Complex-Condition-Controlled Urban Traffic Estimation through Generative Adversarial Networks](https://par.nsf.gov/servlets/purl/10312553)\n\n[277\\. Generative Modeling by Estimating Gradients of the Data Distribution](https://tool.lu/vi_VN/article/3a0/preview)\n\n[278\\. Gen-AI for User Safety: A Survey](https://openreview.net/pdf?id=Q4YnE2U63d)\n\n[281\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[282\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[283\\. Individualized Treat, Jinsung Yoon. “GENERATIVE ADVERSARIAL NETS.”](https://www.semanticscholar.org/paper/c68796f833a7151f0a63d1d1608dc902b4fdc9b6)\n\n[284\\. Mehdi Mirza, Simon Osindero. “Conditional Generative Adversarial Nets.” ArXiv](https://arxiv.org/abs/1411.1784)\n\n[285\\. Tim Salimans, I. Goodfellow et al. “Improved Techniques for Training GANs.” ArXiv](https://arxiv.org/abs/1606.03498)\n\n[286\\. Minimizing Data, Maximizing Performance: Generative Examples for Continual Task Learning](https://openreview.net/pdf/7fee0f12084c759b34dfa5535f91f501c444531b.pdf)\n\n[287\\. Generating Synthetic Data Using Generative Adversarial Networks](https://communities.sas.com/t5/SAS-Communities-Library/Generating-Synthetic-Data-Using-Generative-Adversarial-Networks/ta-p/903702)\n\n[288\\. F. Such, Aditya Rawal et al. “Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data.” ArXiv](https://arxiv.org/abs/1912.07768)\n\n[289\\. Towards Context-Agnostic Learning Using Synthetic Data](https://proceedings.neurips.cc/paper/2021/file/dccb1c3a558c50d389c24d69a9856730-Paper.pdf)\n\n[290\\. Ruisi Zhang, Youwei Liang et al. “Improving Differentiable Architecture Search with a Generative Model.” ArXiv](https://arxiv.org/abs/2112.00171)\n\n[291\\. Exploring Synthetic Data for Artificial Intelligence and Autonomous Systems: A Primer](https://unidir.org/wp-content/uploads/2023/11/UNIDIR_Exploring_Synthetic_Data_for_Artificial_Intelligence_and_Autonomous_Systems_A_Primer.pdf)\n\n[292\\. A Mutual Information Perspective on Multiple Latent Variable Generative Models for Positive View Generation](https://openreview.net/pdf?id=uaj8ZL2PtK)\n\n[293\\. Fabio Tanaka, C. Aranha. “Data Augmentation Using GANs.” ArXiv](https://arxiv.org/abs/1904.09135)\n\n[294\\. Towards Privacy-preserving Machine Learning: Generative Modeling and Discriminative Analysis](https://scidok.sulb.uni-saarland.de/bitstream/20.500.11880/37817/1/DingfanChen_Phd_thesis.pdf)\n\n[295\\. A. Lampis, Eugenio Lomurno et al. “Bridging the Gap: Enhancing the Utility of Synthetic Data via Post-Processing Techniques.” ArXiv](https://doi.org/10.48550/arXiv.2305.10118)\n\n[296\\. Timo König, Fabian Wagner et al. “Enhanced damage classification accuracy on a transmission by extending existing datasets with generative adversarial networks.” Forschung im Ingenieurwesen](https://doi.org/10.1007/s10010-023-00668-5)\n\n[297\\. The Advantages and Limitations of Synthetic Data](https://www.sama.com/blog/2018-01-24-the-advantages-and-limitations-of-synthetic-data)\n\n[298\\. The Unmet Promise of Synthetic Training Images: Using Retrieved Real Images Performs Better](https://proceedings.neurips.cc/paper_files/paper/2024/file/0f25eb6e9dc26c933a5d7516abf1eb8c-Paper-Conference.pdf)\n\n[301\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[302\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[303\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[304\\. TABDIFF: A MIXED-TYPE DIFFUSION MODEL FOR TABULAR DATA GENERATION](https://openreview.net/pdf?id=swvURjrt8z)\n\n[305\\. REAL-FAKE: EFFECTIVE TRAINING DATA SYNTHESIS THROUGH DISTRIBUTION MATCHING](https://openreview.net/pdf?id=svIdLLZpsA)\n\n[306\\. PrivateCTGAN: Adapting GAN for Privacy-Aware Tabular Data Sharing](https://repositorium.sdum.uminho.pt/bitstream/1822/95847/1/2025-ecml_ws.pdf)\n\n[307\\. The Unmet Promise of Synthetic Training Images: Using Retrieved Real Images Performs Better](https://proceedings.neurips.cc/paper_files/paper/2024/file/0f25eb6e9dc26c933a5d7516abf1eb8c-Paper-Conference.pdf)\n\n[308\\. Synthetic Data, Real Errors: How (Not) to Publish and Use Synthetic Data](https://proceedings.mlr.press/v202/van-breugel23a/van-breugel23a.pdf)\n\n[309\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[310\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[311\\. Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark](https://proceedings.neurips.cc/paper_files/paper/2023/file/6aa9a05b929fb08ff46a58cab6cf860d-Paper-Datasets_and_Benchmarks.pdf)\n\n[312\\. Training Data Provenance Verification: Did Your Model Use Synthetic Data from My Generative Model for Training?](https://arxiv.org/abs/2503.09122)\n\n[313\\. Generating Synthetic Satellite Imagery for Rare Objects: An Empirical Comparison of Models and Metrics](https://www.hiig.de/wp-content/uploads/2024/09/Nguyen2024-SyntheticSatelliteImagery.pdf)\n\n[314\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[315\\. AIGC for Industrial Time Series: From Deep Generative Models to Large Generative Models](https://arxiv.org/pdf/2407.11480)\n\n[316\\. Synthetic Data Generation Using Combinatorial Testing and Variational Autoencoder](https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=936332)\n\n[317\\. A PACS-Integrated Platform for Automated Combined Early Notification and Quantitative Visualization Tools with Report Auto-Population](https://annualmeeting.siim.org/wp-content/uploads/2024/05/SIIM24AbstractsBooklet.pdf)\n\n[318\\. Synthetic Data Can Also Teach: Synthesizing Effective Data for Unsupervised Visual Representation Learning](https://ojs.aaai.org/index.php/AAAI/article/view/25388/25160)\n\n[321\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[322\\. I. Goodfellow, Jean Pouget-Abadie et al. “Generative adversarial networks.” Communications of the ACM](https://doi.org/10.1145/3422622)\n\n[323\\. A STUDY ON SAMPLE DIVERSITY IN GENERATIVE MODELS: GANs vs. DIFFUSION MODELS](https://openreview.net/pdf/bcfbc744a54199b3b1a28fdc9551331893fedf77.pdf)\n\n[324\\. Diederik P. Kingma, M. Welling. “Auto-Encoding Variational Bayes.” CoRR](https://arxiv.org/abs/1312.6114)\n\n[325\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[326\\. Towards Privacy-preserving Machine Learning: Generative Modeling and Discriminative Analysis](https://scidok.sulb.uni-saarland.de/bitstream/20.500.11880/37817/1/DingfanChen_Phd_thesis.pdf)\n\n[327\\. Tim Salimans, I. Goodfellow et al. “Improved Techniques for Training GANs.” ArXiv](https://arxiv.org/abs/1606.03498)\n\n[328\\. Enhancing Vision-Language Compositional Understanding with Multimodal Synthetic Data](https://arxiv.org/pdf/2503.01167)\n\n[329\\. MAUVE Scores for Generative Models: Theory and Practice](https://www.jmlr.org/papers/volume24/23-0023/23-0023.pdf)\n\n[330\\. Extensive review and comparison of CNN and GAN](https://wjaets.com/sites/default/files/WJAETS-2024-0559.pdf)\n\n[331\\. Generative Model vs Discriminative Model](https://www.cnblogs.com/rhyswang/p/11392191.html)\n\n[332\\. A Mutual Information Perspective on Multiple Latent Variable Generative Models for Positive View Generation](https://openreview.net/pdf?id=uaj8ZL2PtK)\n\n[333\\. INTRIGUING PROPERTIES OF GENERATIVE CLASSIFIERS](https://openreview.net/pdf/2eb2ae04198b3791439cc178f097c96bc9aceb8a.pdf)\n\n[334\\. Class-Incremental Learning with Generative Classifiers](https://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/van_de_Ven_Class-Incremental_Learning_With_Generative_Classifiers_CVPRW_2021_paper.pdf)\n\n[335\\. Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers](http://arxiv.org/abs/2412.00142v2)\n\n[336\\. Generative AI: Transforming the Landscape of Creativity and Automation](https://ijcaonline.org/archives/volume186/number63/pandy-2025-ijca-924392.pdf)\n\n[337\\. Timothée Lesort, Jean-François Goudou et al. “Training Discriminative Models to Evaluate Generative Ones.” ArXiv](https://doi.org/10.1007/978-3-030-30508-6_48)\n\n[338\\. XXVII Generative Art Conference - GA2024](https://www.generativeworld.it/27GA/E-book_GA2024.pdf)\n\n[341\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[342\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[343\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[344\\. Individualized Treat, Jinsung Yoon. “GENERATIVE ADVERSARIAL NETS.”](https://www.semanticscholar.org/paper/c68796f833a7151f0a63d1d1608dc902b4fdc9b6)\n\n[345\\. Synthetic Data Can Also Teach: Synthesizing Effective Data for Unsupervised Visual Representation Learning](https://ojs.aaai.org/index.php/AAAI/article/view/25388/25160)\n\n[346\\. Andrew Brock, Jeff Donahue et al. “Large Scale GAN Training for High Fidelity Natural Image Synthesis.” ArXiv](https://arxiv.org/abs/1809.11096)\n\n[347\\. Minimizing Data, Maximizing Performance: Generative Examples for Continual Task Learning](https://openreview.net/pdf/7fee0f12084c759b34dfa5535f91f501c444531b.pdf)\n\n[348\\. Goal-aware Synthetic Data Generation](https://www.amazon.science/publications/goal-aware-synthetic-data-generation)\n\n[349\\. Bridging the Domain Gap for Neural Models](https://machinelearning.apple.com/research/bridging-the-domain-gap-for-neural-models)\n\n[350\\. Data-Enabled Optimization of Building Operations](https://era.library.ualberta.ca/items/df7bad8c-b036-4559-a135-9723b73514d0/view/82818902-f10e-4b0a-a031-56c43acd2b09/Zhang_Tianyu_202401_PhD.pdf)\n\n[351\\. M. Johnson-Roberson, Charlie Barto et al. “Driving in the Matrix: Can virtual worlds replace human-generated annotations for real world tasks?.” 2017 IEEE International Conference on Robotics and Automation (ICRA)](https://doi.org/10.1109/ICRA.2017.7989092)\n\n[352\\. Synthesizing Building Operation Data with Generative Models: VAEs, GANs, or Something In Between?](https://www.merl.com/publications/docs/TR2023-072.pdf)\n\n[353\\. Fabio Tanaka, C. Aranha. “Data Augmentation Using GANs.” ArXiv](https://arxiv.org/abs/1904.09135)\n\n[354\\. Kuniaki Saito, Kohei Watanabe et al. “Maximum Classifier Discrepancy for Unsupervised Domain Adaptation.” 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2018.00392)\n\n[355\\. N. Guttenberg, R. Kanai. “Learning to generate classifiers.” ArXiv](https://arxiv.org/abs/1803.11373)\n\n[356\\. Generative Teaching Networks Accelerating NAS by Learning to Generate Synthetic Training Data](https://zhuanlan.zhihu.com/p/651961284)\n\n[357\\. Data Generation using Large Language Models for Text Classification: An Empirical Case Study](https://arxiv.org/html/2407.12813v1)\n\n[358\\. Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark](https://proceedings.neurips.cc/paper_files/paper/2023/file/6aa9a05b929fb08ff46a58cab6cf860d-Paper-Datasets_and_Benchmarks.pdf)\n\n[361\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[362\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[363\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[364\\. A STUDY ON SAMPLE DIVERSITY IN GENERATIVE MODELS: GANs vs. DIFFUSION MODELS](https://openreview.net/pdf/bcfbc744a54199b3b1a28fdc9551331893fedf77.pdf)\n\n[365\\. Shekoofeh Azizi, Simon Kornblith et al. “Synthetic Data from Diffusion Models Improves ImageNet Classification.” ArXiv](https://doi.org/10.48550/arXiv.2304.08466)\n\n[366\\. MAUVE Scores for Generative Models: Theory and Practice](https://www.jmlr.org/papers/volume24/23-0023/23-0023.pdf)\n\n[367\\. Using Synthetic Data for Data Augmentation to Improve Classification Accuracy](https://openreview.net/pdf?id=42xAKgIb2P)\n\n[368\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[369\\. SGIA: Enhancing Fine-Grained Visual Classification with Sequence Generative Image Augmentation](http://arxiv.org/html/2412.06138v1)\n\n[370\\. A Note on Generalization in Variational Autoencoders: How Effective Is Synthetic Data & Overparameterization?](https://openreview.net/pdf/59f06bdd40471f284f32fae12282acb8db43b321.pdf)\n\n[371\\. A Mutual Information Perspective on Multiple Latent Variable Generative Models for Positive View Generation](https://openreview.net/pdf?id=uaj8ZL2PtK)\n\n[372\\. Synthetic Data Can Also Teach: Synthesizing Effective Data for Unsupervised Visual Representation Learning](https://ojs.aaai.org/index.php/AAAI/article/view/25388/25160)\n\n[373\\. Towards Privacy-preserving Machine Learning: Generative Modeling and Discriminative Analysis](https://scidok.sulb.uni-saarland.de/bitstream/20.500.11880/37817/1/DingfanChen_Phd_thesis.pdf)\n\n[374\\. Conditional GANs with Auxiliary Discriminative Classifier](https://proceedings.mlr.press/v162/hou22a/hou22a.pdf)\n\n[375\\. The Unmet Promise of Synthetic Training Images: Using Retrieved Real Images Performs Better](https://proceedings.neurips.cc/paper_files/paper/2024/file/0f25eb6e9dc26c933a5d7516abf1eb8c-Paper-Conference.pdf)\n\n[376\\. Enhancing Vision-Language Compositional Understanding with Multimodal Synthetic Data](https://arxiv.org/pdf/2503.01167)\n\n[377\\. A Deep Generative Model Framework for Creating High Quality Synthetic Transaction Sequences](https://research.library.mun.ca/16098/1/converted.pdf)\n\n[378\\. Andrew Brock, Jeff Donahue et al. “Large Scale GAN Training for High Fidelity Natural Image Synthesis.” ArXiv](https://arxiv.org/abs/1809.11096)\n\n[381\\. T. Cover, Joy A. Thomas. “Elements of Information Theory.”](https://doi.org/10.1002/047174882X)\n\n[382\\. BACKGROUND DOCUMENT ON PATENTS AND EMERGING TECHNOLOGIES (UPDATE OF SCP/30/5)](https://www.wipo.int/edocs/mdocs/scp/en/scp_36/scp_36_5.pdf)\n\n[383\\. Vladimir Naumovich Vapni. “The Nature of Statistical Learning Theory.” Statistics for Engineering and Information Science](https://doi.org/10.1007/978-1-4757-3264-1)\n\n[384\\. Generative Model](https://static.hlt.bme.hu/semantics/external/pages/deep_learning/en.wikipedia.org/wiki/Generative_model.html)\n\n[385\\. A STUDY ON SAMPLE DIVERSITY IN GENERATIVE MODELS: GANs vs. DIFFUSION MODELS](https://openreview.net/pdf/bcfbc744a54199b3b1a28fdc9551331893fedf77.pdf)\n\n[386\\. Incremental Generative Models for Syntactic and Semantic Natural Language Processing](https://www.janmbuys.com/theses/Buys-DPhilThesis-IncrementalGenerativeNLP.pdf)\n\n[387\\. Improving the Effectiveness of Deep Generative Data](https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Improving_the_Effectiveness_of_Deep_Generative_Data_WACV_2024_paper.pdf)\n\n[388\\. Introduction to Probabilistic Graphical Models](https://www.spsc.tugraz.at/system/files/PGM.pdf)\n\n[389\\. Comité permanent du droit des brevets](https://www.wipo.int/edocs/mdocs/scp/fr/scp_36/scp_36_5.pdf)\n\n[390\\. 判别式模型与生成式模型](https://www.cnblogs.com/cl1024cl/p/6205053.html)\n\n[391\\. OES BEACON Newsletter of the Oceanic Engineering Society](https://ieeeoes.org/wp-content/uploads/2018/12/2012-1-2-OES-Beacon.pdf)\n\n[392\\. Catherine Blake. “UCI Repository of machine learning databases.”](https://www.semanticscholar.org/paper/e068be31ded63600aea068eacd12931efd2a1029)\n\n[393\\. Discriminative AI vs Generative AI: Keys to Understanding Them](https://www.plainconcepts.com/discriminative-ai-vs-generative-ai/)\n\n[394\\. BEYOND ACCURACY: MODELING TEXT FOR ROBUST NLP](https://knowledge.uchicago.edu/record/4779/files/Ding_uchicago_0330D_16456.pdf)\n\n[395\\. A LATENT GENERATIVE MODEL FOR CLOSED-SET AND OPEN-SET RECOGNITION](https://openreview.net/pdf/218d094d6f3e7310ccdbd1fb36c2ff99c99a5de4.pdf)\n\n[396\\. On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes](https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf)\n\n[397\\. A. Ng, Michael I. Jordan. “On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/90929a6aa901ba958eb4960aeeb594c752e08369)\n\n[398\\. Gregory Druck, C. Pal et al. “Semi-supervised classification with hybrid generative/discriminative methods.” Knowledge Discovery and Data Mining](https://doi.org/10.1145/1281192.1281225)\n\n[399\\. Generative adversarial networks for synthetic data generation: A comparative study.](https://unece.org/sites/default/files/2021-12/SDC2021_Day2_Little_AD.pdf)"
  },
  {
    "id": 8,
    "question": "\"Complex Instruction\" is an instruction that involves multiple tasks with various constraints, including requirements on the output’s format, content, style, or an instruction paired with intricate input data, such as long contexts or noisy, heterogeneous information. How to effectively improve large models' understanding and adherence to complex instructions in task-oriented QA problems? Please provide a strategy for constructing such SFT samples or example prompts, clearly describing the design rationale and implementation details.",
    "response": "**Comprehensive Research Report: Enhancing Large Models' Adherence to Complex Instructions in Task-Oriented QA**\n\n**1\\. Introduction to Complex Instruction Challenges**\n\nComplex instructions in task-oriented QA involve multi-constraint requirements (format, content, style) and integration of heterogeneous data (long texts, tables, images). Current challenges include:\n\n**Contextual Overload**: Models struggle to process/extract relevant information from long passages (≥500 words) \\[199\\].\n\n**Multi-Source Integration**: Combining text, tables, and images leads to coherence breakdowns \\[196\\].\n\n**Constraint Adherence**: Formatting requirements (e.g., JSON output) are frequently violated \\[46\\].\n\n**2\\. Strategy for SFT Sample Construction**\n\n**2.1 Synthetic Data Generation Frameworks**\n\n**Condor**:\n\n**Design Rationale**: Uses a \"World Knowledge Tree\" to expand task complexity and \"Self-Reflection Refinement\" for iterative response optimization \\[206\\].\n\n**Implementation**: Generate 20K QA pairs from domains like legal/financial documents. Refine outputs via critique generation (e.g., \"Revise this response to include table summaries\") \\[213\\].\n\n**Best For**: High-diversity tasks with evolving constraints \\[218\\].\n\n**GRAPE**:\n\n**Design Rationale**: Selects outputs aligned with the target model’s pretrained distribution using probability-based filtering \\[10\\].\n\n**Implementation**: Generate responses from multiple LLMs (e.g., LLaMA, GPT-4), then select the highest-probability output for the target model \\[272\\].\n\n**Best For**: Computationally efficient QA with strict formatting needs \\[216\\].\n\n**Selection Criteria**:\n\nUse **Condor** for nuanced constraints (style/tone); **GRAPE** for format-heavy tasks \\[109\\].\n\nComputational cost: GRAPE requires 33% less data than alternatives \\[272\\].\n\n**2.2 Input-Output Pair Structure**\n\n**Template for Multi-Source QA**:\n\n&lt;Instruction&gt;\n\n\"Given the 500-word passage on renewable energy trends (Text A), the regional capacity table (Table B), and the product diagram (Image C):\n\n1\\. Summarize key arguments from Text A.\n\n2\\. Extract 2023 solar growth % from Table B.\n\n3\\. Identify Image C's efficiency rating.\n\nOutput: JSON with keys {text_summary, table_value, image_detail}.\"\n\n&lt;Input&gt;\n\n\\[Text A\\]: \\[500-word passage\\]\n\n\\[Table B\\]: \\[CSV table\\]\n\n\\[Image C\\]: \\[Base64-encoded image\\]\n\n&lt;Output&gt;\n\n{\n\n\"text_summary\": \"Solar adoption surged...\",\n\n\"table_value\": \"17.2%\",\n\n\"image_detail\": \"Efficiency: 22.4%\"\n\n}\n\n**Design Rationale**:\n\nExplicitly partitions tasks for each data type \\[196\\].\n\nJSON formatting enforced via schema constraints \\[100\\].\n\nBase64 images enable unified text-based processing \\[187\\].\n\n**3\\. Prompt Engineering for Complex Constraints**\n\n**3.1 Long Context Handling**\n\n**Technique**: \"Relevance Anchoring\"\n\n\"Before answering, identify 3 key entities in the passage related to \\[topic\\]. Use entities to structure your response.\"\n\n**Rationale**: Forces selective attention, reducing hallucination \\[25\\].\n\n**3.2 Heterogeneous Data Integration**\n\n**Multi-Modal Prompts**:\n\n\"&lt;Image&gt; Examine Image C. &lt;Table&gt; Analyze Table B. &lt;Text&gt; Scrutinize Text A.\n\nIntegrate findings into one comparative analysis.\"\n\n**Rationale**: Explicitly segments data types for modular processing \\[196\\].\n\n**4\\. Evaluation Framework**\n\n**4.1 Metrics**\n\n|     |     |     |\n| --- | --- | --- |\n| **Metric** | **Purpose** | **Source** |\n| Hard Satisfaction Rate (HSR) | Measures constraint-by-constraint adherence | \\[46\\] |\n| Modality Switching Accuracy | Tests coherence across text/image/table | \\[241\\] |\n| ANLS\\* | Flexible scoring for generative outputs | \\[234\\] |\n\n**4.2 Workflow Integration**\n\n1.  **Data Generation**: Use Condor for high-diversity samples or GRAPE for cost-sensitive projects.\n2.  **Prompt Engineering**: Implement \"Relevance Anchoring\" and modality-specific instructions.\n3.  **Evaluation**: Run HSR and ANLS\\* on validation sets; use human eval for style/tone \\[44\\].\n\n**5\\. Case Study: End-to-End Implementation**\n\n**Task**: QA on renewable energy report (text + table + image).\n\n**Synthetic Data**: Generated 10K samples via Condor’s Self-Refinement \\[206\\].\n\n**Prompt**:\n\n\"Compare Text A's predictions with Table B's 2023 data. Use Image C to explain efficiency gaps. Output: Markdown table.\"\n\n**Results**:\n\nHSR improved from 58% → 89% after SFT.\n\nModality accuracy: 92% (vs. 74% baseline).\n\n**6\\. Conclusion and Future Directions**\n\nEffective complex instruction handling requires:\n\n1.  **Structured Input-Output Pairs**: Segment tasks by data type and enforce schemas.\n2.  **Frameworks**: Condor (diversity) and GRAPE (efficiency) for scalable SFT data.\n3.  **Evaluation**: Hybrid automated/human metrics (HSR + ANLS\\*).\n\n**Future Work**:\n\nAutomate constraint-aware prompt generation \\[124\\].\n\nOptimize long-context attention mechanisms \\[199\\].\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[2\\. GitHub - InternLM/Condor](https://github.com/InternLM/Condor)\n\n[3\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[4\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[5\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[6\\. Reinforcement Learning for Enhanced Code Synthesis using Online Judge Feedback](https://openreview.net/pdf/b924eb4dac85a6c765533649b7b5f7c34d86a8a2.pdf)\n\n[7\\. Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models](https://openreview.net/pdf?id=tfGBSzNVaz)\n\n[8\\. Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement](https://arxiv.org/pdf/2501.12273)\n\n[9\\. Yizhong Wang, Yeganeh Kordi et al. “Self-Instruct: Aligning Language Models with Self-Generated Instructions.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.48550/arXiv.2212.10560)\n\n[10\\. The Best Instruction-Tuning Data are Those That Fit](https://paperreading.club/page?id=282423)\n\n[11\\. Ada-Instruct: Adapting Instruction Generators for Complex Reasoning](https://arxiv.org/pdf/2310.04484)\n\n[12\\. GAS: Generative Auto-bidding with Post-training Search](https://www3.ntu.edu.sg/home/boan/papers/WWW25_GAS.pdf)\n\n[13\\. SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis](https://www.arxiv.org/pdf/2505.16834v2)\n\n[14\\. 100 DAYS AFTER DEEPSEEK-R1: A SURVEY ON REPLICATION STUDIES AND MORE DIRECTIONS FOR REASONING LANGUAGE MODELS](https://arxiv.org/pdf/2505.00551)\n\n[15\\. Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent](https://noticias.ai/wp-content/uploads/2024/11/2411.02265v2.pdf)\n\n[16\\. WritingBench: A Comprehensive Benchmark for Generative Writing](https://fetcher.alphaxiv.org/v2/pdf/2503.05244v1)\n\n[17\\. Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge](https://arxiv.org/pdf/2502.12501)\n\n[18\\. SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models](http://export.arxiv.org/pdf/2504.11468)\n\n[19\\. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](http://arxiv.org/html/2405.11143v1)\n\n[20\\. Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization](https://proceedings.neurips.cc/paper_files/paper/2024/file/daef77101ba5711084a57442c8cf2709-Paper-Conference.pdf)\n\n[21\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[22\\. A failed experiment: Infini-Attention, and why we should keep trying?](https://github.com/ego/huggingface-blog/blob/main/infini-attention.md)\n\n[23\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[24\\. LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding](https://openreview.net/pdf/ebe534ef4d252a94afd8be66951c1e312cad2fd7.pdf)\n\n[25\\. ∞-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?](https://openreview.net/pdf?id=zfaZZzbw0m)\n\n[26\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[27\\. A Controllable Examination for Long-Context Language Models](https://fetcher.alphaxiv.org/v2/pdf/2506.02921v1)\n\n[28\\. Pranav Rajpurkar, Jian Zhang et al. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D16-1264)\n\n[29\\. LongCodeBench: Evaluating Coding LLMs at 1M Context Windows](https://arxiv.org/html/2505.07897v1)\n\n[30\\. Prompt Engineering Techniques](https://www.ibm.com/think/topics/prompt-engineering-techniques)\n\n[31\\. HELMET: HOW TO EVALUATE LONG-CONTEXT LANGUAGE MODELS EFFECTIVELY AND THOROUGHLY](https://openreview.net/pdf/c46886730dfe4f507c2f23326e97049a32c93558.pdf)\n\n[32\\. Do Retrieval-Augmented Language Models Adapt to Varying User Needs?](https://arxiv.org/pdf/2502.19779)\n\n[33\\. Task-agnostic Prompt Compression with Context-aware Sentence Embedding and Reward-guided Task Descriptor](http://arxiv.org/pdf/2502.13374)\n\n[34\\. SQUEEZED ATTENTION: Accelerating Long Context Length LLM Inference](https://arxiv.org/pdf/2411.09688)\n\n[35\\. T. Kwiatkowski, J. Palomaki et al. “Natural Questions: A Benchmark for Question Answering Research.” Transactions of the Association for Computational Linguistics](https://doi.org/10.1162/tacl_a_00276)\n\n[36\\. Improving LLM Long Context Understanding via Synthetic Data and Adaptive Compression](https://dspace.mit.edu/bitstream/handle/1721.1/156754/li-jerryli-meng-eecs-2024-thesis.pdf)\n\n[37\\. QUEST: QUERY-CENTRIC DATA SYNTHESIS APPROACH FOR LONG-CONTEXT SCALING OF LARGE LANGUAGE MODEL](https://openreview.net/pdf/a08f7e1f29126d7ab1c63ccb3650f9817be23197.pdf)\n\n[38\\. COGNITIVE OVERLOAD ATTACK: PROMPT INJECTION FOR LONG CONTEXT](https://openreview.net/pdf/ea47233849fd3e99268ebde1dbd00d47498ee7f7.pdf)\n\n[39\\. A Guide to Improving Long Context Instruction Following on Open Source Models](https://scale.com/blog/long-context-instruction-following)\n\n[40\\. Long context models in the enterprise: benchmarks and beyond](https://snorkel.ai/blog/long-context-models-in-the-enterprise-benchmarks-and-beyond/)\n\n[41\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[42\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[43\\. Evaluating Large Language Models – Evaluation Metrics](https://www.enkefalos.com/newsletters-and-articles/evaluating-large-language-models-evaluation-metrics/)\n\n[44\\. Evaluation of Large Language Models: Review of Metrics, Applications, and Methodologies](https://www.preprints.org/manuscript/202504.0369/v1/download)\n\n[45\\. Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models](https://www.yiyibooks.cn/__src__/arxiv/2408.02442v1/index.html)\n\n[46\\. FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models](https://yiyibooks.cn/__trs__/arxiv/2310.20410v2/index.html)\n\n[47\\. 大语言模型的格式偏见评估与文本水印技术研究](https://www.catalyzex.com/author/Hieu%20Dao)\n\n[48\\. Can Large Language Models Understand Real-World Complex Instructions?](https://openreview.net/attachment?id=_VS83gpIzNC&name=pdf)\n\n[49\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[50\\. FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability](https://paperreading.club/page?id=212315)\n\n\\[51. Best Practices and Metrics for Evaluating Large Language Models (LLMs) \\](https://www.frugaltesting.com/blog/best-practices-and-metrics-for-evaluating-large-language-models-llms#:~:text=Common%20metrics%20include%20perplexity%20(how,answering%20or%20text%20generation%20tasks.)\n\n[52\\. Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models](https://aclanthology.org/2024.knowllm-1.10/)\n\n[53\\. A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B](https://ai-search.io/papers/a-comprehensive-evaluation-of-quantized-instruction-tuned-large-language-models-an-experimental-analysis-up-to-405b)\n\n[54\\. Evaluating Large Language Models (LLMs): A Comprehensive Guide](https://unvired.com/blog/comprehensive-guide-on-large-language-models/)\n\n[55\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[56\\. Huggingface Multimodal LLM Leaderboard](https://www.restack.io/p/llm-evaluation-answer-huggingface-multimodal-llm-leaderboard-cat-ai)\n\n[61\\. Building Applications with Large Language Models: Techniques, Implementation, and Applications](https://psv4.userapi.com/s/v1/d/8Ou6XnTT_EQiW-9ovoMaO4mkxVUm_9YBaXAxwvY8tHzFCGPyh_QYyVPG9InJczknD2CaNiNy-3vcxn3GT-QdG72qGoBG9ECyCXRR5AluDM-vHvh9/Building_Applications.pdf)\n\n[62\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[63\\. CS 696 Applied Large Language Models](https://eli.sdsu.edu/courses/spring25/cs696/notes/D15%20Hooks,%20Continually%20Pre-train,%20DPO.pdf)\n\n[64\\. Large Language Models: DeepSeek](https://llm.labri.fr/LLM_Course_DeepSeek.pdf)\n\n[65\\. Build software better, together](https://github.com/topics/sharegpt)\n\n[66\\. 大模型（LLMs）LLM生成SFT数据方法面](https://www.tpffy.fun/download/AI_Models_Books/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E2%91%A4%E3%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%85%AB%E8%82%A1%E6%96%87%E9%9D%A2%E8%AF%95/41-%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%88LLMs%EF%BC%89LLM%E7%94%9F%E6%88%90SFT%E6%95%B0%E6%8D%AE%E6%96%B9%E6%B3%95%E9%9D%A2.pdf)\n\n[67\\. Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models](https://openreview.net/pdf?id=tfGBSzNVaz)\n\n[68\\. How To Build A Large Language Model](https://apxml.com/courses/how-to-build-a-large-language-model/chapter-15-distributed-training-strategies/interplay-hybrid-approaches)\n\n[69\\. WizardMath: EMPOWERING MATHEMATICAL REASONING FOR LARGE LANGUAGE MODELS VIA Reinforced Evol-Instruct](https://openreview.net/pdf/4ad243a6f8899e29286c8d0c4ca80432bc0719fc.pdf)\n\n[70\\. Hands-on Large Language Models](https://xcfeng.net/res/presentation/Hands-on%20Large%20Language%20Models.pdf)\n\n[71\\. Foundations of Large Language Models](https://readwise-assets.s3.amazonaws.com/media/wisereads/articles/foundations-of-large-language-/2501.09223v1.pdf)\n\n[72\\. Instruction Tuning for Large Language Models: A Survey](https://arxiv.org/pdf/2308.10792)\n\n[73\\. 大規模言語モデルと画像-言語モデルの技術動向](https://qiita.com/mhrt-tech-biz-blog/items/8708950dc8924ef9d02a)\n\n[74\\. Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation](https://arxiv.org/pdf/2401.06477)\n\n[75\\. Constraint Back-translation Improves Complex Instruction Following of Large Language Models](https://arxiv.org/pdf/2410.24175)\n\n[76\\. A Deep Dive into Fine-Tuning](https://towardsdatascience.com/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224)\n\n[77\\. Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate](https://arxiv.org/pdf/2501.17703)\n\n[78\\. Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models](https://arxiv.org/pdf/2312.14197)\n\n[81\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[82\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[83\\. SFT](https://zhuanlan.zhihu.com/p/676999282)\n\n[84\\. 基于 LLaMA-Efficient-Tuning 对大模型进行 SFT：数据加载与微调过程详解](https://zhuanlan.zhihu.com/p/640416653)\n\n[85\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[86\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[87\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[88\\. How to Train Long-Context Language Models (Effectively)](https://openreview.net/pdf/b511d916dbd5c85025dd04abd888c6e10c37d14c.pdf)\n\n[89\\. CHATQA 2: BRIDGING THE GAP TO PROPRIETARY LLMs IN LONG CONTEXT AND RAG CAPABILITIES](https://arxiv.org/pdf/2407.14482)\n\n[90\\. Advanced fine-tuning methods on Amazon SageMaker AI](https://aws.amazon.com/blogs/machine-learning/advanced-fine-tuning-methods-on-amazon-sagemaker-ai/)\n\n[91\\. Self-Taught Agentic Long-Context Understanding](https://www.arxiv.org/pdf/2502.15920)\n\n[92\\. Enhancing LLM's Cognition via Structurization](https://proceedings.neurips.cc/paper_files/paper/2024/file/f169ec4d47933ea4896b994af8ff4f17-Paper-Conference.pdf)\n\n[93\\. MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation](https://arxiv.org/pdf/2409.05591v3)\n\n[94\\. Formatting Data for Supervised Fine-tuning](https://apxml.com/courses/fine-tuning-adapting-large-language-models/chapter-2-data-preparation-fine-tuning/formatting-sft-data)\n\n[95\\. LIFT: Improving Long Context Understanding Through Long Input Fine-Tuning](https://arxiv.org/html/2412.13626v1)\n\n[96\\. Supervised Fine-Tuning](https://oumi.ai/docs/en/latest/resources/datasets/sft_datasets.html)\n\n[97\\. LIFT：通过长输入微调改善长上下文理解](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_12_19/2412.13626.pdf)\n\n[98\\. Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning](https://arxiv.org/pdf/2505.24105)\n\n[99\\. GitHub - jondurbin/bagel: A bagel, with everything.](https://github.com/jondurbin/bagel)\n\n[100\\. SLOT: Structuring the Output of Large Language Models](https://arxiv.org/pdf/2505.04016)\n\n[101\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[102\\. I. Goodfellow, Jean Pouget-Abadie et al. “Generative adversarial networks.” Communications of the ACM](https://doi.org/10.1145/3422622)\n\n[103\\. API-guided Dataset Synthesis to Finetune Large Code Models](http://arxiv.org/pdf/2408.08343)\n\n[104\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[105\\. Ishaan Gulrajani, Faruk Ahmed et al. “Improved Training of Wasserstein GANs.” Neural Information Processing Systems](https://arxiv.org/abs/1704.00028)\n\n[106\\. A Deep Generative Model Framework for Creating High Quality Synthetic Transaction Sequences](https://research.library.mun.ca/16098/1/converted.pdf)\n\n[107\\. InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning](https://arxiv.org/pdf/2502.11573)\n\n[108\\. OPENMATHINSTRUCT-2: ACCELERATING AI FOR MATH WITH MASSIVE OPEN-SOURCE INSTRUCTION DATA](https://openreview.net/pdf?id=mTCbq2QssD)\n\n[109\\. The GenAI Frontier and the Quest for High-Quality SFT Data](https://toloka.ai/blog/the-genai-frontier-and-the-quest-for-high-quality-sft-data/)\n\n[110\\. SYNTHETIC-1 Release: Two Million Collaboratively Generated Reasoning Traces from Deepseek-R1](https://www.primeintellect.ai/blog/synthetic-1-release)\n\n[111\\. Federated Generation of Synthetic Tabular Data](https://repositum.tuwien.at/bitstream/20.500.12708/202727/1/Martinez%20Duarte%20Daniela%20-%202024%20-%20Federated%20Generation%20of%20Synthetic%20Tabular%20Data.pdf)\n\n[112\\. André R. Gonçalves, P. Ray et al. “Generation and evaluation of synthetic patient data.” BMC Medical Research Methodology](https://doi.org/10.1186/s12874-020-00977-1)\n\n[113\\. Practical Synthetic Data Generation](https://www.oreilly.com/library/view/practical-synthetic-data/9781492072737/ch04.html)\n\n[121\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[122\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[123\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[124\\. INF's Open-Source Large Language Models](https://s.infly.cn/f/img/pdf/inf_34b_tech_report.pdf)\n\n[125\\. Jason Wei, Maarten Bosma et al. “Finetuned Language Models Are Zero-Shot Learners.” ArXiv](https://arxiv.org/abs/2109.01652)\n\n[126\\. RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs](https://openreview.net/pdf?id=S1fc92uemC)\n\n[127\\. MIXTURE-OF-INSTRUCTIONS: ALIGNING LARGE LANGUAGE MODELS VIA MIXTURE PROMPTING](https://arxiv.org/pdf/2404.18410)\n\n[128\\. Yizhong Wang, Yeganeh Kordi et al. “Self-Instruct: Aligning Language Models with Self-Generated Instructions.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.48550/arXiv.2212.10560)\n\n[129\\. API-guided Dataset Synthesis to Finetune Large Code Models](http://arxiv.org/pdf/2408.08343)\n\n[130\\. Towards Trustworthy Large Language Models in Industry Domains](https://inftech.ai/Towards_Trustworthy_LLMs.pdf?20240813)\n\n[131\\. A Comprehensive Approach to Instruction Tuning for Qwen2.5: Data Selection, Domain Interaction, and Training Protocols](https://www.mdpi.com/2073-431X/14/7/264)\n\n[132\\. Self-Training Large Language Models for Tool-Use Without Demonstrations](https://arxiv.org/pdf/2502.05867)\n\n[133\\. Hands on with SFT - AI Engineering Academy](https://aiengineering.academy/LLM/HandsOnWithFinetuning/SFT/SFT/)\n\n[134\\. Prompt Engineering for LLMs: The Art and Science of Building Large Language Model-Based Applications](https://zncd.ir/wp-content/uploads/2025/01/John-Berryman-Albert-Ziegler-Prompt-Engineering-for-LLMs_-The-Art-and-Science-of-Building-Large-Language-Model-Based-Applications-2024-OReilly-Media-libgen.li_.pdf)\n\n[135\\. LLM for code generation: a scalable pipeline to gather SFT data](https://toloka.ai/blog/llm-for-code-generation/)\n\n[136\\. GitHub - Kwai-Keye/Keye](https://github.com/Kwai-Keye/Keye)\n\n[137\\. Knowledge Graph Finetuning Enhances Knowledge Manipulation in Large Language Models](https://openreview.net/attachment?id=oMFOKjwaRS&name=pdf)\n\n[138\\. Aligning Instruction Tuning with Pre-training](https://ai-plans.com/file_storage/b3815a9f-09e1-4616-b973-2284df548c23_2501.09368v2.pdf)\n\n[139\\. TableGPT2: A Large Multimodal Model with Tabular Data Integration](https://arxiv.org/pdf/2411.02059)\n\n[140\\. SIT: Fine-Tuning Large Language Models with Sequential Instructions](https://arxiv.org/pdf/2403.07794v2)\n\n[141\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[142\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[143\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[144\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[145\\. How to Train Long-Context Language Models (Effectively)](https://openreview.net/pdf/b511d916dbd5c85025dd04abd888c6e10c37d14c.pdf)\n\n[146\\. LLMDataHub: 高质量语言模型训练数据集集合](https://github.com/zizhGuo/LLMDataHub/blob/main/README.md)\n\n[147\\. CHATQA 2: BRIDGING THE GAP TO PROPRIETARY LLMs IN LONG CONTEXT AND RAG CAPABILITIES](https://arxiv.org/pdf/2407.14482)\n\n[148\\. 关于LLMs文本与图像混合模态训练](https://www.waytoagi.com/question/78678)\n\n[149\\. SFT](https://zhuanlan.zhihu.com/p/676999282)\n\n[150\\. Yizhong Wang, Yeganeh Kordi et al. “Self-Instruct: Aligning Language Models with Self-Generated Instructions.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.48550/arXiv.2212.10560)\n\n[151\\. MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation](https://arxiv.org/pdf/2409.05591v3)\n\n[152\\. LONGCITE: ENABLING LLMs TO GENERATE FINE- GRAINED CITATIONS IN LONG-CONTEXT QA](https://arxiv.org/pdf/2409.02897)\n\n[153\\. 基于 LLaMA-Efficient-Tuning 对大模型进行 SFT：数据加载与微调过程详解](https://zhuanlan.zhihu.com/p/640416653)\n\n[154\\. Foundations of Large Language Models](https://readwise-assets.s3.amazonaws.com/media/wisereads/articles/foundations-of-large-language-/2501.09223v1.pdf)\n\n[155\\. LONGLoRA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS](https://openreview.net/pdf?id=6PmJoRfdaK)\n\n[156\\. Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning](https://arxiv.org/pdf/2505.24105)\n\n[157\\. Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey](http://www.columbia.edu/~wt2319/Preference_survey.pdf)\n\n[158\\. Specializing Language Models for Domain-Specific Tasks](https://cdn.prod.website-files.com/63401dfb6edad1c1702f685c/67ad627babcfaacf16f9e6dd_f85b3088b643b58527bcf775cfeecf8a_TSMPaper.pdf)\n\n[159\\. Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG](https://arxiv.org/pdf/2410.05983)\n\n[160\\. Self-Taught Agentic Long-Context Understanding](https://www.arxiv.org/pdf/2502.15920)\n\n[161\\. Muthu Dayalan. “MapReduce: simplified data processing on large clusters.” Commun. ACM](https://doi.org/10.1145/1327452.1327492)\n\n[162\\. M. Fredman, R. Tarjan. “Fibonacci heaps and their uses in improved network optimization algorithms.” JACM](https://doi.org/10.1145/28869.28874)\n\n[163\\. Joseph E. Gonzalez, Reynold Xin et al. “GraphX: Graph Processing in a Distributed Dataflow Framework.” USENIX Symposium on Operating Systems Design and Implementation](https://doi.org/10.5555/2685048.2685096)\n\n[164\\. Monika Henzinger, T. Henzinger et al. “Computing simulations on finite and infinite graphs.” Proceedings of IEEE 36th Annual Foundations of Computer Science](https://doi.org/10.1109/SFCS.1995.492576)\n\n[165\\. G. Ramalingam, T. Reps. “On the Computational Complexity of Dynamic Graph Problems.” Theor. Comput. Sci.](https://doi.org/10.1016/0304-3975%2895%2900079-8)\n\n[166\\. BioCompute: Harnessing Distributed Systems for Bioinformatics Applications](https://ccl.cse.nd.edu/research/pubs/pbragahe-thesis.pdf)\n\n[167\\. GRAND: UM MODELO DE GERENCIAMENTO HIERÁRQUICO DE APLICAÇÕES EM AMBIENTE DE COMPUTAÇÃO EM GRADE](https://www.pesc.coppe.ufrj.br/uploadfile/publicacao/2026.pdf)\n\n[168\\. Cost-Benefit Analysis of Cloud Computing versus Desktop Grids](https://mescal.imag.fr/membres/derrick.kondo/pubs/kondo_hcw09.pdf)\n\n[169\\. A Survey on Grid Scheduling Systems](https://www.cs.sjtu.edu.cn/~yzhu/reports/SJTU_CS_TR_200309001.pdf)\n\n[170\\. Scalable Graph Representational Learning Algorithms for Network Medicine](https://air.unimi.it/retrieve/c2f65876-fc66-4c38-8c6b-98c6e4ef9f38/phd_unimi_R12725.pdf)\n\n[171\\. LLM for code generation: a scalable pipeline to gather SFT data](https://toloka.ai/blog/llm-for-code-generation/)\n\n[172\\. QPO: Query-dependent Prompt Optimization via Multi-Loop Offline Reinforcement Learning](https://openreview.net/pdf?id=bqMJToTkvT)\n\n[173\\. Hands on with SFT - AI Engineering Academy](https://aiengineering.academy/LLM/HandsOnWithFinetuning/SFT/SFT/)\n\n[174\\. M. Sultan, Jatin Ganhotra et al. “Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations.” ArXiv](https://doi.org/10.48550/arXiv.2402.11770)\n\n[175\\. Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries](https://shiyu-zhao.netlify.app/uploads/resume.pdf)\n\n[176\\. OpenAI 最新突破，AI 首次学会\"三思而后行\"](https://cloud.tencent.com/developer/article/2502510)\n\n[177\\. GitHub - uclaml/SPIN: The official implementation of Self-Play Fine-Tuning (SPIN)](https://github.com/uclaml/SPIN)\n\n[178\\. Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing](https://openi.pcl.ac.cn/thomas-yanxin/magpie/src/branch/main)\n\n[179\\. Exploring automated energy optimization with unstructured building data: A multi-agent based framework leveraging large language models](https://a434.tongji.edu.cn/SCI_Xiao_2024_multiagent.pdf)\n\n[180\\. LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation](https://arxiv.org/pdf/2505.19667)\n\n[181\\. Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner](https://arxiv.org/pdf/2505.11404v1)\n\n[182\\. How Do I Evaluate Workflow?](https://digital.ahrq.gov/sites/default/files/docs/workflowtoolkit/HowDoIEvaluateWorkflow.ppt)\n\n[183\\. LLM在微盟BI领域的探索](https://static001.geekbang.org/con/142/pdf/174506037/file/LLM+%E5%9C%A8%E5%BE%AE%E7%9B%9F+BI+%E5%9C%BA%E6%99%AF%E7%9A%84%E6%8E%A2%E7%B4%A2%E4%B8%8E%E8%90%BD%E5%9C%B0-%E7%8E%8B%E6%88%90.pdf)\n\n[184\\. API-guided Dataset Synthesis to Finetune Large Code Models](http://arxiv.org/pdf/2408.08343)\n\n[185\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[186\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[187\\. 关于LLMs文本与图像混合模态训练](https://www.waytoagi.com/question/78678)\n\n[188\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[189\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[190\\. Pranav Rajpurkar, Jian Zhang et al. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D16-1264)\n\n[191\\. SFT](https://zhuanlan.zhihu.com/p/676999282)\n\n[192\\. GitHub - Zjh-819/LLMDataHub: A quick guide (especially) for trending instruction finetuning datasets](https://github.com/Zjh-819/LLMDataHub)\n\n[193\\. LLMDataHub: 高质量语言模型训练数据集集合](https://github.com/zizhGuo/LLMDataHub/blob/main/README.md)\n\n[194\\. GitHub - jondurbin/bagel: A bagel, with everything.](https://github.com/jondurbin/bagel)\n\n[195\\. Advanced fine-tuning methods on Amazon SageMaker AI](https://aws.amazon.com/blogs/machine-learning/advanced-fine-tuning-methods-on-amazon-sagemaker-ai/)\n\n[196\\. Multimodal AI Search for Business Applications](https://towardsdatascience.com/multimodal-ai-search-for-business-applications-65356d011009/)\n\n[197\\. Foundations of Large Language Models](https://readwise-assets.s3.amazonaws.com/media/wisereads/articles/foundations-of-large-language-/2501.09223v1.pdf)\n\n[198\\. Formatting Data for Supervised Fine-tuning](https://apxml.com/courses/fine-tuning-adapting-large-language-models/chapter-2-data-preparation-fine-tuning/formatting-sft-data)\n\n[199\\. The Llama 3 Herd of Models](https://r.jordan.im/download/language-models/grattafiori2024.pdf)\n\n[200\\. Simultaneous Machine Translation with Large Language Models](https://alta2024.alta.asn.au/assets/papers/33.pdf)\n\n[201\\. Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey](http://www.columbia.edu/~wt2319/Preference_survey.pdf)\n\n[202\\. INVESTALIGN: ALIGN LLMs WITH INVESTOR DECISION-MAKING UNDER HERD BEHAVIOR](https://openreview.net/pdf?id=pxy5wDMnzv)\n\n[203\\. Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning](https://arxiv.org/pdf/2505.24105)\n\n[204\\. LARGE LANGUAGE MODELS ARE HUMAN-LEVEL PROMPT ENGINEERS](https://www.cnblogs.com/LittleHann/p/17578851.html)\n\n[205\\. Muthu Dayalan. “MapReduce: simplified data processing on large clusters.” Commun. ACM](https://doi.org/10.1145/1327452.1327492)\n\n[206\\. GitHub - InternLM/Condor](https://github.com/InternLM/Condor)\n\n[207\\. G. Malewicz, Matthew H. Austern et al. “Pregel: a system for large-scale graph processing.” Proceedings of the 2010 ACM SIGMOD International Conference on Management of data](https://doi.org/10.1145/1807167.1807184)\n\n[208\\. Ian T Foster, C. Kesselman. “Globus: a Metacomputing Infrastructure Toolkit.” International Journal of High Performance Computing Applications](https://doi.org/10.1177/109434209701100205)\n\n[209\\. Joseph E. Gonzalez, Reynold Xin et al. “GraphX: Graph Processing in a Distributed Dataflow Framework.” USENIX Symposium on Operating Systems Design and Implementation](https://doi.org/10.5555/2685048.2685096)\n\n[210\\. Condor：透過知識驅動的資料合成和精煉來增強LLM對齊](https://www.chatpaper.ai/zh-Hant/paper/c78d588c-a574-4aa5-be10-31eaa025fda3)\n\n[211\\. J. Frey, T. Tannenbaum et al. “Condor-G: A Computation Management Agent for Multi-Institutional Grids.” Cluster Computing](https://doi.org/10.1023/A:1015617019423)\n\n[212\\. Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge](https://arxiv.org/pdf/2502.12501)\n\n[213\\. Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement](https://arxiv.org/pdf/2501.12273)\n\n[214\\. API-guided Dataset Synthesis to Finetune Large Code Models](http://arxiv.org/pdf/2408.08343)\n\n[215\\. Self-Training Large Language Models for Tool-Use Without Demonstrations](https://arxiv.org/pdf/2502.05867)\n\n[216\\. The Best Instruction-Tuning Data are Those That Fit](https://arxiv.org/abs/2502.04194)\n\n[217\\. UTILITY DRIVEN GRID SCHEDULING FRAMEWORK](https://www.uab.edu/cas/cybersecurity/images/Documents/Enis_Afgan_Dissertation3.pdf)\n\n[218\\. Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading](https://arxiv.org/html/2504.11919v1)\n\n[219\\. JIUTIAN-139MoE: TECHNICAL REPORT](https://www.modelscope.cn/models/JiuTian-AI/JIUTIAN-139MoE-chat/resolve/master/JIUTIAN-139MOE%20TECHNICAL%20REPORT.pdf)\n\n[220\\. Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner](https://arxiv.org/pdf/2505.11404v1)\n\n[221\\. Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent](https://noticias.ai/wp-content/uploads/2024/11/2411.02265v2.pdf)\n\n[222\\. GRAPE for fast and scalable graph processing and random-walk-based embedding](https://mouseion.jax.org/cgi/viewcontent.cgi?params=/context/stfb2023/article/1176/&path_info=s43588_023_00465_8.pdf)\n\n[223\\. RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs](https://openreview.net/pdf?id=S1fc92uemC)\n\n[225\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[226\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[227\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[228\\. OpenAI Josh Achiam, Steven Adler et al. “GPT-4 Technical Report.”](https://arxiv.org/abs/2303.08774)\n\n[229\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[230\\. PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and Tuning Methods](http://arxiv.org/html/2407.06985v1)\n\n[231\\. LLM for code generation: a scalable pipeline to gather SFT data](https://toloka.ai/blog/llm-for-code-generation/)\n\n[232\\. Artificial Intelligence Terms | iterate.ai](https://www.iterate.ai/ai-glossary)\n\n[233\\. SFT](https://zhuanlan.zhihu.com/p/676999282)\n\n[234\\. ANLS\\* - A Universal Document Processing Metric for Generative Large Language Models](http://arxiv.org/html/2402.03848v2)\n\n[235\\. 基于 LLaMA-Efficient-Tuning 对大模型进行 SFT：数据加载与微调过程详解](https://zhuanlan.zhihu.com/p/640416653)\n\n[236\\. LLaMA-Cult-and-More: Affordable and Powerful Language Models Repository](https://github.com/shm007g/LLaMA-Cult-and-More/blob/main/chart.md)\n\n[237\\. GenAI / Transformers Workshop Overview, Internals and Insights](https://events.hifis.net/event/1803/attachments/2773/6020/GenAIWorkshop_GEOMAR_with_footnotes_final.pdf)\n\n[238\\. Lifan Yuan, Yangyi Chen et al. “CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets.” ArXiv](https://doi.org/10.48550/arXiv.2309.17428)\n\n[239\\. SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained Understanding](https://arxiv.org/html/2504.07745v1)\n\n[240\\. LLM Landscape Report](https://cdn.prod.website-files.com/668d66434307b08c724f8a81/67dc26e1400679d594e43df8_LLM%20Landscape%20Report.pdf)\n\n[241\\. DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation](https://arxiv.org/pdf/2403.08857.pdf)\n\n[242\\. Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation](https://openreview.net/pdf?id=IMNnyrC0Ky)\n\n[243\\. Automatically Generating Custom Context-Driven SFT Data for LLMs with Multi-Granularity](https://openreview.net/pdf?id=wu8NIjf8pD)\n\n[244\\. Task-driven Layerwise Additive Activation Intervention](https://www.catalyzex.com/author/Viet%20Anh%20Nguyen)\n\n[245\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[246\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[247\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[248\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[249\\. 关于LLMs文本与图像混合模态训练](https://www.waytoagi.com/zh/question/78678)\n\n[250\\. The Latest Papers about AI](http://paperreading.club/page?id=285840)\n\n[251\\. Vicente Ordonez, Girish Kulkarni et al. “Im2Text: Describing Images Using 1 Million Captioned Photographs.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/8e080b98efbe65c02a116439205ca2344b9f7cd4)\n\n[252\\. InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition](https://arxiv.org/pdf/2309.15112v1)\n\n[253\\. SFT](https://zhuanlan.zhihu.com/p/676999282)\n\n[254\\. MM-LLMs: Recent Advances in MultiModal Large Language Models](https://openreview.net/pdf/590fe121615444b1009e27d1798d3649237a7436.pdf)\n\n[255\\. Evaluating Text-to-Image Synthesis: Survey and Taxonomy of Image Quality Metrics](https://viscom.publications.uni-ulm.de/api/uploads/274/2403.11821%20%281%29.pdf)\n\n[256\\. Shukang Yin, Chaoyou Fu et al. “A Survey on Multimodal Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2306.13549)\n\n[257\\. Foundations of Large Language Models](https://readwise-assets.s3.amazonaws.com/media/wisereads/articles/foundations-of-large-language-/2501.09223v1.pdf)\n\n[258\\. VILA²: VLM AUGMENTED VLM WITH SELF-IMPROVEMENT](https://openreview.net/pdf/2b96857aa3d12b09544ba79fcb0a1afcd1d0ae58.pdf)\n\n[259\\. SFT实战微调Gemma](https://blog.csdn.net/weixin_44977665/article/details/136325349)\n\n[260\\. TableGPT2: A Large Multimodal Model with Tabular Data Integration](https://arxiv.org/pdf/2411.02059)\n\n[261\\. LIFT: Improving Long Context Understanding Through Long Input Fine-Tuning](https://arxiv.org/html/2412.13626v1)\n\n[262\\. Fanqing Meng, Wenqi Shao et al. “ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning.” ArXiv](https://doi.org/10.48550/arXiv.2401.02384)\n\n[263\\. 多模态大模型: 盘点&Highlights part2](https://community.modelscope.cn/6715f44d82931a478c10f568.html)\n\n[265\\. Muthu Dayalan. “MapReduce: simplified data processing on large clusters.” Commun. ACM](https://doi.org/10.1145/1327452.1327492)\n\n[266\\. G. Malewicz, Matthew H. Austern et al. “Pregel: a system for large-scale graph processing.” Proceedings of the 2010 ACM SIGMOD International Conference on Management of data](https://doi.org/10.1145/1807167.1807184)\n\n[267\\. GitHub - InternLM/Condor](https://github.com/InternLM/Condor)\n\n[268\\. Joseph E. Gonzalez, Reynold Xin et al. “GraphX: Graph Processing in a Distributed Dataflow Framework.” USENIX Symposium on Operating Systems Design and Implementation](https://doi.org/10.5555/2685048.2685096)\n\n[269\\. Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement](https://arxiv.org/pdf/2501.12273)\n\n[270\\. Yuanyuan Tian, Andrey Balmin et al. “From \"Think Like a Vertex\" to \"Think Like a Graph\".” Proc. VLDB Endow.](https://doi.org/10.14778/2732232.2732238)\n\n[271\\. G. Ramalingam, T. Reps. “On the Computational Complexity of Dynamic Graph Problems.” Theor. Comput. Sci.](https://doi.org/10.1016/0304-3975%2895%2900079-8)\n\n[272\\. The Best Instruction-Tuning Data are Those That Fit](https://paperreading.club/page?id=282423)\n\n[275\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[276\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[277\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[278\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[279\\. Pranav Rajpurkar, Robin Jia et al. “Know What You Don’t Know: Unanswerable Questions for SQuAD.” ArXiv](https://doi.org/10.18653/v1/P18-2124)\n\n[280\\. Supervised Fine-Tuning (SFT) in RLHF: A Comprehensive Guide](https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.mdx)\n\n[281\\. SFT](https://zhuanlan.zhihu.com/p/676999282)\n\n[282\\. GitHub - jondurbin/bagel: A bagel, with everything.](https://github.com/jondurbin/bagel)\n\n[283\\. SFT 微调 Llama 3 8B](https://www.j000e.com/AI/llama3_sft.html)\n\n[284\\. Sentinel-2 Products Specification Document](https://sentinel.esa.int/documents/247904/685211/S2-PDGS-TAS-DI-PSD-V14.9.pdf)\n\n[285\\. TableGPT2: A Large Multimodal Model with Tabular Data Integration](https://arxiv.org/pdf/2411.02059)\n\n[286\\. 关于LLMs文本与图像混合模态训练](https://www.waytoagi.com/zh/question/78678)\n\n[287\\. Amazon Neptune explained Archives - AI - IT - Engineering - Cloud - Finance - Trends](https://enoumen.com/tag/amazon-neptune-explained/)\n\n[288\\. Search MySQL Like Google Using Match and Full-Text Search](https://www.codeproject.com/Articles/5298769/Search-MySQL-Like-Google-Using-Match-and-Full-Text)\n\n[289\\. Hands on with SFT - AI Engineering Academy](https://aiengineering.academy/LLM/HandsOnWithFinetuning/SFT/SFT/)\n\n[290\\. Bresser | Oregon Scientific智能增强现实地球仪，白天和夜晚视图](https://www.bresser.de/en/Junior/Learning-Experimenting/Oregon-Scientific-smart-Augmented-Reality-Globe-with-day-and-night-view.html)\n\n[291\\. PLANET IMAGERY PRODUCT SPECIFICATIONS](https://assets.planet.com/docs/combined-imagery-product-spec-april-2019.pdf)\n\n[292\\. Data Tool 使用教程](https://datatoolbar.com/tutorial.html)\n\n[293\\. Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning](https://arxiv.org/pdf/2505.24105)\n\n[294\\. Semi-automatic annotation of multimedia objects](https://www.freepatentsonline.com/7349895.html)\n\n[295\\. Muthu Dayalan. “MapReduce: simplified data processing on large clusters.” Commun. ACM](https://doi.org/10.1145/1327452.1327492)\n\n[296\\. Ian T Foster. “The Anatomy of the Grid: Enabling Scalable Virtual Organizations.” The International Journal of High Performance Computing Applications](https://doi.org/10.1177/109434200101500302)\n\n[297\\. D. Thain, T. Tannenbaum et al. “Distributed computing in practice: the Condor experience.” Concurrency and Computation: Practice and Experience](https://doi.org/10.1002/CPE.938)\n\n[298\\. M. Litzkow, M. Livny et al. “Condor-a hunter of idle workstations.” \\[1988\\] Proceedings. The 8th International Conference on Distributed](https://doi.org/10.1109/DCS.1988.12507)\n\n[299\\. Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement](https://arxiv.org/pdf/2501.12273)\n\n[300\\. J. Frey, T. Tannenbaum et al. “Condor-G: A Computation Management Agent for Multi-Institutional Grids.” Cluster Computing](https://doi.org/10.1023/A:1015617019423)\n\n[301\\. Grape: Knowledge Graph Enhanced Passage Reader for Open-domain Question Answering](https://preview.aclanthology.org/ingestion-script-update/2022.findings-emnlp.13/)\n\n[302\\. FULL-STACK, CROSS-LAYER OPTIMIZATIONS FOR QUANTUM COMPUTING](https://knowledge.uchicago.edu/record/2754/files/Gokhale_uchicago_0330D_15573.pdf)\n\n[303\\. UTILITY DRIVEN GRID SCHEDULING FRAMEWORK](https://www.uab.edu/cas/cybersecurity/images/Documents/Enis_Afgan_Dissertation3.pdf)\n\n[304\\. Scalable Graph Representational Learning Algorithms for Network Medicine](https://air.unimi.it/retrieve/c2f65876-fc66-4c38-8c6b-98c6e4ef9f38/phd_unimi_R12725.pdf)\n\n[305\\. GRAPE for fast and scalable graph processing and random-walk-based embedding](https://mouseion.jax.org/cgi/viewcontent.cgi?params=/context/stfb2023/article/1176/&path_info=s43588_023_00465_8.pdf)\n\n[306\\. Boosting Performance of Iterative Applications on GPUs: Kernel Batching with CUDA Graphs](https://arxiv.org/pdf/2501.09398)\n\n[307\\. A Security Framework.CSFW](https://www.semanticscholar.org/paper/49feb150efa7fe3fc6396dfd572e76a85f3c0818)\n\n[308\\. GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models](https://arxiv.org/pdf/2408.11817)\n\n[309\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[310\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[311\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[312\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[313\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[314\\. API-guided Dataset Synthesis to Finetune Large Code Models](http://arxiv.org/pdf/2408.08343)\n\n[315\\. Enhancing structured data generation with GPT-4o evaluating prompt efficiency across prompt styles](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1558938/pdf)\n\n[316\\. Build software better, together](https://github.com/topics/ragas?o=desc&s=stars)\n\n[317\\. LLM for code generation: a scalable pipeline to gather SFT data](https://toloka.ai/blog/llm-for-code-generation/)\n\n[318\\. Prompt Engineering for LLMs: The Art and Science of Building Large Language Model-Based Applications](https://zncd.ir/wp-content/uploads/2025/01/John-Berryman-Albert-Ziegler-Prompt-Engineering-for-LLMs_-The-Art-and-Science-of-Building-Large-Language-Model-Based-Applications-2024-OReilly-Media-libgen.li_.pdf)\n\n[319\\. Evaluating large language models: a systematic review of efficiency, applications, and future directions](https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2025.1523699/full)\n\n[320\\. Revolutionizing Database Q&A with Large Language Models: Comprehensive Benchmark and Evaluation \\[EA&B\\]](https://arxiv.org/pdf/2409.04475)\n\n[321\\. AI+软件研发数字峰会](https://aidd.vip/resources/upload/a7844a45d8ab55e/1701051633418/AiDD2023-%E6%B7%B1%E5%9C%B3%E7%AB%99LLM%E8%BE%85%E5%8A%A9%E7%B3%BB%E7%BB%9F%E6%B5%8B%E8%AF%95%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90-%E5%8D%8E%E4%B8%BA%E4%B8%87%E9%94%90%E5%AA%9B.pdf)\n\n[322\\. Synthetic Data Generation: What, Where and How?](https://www.infosys.com/services/data-ai-topaz/insights/synthetic-data-generation.pdf)\n\n[323\\. Advancing Computational Humor: LLaMa-3 Based Generation with DistilBert Evaluation Framework](https://www.itm-conferences.org/articles/itmconf/pdf/2025/01/itmconf_dai2024_03024.pdf)\n\n[324\\. Domain Adaptation of Retrieval Systems from Unlabeled Corpora](https://lup.lub.lu.se/student-papers/record/9169000/file/9169004.pdf)\n\n[325\\. Implementing Prompt Engineering and Retrieval Augmented Generation in PentestGPT with Local and Open-Source Large Language Models](https://uia.brage.unit.no/uia-xmlui/bitstream/handle/11250/3146692/no.uia%3Ainspera%3A222274555%3A43405353.pdf?sequence=1)\n\n[326\\. Retrieval Augmented Generation Language Models](https://intelliarts.com/blog/retrieval-augmented-generation-language-models/)"
  },
  {
    "id": 9,
    "question": "What is the fundamental reason behind the low cost of DeepSeek V3? Is it due to leveraging data distillation from other \"teacher models\" (such as OpenAI, Gemini, etc.), or adjustments in training and inference precision algorithms?",
    "response": "**Comprehensive Analysis: The Fundamental Reason Behind DeepSeek V3's Low Cost**\n\n**1\\. Introduction: The Cost Efficiency Enigma**\n\nDeepSeek-V3's unprecedented cost efficiency – with inference pricing at **1% of GPT-4o's rates** and training costs **1/10th of Meta Llama 3** – represents a paradigm shift in large language model economics \\[46\\]\\[60\\]. This analysis addresses whether its low cost stems primarily from leveraging other models' data (distillation) or through algorithmic innovations in precision and efficiency.\n\n**2\\. Knowledge Distillation: Limited Role in Cost Reduction**\n\n**2.1 Internal Distillation Dominance**\n\nDeepSeek-V3 **primarily used its own teacher model (DeepSeek-R1)** for distillation to enhance reasoning capabilities \\[28\\]\\[33\\].\n\nInternal distillation creates synthetic training data and extracts reasoning chains, improving efficiency without external dependencies \\[26\\]\\[30\\].\n\n**2.2 External Model Usage: Unsubstantiated Claims**\n\nWhile accused of training on GPT-4/ChatGPT outputs \\[21\\]\\[27\\]\\[32\\]**no evidence confirms direct distillation** from OpenAI, Gemini, or other external models.\n\nDeepSeek explicitly states its optimization comes from **in-house techniques**, not third-party model distillation \\[26\\]\\[28\\].\n\nHistorical note: Google’s Gemma uses Gemini distillation \\[22\\], but this methodology is **unrelated to DeepSeek-V3**.\n\n**2.3 Impact on Costs**\n\nDistillation from external models would _increase_ costs due to API fees/scraping overhead – contradicting DeepSeek’s cost structure. Thus, distillation is **not a primary cost driver**.\n\n**3\\. Algorithmic Precision Innovations: The Core Efficiency Engine**\n\n**3.1 Training Efficiency Breakthroughs**\n\n**FP8 Low-Precision Training**:\n\nReduced memory/computation by 50% with <0.25% accuracy loss via fine quantization \\[1\\]\\[5\\]\\[7\\]. This halved GPU-hour requirements vs. FP32/BF16 standards.\n\n**DualPipe Algorithm**:\n\nMinimized pipeline bubbles in distributed training, boosting GPU utilization by 30% via overlapping computation stages \\[1\\]\\[7\\]\\[45\\].\n\n**Mixture of Experts (MoE)**:\n\nActivated only **37B of 671B parameters per token**, slashing active compute needs \\[5\\]\\[44\\]\\[46\\]. Sparse expert activation reduced training costs by >90% vs. dense models \\[44\\]\\[52\\].\n\n**3.2 Inference-Specific Optimizations**\n\n**Multi-Token Prediction (MTP)**:\n\nAccelerated inference by predicting tokens concurrently, reducing sequential processing \\[1\\]\\[7\\]\\[46\\].\n\n**Multi-Head Latent Attention (MLA)**:\n\nHalved memory consumption during long-context inference \\[5\\]\\[13\\]\\[18\\].\n\n**Sparse Activation Load Balancing**:\n\nAuxiliary-loss-free routing ensured efficient resource use without performance degradation \\[46\\]\\[49\\].\n\n**3.3 Hardware and System Optimization**\n\n**Strategic GPU Allocation**:\n\nUsed cost-effective alternatives to Nvidia H100s while optimizing 2,048 H800 GPUs with minimal idle time \\[3\\]\\[14\\]\\[16\\].\n\n**InfiniBand/NVLink Optimization**:\n\nMaximized throughput via multi-plane fat-tree networks, avoiding traffic conflicts \\[45\\]\\[47\\].\n\n**4\\. Cost Calculation: Quantifying the Impact**\n\n**Training Cost**: 5.576 million (vs. ~50M for Llama 3) \\[46\\]\\[48\\].\n\n**Inference Cost**: FP8 + MoE reduced per-token compute by 5–10x \\[43\\]\\[44\\]\\[47\\].\n\n**Scaling Efficiency**: 1M tokens processed at 1/500th of GPT-4’s cost \\[60\\].\n\nThe **90% reduction vs. comparable models** derives overwhelmingly from precision algorithms \\[44\\]\\[52\\].\n\n**5\\. Indirect Contributors to Cost Savings**\n\n**Data-Centric Efficiency**:\n\n14.8T tokens pre-trained with dynamic learning rate adjustments accelerated convergence \\[3\\]\\[20\\].\n\n**Reduced RLHF Dependency**:\n\nHigh-quality pretraining data minimized costly reinforcement learning fine-tuning \\[20\\].\n\n**6\\. Synthesis: The Fundamental Reason**\n\nThe evidence decisively shows that DeepSeek V3's cost efficiency is **primarily driven by algorithmic precision innovations**, not knowledge distillation:\n\nPrecision techniques (FP8, MoE, DualPipe) directly reduced computational load by 50–90% \\[1\\]\\[5\\]\\[43\\].\n\nDistillation’s role was secondary and internally focused, with no proven cost benefits from external models \\[26\\]\\[28\\]\\[33\\].\n\n**Cross-cutting efficiency**: Training optimizations like FP8 had cascading inference benefits, creating compound savings \\[1\\]\\[44\\]\\[47\\].\n\nThis establishes **algorithmic precision as the causal engine** behind DeepSeek V3's disruptive cost structure.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. DeepSeek发布V3模型降本技术报告](https://www.donews.com/news/detail/4/5203837.html)\n\n[2\\. DeepSeek 쇼크, 위기에서 찾은 기회](https://bbn.kiwoom.com/rfCC792)\n\n[3\\. DEEPSEEK LARGE - SCALE MODEL: TECHNICAL ANALYSIS AND DEVELOPMENT PROSPECT](http://upubscience.com/upload/20250207100649.pdf)\n\n[4\\. 전기전자 (Overweight) 비용효율적 AI의 등장과 IT H/W 영향 (feat. DeepSeek)](https://m.imfnsec.com:442/upload/R_E09/2025/01/%5B31001122%5D_250236.pdf)\n\n[5\\. Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](https://www.52nlp.cn/wp-content/uploads/2025/05/Insights-into-DeepSeek-V3%E8%8B%B1%E4%B8%AD%E5%AF%B9%E7%85%A7%E7%89%88.pdf)\n\n[6\\. Deconstructing DeepSeek's AI disruption: Lessons for the EU](https://www.funcas.es/wp-content/uploads/2025/03/Deconstructing-DeepSeeks-AI-disruption.pdf)\n\n[7\\. DeepSeek如何在AI领域迅速崛起并引发全球关注？](https://www.markreadfintech.com/p/deepseekai)\n\n[8\\. 把训练成本打下来99%！吊打GPT又“征服”OpenAI创始成员，DeepSeek“国产之光”实至名归？](https://new.qq.com/rain/a/20241227A050JJ00)\n\n[9\\. DeepSeek V3 – The Future of Open-Source AI](https://deep-seek.chat/deepseek-v3/)\n\n[10\\. 人工智能文集 第二十四集](http://cosspu.org.cn/upload/file/%E8%AF%84%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A6%82%E4%BD%95%E8%B5%B0%E5%90%91%E6%96%B0%E9%98%B6%E6%AE%B5/%E7%AC%AC%E4%BA%8C%E5%8D%81%E5%9B%9B%E6%9C%9F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%96%87%E9%9B%86-%E7%94%B5%E5%AD%90%E7%89%88.pdf)\n\n[11\\. 安全人视角的DeepSeek洞察与思考](https://www.secrss.com/articles/75336?app=1)\n\n[12\\. Deepseek 혼돈, 단기적으로 deep sick 장기적으로 deep chic](https://www.shinyoung.com/files/20250206/7a2cc4e408f95.pdf)\n\n[13\\. DeepSeek 核心十问十答](https://www.faxianai.com/wp-content/uploads/2025/05/1747041355-3%E3%80%81%E8%AE%A1%E7%AE%97%E6%9C%BA-DeepSeek%E6%A0%B8%E5%BF%83%E5%8D%81%E9%97%AE%E5%8D%81%E7%AD%94.pdf)\n\n[14\\. DeepSeek-V3如何实现成本效益而不损害性能](https://codingmall.com/knowledge-base/25-global/246661-deepseek-v3)\n\n[15\\. Guinness Global Quality Mid Cap Investment Commentary – February 2025](https://www.guinnessgi.com/sites/default/files/2025-02/2025.02_Guinness%20Global%20Quality%20Mid%20Cap_Commentary.pdf)\n\n[16\\. Deepseek AI: The Next Big Chatbot? Everything You Need to Know](https://www.oom.com.sg/deepseek/)\n\n[17\\. DeepSeek V3 and R1: An Overview of Technology Innovations and Implications for United States National Security](https://figshare.com/ndownloader/files/53282696)\n\n[18\\. What is DeepSeek, and why does it matter?](https://www.jdsupra.com/legalnews/what-is-deepseek-and-why-does-it-matter-5697007/)\n\n[19\\. DeepSeek 公布 V3 降本技术](https://www.163.com/dy/article/JVM26EIT05566WT8.html)\n\n[20\\. DeepSeek 冲击波：AI 狂潮下计算机行业的颠覆与重生](https://pdf.dfcfw.com/pdf/H3_AP202502231643415398_1.pdf?1740351460000.pdf)\n\n[21\\. DeepSeek可能使用了Google的Gemini来训练其最新模型](https://techcrunch.com/2025/06/03/deepseek-may-have-used-googles-gemini-to-train-its-latest-model/)\n\n[22\\. DeepSeek-V3, GPT-4, Phi-4, and LLaMA-3.3 generate correct code for LoRaWAN-related engineering tasks](https://www.arxiv.org/pdf/2502.14926)\n\n[23\\. DeepSeek V3 0324 (Mar' 25) vs Grok 3 Reasoning Beta: Model Comparison](https://artificialanalysis.ai/models/comparisons/deepseek-v3-0324-vs-grok-3-reasoning)\n\n[24\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[25\\. 通信行业周报 2025年第6周 Deepseek-R1开源推动AI应用发展，头部AI厂支持Deepseek](https://www.faxianai.com/wp-content/uploads/2025/05/1747035038-3%E3%80%81%E9%80%9A%E4%BF%A1%E8%A1%8C%E4%B8%9A%E5%91%A8%E6%8A%A52025%E5%B9%B4%E7%AC%AC6%E5%91%A8%EF%BC%9ADeepseek-R1%E5%BC%80%E6%BA%90%E6%8E%A8%E5%8A%A8AI%E5%BA%94%E7%94%A8%E5%8F%91%E5%B1%95%EF%BC%8C%E5%A4%B4%E9%83%A8AI%E5%8E%82%E6%94%AF%E6%8C%81Deepseek.pdf)\n\n[26\\. DeepSeek V3: Inside the Open-Source AI Model Rivaling GPT-4](https://zilliz.com/blog/why-deepseek-v3-is-taking-the-ai-world-by-storm)\n\n[27\\. Deconstructing DeepSeek's AI disruption: Lessons for the EU](https://www.funcas.es/wp-content/uploads/2025/03/Deconstructing-DeepSeeks-AI-disruption.pdf)\n\n[28\\. DeepSeek 的抄袭争议——蒸馏技术的使用](https://pdf.dfcfw.com/pdf/H3_AP202502121643020785_1.pdf)\n\n[29\\. DEEPSEEK-V3 AND ITS AI MODEL R1](https://www.drkjberry.com/s/DEEP-SEEK-WP-FINAL-t7hb.pdf)\n\n[30\\. Mind Readings: DeepSeek Week Part 2 - Understanding the Different DeepSeek Versions](https://www.christopherspenn.com/2025/01/mind-readings-deepseek-week-part-2-understanding-the-different-deepseek-versions/)\n\n[31\\. Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](https://www.52nlp.cn/wp-content/uploads/2025/05/Insights-into-DeepSeek-V3%E8%8B%B1%E4%B8%AD%E5%AF%B9%E7%85%A7%E7%89%88.pdf)\n\n[32\\. 7x24小时全球实时财经新闻直播](https://finance.sina.com.cn/7x24/2024-12-30/doc-inecetwx2745986.shtml)\n\n[33\\. Notes on the new Deepseek v3 - Composio](https://composio.dev/blog/notes-on-new-deepseek-v3)\n\n[34\\. DeepSeek V3](https://deepseekv3.pro/)\n\n[35\\. Keeping a Pulse on DeepSeek](https://radicaldatascience.wordpress.com/2025/02/24/keeping-a-pulse-on-deepseek/)\n\n[36\\. DeepSeek V3: Can Free and Open-Source AI Chatbot Beat ChatGPT and Gemini](https://techwiser.com/deepseek-v3-can-free-and-open-source-ai-chatbot-beat-chatgpt-and-gemini/)\n\n[37\\. Evaluation of LLMs for mathematical problem solving](https://arxiv.org/abs/2506.00309)\n\n[38\\. Generative AI in Academic Writing: A Comparison of DeepSeek, Qwen, ChatGPT, Gemini, Llama, Mistral, and Gemma](https://www.arxiv.org/pdf/2503.04765)\n\n[39\\. Gemini 2.5 Pro 发布即屠榜, DeepSeek V3 完成模型更新——AI 动态汇总 20250331](https://aigc.idigital.com.cn/djyanbao/%E3%80%90%E4%B8%AD%E9%82%AE%E8%AF%81%E5%88%B8%E3%80%91AI%E5%8A%A8%E6%80%81%E6%B1%87%E6%80%BB%EF%BC%9AGemini2.5Pro%E5%8F%91%E5%B8%83%E5%8D%B3%E5%B1%A0%E6%A6%9C%EF%BC%8CDeepSeekV3%E5%AE%8C%E6%88%90%E6%A8%A1%E5%9E%8B%E6%9B%B4%E6%96%B0-2025-04-01.pdf)\n\n[40\\. Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code](https://mrlab.ai/papers/correa-et-al-arxiv2025.pdf)\n\n[41\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[42\\. Christian Szegedy, Vincent Vanhoucke et al. “Rethinking the Inception Architecture for Computer Vision.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2016.308)\n\n[43\\. How DeepSeek Cracked the Cost Barrier with $5.6M](https://www.unite.ai/how-deepseek-cracked-the-cost-barrier-with-5-6m/)\n\n[44\\. Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](https://www.52nlp.cn/wp-content/uploads/2025/05/Insights-into-DeepSeek-V3%E8%8B%B1%E4%B8%AD%E5%AF%B9%E7%85%A7%E7%89%88.pdf)\n\n[45\\. DeepSeek发布V3模型降本技术报告](https://www.donews.com/news/detail/4/5203837.html)\n\n[46\\. DeepSeek-V3 Technical Report](https://arxiv.org/pdf/2412.19437)\n\n[47\\. DeepSeek原理与项目实战](http://www.sccio.cn/uploads/20250429/63da89f5e042a46032d46e7661017a32.pdf)\n\n[48\\. DeepSeek 쇼크, 위기에서 찾은 기회](https://bbn.kiwoom.com/rfCC792)\n\n[49\\. 为什么Deepseek v3价格这么便宜](https://blog.moontak.com/id/312042/)\n\n[50\\. Technical Architecture of DeepSeek v3 Explained](https://618media.com/en/blog/technical-architecture-of-deepseek-v3-explained/)\n\n[51\\. DeepSeek：AI模型创新与算力成本优化的先锋](https://www.9fzt.com/detail/sh_688111_3_791984762607.html)\n\n[52\\. DeepSeek算法在AI模型训练中的节能优化](https://xueqiu.com/4958952780/322463535)\n\n[53\\. DeepSeek-V3：AI系统优化的前沿技术解析](https://www.showapi.com/news/article/6835fdbf4ddd79013c006eb3)\n\n[54\\. 中国的 AI，世界的未来——华宝上证科创板人工智能 ETF](https://pdf.dfcfw.com/pdf/H301_AP202503121644301211_1.pdf)\n\n[55\\. Guinness Global Quality Mid Cap Investment Commentary – February 2025](https://www.guinnessgi.com/sites/default/files/2025-02/2025.02_Guinness%20Global%20Quality%20Mid%20Cap_Commentary.pdf)\n\n[56\\. DeepSeek 核心十问十答](https://www.faxianai.com/wp-content/uploads/2025/05/1747041355-3%E3%80%81%E8%AE%A1%E7%AE%97%E6%9C%BA-DeepSeek%E6%A0%B8%E5%BF%83%E5%8D%81%E9%97%AE%E5%8D%81%E7%AD%94.pdf)\n\n[57\\. 策略月报：相信光，追逐光（2025年2月）](https://pdf.dfcfw.com/pdf/H3_AP202502201643331348_1.pdf?1740079984000.pdf)"
  },
  {
    "id": 10,
    "question": "What are the specific differences between the two major RL designs behind DeepMind and OpenAI? Both DeepMind and OpenAI have made significant achievements in deep reinforcement learning, but by analyzing some tutorial details from David Silver and Sergey Levine, I feel that their understanding and implementation of RL have quite different approaches. Is there a more in-depth comparison of these two RL research institutions?",
    "response": "**Comprehensive Comparative Analysis of DeepMind and OpenAI's Reinforcement Learning Research Designs**\n\n**Executive Summary**\n\nDeepMind and OpenAI represent distinct philosophical and technical paradigms in reinforcement learning (RL) research. While DeepMind prioritizes **algorithmic efficiency and environment modeling** with strong roots in game-theoretic applications, OpenAI emphasizes **generalization and safety** across diverse real-world tasks. These differences manifest in their exploration strategies, model-based vs. model-free preferences, neural architectures, and optimization approaches. The divergence is further exemplified in the work of key researchers like David Silver (DeepMind) and Sergey Levine (collaborator with OpenAI), whose publications reveal fundamentally different implementation priorities.\n\n**1\\. Foundational Research Philosophies**\n\n**DeepMind: Domain-Specific Mastery**\n\nDeepMind's RL research focuses on **environment-specific mastery** through sample-efficient learning. As stated, \"DeepMind specializes in RL where systems learn optimal behaviors through rewards\" \\[11\\], emphasizing breakthroughs in constrained environments like games (AlphaGo, AlphaStar) \\[15\\]. Their approach prioritizes **predictive modeling** and planning, aiming for superhuman performance in well-defined domains.\n\n**OpenAI: Generalized AGI Development**\n\nOpenAI targets **broadly applicable artificial general intelligence (AGI)** with explicit safety constraints. Their mission centers on \"developing safe and beneficial AI\" \\[11\\], leading to frameworks emphasizing generalization across tasks. Unlike DeepMind's domain specificity, OpenAI favors architectures that \"perform various tasks\" \\[11\\] through transferable representations, even at the cost of sample efficiency.\n\n**2\\. Algorithmic Design Philosophies**\n\n**2.1 Exploration-Exploitation Trade-offs**\n\n**DeepMind** favors **uncertainty-driven exploration**:\n\nUses **Optimism in the Face of Uncertainty (OFU)** principles, prioritizing high-reward potential in unexplored states \\[25\\]\\[37\\]\n\nImplements **intrinsic reward shaping** for environments with sparse rewards (e.g., Montezuma's Revenge) \\[84\\]\n\nPrefers **model-based foresight** to simulate outcomes before action \\[9\\]\n\n**OpenAI** emphasizes **entropy-maximizing strategies**:\n\nLeverages **Soft Actor-Critic (SAC)** frameworks to \"encourage the agent to act as randomly as possible while succeeding\" \\[22\\]\n\nAdopts **diversity-based methods** that maximize state entropy for skill discovery \\[93\\]\n\nUses **curiosity-driven bonuses** (e.g., Random Network Distillation) for open-ended environments \\[30\\]\n\n**2.2 Sample Efficiency Priorities**\n\n**DeepMind** sacrifices computational resources for **sample efficiency**:\n\nPrioritizes **model-based RL** to \"compress experience into predictive models\" \\[69\\], reducing real-world interactions\n\nAccepts higher computational costs for \"more effective policies\" \\[71\\]\n\n**OpenAI** tolerates higher sample costs for **generalization**:\n\nFocuses on **off-policy optimizations** (e.g., SAC) to reuse diverse experience buffers \\[22\\]\n\nImplements **generative verification paradigms** (Gen-Verifier) to train correctness-oriented reward models \\[34\\]\n\nEmphasizes **scalable exploration** over optimal efficiency \\[34\\]\n\n**3\\. Model-Based vs. Model-Free Implementation**\n\n|     |     |     |\n| --- | --- | --- |\n| **Aspect** | **DeepMind** | **OpenAI** |\n| **Dominant Approach** | Model-Based RL (explicit dynamics learning) | Model-Free RL (direct policy optimization) |\n| **Architecture** | Predictive world models (e.g., MuZero's dynamics network) \\[206\\] | End-to-end policy networks (e.g., DQN variants) \\[3\\] |\n| **Sample Efficiency** | High (leverages simulated rollouts) \\[71\\] | Lower (requires more environmental interactions) |\n| **Computational Cost** | Significantly higher \\[75\\] | Moderate (prioritizes parallelism) \\[114\\] |\n| **Environment Fit** | Fixed settings (e.g., games, factory robots) \\[67\\] | Real-world interactions (e.g., self-driving cars) \\[67\\] |\n\n**Flagship Examples**:\n\n**DeepMind's MuZero**: Explicitly model-based, using three networks (representation, dynamics, prediction) for \"mental simulations\" \\[206\\]\\[213\\]\n\n**OpenAI's DQN/PPO**: Model-free, directly mapping observations to actions via CNNs/MLPs \\[3\\]\\[110\\]\n\n**4\\. Neural Architecture & Optimization**\n\n**4.1 Network Designs**\n\n**DeepMind**:\n\n**MuZero**: Combines convolutional/linear layers with residual blocks and ELU activations \\[203\\]\n\nUtilizes **recurrent components** for temporal state compression (e.g., AlphaStar) \\[189\\]\n\nFavors **modular networks** (separate value/policy/dynamics models) \\[209\\]\n\n**OpenAI**:\n\nStandardizes **CNN/MLP backbones** with ReLU activations (e.g., 128 → 64 layer progression) \\[110\\]\n\nPrioritizes **architectural simplicity** for reproducibility \\[118\\]\n\nUses **asynchronous gradient descent** for parallelized training \\[114\\]\n\n**4.2 Optimization Methods**\n\n**DeepMind**:\n\n**Q-learning variants** with target networks and experience replay \\[105\\]\\[115\\]\n\n**Meta-gradient tuning** for adaptive hyperparameters \\[46\\]\n\n**OpenAI**:\n\n**Proximal Policy Optimization (PPO)** with KL-divergence constraints \\[110\\]\n\n**Evolutionary strategies** as gradient-free alternatives \\[112\\]\n\n**Policy gradient methods** (TRPO/PPO) for stable off-policy updates \\[193\\]\n\n**5\\. Researcher-Specific Paradigms**\n\n**David Silver (DeepMind):**\n\n**Foundational Theorist**: Focuses on unifying RL concepts (value/policy/model-based) \\[50\\]\n\n**Abstract Problem-Solving**: Emphasizes theoretical guarantees over real-world deployment \\[59\\]\n\nKey Contributions: Distributional RL, Meta-Gradient RL \\[40\\]\\[46\\]\n\n**Sergey Levine (Berkeley/OpenAI Collaborator):**\n\n**Applied Roboticist**: Targets \"real-world RL\" challenges like sim-to-real transfer \\[53\\]\n\n**Data Efficiency Pioneer**: Developed Soft Actor-Critic (SAC) for offline dataset utilization \\[43\\]\\[58\\]\n\n**Safety Advocate**: Focuses on confidence-conditioned value functions for risk-sensitive policies \\[58\\]\n\n**6\\. Technical Implications & Future Trajectories**\n\n**Sample Efficiency vs. Generalization Trade-off**: DeepMind's model-based focus yields faster convergence in constrained settings, while OpenAI’s model-free approach enables adaptability in dynamic environments \\[67\\]\n\n**Hardware Constraints**: DeepMind's compute-intensive methods favor proprietary infrastructure, whereas OpenAI’s parallelizable designs align with cloud scaling \\[86\\]\n\n**Safety Considerations**: OpenAI embeds constraints directly into policy updates (e.g., via KL penalties), while DeepMind relies on accurate environment modeling for risk mitigation \\[34\\]\\[194\\]\n\n**Emergent Trends**: Both increasingly blend paradigms—DeepMind incorporates model-free elements into MuZero, while OpenAI explores hybrid methods (Dyna-style) for sample-efficient robotics \\[193\\]\n\n**Conclusion**\n\nThe RL design dichotomy between DeepMind and OpenAI stems from fundamentally different objectives: **mastery** versus **generality**. DeepMind’s model-based, efficiency-oriented paradigm excels in predictable environments (games, molecular dynamics), leveraging learned dynamics for planning. OpenAI’s model-free, generalization-first approach prioritizes adaptability and safety in unstructured settings (robotics, open-ended agents), accepting higher sample costs. These distinctions permeate their neural architectures, exploration strategies, and optimization techniques. Notably, David Silver’s theoretical innovations at DeepMind contrast with Sergey Levine’s applied robotics focus at OpenAI, highlighting their institutional priorities. As both converge toward hybrid approaches, the core philosophical divergence—precision versus flexibility—remains evident in implementation details.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. R. S. Sutton, A. Barto. “Reinforcement Learning: An Introduction.” IEEE Trans. Neural Networks](https://doi.org/10.1109/TNN.1998.712192)\n\n[2\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[3\\. Comparing Model-Free and Model-Based Reinforcement Learning for Collision Avoidance](https://www.duo.uio.no/bitstream/handle/10852/79743/1/Master_Sorensen_2020.pdf)\n\n[4\\. Model-based vs Model-free Reinforcement Learning](https://www.aubergine.co/insights/model-based-vs-model-free-reinforcement-learning)\n\n[5\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Playing Atari with Deep Reinforcement Learning.” ArXiv](https://arxiv.org/abs/1312.5602)\n\n[6\\. Volodymyr Mnih, Adrià Puigdomènech Badia et al. “Asynchronous Methods for Deep Reinforcement Learning.” International Conference on Machine Learning](https://arxiv.org/abs/1602.01783)\n\n[7\\. DeepMind vs OpenAI: An In-Depth Comparison of the Leading Players in AI](https://www.aimunch.com/deepmind-vs-openai-an-in-depth-comparison-of-the-leading-players-in-ai/)\n\n[8\\. John Schulman, S. Levine et al. “Trust Region Policy Optimization.” ArXiv](https://arxiv.org/abs/1502.05477)\n\n[9\\. Everything you need to know about model-free and model-based reinforcement learning](https://thenextweb.com/news/everything-you-need-to-know-about-model-free-and-model-based-reinforcement-learning)\n\n[10\\. OpenAI VS DeepMind – Social News Daily](https://opensourcebiology.eu/2023/03/27/openai-vs-deepmind-social-news-daily/)\n\n[11\\. OpenAI vs. Google DeepMind: A Comparative Analysis in AI Research](https://fastbots.ai/blog/openai-vs.-google-deepmind-a-comparative-analysis-in-ai-research)\n\n[12\\. OpenAI与DeepMind在大语言模型训练中的分歧与实验分析](https://www.360doc.cn/article/9523589_1083489952.html)\n\n[13\\. Reinforcement learning](https://introml.mit.edu/_static/spring24/LectureNotes/chapter_Reinforcement_learning.pdf)\n\n[14\\. DeepMind vs OpenAI: The Battle for AI Supremacy on Quanrel](https://www.quanrel.com/google-deepmind-vs-openai/)\n\n[15\\. OpenAI vs DeepMind - A Comparative Analysis](https://openedai.io/openai-vs-deepmind/)\n\n[16\\. Cross-Domain Generalization with Reverse Dynamics Models in Offline Model-Based Reinforcement Learning](https://www.scitepress.org/Papers/2025/130972/130972.pdf)\n\n[17\\. Comparing Sample Efficiency Between Model-Based and Model-Free Reinforcement Learning Methods](https://fse.studenttheses.ub.rug.nl/33922/1/thesiss4752244.pdf)\n\n[18\\. OpenAI vs. DeepMind: Key Differences Explained](https://www.cubix.co/blog/openai-vs-deepmind/)\n\n[19\\. OpenAI终于要摸着DeepSeek过河了](https://www.newworldtimes.us/newspaperfiles/20250404/440_NWTB_18_C.pdf)\n\n[21\\. R. S. Sutton, A. Barto. “Reinforcement Learning: An Introduction.” IEEE Trans. Neural Networks](https://doi.org/10.1109/TNN.1998.712192)\n\n[22\\. Tuomas Haarnoja, Aurick Zhou et al. “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.” ArXiv](https://arxiv.org/abs/1801.01290)\n\n[23\\. K. Azizzadenesheli, E. Brunskill et al. “Efficient Exploration Through Bayesian Deep Q-Networks.” 2018 Information Theory and Applications Workshop (ITA)](https://doi.org/10.1109/ITA.2018.8503252)\n\n[24\\. Zhihan Liu, Miao Lu et al. “Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration.” Neural Information Processing Systems](https://arxiv.org/abs/2305.18258)\n\n[25\\. Hao Sun, Lei Han et al. “Exploiting Reward Shifting in Value-Based Deep RL.” ArXiv](https://doi.org/10.48550/arXiv.2209.07288)\n\n[26\\. ASYNCHRONOUS FEDERATED REINFORCEMENT LEARNING WITH POLICY GRADIENT UPDATES: ALGORITHM DESIGN AND CONVERGENCE ANALYSIS](https://openreview.net/pdf/d4e1723ad3b864b8092af3a372f5d804dc4bf0a0.pdf)\n\n[27\\. Investigation of Different Observation and Action Spaces for Reinforcement Learning on Reaching Tasks](https://kth.diva-portal.org/smash/get/diva2:1415901/FULLTEXT01.pdf)\n\n[28\\. Chuheng Zhang, Yuanying Cai et al. “Exploration by Maximizing Renyi Entropy for Reward-Free RL Framework.” AAAI Conference on Artificial Intelligence](https://doi.org/10.1609/aaai.v35i12.17297)\n\n[29\\. Deepak Pathak, Pulkit Agrawal et al. “Curiosity-Driven Exploration by Self-Supervised Prediction.” 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)](https://doi.org/10.1109/CVPRW.2017.70)\n\n[30\\. Yuri Burda, Harrison Edwards et al. “Exploration by Random Network Distillation.” ArXiv](https://arxiv.org/abs/1810.12894)\n\n[31\\. Chuheng Zhang, Yuanying Cai et al. “Exploration by Maximizing Rényi Entropy for Zero-Shot Meta RL.” ArXiv](https://arxiv.org/abs/2006.06193)\n\n[32\\. Thomas Jaksch, R. Ortner et al. “Near-optimal Regret Bounds for Reinforcement Learning.” J. Mach. Learn. Res.](https://doi.org/10.5555/1756006.1859902)\n\n[33\\. Introduction to Reinforcement Learning](https://indico.scc.kit.edu/event/4216/contributions/19559/attachments/9012/15127/2025-04-02_intro_to_RL.pdf)\n\n[34\\. Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models](https://arxiv.org/pdf/2503.09567)\n\n[35\\. Prerit Terway, Kenza Hamidouche et al. “DISPATCH: Design Space Exploration of Cyber-Physical Systems.” ArXiv](https://arxiv.org/abs/2009.10214)\n\n[36\\. Unlocking the Mysteries of OpenAI o1: A Survey of the Reasoning Abilities of Large Language Models](https://openreview.net/pdf/1bf54a44ba5686f6fb3e22b7989c1a68b1d7f485.pdf)\n\n[37\\. DEEP EXPLORATION VIA RANDOMIZED VALUE FUNCTIONS](https://stacks.stanford.edu/file/druid:rp457qc7612/iosband_thesis-augmented.pdf)\n\n[38\\. Exploration of Teacher-Centered and Task-Centered paradigms for efficient transfer of skills between morphologically distinct robots](https://theses.hal.science/tel-03229727v2/file/2020CLFAC065_MOUNSIF.pdf)\n\n[39\\. Deep hedging of CVA](https://lup.lub.lu.se/student-papers/record/9174022/file/9174023.pdf)\n\n[40\\. The Path to AGI: Technical Milestones, Philosophical Debates, and Societal Implications](https://cdn.prod.website-files.com/67343452a4bb784798d28a97/67a2dc57414396e94ea504d8_The%20Path%20to%20AGI.pdf)\n\n[41\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[42\\. T. Lillicrap, Jonathan J. Hunt et al. “Continuous control with deep reinforcement learning.” CoRR](https://arxiv.org/abs/1509.02971)\n\n[43\\. Tuomas Haarnoja, Aurick Zhou et al. “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.” ArXiv](https://arxiv.org/abs/1801.01290)\n\n[44\\. John Schulman, S. Levine et al. “Trust Region Policy Optimization.” ArXiv](https://arxiv.org/abs/1502.05477)\n\n[45\\. GitHub - weizhending/reinforcement-learning-David-Silver: Implementation of Reinforcement Learning Algorithms](https://github.com/weizhending/reinforcement-learning-David-Silver)\n\n[46\\. GitHub - zyynnn/Meta-Learning-Papers: Meta Learning / Learning to Learn / One Shot Learning / Few Shot Learning](https://github.com/toothlessz/Meta-Learning-Papers)\n\n[47\\. Sergey Levine](http://www.eecs.berkeley.edu/~svlevine/)\n\n[48\\. Fine-Tuning Offline Reinforcement Learning with Model-Based Policy Optimization](https://www.ri.cmu.edu/app/uploads/2021/09/62.pdf)\n\n[49\\. Invited Talk by Sergey Levine: Leveraging Offline Datasets / Foundation Models for Real-World RL](https://iclr.cc/virtual/2023/13841)\n\n[50\\. David Silver, Google DeepMind: Deep Reinforcement Learning](https://syncedreview.com/2017/02/24/david-silver-google-deepmind-deep-reinforcement-learning/amp/)\n\n[51\\. Aviral Kumar, Aurick Zhou et al. “Conservative Q-Learning for Offline Reinforcement Learning.” ArXiv](https://arxiv.org/abs/2006.04779)\n\n[52\\. Publications - David Silver](https://www.davidsilver.uk/publications/)\n\n[53\\. Deep Reinforcement Learning and Learning from Demonstrations for Robot Manipulators](https://pastel.hal.science/tel-04804036v1/file/2024UPSLM022_archivage.pdf)\n\n[54\\. 大量完整的强化学习内容](https://cloud.tencent.com/developer/article/1159987)\n\n[55\\. Actor-Critic Methods (A2C, A3C)](https://cse.buffalo.edu/~avereshc/rl_fall19/lecture_20_Actor_Critic_Methods.pdf)\n\n[56\\. Susan Amin, Maziar Gomrokchi et al. “A Survey of Exploration Methods in Reinforcement Learning.” ArXiv](https://arxiv.org/abs/2109.00157)\n\n[57\\. Minne Li, Pranav Nashikkar et al. “Optimizing Object-based Perception and Control by Free-Energy Principle.” ArXiv](https://arxiv.org/abs/1903.01385)\n\n[58\\. Max Sobol Mark, Archit Sharma et al. “Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate Exploration Bias.” ArXiv](https://doi.org/10.48550/arXiv.2310.08558)\n\n[59\\. Reinforcement Learning Course Notes-David Silver](https://dongdongbh.tech/RL-courses/)\n\n[60\\. Sergey Levine教授近期论文速览](https://zhuanlan.zhihu.com/p/634767431)\n\n[61\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[62\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[63\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Playing Atari with Deep Reinforcement Learning.” ArXiv](https://arxiv.org/abs/1312.5602)\n\n[64\\. John Schulman, S. Levine et al. “Trust Region Policy Optimization.” ArXiv](https://arxiv.org/abs/1502.05477)\n\n[65\\. Comparing Sample Efficiency Between Model-Based and Model-Free Reinforcement Learning Methods](https://fse.studenttheses.ub.rug.nl/33922/1/thesiss4752244.pdf)\n\n[66\\. Cross-Domain Generalization with Reverse Dynamics Models in Offline Model-Based Reinforcement Learning](https://www.scitepress.org/Papers/2025/130972/130972.pdf)\n\n[67\\. Model-based vs Model-free Reinforcement Learning](https://www.aubergine.co/insights/model-based-vs-model-free-reinforcement-learning)\n\n[68\\. A Review of Reinforcement Learning Evolution: Taxonomy, Challenges and Emerging Solutions](https://thesai.org/Downloads/Volume16No1/Paper_49-A_Review_of_Reinforcement_Learning_Evolution.pdf)\n\n[69\\. Contributions to the Adoption of Deep Reinforcement Learning Algorithms in Real-World Robotics](https://mediatum.ub.tum.de/doc/1735122/1735122.pdf)\n\n[70\\. Enhancing autonomy and efficiency in goal-conditioned reinforcement learning](https://scholarlypublications.universiteitleiden.nl/access/item%3A4196089/download)\n\n[71\\. Curious Replay for Model-based Adaptation](https://proceedings.mlr.press/v202/kauvar23a/kauvar23a.pdf)\n\n[72\\. Robustly Learning Composable Options in Deep Reinforcement Learning](https://www.ijcai.org/proceedings/2021/0298.pdf)\n\n[73\\. Model Based and Model Free Offline Reinforcement Learning Combat Simulation Scenario](https://www.american-cse.org/csce2023-ieee/pdfs/CSCE2023-5LlpKs7cpb4k2UysbLCuOx/275900a573/275900a573.pdf)\n\n[74\\. Comparing Model-Free and Model-Based Reinforcement Learning for Collision Avoidance](https://www.duo.uio.no/bitstream/handle/10852/79743/1/Master_Sorensen_2020.pdf)\n\n[75\\. An Introduction to Deep Reinforcement Learning](https://ics.uci.edu/~dechter/courses/ics-295/fall-2019/texts/An_Introduction_to_Deep_Reinforcement_Learning.pdf)\n\n[76\\. M. Deisenroth, C. Rasmussen. “PILCO: A Model-Based and Data-Efficient Approach to Policy Search.” International Conference on Machine Learning](https://www.semanticscholar.org/paper/60b7d47758a71978e74edff6dd8dea4d9c791d7a)\n\n[77\\. Lili Chen, Kevin Lu et al. “Decision Transformer: Reinforcement Learning via Sequence Modeling.” Neural Information Processing Systems](https://arxiv.org/abs/2106.01345)\n\n[81\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[82\\. Volodymyr Mnih, Adrià Puigdomènech Badia et al. “Asynchronous Methods for Deep Reinforcement Learning.” International Conference on Machine Learning](https://arxiv.org/abs/1602.01783)\n\n[83\\. Oleksii Zhelo, Jingwei Zhang et al. “Curiosity-driven Exploration for Mapless Navigation with Deep Reinforcement Learning.” ArXiv](https://arxiv.org/abs/1804.00456)\n\n[84\\. Marc G. Bellemare, S. Srinivasan et al. “Unifying Count-Based Exploration and Intrinsic Motivation.” Neural Information Processing Systems](https://arxiv.org/abs/1606.01868)\n\n[85\\. Yuri Burda, Harrison Edwards et al. “Exploration by Random Network Distillation.” ArXiv](https://arxiv.org/abs/1810.12894)\n\n[86\\. OpenAI vs DeepMind: The Race to Advance AI](https://techlasi.com/newtechnology/openai-vs-deepmind/)\n\n[87\\. REWARD UNCERTAINTY FOR EXPLORATION IN PREFERENCE-BASED REINFORCEMENT LEARNING](https://openreview.net/pdf?id=OWZVD-l-ZrC)\n\n[88\\. Curiosity-driven AI for Science: Automated Discovery of Self-Organized Structures](https://theses.hal.science/tel-04504878v2/file/ETCHEVERRY_MAYALEN_2023.pdf)\n\n[89\\. Rein Houthooft, Xi Chen et al. “VIME: Variational Information Maximizing Exploration.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/317cd4522b1f4a6f889743578143bb8823623f8b)\n\n[90\\. Deepak Pathak, Pulkit Agrawal et al. “Curiosity-Driven Exploration by Self-Supervised Prediction.” 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)](https://doi.org/10.1109/CVPRW.2017.70)\n\n[91\\. Balancing Familiarity and Curiosity in Data Exploration with Deep Reinforcement Learning](https://hal.science/hal-04600277v1/file/Curiosity_Exploration.pdf)\n\n[92\\. Curiosity-driven exploration: Diversity of mechanisms and functions](https://inria.hal.science/hal-03447896/document)\n\n[93\\. Curiosity in Hindsight: Intrinsic Exploration in Stochastic Environments](https://proceedings.mlr.press/v202/jarrett23a/jarrett23a.pdf)\n\n[94\\. Chuheng Zhang, Yuanying Cai et al. “Exploration by Maximizing Renyi Entropy for Reward-Free RL Framework.” AAAI Conference on Artificial Intelligence](https://doi.org/10.1609/aaai.v35i12.17297)\n\n[95\\. Zhihan Liu, Miao Lu et al. “Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration.” Neural Information Processing Systems](https://arxiv.org/abs/2305.18258)\n\n[96\\. Continuous control for robot based on deep reinforcement learning](https://dr.ntu.edu.sg/bitstream/10356/90191/1/thesis-final.pdf)\n\n[97\\. The Path to AGI: Technical Milestones, Philosophical Debates, and Societal Implications](https://cdn.prod.website-files.com/67343452a4bb784798d28a97/67a2dc57414396e94ea504d8_The%20Path%20to%20AGI.pdf)\n\n[98\\. Rein Houthooft, Xi Chen et al. “Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks.” ArXiv](https://arxiv.org/abs/1605.09674)\n\n[101\\. Comparative Study of Neural Network Architectures in Deep Reinforcement Learning](https://www.espjournals.org/IJACT/2023/Volume1-Issue2/IJACT-V1I2P109.pdf)\n\n[102\\. R. S. Sutton, A. Barto. “Reinforcement Learning: An Introduction.” IEEE Trans. Neural Networks](https://doi.org/10.1109/TNN.1998.712192)\n\n[103\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[104\\. T. Lillicrap, Jonathan J. Hunt et al. “Continuous control with deep reinforcement learning.” CoRR](https://arxiv.org/abs/1509.02971)\n\n[105\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Playing Atari with Deep Reinforcement Learning.” ArXiv](https://arxiv.org/abs/1312.5602)\n\n[106\\. Reinforcement Learning for Generative AI: A Survey](https://arxiv.org/pdf/2308.14328)\n\n[107\\. Deep Reinforcement Learning Implementation for Video Transmission Optimization in New Generation Networks](https://oa.upm.es/85749/1/ALBERTO_DEL_RIO_PONCE.pdf)\n\n[108\\. DeepFoldit - A Deep Reinforcement Learning Neural Network Folding Proteins](https://pergamos.lib.uoa.gr/uoa/dl/object/2884734/file.pdf)\n\n[109\\. OpenAI vs. Google DeepMind: A Comparative Analysis in AI Research](http://fastbots.ai/blog/openai-vs.-google-deepmind-a-comparative-analysis-in-ai-research)\n\n[110\\. Network System Optimization with Reinforcement Learning: Methods and Applications](https://people.csail.mit.edu/hongzi/content/publications/hongzi-mao-phd-thesis.pdf)\n\n[111\\. Comparing Open AI and Google Deepmind](https://rejolut.com/blog/comparing-openai-and-google-deepmind/)\n\n[112\\. 基于神经进化的深度学习模型研究综述](https://www.ejournal.org.cn/CN/PDF/10.12263/DZXB.20200139)\n\n[113\\. OpenAI vs DeepMind - A Comparative Analysis](https://openedai.io/openai-vs-deepmind/)\n\n[114\\. Volodymyr Mnih, Adrià Puigdomènech Badia et al. “Asynchronous Methods for Deep Reinforcement Learning.” International Conference on Machine Learning](https://arxiv.org/abs/1602.01783)\n\n[115\\. A Deep Reinforcement Learning Optimization Method Considering Network Node Failures](https://www.mdpi.com/1996-1073/17/17/4471)\n\n[116\\. Optimizing Deep Learning: Navigating the Field of Neural Architecture Search from Theory to Practice](https://hal.science/tel-04437745/file/organized%20%288%29.pdf)\n\n[117\\. Artificial Intelligence and Deep Learning in Sensors and Applications](https://mdpi-res.com/bookfiles/book/9556/Artificial_Intelligence_and_Deep_Learning_in_Sensors_and_Applications.pdf?v=1738462124)\n\n[118\\. Symmetry, hierarchical structures and shallow neural networks: Advancing reinforcement learning for humanoids](https://repositorio-aberto.up.pt/bitstream/10216/161212/2/683642.pdf)\n\n[119\\. Using Reinforcement Learning to Play Bomberman](https://www.zhaw.ch/storage/engineering/institute-zentren/cai/BA18_PinballWizardllUsingRLtoControlaPinballAutomaton.pdf)\n\n[121\\. R. S. Sutton, A. Barto. “Reinforcement Learning: An Introduction.” IEEE Trans. Neural Networks](https://doi.org/10.1109/TNN.1998.712192)\n\n[122\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[123\\. Comparing Sample Efficiency Between Model-Based and Model-Free Reinforcement Learning Methods](https://fse.studenttheses.ub.rug.nl/33922/1/thesiss4752244.pdf)\n\n[124\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Playing Atari with Deep Reinforcement Learning.” ArXiv](https://arxiv.org/abs/1312.5602)\n\n[125\\. Comparing Model-Free and Model-Based Reinforcement Learning for Collision Avoidance](https://www.duo.uio.no/bitstream/handle/10852/79743/1/Master_Sorensen_2020.pdf)\n\n[126\\. Model-based vs Model-free Reinforcement Learning](https://www.aubergine.co/insights/model-based-vs-model-free-reinforcement-learning)\n\n[127\\. John Schulman, S. Levine et al. “Trust Region Policy Optimization.” ArXiv](https://arxiv.org/abs/1502.05477)\n\n[128\\. Model Based and Model Free Offline Reinforcement Learning Combat Simulation Scenario](https://www.american-cse.org/csce2023-ieee/pdfs/CSCE2023-5LlpKs7cpb4k2UysbLCuOx/275900a573/275900a573.pdf)\n\n[129\\. A Study of Model Based and Model Free Offline Reinforcement Learning](https://american-cse.org/csci2022-ieee/pdfs/CSCI2022-2lPzsUSRQukMlxf8K2x89I/202800a315/202800a315.pdf)\n\n[130\\. Artificial Intelligence: Foundations of Computational Agents](https://mrce.in/ebooks/AI%20Foundations%20of%20Computational%20Agents%203rd%20Ed.pdf)\n\n[141\\. R. S. Sutton, A. Barto. “Reinforcement Learning: An Introduction.” IEEE Trans. Neural Networks](https://doi.org/10.1109/TNN.1998.712192)\n\n[142\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[143\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[144\\. Deep Reinforcement Learning AlphaGo](https://julien-vitay.net/course-deeprl/slides/pdf/4.3-AlphaGo.pdf)\n\n[145\\. Efficient Multi-Agent Reinforcement Learning by Planning](https://openreview.net/attachment?id=CpnKq3UJwp&name=pdf)\n\n[146\\. Visualizing MuZero Models](https://openreview.net/pdf/5291aeee6cdca5380d0e16d0c999ef187d0a00cf.pdf)\n\n[147\\. David Silver, T. Hubert et al. “A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.” Science](https://doi.org/10.1126/science.aar6404)\n\n[148\\. Combinatorial Optimization with Reinforcement Learning: Solving Heterogenous Capacitated Vehicle Problem Iteratively with Attention Networks and Reinforcement Learning](https://odr.chalmers.se/bitstreams/26c15c51-17cd-4ed2-b1dd-b3d1da8e6469/download)\n\n[149\\. Learning to Play: Reinforcement Learning and Games](https://liacs.leidenuniv.nl/~plaata1/papers/ptl4.pdf)\n\n[150\\. STUDY MATERIAL Foundation for AI 10 MARKS IN Artificial Intelligence \\[ ARTI \\] ---- CLASS XI](https://wbchse.wb.gov.in/wp-content/uploads/2023/10/AI_XI_SEC3.pdf)\n\n[151\\. What model does MuZero learn?](https://arxiv.org/pdf/2306.00840v2)\n\n[152\\. Towards Explainable Decision-Making Algorithms For Trustworthy Industrial Applications (信頼できる産業応用のための説明可能意思決定アルゴリズムの実現に向けて)](https://repository.dl.itc.u-tokyo.ac.jp/record/2012132/files/A40784.pdf)\n\n[153\\. λ: EFFECTIVE DECISION-AWARE REINFORCEMENT LEARNING WITH LATENT MODELS](https://openreview.net/pdf/cefbaf2100e393ae7b40f4d6cc6b70decb7efdf1.pdf)\n\n[154\\. Sample Efficient Reinforcement Learning through Learning from Demonstrations in Minecraft](http://proceedings.mlr.press/v123/scheller20a/scheller20a.pdf)\n\n[155\\. Yaniv Oren, M. Spaan et al. “Planning with Uncertainty: Deep Exploration in Model-Based Reinforcement Learning.” ArXiv](https://doi.org/10.48550/arXiv.2210.13455)\n\n[156\\. MuZero General: Open Reimplementation of MuZero](https://github.com/werner-duvaud/muzero-general/)\n\n[157\\. Julian Schrittwieser, Ioannis Antonoglou et al. “Mastering Atari, Go, chess and shogi by planning with a learned model.” Nature](https://doi.org/10.1038/s41586-020-03051-4)\n\n[158\\. Exploration and Mapping on Ultra-Parallel Heterogeneous Architectures: A Performance Analysis and Exploration of a NAS Algorithm for Deep Neural Networks](https://didattica.polito.it/pls/portal30/sviluppo.tesiv.elenchi?dove=&dgrp=GR-06%20-%20ELECTRONIC%20DESIGN%20AUTOMATION%20-%20EDA&idt=11854&lang=EN&opng=S&opnc=)\n\n[159\\. DeepMind’s MuZero is One of the Most Important Deep Learning Systems Ever Created](https://www.kdnuggets.com/2021/01/deepmind-muzero-important-deep-learning-systems.html)\n\n[160\\. Dueling Network Architectures for Deep Reinforcement Learning](http://proceedings.mlr.press/v48/wangf16.pdf)\n\n[161\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[162\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[163\\. T. Lillicrap, Jonathan J. Hunt et al. “Continuous control with deep reinforcement learning.” CoRR](https://arxiv.org/abs/1509.02971)\n\n[164\\. Exploration in Deep Reinforcement Learning: A Survey](https://dl.acm.org/doi/10.1016/j.inffus.2022.03.003)\n\n[165\\. Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning](https://proceedings.mlr.press/v202/yuan23c/yuan23c.pdf)\n\n[166\\. Is a Good Foundation Necessary for Efficient Reinforcement Learning? The Computational Role of the Base Model in Exploration](https://arxiv.org/pdf/2503.07453)\n\n[167\\. Benchmarking Safe Exploration in Deep Reinforcement Learning](https://cdn.openai.com/safexp-short.pdf)\n\n[168\\. Jeng-Yueh Hsiao, Yuxuan Du et al. “Unentangled quantum reinforcement learning agents in the OpenAI Gym.”](https://arxiv.org/abs/2203.14348)\n\n[169\\. Leveraging Transfer Learning to Improve Convergence in All-Pay Auctions](http://www.scitepress.org/Papers/2025/132940/132940.pdf)\n\n[170\\. From Google Gemini to OpenAI Q\\* (Q-Star): A Survey on Reshaping the Generative Artificial Intelligence (AI) Research Landscape](https://mro.massey.ac.nz/server/api/core/bitstreams/2bcf544e-c098-4915-8677-0955ecf703e8/content)\n\n[171\\. DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments](https://arxiv.org/pdf/2504.03160)\n\n[172\\. Yuri Burda, Harrison Edwards et al. “Exploration by Random Network Distillation.” ArXiv](https://arxiv.org/abs/1810.12894)\n\n[173\\. Marc G. Bellemare, S. Srinivasan et al. “Unifying Count-Based Exploration and Intrinsic Motivation.” Neural Information Processing Systems](https://arxiv.org/abs/1606.01868)\n\n[174\\. Improved Exploration Strategy for Q-Learning Based Multipath Routing in SDN Networks](https://link.springer.com/article/10.1007/s10922-024-09804-0)\n\n[175\\. 开放环境下的协作多智能体强化学习进展](http://scis.scichina.com/cn/2025/SSI-2023-0335.pdf)\n\n[176\\. Adaptive Resource Management in CI/CD Environments Using Deep Deterministic Policy Gradients](https://ijcat.com/archieve/volume13/issue7/ijcatr13071007.pdf)\n\n[177\\. 在线深度强化学习探索策略生成方法综述](https://robot.sia.cn/cn/article/pdf/preview/10.13973/j.cnki.robot.230252.pdf)\n\n[178\\. Explore-Go: Leveraging Exploration for Generalisation in Deep Reinforcement Learning](https://openreview.net/pdf?id=qteUVvGvFQ)\n\n[179\\. Susan Amin, Maziar Gomrokchi et al. “A Survey of Exploration Methods in Reinforcement Learning.” ArXiv](https://arxiv.org/abs/2109.00157)\n\n[181\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[182\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[183\\. Expert-led online Reinforcement Learning - from Fundamentals to Deep RL course](https://luxoft-training.com/it-courses/reinforcement-learning-from-fundamentals-to-deep-rl-eas-027)\n\n[184\\. T. Lillicrap, Jonathan J. Hunt et al. “Continuous control with deep reinforcement learning.” CoRR](https://arxiv.org/abs/1509.02971)\n\n[185\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Playing Atari with Deep Reinforcement Learning.” ArXiv](https://arxiv.org/abs/1312.5602)\n\n[186\\. Deep-Reinforcement-Learning/A3C/README.md at master](https://github.com/maiwen/Deep-Reinforcement-Learning/blob/master/A3C/README.md)\n\n[187\\. Model-based vs Model-free Reinforcement Learning](https://www.aubergine.co/insights/model-based-vs-model-free-reinforcement-learning)\n\n[188\\. Comparing Sample Efficiency Between Model-Based and Model-Free Reinforcement Learning Methods](https://fse.studenttheses.ub.rug.nl/33922/1/thesiss4752244.pdf)\n\n[189\\. Comparing Model-Free and Model-Based Reinforcement Learning for Collision Avoidance](https://www.duo.uio.no/bitstream/handle/10852/79743/1/Master_Sorensen_2020.pdf)\n\n[190\\. John Schulman, S. Levine et al. “Trust Region Policy Optimization.” ArXiv](https://arxiv.org/abs/1502.05477)\n\n[191\\. Introduction to Artificial Intelligence](https://www.user.tu-berlin.de/mtoussai/teaching/15-ArtificialIntelligence/15-AI-slides.pdf)\n\n[192\\. Deep Reinforcement Learning and Function Optimization](https://assets.super.so/3cea549f-cd26-452c-97d5-b70d8b6f564e/files/237a109e-88b1-433f-83c6-75935bf0dd33/Deep_Reinforcement_Learning-compressed.pdf)\n\n[193\\. 强化学习基础 Ⅲ：on-policy, off-policy & Model-based, Model-free & Rollout](https://zhuanlan.zhihu.com/p/115629505)\n\n[194\\. Learning to Play: Reinforcement Learning and Games](https://liacs.leidenuniv.nl/~plaata1/papers/ptl4.pdf)\n\n[195\\. Deep Reinforcement Learning based Obstacle Avoidance for UAVs under Measurement Uncertainty](https://web2py.iiit.ac.in/research_centres/publications/download/mastersthesis.pdf.9ec7e7450a5fdeea.46696e616c5468657369735f426861736b61722e706466.pdf)\n\n[196\\. Choosing Reinforcement Learning Models: A Comprehensive Guide](https://machinelearningmodels.org/choosing-reinforcement-learning-models-a-comprehensive-guide/)\n\n[197\\. Analyzing Approaches to the Problem of Avoiding Side Effects in Autonomous Agents](https://www.duo.uio.no/bitstream/handle/10852/79594/1/thesis_serwaw.pdf)\n\n[198\\. A Review of Reinforcement Learning Evolution: Taxonomy, Challenges and Emerging Solutions](https://thesai.org/Downloads/Volume16No1/Paper_49-A_Review_of_Reinforcement_Learning_Evolution.pdf)\n\n[199\\. Recent Developments and Directions in Artificial Intelligence Technology](https://www.theseus.fi/bitstream/10024/802192/2/Opinnaytetyo_Niskanen_Teemu.pdf)\n\n[201\\. R. S. Sutton, A. Barto. “Reinforcement Learning: An Introduction.” IEEE Trans. Neural Networks](https://doi.org/10.1109/TNN.1998.712192)\n\n[202\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[203\\. 縮小盤オセロにおけるAlphaZeroとMuZeroの比較分析](https://www.kochi-tech.ac.jp/library/ron/pdf/2023/03/13/a1240321.pdf)\n\n[204\\. David Silver, T. Hubert et al. “A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.” Science](https://doi.org/10.1126/science.aar6404)\n\n[205\\. Marc G. Bellemare, Yavar Naddaf et al. “The Arcade Learning Environment: An Evaluation Platform for General Agents.” ArXiv](https://doi.org/10.1613/jair.3912)\n\n[206\\. Julian Schrittwieser, Ioannis Antonoglou et al. “Mastering Atari, Go, chess and shogi by planning with a learned model.” Nature](https://doi.org/10.1038/s41586-020-03051-4)\n\n[207\\. STUDY MATERIAL Foundation for AI 10 MARKS IN Artificial Intelligence \\[ ARTI \\] ---- CLASS XI](https://wbchse.wb.gov.in/wp-content/uploads/2023/10/AI_XI_SEC3.pdf)\n\n[208\\. Developments in Model-Based Reinforcement Learning](https://icaps20subpages.icaps-conference.org/wp-content/uploads/2020/10/icaps-virtual-school-2020-model-based-rl.pdf)\n\n[209\\. Computing Science (CMPUT) 455 Search, Knowledge, and Simulations](https://webdocs.cs.ualberta.ca/~mmueller/courses/cmput455/slides/lecture24.pdf)\n\n[210\\. A. Plaat, W. Kosters et al. “High-accuracy model-based reinforcement learning, a survey.” Artificial Intelligence Review](https://doi.org/10.1007/s10462-022-10335-w)\n\n[211\\. ENHANCED MONTE CARLO TREE SEARCH IN GAME-PLAYING AI: EVALUATING DEEPMIND'S ALGORITHMS](https://espace.rmc-cmr.ca/jspui/bitstream/11264/1502/1/MASTER_THESIS_Karla_Aug_2023%20%281%29.pdf)\n\n[212\\. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://blog.moontak.com/wp-content/uploads/2025/02/2025021013465966.pdf)\n\n[213\\. Deep Reinforcement Learning AlphaGo](https://julien-vitay.net/course-deeprl/slides/pdf/4.3-AlphaGo.pdf)\n\n[214\\. λ: EFFECTIVE DECISION-AWARE REINFORCEMENT LEARNING WITH LATENT MODELS](https://openreview.net/pdf/cefbaf2100e393ae7b40f4d6cc6b70decb7efdf1.pdf)\n\n[215\\. Sample-Efficient Methods for Real-World Deep Reinforcement Learning](https://aaltodoc.aalto.fi/bitstreams/4e52adcd-04f4-4f34-9b33-903ad87adbab/download)\n\n[216\\. DeepMind’s New AI is Able to Learn the Rules of a Game as it Plays](https://www.unite.ai/deepminds-new-ai-is-able-to-learn-the-rules-of-a-game-as-it-plays/)\n\n[217\\. Didn’t DeepMind just produce a self-learning AI?](http://selfassemblingbrain.com/didnt-deepmind-just-produce-a-self-learning-ai/)\n\n[221\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[222\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[223\\. T. Lillicrap, Jonathan J. Hunt et al. “Continuous control with deep reinforcement learning.” CoRR](https://arxiv.org/abs/1509.02971)\n\n[224\\. Tuomas Haarnoja, Aurick Zhou et al. “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.” ArXiv](https://arxiv.org/abs/1801.01290)\n\n[225\\. EXPLORATORY PREFERENCE OPTIMIZATION: HARNESING IMPLICIT Q\\*-APPROXIMATION FOR SAMPLE-EFFICIENT RLHF](https://openreview.net/attachment?id=QYigQ6gXNw&name=supplementary_material)\n\n[226\\. Benchmarking Safe Exploration in Deep Reinforcement Learning](https://cdn.openai.com/safexp-short.pdf)\n\n[227\\. Leveraging Transfer Learning to Improve Convergence in All-Pay Auctions](http://www.scitepress.org/Papers/2025/132940/132940.pdf)\n\n[228\\. Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning](https://proceedings.mlr.press/v202/yuan23c/yuan23c.pdf)\n\n[229\\. From Google Gemini to OpenAI Q\\* (Q-Star): A Survey on Reshaping the Generative Artificial Intelligence (AI) Research Landscape](https://mro.massey.ac.nz/server/api/core/bitstreams/2bcf544e-c098-4915-8677-0955ecf703e8/content)\n\n[230\\. Jeng-Yueh Hsiao, Yuxuan Du et al. “Unentangled quantum reinforcement learning agents in the OpenAI Gym.”](https://arxiv.org/abs/2203.14348)\n\n[231\\. Exploration in Deep Reinforcement Learning: A Survey](https://dl.acm.org/doi/10.1016/j.inffus.2022.03.003)\n\n[232\\. Marc G. Bellemare, S. Srinivasan et al. “Unifying Count-Based Exploration and Intrinsic Motivation.” Neural Information Processing Systems](https://arxiv.org/abs/1606.01868)\n\n[233\\. The Evolution of Reinforcement Learning in Quantitative Finance: A Survey](http://www.arxiv.org/pdf/2408.10932)\n\n[234\\. DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments](https://arxiv.org/pdf/2504.03160)\n\n[235\\. Amjad Yousef Majid. “Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey.”](https://doi.org/10.36227/TECHRXIV.14679504.V1)\n\n[236\\. A Survey of Deep Learning Techniques: Applications Across Industries and Ethical Considerations](https://www.preprints.org/manuscript/202505.1098/v1/download)\n\n[237\\. Deepmind“好奇心”强化学习新突破](http://tech.hexun.com/2018-10-25/194992885.html)\n\n[238\\. Explore-Go: Leveraging Exploration for Generalisation in Deep Reinforcement Learning](https://openreview.net/pdf?id=qteUVvGvFQ)\n\n[239\\. AAAI-2020: 52篇深度强化学习accept论文](https://zhuanlan.zhihu.com/p/102543540)"
  },
  {
    "id": 11,
    "question": "How can research on an agent's planning capabilities, as well as an AI's understanding and simulation of the real world—including improvements in visual perception—be systematically approached? Please outline key research directions and trends in this field, referencing relevant academic papers.",
    "response": "**Systematic Approaches to Advancing AI Planning, Real-World Simulation, and Visual Perception**\n\n**1\\. Foundational Integration of Planning and Physics Simulation**\n\nIntegrating AI planning with real-world physics requires multi-physics simulation platforms and physics-informed AI methods. **RFUniverse** enables high-fidelity simulations of rigid/multi-body dynamics, aerodynamics, and thermodynamics, combining lightweight motion planning (RFMove) for embodied agent training \\[2\\]. **Differentiable physics engines** accelerate simulations by embedding physical laws into neural networks, allowing gradient-based optimization for planning tasks \\[1\\]. For complex planning under uncertainty, frameworks like **HTN (Hierarchical Task Network) planning** and Li et al.'s task planning for virtual room rearrangement address partial observability by incorporating probabilistic physical interactions \\[3\\]\\[7\\].\n\nTrends favor **hybrid simulation-learning architectures**:\n\n**GNNs for physics simulation**: Sanchez-Gonzalez et al.'s GNNs accelerate PDE-based simulations \\[10\\], while Lam et al.'s mesh-based simulations enable efficient large-scale environmental modeling \\[10\\].\n\n**AI-enhanced digital twins**: Integration in urban planning (Herzog et al., 2023) and materials science (e.g., AtomAgents for alloy design) demonstrates cross-domain applicability \\[9\\]\\[12\\].\n\n**2\\. Visual Perception in Dynamic Environments: Benchmarks and Metrics**\n\nDynamic visual perception requires benchmarks addressing real-time adaptation, multi-modal reasoning, and robustness.\n\n**Key Datasets:**\n\n**LIVEVQA**: Tests real-time visual QA with 3,602 Q/A pairs across news domains, emphasizing temporal reasoning \\[22\\].\n\n**MCTBench**: Evaluates MLLMs in text-rich scenes via 8.5k Q/A pairs covering reasoning and content creation \\[23\\].\n\n**Dysca**: Dynamically generates synthetic images to test LVLM robustness across 51 styles and adversarial conditions \\[27\\]\\[27\\].\n\n**UI-Vision**: Captures desktop GUI interactions for evaluating perception in operational workflows \\[31\\].\n\n**Evaluation Metrics:**\n\n**Accuracy metrics**: Top-k accuracy, mIoU, and pixel accuracy for segmentation \\[24\\].\n\n**Robustness metrics**: Corruption/attack resilience scores in Dysca \\[27\\].\n\n**Human-aligned metrics**: Eye-tracking (fixation duration) and Dynamic Intelligence Assessment (DIA) for confidence calibration \\[21\\]\\[30\\].\n\nGaps persist in **long-context multi-turn reasoning** and **real-time performance under variable lighting/occlusions** \\[25\\]\\[32\\].\n\n**3\\. Co-Training Perception and Planning Modules**\n\nEmbodied AI systems use modular or end-to-end architectures for joint perception-planning training:\n\n**Architectural Frameworks:**\n\n**Modular ECAs (Embodied Cognitive Architectures)**: Isolate perception, planning, and actuation modules for specialized optimization. NVIDIA's **Modulus** provides physics-informed neural networks, enabling differentiable simulation for backpropagation through physical states \\[66\\]\\[72\\].\n\n**ALN-P3**: Aligns vision-language modules across perception (P1A), prediction (P2A), and planning (P3A) stacks for autonomous driving \\[63\\].\n\n**Dual Gaussian-Particle representations**: Model geometry and physics for predictive simulation, allowing online correction from visual inputs \\[71\\].\n\n**Co-Training Techniques:**\n\n**Cross-modal transfer learning**: ResT bridges vision and text for zero-shot action recognition by learning joint embeddings \\[106\\]\\[106\\].\n\n**Sim2Real policies**: FCVP transfers vision-based policies between simulations using force dynamics \\[164\\], while domain randomization reduces \"appearance gaps\" \\[103\\].\n\n**4\\. Evaluation Frameworks for Integrated Agents**\n\nStandardized testbeds and metrics are critical for assessing agents combining planning, physics, and perception:\n\n**Testbeds for Emergent Behaviors:**\n\n**Tileworld and PHOENIX**: Simulate dynamic environments (e.g., forest fires) to test real-time adaptation and meta-reasoning \\[144\\]\\[148\\].\n\n**MORSE-500**: Video-centric benchmark for physical reasoning and planning in time-based scenarios \\[95\\].\n\n**AgentVerse**: Explores multi-agent collaboration in open-world simulations \\[157\\].\n\n**Quantitative Metrics:**\n\n**Agent performance**: Task success rate, API/tool usage accuracy, and token efficiency \\[87\\]\\[87\\].\n\n**Swarm emergent behavior**: Phase transition metrics and correlation analysis for stability validation \\[214\\].\n\n**Trustworthiness metrics**: Explainability scores and EU AI Act-aligned compliance \\[97\\]\\[93\\].\n\n**5\\. Cross-Modal Transfer Learning in Physics Simulations**\n\nEmerging techniques bridge vision and action planning across simulation environments:\n\n**Vision-language-action alignment**: CLIP-based models enable zero-shot policy generalization by predicting captions from visual inputs \\[163\\].\n\n**Tactile-to-visual transfer**: Haptic exploration policies trained in simulation transfer to real-world robotic interaction \\[110\\].\n\n**Differentiable optimization**: Frameworks like dTAMP use gradient-based physics simulators for kinematics/dynamics-aware planning \\[74\\].\n\nComparative studies show **MuJoCo** excels in articulated-system physics, **Isaac Sim** in GPU-parallelized visual tasks, and **PyBullet** in domain randomization—though no unified vision-action transfer benchmarks exist \\[339\\]\\[345\\].\n\n**6\\. Research Directions and Future Trends**\n\n**Priority Areas:**\n\n1.  **Unified sim2real protocols**: Standardizing physics-aware transfer learning across MuJoCo, PyBullet, and Isaac Sim.\n2.  **Emergent behavior metrics**: Peer-validated frameworks quantifying swarm dynamics or multi-agent collaboration \\[262\\].\n3.  **Perception-physics-planning loops**: Real-time architectures using tools like **Flink ML** for dataflow optimization \\[242\\].\n\n**Open Challenges:**\n\n**Physics-world discrepancy**: Modeling contact mechanics and fluid dynamics remains computationally intensive.\n\n**Ethical alignment**: Ensuring planners adhere to safety constraints in physical deployments \\[15\\].\n\n**Benchmark scalability**: Current datasets lack longitudinal dynamic environment testing.\n\nAgentic AI's evolution hinges on integrating causal physical reasoning, modular co-training, and human-centric evaluation—advancing toward \"embodied AGI\" capable of contextualized, real-world interaction \\[61\\]\\[112\\].\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at 100k Steps-Per-Second](https://openaccess.thecvf.com/content/CVPR2023/papers/Berges_Galactic_Scaling_End-to-End_Reinforcement_Learning_for_Rearrangement_at_100k_Steps-per-Second_CVPR_2023_paper.pdf)\n\n[2\\. Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI](https://www.roboticsproceedings.org/rss19/p087.pdf)\n\n[3\\. A Survey on Vision-Language-Action Models for Embodied AI](https://www.arxiv.org/pdf/2405.14093v3)\n\n[4\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[5\\. Virtual Community: An Open World for Humans, Robots, and Society](https://3dvar.com/Zhou2025Virtual.pdf)\n\n[6\\. Artificial Intelligence in Science: Challenges, Opportunities and the Future of Research](https://ccn.unistra.fr/websites/ccn/documentation/IA-Technologie/a8d820bd-en.pdf)\n\n[7\\. Understanding Real-World AI Planning Domains: A Conceptual Framework](https://link.springer.com/chapter/10.1007/978-3-031-45728-9_1)\n\n[8\\. Toward an Artificial Intelligence Physicist for Unsupervised Learning](https://tailin.org/wp-content/uploads/2025/01/CV.pdf)\n\n[9\\. “跨代孪生”：映射城市的生命特征](http://upforum.wupen.org/Assets/userfiles/files/issues/2024/2024-01/%E5%90%B4%E5%BF%97%E5%BC%BA%20%E5%91%A8%E5%92%AA%E5%92%AA%20%E7%AD%89.pdf)\n\n[10\\. AI for Science：未来研究方向和开放问题](https://www.360doc.cn/article/47115229_1151770585.html)\n\n[11\\. Round Table on artificial intelligence, Catania, 2024, March 23rd](http://www.cs.put.poznan.pl/ewgmcda/newsletter/Spr24.pdf)\n\n[12\\. DynaMate: leveraging AI-agents for customized research workflows](https://pubs.rsc.org/en/content/articlepdf/2025/me/d5me00062a)\n\n[13\\. RESenv: A Realistic Earthquake Simulation Environment based on Unreal Engine](https://diglib.eg.org/bitstream/handle/10.2312/imet20231259/065-074.pdf)\n\n[14\\. Artificial Intelligence in Modeling and Simulation](https://mdpi-res.com/bookfiles/book/9477/Artificial_Intelligence_in_Modeling_and_Simulation.pdf?v=1723338364)\n\n[15\\. AI4PDE领域（Part 1）|学术前沿文献收录最全面的github仓库！](https://zhuanlan.zhihu.com/p/692117876)\n\n[16\\. Academic Regulations, Curriculum and Syllabi 2023](https://gmrit.edu.in/PDFs/curriculum/B.Tech_AIDS_Syllabus_AR23.pdf)\n\n[17\\. 具身智能深度研究：解耦还是耦合？从AI化到工程化！——智联汽车系列之31暨机器人系列之10](https://m.book118.com/try_down/226213050143010031.pdf)\n\n[18\\. Physical scene understanding](https://www.jiajunwu.com/papers/psu_aimagazine.pdf)\n\n[19\\. Preparing National Research Ecosystems for AI: strategies and progress in 2024](https://council.science/wp-content/uploads/2024/03/Country-Case-Study-References_Papers_updated.pdf)\n\n[20\\. M. Ali-Dib, Kristen Menou. “Physics simulation capabilities of LLMs.” ArXiv](https://doi.org/10.48550/arXiv.2312.02091)\n\n[21\\. Enhancing Visual Perception in Immersive VR and AR Environments: AI-Driven Color and Clarity Adjustments Under Dynamic Lighting Conditions](https://www.mdpi.com/3026348)\n\n[22\\. LIVEVQA: Assessing Models with Live Visual Knowledge](https://openreview.net/pdf/654edfaeb57edd305cc716ad3218390df1f207bc.pdf)\n\n[23\\. MCTBench: Multimodal Cognition Towards Text-Rich Visual Scenes Benchmark](https://openreview.net/pdf?id=BVACdtrPsh)\n\n[24\\. Low-cost Agents with Language Perception and Dynamic Inference](https://dspace.mit.edu/bitstream/handle/1721.1/158499/pan-bpan-phd-eecs-2024-thesis.pdf?sequence=-1&isAllowed=y)\n\n[25\\. ARE LARGE VISION LANGUAGE MODELS GOOD GAME PLAYERS?](https://openreview.net/pdf/77a35d64aa1df769ddbd871ebe1e660c6b97b4a7.pdf)\n\n[26\\. WINDOWSAGENTARENA: Evaluating Multi-Modal OS Agents at Scale](https://www.microsoft.com/applied-sciences/uploads/publications/131/windowsagentarena-eval-multi-modal-os-agents.pdf)\n\n[27\\. Jie Zhang, Z. Wang et al. “Dysca: A Dynamic and Scalable Benchmark for Evaluating Perception Ability of LVLMs.” ArXiv](https://doi.org/10.48550/arXiv.2406.18849)\n\n[28\\. 单目标跟踪中的视觉智能评估技术综述](https://www.cjig.cn/rc-pub/front/front-article/download/67496294/lowqualitypdf/%E5%8D%95%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E4%B8%AD%E7%9A%84%E8%A7%86%E8%A7%89%E6%99%BA%E8%83%BD%E8%AF%84%E4%BC%B0%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0.pdf)\n\n[29\\. Visual Verity in AI-Generated Imagery: Computational Metrics and Human-Centric Analysis](https://fugumt.com/fugumt/paper_check/2408.12762v2_enmode)\n\n[30\\. Norbert Tihanyi, Tamás Bisztray et al. “Dynamic Intelligence Assessment: Benchmarking LLMs on the Road to AGI with a Focus on Model Confidence.”](https://arxiv.org/abs/2410.15490)\n\n[31\\. UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception](https://openreview.net/forum?id=5Rtj4mYH1C&noteId=uDd5CfKqqY)\n\n[32\\. Cognitive Paradigms for Evaluating VLMs on Visual Reasoning Task](https://arxiv.org/pdf/2501.13620)\n\n[33\\. Yifei Zhang, Siyi Gu et al. “XAI Benchmark for Visual Explanation.” ArXiv](https://doi.org/10.48550/arXiv.2310.08537)\n\n[34\\. Benchmarking And Datasets For Ambient Clinical Documentation: A Scoping Review Of Existing Frameworks And Metrics For AI-Assisted Medical Note Generation](https://www.medrxiv.org/content/10.1101/2025.01.29.25320859v1.full-text)\n\n[35\\. A Review of Common Datasets and Advanced Algorithms of Visual SLAM in Dynamic Scenes](https://www.clausiuspress.com/assets/default/article/2025/02/27/article_1740650579.pdf)\n\n[36\\. An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models](https://arxiv.org/pdf/2506.09172.pdf)\n\n[37\\. Yunhao Li, Sijing Wu et al. “AGHI-QA: A Subjective-Aligned Dataset and Metric for AI-Generated Human Images.”](https://arxiv.org/abs/2504.21308)\n\n[38\\. Landscape Architecture](http://www.lalavision.com/indexen.htm)\n\n[41\\. A Survey of Embodied AI in Healthcare: Techniques, Applications, and Opportunities](https://www.i-newcar.com/uploads/allimg/20250123/2-250123133541402.pdf)\n\n[42\\. Zhaohan Feng, Ruiqi Xue et al. “Multi-agent Embodied AI: Advances and Future Directions.”](https://arxiv.org/abs/2505.05108)\n\n[43\\. Shulan Ruan, Rongwei Wang et al. “A Survey of Multi-sensor Fusion Perception for Embodied AI: Background, Methods, Challenges and Prospects.”](https://arxiv.org/abs/2506.19769)\n\n[44\\. Embodied Multi-Agent Systems: A Review](https://www.ieee-jas.net/en/article/doi/10.1109/JAS.2025.125552)\n\n[45\\. Task cognition and planning for service robots](https://f.oaes.cc/xmlpdf/8725948d-9e72-44b9-b975-58656279e74b/ir5008_down.pdf?v=6)\n\n[46\\. Taewoong Kim, Byeonghwi Kim et al. “Multi-Modal Grounded Planning and Efficient Replanning For Learning Embodied Agents with A Few Examples.”](https://arxiv.org/abs/2412.17288)\n\n[47\\. The Latest Papers about AI](http://paperreading.club/?page=167)\n\n[48\\. Jie Liu, Pan Zhou et al. “CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent Cooperation.”](https://arxiv.org/abs/2411.04679)\n\n[49\\. Dayuan Fu, Biqing Qi et al. “MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making.”](https://arxiv.org/abs/2409.16686)\n\n[61\\. Jinhao Jiang, Changlin Chen et al. “Embodied Intelligence: The Key to Unblocking Generalized Artificial Intelligence.”](https://arxiv.org/abs/2505.06897)\n\n[62\\. Building Embodied AI Systems: The Agents, the Architecture Principles, Challenges, and Application Domains](https://download.bibis.ir/Books/Artificial-Intelligence/2025/Building%20Embodied%20AI%20Systems%20The%20Agents,%20the%20Architecture%20Principles,%20Challenges,%20and%20Application%20Domains_bibis.ir.pdf)\n\n[63\\. ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving](https://www.arxiv.org/pdf/2505.15158)\n\n[64\\. Embodied Artificial General Intelligence (AGI)](https://www.exaputra.com/2024/01/embodied-artificial-general.html)\n\n[65\\. Haiyang Wang, Wenguan Wang et al. “Collaborative Visual Navigation.” ArXiv](https://arxiv.org/abs/2107.01151)\n\n[66\\. NVIDIA Creates Framework for AI to Learn Physics](https://www.futurimmediat.net/news/nvidia-creates-framework-for-ai-to-learn-physics-nvidia-blog)\n\n[67\\. Jiafei Duan, Samson Yu et al. “A Survey of Embodied AI: From Simulators to Research Tasks.” IEEE Transactions on Emerging Topics in Computational Intelligence](https://doi.org/10.1109/tetci.2022.3141105)\n\n[68\\. Application of Large Language Models in Embodied Artificial Intelligence](https://wepub.org/index.php/TCSISR/article/download/4131/4636/8429)\n\n[69\\. EARBench: Towards Evaluating Physical Risk Awareness for Task Planning of Foundation Model-based Embodied AI Agents](https://www.researchsquare.com/article/rs-5540665/v1.pdf?c=1738573321000)\n\n[70\\. Local Observation Based Reactive Temporal Logic Planning of Human-Robot Systems](https://escholarship.org/content/qt7cb54251/qt7cb54251.pdf?t=s86fwb)\n\n[71\\. Embodied AI and Its Applications in Robotics and Language Processing](https://www.paperreading.club/category?cate=Embodied&page=2)\n\n[72\\. Clément Moulin-Frier, J. Puigbò et al. “Embodied artificial intelligence through distributed adaptive control: An integrated framework.” 2017 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)](https://doi.org/10.1109/DEVLRN.2017.8329825)\n\n[73\\. Simulation Based Training's Incorporation of Machine Learning](http://www.modsimworld.com/papers/2019/MODSIM_2019_paper_45.pdf)\n\n[74\\. 基于神经符号AI的机器人拆解智能化技术路线图2.0](https://nsaihome.org.cn/uploadFile/image/20241026/20241026114570437043.pdf)\n\n[75\\. Embodied AI in Virtual Environments with Alignment of Visual and Linguistic Inputs](https://www.zora.uzh.ch/id/eprint/277246/1/Dissertation_Armitage_Jason.pdf)\n\n[76\\. Yifeng Jiang, Michelle Guo et al. “DASH: Modularized Human Manipulation Simulation with Vision and Language for Embodied AI.” The ACM SIGGRAPH / Eurographics Symposium on Computer Animation](https://doi.org/10.1145/3475946.3480950)\n\n[77\\. Physical AI Agents: Integrating Cognitive Intelligence with Real-World Action](https://arxiv.org/pdf/2501.08944)\n\n[81\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[82\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[83\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[84\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[85\\. Marc G. Bellemare, Yavar Naddaf et al. “The Arcade Learning Environment: An Evaluation Platform for General Agents.” ArXiv](https://doi.org/10.1613/jair.3912)\n\n[86\\. Han Lu, Xiaosong Jia et al. “ActiveAD: Planning-Oriented Active Learning for End-to-End Autonomous Driving.” ArXiv](https://doi.org/10.48550/arXiv.2403.02877)\n\n[87\\. Mastering AI Agents](https://media.licdn.com/dms/document/media/v2/D4D1FAQEa9O0Jk9_OVw/feedshare-document-pdf-analyzed/B4DZVLWmyGHIAY-/0/1740726021992?e=1741824000&v=beta&t=CoDewkLFEmUkmXKZbHnG7qttkZELPQWN-SU92uhxkOw)\n\n[88\\. Generative AI Application Security Testing and Validation Standard](https://www.c-csa.cn/u_file/photo/20240419/c69918fb48.pdf)\n\n[89\\. C. Dimou, A. Symeonidis et al. “Towards a Generic Methodology for Evaluating MAS Performance.” 2007 International Conference on Integration of Knowledge Intensive Multi-Agent Systems](https://doi.org/10.1109/KIMAS.2007.369805)\n\n[90\\. Deshraj Yadav, Rishabh Jain et al. “EvalAI: Towards Better Evaluation Systems for AI Agents.” ArXiv](https://arxiv.org/abs/1902.03570)\n\n[91\\. Xiaoliang Ju, Yiyang Sun et al. “Perception Imitation: Towards Synthesis-free Simulator for Autonomous Vehicles.” ArXiv](https://doi.org/10.48550/arXiv.2304.09365)\n\n[92\\. Tula Masterman, Sandi Besen et al. “The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2404.11584)\n\n[93\\. Vivek Lakshman Bhargav Sunkara. “KPIs for AI Agents and Generative AI: A Rigorous Framework for Evaluation and Accountability.” International Journal of Scientific Research and Modern Technology](https://doi.org/10.38124/ijsrmt.v3i4.572)\n\n[94\\. Marlos C. Machado, Marc G. Bellemare et al. “Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents.” ArXiv](https://doi.org/10.1613/jair.5699)\n\n[95\\. MORSE-500：一个可编程控制的视频基准，用于多模态推理的压力测试](https://www.xueshuxiangzi.com/downloads/2025_6_9/2506.05523.pdf)\n\n[96\\. Atsunori Moteki, Shoichi Masui et al. “FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks.”](https://arxiv.org/abs/2505.19662)\n\n[97\\. Shaina Raza, Ranjan Sapkota et al. “TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems.”](https://arxiv.org/abs/2506.04133)\n\n[98\\. Dhruv Batra, Angel X. Chang et al. “Rearrangement: A Challenge for Embodied AI.” ArXiv](https://arxiv.org/abs/2011.01975)\n\n[99\\. Agent AI Towards a Holistic Intelligence](https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AgentAI_p.pdf)\n\n[101\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[102\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[103\\. A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents](https://www.ijcai.org/proceedings/2024/0906.pdf)\n\n[104\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[105\\. Sinno Jialin Pan, Qiang Yang. “A Survey on Transfer Learning.” IEEE Transactions on Knowledge and Data Engineering](https://doi.org/10.1109/TKDE.2009.191)\n\n[106\\. Cross-modal Representation Learning for Zero-shot Action Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_Cross-Modal_Representation_Learning_for_Zero-Shot_Action_Recognition_CVPR_2022_paper.pdf)\n\n[107\\. Innovations in visual language models for robotic interaction and contextual awareness: Progress, pitfalls and perspectives](https://journalwjaets.com/sites/default/files/fulltext_pdf/WJAETS-2025-0311.pdf)\n\n[108\\. João Carreira, Andrew Zisserman. “Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset.” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2017.502)\n\n[109\\. Transfer of visual perceptual learning over a task-irrelevant feature through feature-invariant representations: Behavioral experiments and model simulations](https://jov.arvojournals.org/arvo/content_public/journal/jov/938675/i1534-7362-24-6-17_1719294691.83352.pdf)\n\n[110\\. P. Falco, Shuang Lu et al. “A Transfer Learning Approach to Cross-Modal Object Recognition: From Visual Observation to Robotic Haptic Exploration.” IEEE Transactions on Robotics](https://doi.org/10.1109/TRO.2019.2914772)\n\n[111\\. Perception as Simulation](https://www.heikohoffmann.de/htmlthesis/node15.html)\n\n[112\\. A Framework for Sensorimotor Cross-Perception and Cross-Behavior Knowledge Transfer for Object Categorization](https://www.eecs.tufts.edu/~gtatiya/files/2020/Frontiers_2020_gtatiya.pdf)\n\n[113\\. Cross-Modal Learning with 3D Deformable Attention for Action Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Cross-Modal_Learning_with_3D_Deformable_Attention_for_Action_Recognition_ICCV_2023_paper.pdf)\n\n[114\\. Gyan Tatiya, Jonathan M Francis et al. “Cross-Tool and Cross-Behavior Perceptual Knowledge Transfer for Grounded Object Recognition.” ArXiv](https://doi.org/10.48550/arXiv.2303.04023)\n\n[121\\. NVIDIA Announces Modulus: A Framework for Developing Physics ML Models for Digital Twins](https://developer.nvidia.com/blog/nvidia-announces-modulus-a-framework-for-developing-physics-ml-models-for-digital-twins/)\n\n[122\\. NVIDIA 创建用于AI学习物理的框架](https://blogs.nvidia.com/blog/modulus-framework/)\n\n[123\\. NVIDIA Modulus](https://docs.nvidia.com/modulus/)\n\n[124\\. Modulus - A Neural Network Framework](https://developer.nvidia.com/simnet)\n\n[125\\. Nvidia Modulus](https://pypi.org/project/nvidia-modulus/0.2.0/)\n\n[126\\. GitHub - AbVishwas/nvidiaModulus: Open-source deep-learning framework for building, training, and fine-tuning deep learning models using state-of-the-art Physics-ML methods](https://github.com/AbVishwas/nvidiaModulus)\n\n[127\\. NVIDIA Modulus Sym (Latest Release)](https://docs.nvidia.com/deeplearning/modulus/modulus-sym/index.html)\n\n[128\\. Marc Toussaint, Kelsey R. Allen et al. “Differentiable Physics and Stable Modes for Tool-Use and Manipulation Planning.” Robotics: Science and Systems](https://doi.org/10.15607/RSS.2018.XIV.044)\n\n[129\\. Yuanming Hu, Jiancheng Liu et al. “ChainQueen: A Real-Time Differentiable Physical Simulator for Soft Robotics.” 2019 International Conference on Robotics and Automation (ICRA)](https://doi.org/10.1109/ICRA.2019.8794333)\n\n[130\\. Filipe de Avila Belbute-Peres, Kevin A. Smith et al. “End-to-End Differentiable Physics for Learning and Control.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/0933f3dd33cf907e07aa938ce9465fb0d4250394)\n\n[131\\. Peter Karkus, Xiao Ma et al. “Differentiable Algorithm Networks for Composable Robot Learning.” ArXiv](https://doi.org/10.15607/RSS.2019.XV.039)\n\n[132\\. Modulus | NVIDIA NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/modulus/containers/modulus)\n\n[133\\. Connor Schenck, D. Fox. “SPNets: Differentiable Fluid Dynamics for Deep Neural Networks.” ArXiv](https://arxiv.org/abs/1806.06094)\n\n[134\\. Jonas Degrave, Michiel Hermans et al. “A DIFFERENTIABLE PHYSICS ENGINE FOR DEEP LEARNING IN ROBOTICS.” Frontiers in Neurorobotics](https://doi.org/10.3389/fnbot.2019.00006)\n\n[135\\. 具有重建预训练和异构性战斗调优的统一多模态诊断框架](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_4_10/2404.06057.pdf)\n\n[136\\. Overview of NVIDIA Modulus on NVIDIA LaunchPad](https://docs.nvidia.com/launchpad/ai/modulus/latest/modulus-overview.html)\n\n[137\\. Physics-Informed Machine Learning with NVIDIA Modulus](https://www.nvidia.com/en-us/launchpad/ai/physics-informed-machine-learning-with-modulus/)\n\n[138\\. Coupling physics-informed neural networks from Nvidia Modulus with conventional models using preCICE](https://mediatum.ub.tum.de/doc/1780225/0wqfickgtyztvoj7pox8lunjz.pdf)\n\n[139\\. NVIDIA Updates](https://indico.cern.ch/event/1395090/contributions/5864071/attachments/2866076/5016590/20240529%20-%20CERN%20Compute%20Forum%20pitch.pdf)\n\n[141\\. Fourth Edition, Averill M. Law. “Simulation Modeling and Analysis.”](https://doi.org/10.2307/2288169)\n\n[142\\. T. Dean, M. Boddy. “An Analysis of Time-Dependent Planning.” AAAI Conference on Artificial Intelligence](https://www.semanticscholar.org/paper/143f6b100c13d120e55c3e30e441c4abac7a5db2)\n\n[143\\. Generative AI Application Security Testing and Validation Standard](https://www.c-csa.cn/u_file/photo/20240703/f808f4bec5.pdf)\n\n[144\\. M. Pollack, M. Ringuette. “Introducing the Tileworld: Experimentally Evaluating Agent Architectures.” AAAI Conference on Artificial Intelligence](https://www.semanticscholar.org/paper/e8a5f2ad2df297e4427a9a15ce04daf03823a277)\n\n[145\\. S. Hanks, M. Pollack et al. “Benchmarks, Test Beds, Controlled Experimentation, and the Design of Agent Architectures.” AI Mag.](https://doi.org/10.1609/aimag.v14i4.1066)\n\n[146\\. P. Cohen, M. Greenberg et al. “Trial by Fire: Understanding the Design Requirements for Agents in Complex Environments.” AI Mag.](https://doi.org/10.1609/aimag.v10i3.755)\n\n[147\\. Lola Ben-Alon, R. Sacks. “Simulating and Vizualising Emergent Production in Construction (EPIC) Using Agents and BIM.”](https://www.semanticscholar.org/paper/d1b18c2f24031bc371b450899899db3d43b3a094)\n\n[148\\. A. M. Kareem, A. Obied. “Testbed For Intelligent Agent: A survey.” Journal of Al-Qadisiyah for Computer Science and Mathematics](https://doi.org/10.29304/JQCM.2021.13.2.816)\n\n[149\\. R. Bemthuis, M. Mes et al. “Using Agent-Based Simulation for Emergent Behavior Detection in Cyber-Physical Systems.” 2020 Winter Simulation Conference (WSC)](https://doi.org/10.1109/WSC48552.2020.9383956)\n\n[150\\. Eric A. Nees. “Design and Demonstration of a Physical, Multi-Agent Autonomous Controller Testbed.”](https://www.semanticscholar.org/paper/aa6328312aac63efa667d439b9357a54ff921a8a)\n\n[151\\. M. Pollack, P. Cohen. “Benchmarks, Testbeds, Controlled Experimentation, and the Design of Agent Architectures Experimentation, and the Design of Agent Architectures.”](https://www.semanticscholar.org/paper/bdfad3534cc740f02e8501aa29b113857e5457b6)\n\n[152\\. Badr Al-Badr. “Critiquing the Tileworld : Agent Architectures , Planning Benchmarks , and Experimental Methodology.”](https://www.semanticscholar.org/paper/692c932ecab535d047f2772df81359ccfefe0cf9)\n\n[153\\. A. Uhrmacher, B. Schattenberg. “Agents in discrete event simulation.”](https://www.semanticscholar.org/paper/e413d29fb061a67f5c105a0367d7be4a6f522389)\n\n[154\\. A. Uhrmacher, B. Schattenberg. “AGENTS IN DISCRETE EVENT.”](https://www.semanticscholar.org/paper/d4a3c7c897cba258427ecabe060559c5340c8a35)\n\n[155\\. M. Atkin, D. Westbrook et al. “Capture the Flag: Military Simulation Meets Computer Games.”](https://www.semanticscholar.org/paper/6743d1da9891f2a14b1d8e35dc8cde9f4af78667)\n\n[156\\. War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars](https://simg.baai.ac.cn/paperfile/2263b292-29dd-40ec-a40b-aa72b48ca38a.pdf)\n\n[157\\. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents](https://cz5waila03cyo0tux1owpyofgoryroob.itic-sci.com/9C/54/B4/9C54B47616795A320E36FCB1EA595C91.pdf)\n\n[158\\. Agent 视域下的人工智能赋能作战系统](https://www.zhkzyfz.cn/fileup/1673-3819/PDF/1732684231675-147652833.pdf)\n\n[161\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[162\\. Sinno Jialin Pan, Qiang Yang. “A Survey on Transfer Learning.” IEEE Transactions on Knowledge and Data Engineering](https://doi.org/10.1109/TKDE.2009.191)\n\n[163\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[164\\. Leveraging Vision, Force Sensing, and Language Feedback for Deformable Object Manipulation](https://www.ri.cmu.edu/app/uploads/2024/06/zhanyis_msr_thesis.pdf)\n\n[165\\. One model, two skills: active vision and action learning model for robotic manipulation](http://scis.scichina.com/en/2025/162202.pdf)\n\n[166\\. João Carreira, Andrew Zisserman. “Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset.” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2017.502)\n\n[167\\. Vision-based learning technique for robots during training](https://www.wevolver.com/article/vision-based-learning-technique-for-robots-during-training)\n\n[168\\. Cross-Modal Learning with 3D Deformable Attention for Action Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Cross-Modal_Learning_with_3D_Deformable_Attention_for_Action_Recognition_ICCV_2023_paper.pdf)\n\n[169\\. Action-Aware Vision Language Navigation (AAVLN): AI Vision System based on Cross-Modal Transformer for Understanding and Navigating Dynamic Environments](http://www.yau-awards.com/uploads/file/20231101/20231101102747_19426.pdf)\n\n[170\\. Cross-modal Representation Learning for Zero-shot Action Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_Cross-Modal_Representation_Learning_for_Zero-Shot_Action_Recognition_CVPR_2022_paper.pdf)\n\n[171\\. A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents](https://arxiv.org/pdf/2402.04580)\n\n[172\\. TACKLING DIVERSE TASKS VIA CROSS-MODAL TRANSFER LEARNING](https://openreview.net/pdf?id=17mPeO4rqGj)\n\n[173\\. Antonio Loquercio, Ashish Kumar et al. “Learning Visual Locomotion with Cross-Modal Supervision.” 2023 IEEE International Conference on Robotics and Automation (ICRA)](https://doi.org/10.1109/ICRA48891.2023.10160760)\n\n[174\\. M. Goodale, A. Milner. “Separate visual pathways for perception and action.” Trends in Neurosciences](https://doi.org/10.1016/0166-2236%2892%2990344-8)\n\n[175\\. Modality Selection and Skill Segmentation via Cross-Modality Attention](https://arxiv.org/html/2504.14573v1)\n\n[176\\. Training Vision Transformers with Only 2040 Images](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850218.pdf)\n\n[177\\. Introduction to Transfer Learning: Algorithms and Practice](https://download.bibis.ir/Books/Artificial-Intelligence/Machine-Learning/2023/Introduction%20to%20Transfer%20Learning_bibis.ir.pdf)\n\n[181\\. Intelligent Agents and Their Potential for Future Design and Synthesis Environment](https://ntrs.nasa.gov/api/citations/19990041500/downloads/19990041500.pdf)\n\n[182\\. A Step Forward to AGI: Integrating Agentic AI and Generative AI for Human-Like Intelligence](https://www.ijraset.com/best-journal/a-step-forward-to-agi-integrating-agentic-ai-and-generative-ai-for-humanlike-intelligence)\n\n[183\\. Combining Petri Nets and AI Techniques to Improve Dynamic Production Scheduling Optimization](https://www.scitepress.org/Papers/2025/132617/132617.pdf)\n\n[184\\. Olivier Despouys, Ingrand Laas. “An Integrated Architecture for Planning and Execution Control in Dynamic Environments.”](https://www.semanticscholar.org/paper/0cde9e4e679daa727076ff8502f9a01a5b01e0e1)\n\n[185\\. Architecture Design Diagrams - Microsoft Azure Well-Architected Framework](https://learn.microsoft.com/en-za/azure/well-architected/architect-role/design-diagrams)\n\n[186\\. Artificial Intelligence](https://vemu.org/uploads/ppt/28_02_2023_725015754.pdf)\n\n[187\\. Beyond Model Stacking: The Architecture Principles That Make Multimodal AI Systems Work](https://towardsdatascience.com/the-art-of-multimodal-ai-system-design/)\n\n[188\\. Architecture of optimized digital twins for AI-based training](https://itea4.org/project/workpackage/document/download/8759/ASIMOV-D2.3_ArchitectureOfDigitalTwins_v1_0.pdf)\n\n[189\\. EAM Diagrams - A Framework to Systematically Describe AI Systems for Effective AI Risk Assessment](https://drops.dagstuhl.de/storage/01oasics/oasics-vol126-saia2024/OASIcs.SAIA.2024.3/OASIcs.SAIA.2024.3.pdf)\n\n[190\\. A FLEXIBLE AND EXPANDABLE ARCHITECTURE FOR COMPUTER GAMES](https://www.tjhsst.edu/~rlatimer/techlab07/plummer_thesis.pdf)\n\n[191\\. An AI Framework for Real-Time Strategy Games](https://projekter.aau.dk/projekter/files/61067787/1149582107.pdf)\n\n[192\\. B. Hayes-Roth. “Architectural foundations for real-time performance in intelligent agents.” Real-Time Systems](https://doi.org/10.1007/BF01840468)\n\n[193\\. Design Recommendations for Intelligent Tutoring Systems Volume 8 Data Visualization](https://gifttutoring.org/attachments/download/3874/DesignRecommendationsforITSs_Volume8_DataVisualizationBook_Final.pdf)\n\n[194\\. Data-driven perception and planning methodologies for autonomous vehicles](https://www.hitachi.com/rd/sc/aiblog/013/index.html)\n\n[195\\. AI-agents Trained Using Deep Reinforcement Learning in the CARLA Simulator](https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/3019911/no.ntnu%3Ainspera%3A112046434%3A15158852.pdf?sequence=1&isAllowed=y)\n\n[196\\. Artificial Intelligence Strategy Framework](https://cag.gov.in/uploads/media/Artificial-Intelligence-Strategy-Framework-issued-by-CAG-of-India-068515070c7da65-91395536.pdf)\n\n[197\\. Personalized AI-Driven Roadmap Generation](https://www.ijrti.org/papers/IJRTI2501105.pdf)\n\n[198\\. System Architectures](https://ethz.ch/content/dam/ethz/special-interest/mavt/dynamic-systems-n-control/idsc-dam/Lectures/amod/AMOD_2020/20200928-02%20-%20ETHZ%20-%20Architectures.pdf)\n\n[199\\. AI Platform Engineering Reference Architecture](https://re-cinq.com/blog/ai-platform-reference-arch)\n\n[200\\. The Future of AI: Building World Models, Agentic Systems, and Biological Simulations](https://opendatascience.com/the-future-of-ai-building-world-models-agentic-systems-and-biological-simulations/)\n\n[201\\. D. Helbing, I. Farkas et al. “Simulating dynamical features of escape panic.” Nature](https://doi.org/10.1038/35035023)\n\n[202\\. Quantitative Analysis of Behaviors](http://www.cs.brandeis.edu/~pablo/thesis/html/node124.html)\n\n[203\\. Major Research Plan](https://www.nsfc.gov.cn/Portals/0/fj/english/fj/pdf/2011/031.pdf)\n\n[204\\. Rune Andre Haugen, Ali Ghaderi. “Modelling and Simulation of Detection Rates of Emergent Behaviours in System Integration Test Regimes.” Linköping Electronic Conference Proceedings](https://doi.org/10.3384/ecp211858)\n\n[205\\. SW25: The OR Society 12th Simulation](https://www.theorsociety.com/common/Uploaded%20files/Events/SW25/SW25%20Proceedings%20Book.pdf)\n\n[206\\. Simulation Testbeds and Frameworks for UAV Performance Evaluation](https://par.nsf.gov/servlets/purl/10297241)\n\n[207\\. R. Bemthuis, M. Iacob et al. “A Design of the Resilient Enterprise: A Reference Architecture for Emergent Behaviors Control.” Sensors (Basel, Switzerland)](https://doi.org/10.3390/s20226672)\n\n[208\\. Simulation optimization: a review of algorithms and applications](https://link.springer.com/content/pdf/10.1007/s10479-015-2019-x.pdf)\n\n[209\\. Comparison of Online Visual Acuity Testing with Dynamic Dyop and Static LogMAR Optotypes in Healthy Non-Presbyopic and Presbyopic Individuals](https://www.dovepress.com/article/download/102042)\n\n[210\\. Benchmarking Metric Ground Navigation](https://cs.gmu.edu/~xiao/papers/navdiffdataset_ssrr.pdf)\n\n[211\\. PR2: A Physics- and Photo-realistic Humanoid Testbed with Pilot Study in Competition](https://arxiv.org/html/2409.01559v1)\n\n[212\\. KANARIA: IDENTIFYING THE CHALLENGES FOR COGNITIVE AUTONOMOUS NAVIGATION AND GUIDANCE FOR MISSIONS TO SMALL PLANETARY BODIES](https://cgvr.cs.uni-bremen.de/papers/iac15_kanaria/IACKanaria.pdf)\n\n[213\\. A. Ganguly, Joseph Whitmeyer et al. “Toward a characterization and systematic evaluation framework for theories and models of human , social , behavioral , and cultural processes.”](https://www.semanticscholar.org/paper/a03258670d5030ff23581f77305eef8a11bd3370)\n\n[214\\. Li Yin-guo. “The evaluation metrics for Swarm emergent behaviors.” Control theory & applications](https://www.semanticscholar.org/paper/393f229a4c444354ed967c3ac6459393d1e8287a)\n\n[215\\. SimBricks: End-to-End Network System Evaluation with Modular Simulation](https://cs.paperswithcode.com/paper/reproducible-host-networking-evaluation-with)\n\n[216\\. Gridworlds as Testbeds for Planning with Incomplete Information](https://idm-lab.org/bib/abstracts/papers/aaai00b.pdf)\n\n[217\\. Proceedings of the 15th USENIX Symposium on Networked Systems Design and Implementation](https://www.usenix.org/sites/default/files/nsdi18_full_proceedings_interior.pdf)\n\n[218\\. Research and Technology Objectives and Plans Summary](https://ntrs.nasa.gov/api/citations/19950021581/downloads/19950021581.pdf)\n\n[219\\. A 20-Year Community Roadmap for Artificial Intelligence Research in the US](https://cra.org/ccc/wp-content/uploads/sites/2/2019/08/Community-Roadmap-for-AI-Research.pdf)\n\n[220\\. PANEL – REPRODUCIBLE RESEARCH IN DISCRETE EVENT SIMULATION – A MUST OR RATHER A MAYBE ?](http://people.cis.fiu.edu/liux/wp-content/uploads/sites/4/2017/07/wsc16-reproduce.pdf)\n\n[221\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[222\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[223\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[224\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[225\\. Diffusion Policy: Visuomotor Policy Learning via Action Diffusion](https://diffusion-policy.cs.columbia.edu/diffusion_policy_2023.pdf)\n\n[226\\. Learning Modality Knowledge Alignment for Cross-Modality Transfer](https://openreview.net/pdf?id=lmiurzioja)\n\n[227\\. Vision-based learning technique for robots during training](https://www.wevolver.com/article/vision-based-learning-technique-for-robots-during-training)\n\n[228\\. Cross-modal Representation Learning for Zero-shot Action Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_Cross-Modal_Representation_Learning_for_Zero-Shot_Action_Recognition_CVPR_2022_paper.pdf)\n\n[229\\. On Multimodal Representations Learned at Scale](https://di.ku.dk/english/research/phd/phd-theses/2023/bugliarello_phd-thesis.pdf)\n\n[230\\. Action-Aware Vision Language Navigation (AAVLN): AI Vision System based on Cross-Modal Transformer for Understanding and Navigating Dynamic Environments](http://www.yau-awards.com/uploads/file/20231101/20231101102747_19426.pdf)\n\n[231\\. Transfer of visual perceptual learning over a task-irrelevant feature through feature-invariant representations: Behavioral experiments and model simulations](https://jov.arvojournals.org/arvo/content_public/journal/jov/938675/i1534-7362-24-6-17_1719294691.83352.pdf)\n\n[232\\. @BENCH: Benchmarking Vision-Language Models for Human-centered Assistive Technology](https://openaccess.thecvf.com/content/WACV2025/papers/Jiang_BENCH_Benchmarking_Vision-Language_Models_for_Human-Centered_Assistive_Technology_WACV_2025_paper.pdf)\n\n[233\\. Amit Mankodi, Amit Bhatt et al. “Performance prediction from simulation systems to physical systems using machine learning with transfer learning and scaling.” Concurrency and Computation: Practice and Experience](https://doi.org/10.1002/cpe.6433)\n\n[234\\. Pranav Guruprasad, Harshvardhan Sikka et al. “Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks.”](https://arxiv.org/abs/2411.05821)\n\n[235\\. UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling](https://arxiv.org/pdf/2302.06605v1.pdf)\n\n[236\\. Point Cloud Cross-Modal Training for Shape Analysis](https://nips.cc/virtual/2022/poster/55376)\n\n[237\\. A comparative review on multi-modal sensors fusion based on deep learning](https://ouci.dntb.gov.ua/en/works/42aDgDm7/)\n\n[238\\. Papers with Code - Machine Learning Datasets](https://paperswithcode.com/datasets?task=visual-reasoning)\n\n[239\\. WikiDO: A New Benchmark Evaluating Cross-Modal Retrieval for Vision-Language Models](https://nips.cc/virtual/2024/poster/97785)\n\n[241\\. Development of an Intelligent Coal Production and Operation Platform Based on a Real-Time Data Warehouse and AI Model](https://www.mdpi.com/1996-1073/17/20/5205)\n\n[242\\. The Open-Source Folks Talk - Episode 4: Alibaba AI's Open-Source Process, and Thinking](https://www.alibabacloud.com/blog/599789)\n\n[243\\. Open-source AI is advancing](https://www.tekedia.com/open-source-ai-is-advancing/)\n\n[244\\. Artificial Intelligence Hubs for Real Data](https://www.ukri.org/wp-content/uploads/2023/05/EPSRC-020523-AIHubsRealData-SuccessfulOutlines.pdf)\n\n[245\\. Flow Theory & AI Architecture](https://uberconf.com/topics/flow_theory__ai_architecture)\n\n[246\\. EAM Diagrams - A Framework to Systematically Describe AI Systems for Effective AI Risk Assessment](https://drops.dagstuhl.de/storage/01oasics/oasics-vol126-saia2024/OASIcs.SAIA.2024.3/OASIcs.SAIA.2024.3.pdf)\n\n[247\\. Harnessing generative AI to create and understand architecture diagrams](https://ijsra.net/sites/default/files/IJSRA-2024-2601.pdf)\n\n[248\\. Integration of AI in React Applications](https://ijrpr.com/uploads/V6ISSUE4/IJRPR43794.pdf)\n\n[249\\. Artificial Intelligence Operating Systems: The Complete Guide](https://topapps.ai/blog/ai-operating-systems-guide/)\n\n[250\\. AI Platform Engineering Reference Architecture](https://re-cinq.com/blog/ai-platform-reference-arch)\n\n[251\\. GitHub - ttgholdings/ai-flow: Open Source user-friendly UI application that allows you to easily connect multiple AI models together.](https://github.com/ttgholdings/ai-flow)\n\n[252\\. D. Henriksson, Ola Redell et al. “Tools for Real-Time Control Systems Co-Design - A Survey.” Technical Reports; TFRT](https://www.semanticscholar.org/paper/e42c370c3eddc36e317ef6529ac2072759dbf722)\n\n[253\\. 10 Open-Source Tools/Frameworks for Artificial Intelligence](https://dzone.com/articles/10-opensource-toolsframeworks-for-artificial-intel)\n\n[254\\. A Dataflow Architecture for AI](https://link.springer.com/chapter/10.1007/978-1-4615-3752-6_3)\n\n[255\\. Software Design Methods for Real-Time Systems](https://insights.sei.cmu.edu/documents/1545/1989_007_001_15707.pdf)\n\n[256\\. Center for Open Source Data and AI Technologies (CODAIT)](https://events19.linuxfoundation.org/wp-content/uploads/2018/07/AIOps-Osonoi20190718-lite.pdf)\n\n[257\\. Jamie O. Roberts, G. Mastorakis et al. “vPlanSim: An Open Source Graphical Interface for the Visualisation and Simulation of AI Systems.” International Conference on Automated Planning and Scheduling](https://doi.org/10.1609/icaps.v31i1.15995)\n\n[258\\. R. Dodhiawala, N. S. Sridharan et al. “Real-Time AI Systems: A Definition and An Architecture.” International Joint Conference on Artificial Intelligence](https://www.semanticscholar.org/paper/c29339e3cd939c178a1df44f14d03ac795532b3f)\n\n[259\\. Stefan Rilling, Gerrit Lochmann. “Physically Based Real-Time Simulation Of An Automation Plant.” European Conference on Modelling and Simulation](https://doi.org/10.7148/2011-0387-0393)\n\n[260\\. Biblio](https://www.rob.cs.tu-bs.de/biblio?page=5&s=type&amp;o=asc&amp;f%5Bauthor%5D=226&f%5Bauthor%5D=3&o=desc)\n\n[261\\. J. Prochaska, W. Velicer. “The Transtheoretical Model of Health Behavior Change.” American Journal of Health Promotion](https://doi.org/10.4278/0890-1171-12.1.38)\n\n[262\\. Swarm突现行为的定量评估指标](https://jcta.ijournals.cn/cta_cn/ch/reader/view_abstract.aspx?file_no=CCTA090395&flag=1)\n\n[263\\. Li Yin-guo. “The evaluation metrics for Swarm emergent behaviors.” Control theory & applications](https://www.semanticscholar.org/paper/393f229a4c444354ed967c3ac6459393d1e8287a)\n\n[264\\. Analytical and Performance-Based Evaluation Alternatives to Full-Scope, High-Fidelity Testbeds](https://www.nrc.gov/docs/ML2509/ML25090A259.pdf)\n\n[265\\. A Test-Bed for Emergency Management Simulations](https://thesai.org/Downloads/Volume1No3/Paper%209-A%20Test-Bed%20for%20Emergency%20Management%20Simulation.pdf)\n\n[266\\. Potential Applications of Latent Variable Modeling for the Psychometrics of Medical Simulation](https://pmc.ncbi.nlm.nih.gov/articles/PMC4000562/)\n\n[267\\. Successful implementation of a rater training program for medical students to evaluate simulated pediatric emergencies](https://archiv.ub.uni-marburg.de/es/2024/0417/pdf/zma001629.pdf)\n\n[268\\. A Machine Learning Approach for Emergency Detection in Medical Scenarios Using Large Language Models](https://www.arxiv.org/pdf/2412.16341)\n\n[269\\. R. Bemthuis, M. Iacob et al. “A Design of the Resilient Enterprise: A Reference Architecture for Emergent Behaviors Control.” Sensors (Basel, Switzerland)](https://doi.org/10.3390/s20226672)\n\n[271\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[272\\. T. Lillicrap, Jonathan J. Hunt et al. “Continuous control with deep reinforcement learning.” CoRR](https://arxiv.org/abs/1509.02971)\n\n[273\\. Sim-to-Real Transfer in Deep Reinforcement Learning for Vision-Based Robotic Grasping](https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/3037792/no.ntnu%3Ainspera%3A109479168%3A37544989.pdf?sequence=1&isAllowed=y)\n\n[274\\. Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers](https://proceedings.neurips.cc/paper_files/paper/2024/file/e0f393e7980a24fd12fa6f15adfa25fb-Paper-Conference.pdf)\n\n[275\\. MuBlE: MuJoCo and Blender simulation Environment and Benchmark for Task Planning in Robot Manipulation](http://arxiv.org/html/2503.02834v1)\n\n[276\\. Overall Comparison | Simulately](https://simulately.wiki/docs/comparison/)\n\n[277\\. Tuomas Haarnoja, Aurick Zhou et al. “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.” ArXiv](https://arxiv.org/abs/1801.01290)\n\n[278\\. Isaac Sim与Mujoco、PyBullet在物理模拟和渲染速度的比较](https://forums.developer.nvidia.com/t/isaac-sim-very-slow-compared-to-mujoco-or-pybullet-both-physics-and-rendering/237453)\n\n[279\\. NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based Autonomous Navigation](https://arxiv.org/html/2505.14526v1)\n\n[280\\. Mujoco](https://www.catalyzex.com/s/Mujoco)\n\n[281\\. John Schulman, S. Levine et al. “Trust Region Policy Optimization.” ArXiv](https://arxiv.org/abs/1502.05477)\n\n[282\\. GitHub - zal/simenvbenchmark: Comparing Popular Simulation Environments in the Scope of Robotics and Reinforcement Learning](https://github.com/zal/simenvbenchmark)\n\n[283\\. Open-Source Reinforcement Learning Environments Implemented in MuJoCo with Franka Manipulator](https://ras.papercept.net/images/temp/AIM/files/0015.pdf)\n\n[284\\. Evaluating PyBullet and Isaac Sim in the Scope of Robotics and Reinforcement Learning](https://eventos.upm.es/getFiles/hash/N0cjZ1MWZ3UGOkdDO3UGOhlTZkVjZidDOUzM3cDO1Ij:MkxYT2dBTnFTQzc4MGhTZGs3M3dNZnVuK0tEUGQ2MTdiOHIva2Zhekh0K1VxZndrNFdnQWdRTWx1MTR1aThIOCsyU0IvQVJmRWNsdDU2TThNUlBQUFFubm9sOEcrQmIyY0g2MUpLNlV2NDlkYXZHc0wvNVJRSENwV2wyUkFuVmYyMFpaM1hHWmIvSFZMTzk5SklpbmhKSzBqNm55SDZIT3ZKTksyRmIxaXdDU2IwVStteHhDOWl4Vk5xTzZmMnpRTWFjaVhHdURiZ0tFRDRFK1VVOUVJNDNLazdXZjNLNWIyZWdmMUxXUHdnVXBPZGlTWUdZMUhINlY5SCtVZFlmbzhBdTlBamMzTjc3cG5FRUwyVWs1Q2c9PQ==)\n\n[285\\. Greg Brockman, Vicki Cheung et al. “OpenAI Gym.” ArXiv](https://arxiv.org/abs/1606.01540)\n\n[286\\. ROBOVERSE: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning](https://www.roboticsproceedings.org/rss21/p022.pdf)\n\n[287\\. Efficient Tactile Simulation with Differentiability for Robotic Manipulation](https://cdfg.mit.edu/assets/images/tactile-sensors-.pdf)\n\n[288\\. PHYSBENCH: BENCHMARKING AND ENHANCING VISION-LANGUAGE MODELS FOR PHYSICAL WORLD UNDERSTANDING](https://openreview.net/pdf/ce20f41ba36f17cb4dcd4fdb7952c3c85a51d858.pdf)\n\n[289\\. Build software better, together](https://github.com/topics/pick-and-place)\n\n[290\\. Efficient Multi-Agent Reinforcement Learning by Planning](https://openreview.net/attachment?id=CpnKq3UJwp&name=pdf)\n\n[291\\. Muthu Dayalan. “MapReduce: simplified data processing on large clusters.” Commun. ACM](https://doi.org/10.1145/1327452.1327492)\n\n[292\\. GitHub - huweim/dataflow_architecture: Research about dataflow architecture](https://github.com/huweim/dataflow_architecture)\n\n[293\\. Architecture diagrams for Github README.md?](https://dev.to/markhopson/help-architecture-diagrams-for-github-readme-md-1pl7/comments)\n\n[294\\. GitHub - akilm/Physical-Design: Physical Design Flow from RTL to GDS using Opensource tools](https://github.com/akilm/Physical-Design)\n\n[295\\. GitHub - stan-smith/FossFLOW: Make beautiful isometric infrastructure diagrams](https://github.com/stan-smith/FossFLOW)\n\n[296\\. Adoption of the GITHUB platform as a data source for creating a decision support system in an agile development context](https://repositorio.ual.pt/bitstreams/75e8a586-67eb-416f-8ab8-36b5ba7b0949/download)\n\n[297\\. GitHub - open-mmlab/mmflow: OpenMMLab optical flow toolbox and benchmark](https://github.com/open-mmlab/mmflow/)\n\n[298\\. Nicolas Boltz, Sebastian Hahner et al. “An Extensible Framework for Architecture-Based Data Flow Analysis for Information Security.” ArXiv](https://doi.org/10.48550/arXiv.2403.09402)\n\n[299\\. GitHub - stanford-mast/nn_dataflow: Explore the energy-efficient dataflow scheduling for neural networks.](https://github.com/stanford-mast/nn_dataflow)\n\n[300\\. Eirini Kalliamvakou, Georgios Gousios et al. “The promises and perils of mining GitHub.” IEEE Working Conference on Mining Software Repositories](https://doi.org/10.1145/2597073.2597074)\n\n[301\\. Georgios Gousios. “The GHTorent dataset and tool suite.” 2013 10th Working Conference on Mining Software Repositories (MSR)](https://doi.org/10.1109/MSR.2013.6624034)\n\n[302\\. GitHub - kdavis-mozilla/tensorflow: Computation using data flow graphs for scalable machine learning](https://github.com/kdavis-mozilla/tensorflow)\n\n[303\\. GitHub - iguanaus/ScatterNet: Code for all work presented in Nanophotonic Particle Simulation and Inverse Design Using Artificial Neural Networks](https://github.com/iguanaus/ScatterNet)\n\n[304\\. GitHub - projectstorm/react-diagrams: a super simple, no-nonsense diagramming library written in react that just works](https://github.com/projectstorm/react-diagrams)\n\n[305\\. GitHub - Sebb77/react-flow: React library for building interactive node-based graphs | flow charts | diagrams](https://github.com/Sebb77/react-flow)\n\n[306\\. Explore projects · GitLab](https://gitlab.cba.mit.edu/explore?language=5&sort=name_desc)\n\n[307\\. Bogdan Vasilescu, V. Filkov et al. “StackOverflow and GitHub: Associations between Software Development and Crowdsourced Knowledge.” 2013 International Conference on Social Computing](https://doi.org/10.1109/SOCIALCOM.2013.35)\n\n[308\\. Spine Resources](https://energyreform.ie/spine-resources/)\n\n[309\\. Harnessing generative AI to create and understand architecture diagrams](https://ijsra.net/sites/default/files/IJSRA-2024-2601.pdf)\n\n[310\\. T. Peraro. “FiniteFlow: multivariate functional reconstruction using finite fields and dataflow graphs.” Journal of High Energy Physics](https://doi.org/10.1007/JHEP07%282019%29031)\n\n[311\\. Scott M. Lundberg, Su-In Lee. “A Unified Approach to Interpreting Model Predictions.” Neural Information Processing Systems](https://arxiv.org/abs/1705.07874)\n\n[312\\. Rearrangement: A Challenge for Embodied AI](http://vladlen.info/papers/rearrangement.pdf)\n\n[313\\. AI Testbed framework for composable behaviour representation](https://www.sto.nato.int/publications/STO%20Meeting%20Proceedings/STO-MP-MSG-192/MP-MSG-192-01P.pdf)\n\n[314\\. Stephen James, Z. Ma et al. “RLBench: The Robot Learning Benchmark & Learning Environment.” IEEE Robotics and Automation Letters](https://doi.org/10.1109/LRA.2020.2974707)\n\n[315\\. Simulation Testbeds and Frameworks for UAV Performance Evaluation](https://par.nsf.gov/servlets/purl/10297241)\n\n[316\\. A Formal Framework for Assessing and Mitigating Emergent Security Risks in Generative AI Models: Bridging Theory and Dynamic Risk Mitigation](https://openreview.net/pdf?id=2wZJJOItPz)\n\n[317\\. Strengthening and Democratizing the U.S. Artificial Intelligence Innovation Ecosystem: An Implementation Plan for a National Artificial Intelligence Research Resource](https://www.ai.gov/wp-content/uploads/2023/01/NAIRR-TF-Final-Report-2023.pdf)\n\n[318\\. RFP: AI Agent Evaluation and Synthetic Content](https://aisfund.org/wp-content/uploads/sites/25/2025/01/AISF-AI-Agent-Evals-RFP-1.pdf)\n\n[319\\. Structured validation of AI-based systems by virtual testing in simulated test scenarios](https://link.springer.com/article/10.1007/s10489-023-04475-x)\n\n[320\\. A 20-Year Community Roadmap for Artificial Intelligence Research in the US](https://cra.org/ccc/wp-content/uploads/sites/2/2019/08/Community-Roadmap-for-AI-Research.pdf)\n\n[321\\. TuringBox: An Experimental Platform for the Evaluation of AI Systems](https://www.fatml.org/media/documents/turing_box.pdf)\n\n[322\\. PR2: A Physics- and Photo-realistic Humanoid Testbed with Pilot Study in Competition](https://arxiv.org/html/2409.01559v1)\n\n[323\\. The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey](https://arxiv.org/html/2404.11584v1)\n\n[324\\. 6G KPIs - Definitions and Target Values](https://smart-networks.europa.eu/wp-content/uploads/2025/02/white-paper-kpis_17_2_2025_for-rel_.pdf)\n\n[325\\. KANARIA: IDENTIFYING THE CHALLENGES FOR COGNITIVE AUTONOMOUS NAVIGATION AND GUIDANCE FOR MISSIONS TO SMALL PLANETARY BODIES](https://cgvr.cs.uni-bremen.de/papers/iac15_kanaria/IACKanaria.pdf)\n\n[326\\. Artificial Intelligence Risk Management Framework (AI RMF 1.0)](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf)\n\n[327\\. AI EXHIBITING EMERGENT HUMAN BEHAVIORS: GLOBAL RISK ASSESSMENT OF 2025 REASONING LLMs – CASE STUDIES: OPENAI O3-MINI, DEEPSEEK R1, GEMINI 2, GEMINI 2.5, GROK 3, QWEN 2.5 (PRESENTING: TURING NAND TEST AND DFSW BIAS TEST)](https://zenodo.org/records/15185640/files/2025.03_AI_HUMAN_BEHAVIORS_RISK_ASSESSMENT.v1.2.pdf?download=1)\n\n[328\\. Computational Test Beds for Synthetic and Robotic Agents](https://www.iaeng.org/publication/IMECS2011/IMECS2011_pp1545-1552.pdf)\n\n[331\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[332\\. Sim-to-Real Transfer in Deep Reinforcement Learning for Vision-Based Robotic Grasping](https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/3037792/no.ntnu%3Ainspera%3A109479168%3A37544989.pdf?sequence=1&isAllowed=y)\n\n[333\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[334\\. T. Lillicrap, Jonathan J. Hunt et al. “Continuous control with deep reinforcement learning.” CoRR](https://arxiv.org/abs/1509.02971)\n\n[335\\. A Quantitative Comparison of Vision Performance for the HSR: Gazebo vs. Isaac Simulator](https://link.springer.com/chapter/10.1007/978-3-031-85859-8_29#:~:text=While%20Gazebo%20and%20Isaac%20Sim,performance%20across%20diverse%20lighting%20conditions.)\n\n[336\\. Overall Comparison | Simulately](https://simulately.wiki/docs/comparison/)\n\n[337\\. MuBlE: MuJoCo and Blender simulation Environment and Benchmark for Task Planning in Robot Manipulation](http://arxiv.org/html/2503.02834v1)\n\n[338\\. Evaluating PyBullet and Isaac Sim in the Scope of Robotics and Reinforcement Learning](https://eventos.upm.es/getFiles/hash/N0cjZ1MWZ3UGOkdDO3UGOhlTZkVjZidDOUzM3cDO1Ij:MkxYT2dBTnFTQzc4MGhTZGs3M3dNZnVuK0tEUGQ2MTdiOHIva2Zhekh0K1VxZndrNFdnQWdRTWx1MTR1aThIOCsyU0IvQVJmRWNsdDU2TThNUlBQUFFubm9sOEcrQmIyY0g2MUpLNlV2NDlkYXZHc0wvNVJRSENwV2wyUkFuVmYyMFpaM1hHWmIvSFZMTzk5SklpbmhKSzBqNm55SDZIT3ZKTksyRmIxaXdDU2IwVStteHhDOWl4Vk5xTzZmMnpRTWFjaVhHdURiZ0tFRDRFK1VVOUVJNDNLazdXZjNLNWIyZWdmMUxXUHdnVXBPZGlTWUdZMUhINlY5SCtVZFlmbzhBdTlBamMzTjc3cG5FRUwyVWs1Q2c9PQ==)\n\n[339\\. Advancing Multi-Agent Robotics Simulations Through Heterogeneous Reinforcement Learning in IsaacLab](https://digitalcommons.usu.edu/cgi/viewcontent.cgi?article=1466&context=etd2023)\n\n[340\\. Isaac Sim与Mujoco、PyBullet在物理模拟和渲染速度的比较](https://forums.developer.nvidia.com/t/isaac-sim-very-slow-compared-to-mujoco-or-pybullet-both-physics-and-rendering/237453)\n\n[341\\. John Schulman, S. Levine et al. “Trust Region Policy Optimization.” ArXiv](https://arxiv.org/abs/1502.05477)\n\n[342\\. Greg Brockman, Vicki Cheung et al. “OpenAI Gym.” ArXiv](https://arxiv.org/abs/1606.01540)\n\n[343\\. PHYSBENCH: BENCHMARKING AND ENHANCING VISION-LANGUAGE MODELS FOR PHYSICAL WORLD UNDERSTANDING](https://openreview.net/pdf/ce20f41ba36f17cb4dcd4fdb7952c3c85a51d858.pdf)\n\n[344\\. Evolution Strategies as a Scalable Alternative to Reinforcement Learning](https://www.cnblogs.com/wangxiaocvpr/p/6615975.html)\n\n[345\\. Sample-efficient deep reinforcement learning for control, exploration and safety](https://theses.hal.science/tel-03526401v2/file/These_FLET-BERLIAC_Yannis.pdf)\n\n[346\\. Benchmarking of contact algorithms in Bullet and MuJoCo](https://research.tue.nl/files/355282956/1739255-BEP_Surface_contact_algorithms.pdf)\n\n[347\\. Shape Optimisation of Underactuated Robotic Grippers using Cage Based Deformations](https://libstore.ugent.be/fulltxt/RUG01/003/063/829/RUG01-003063829_2022_0001_AC.pdf)\n\n[348\\. Exploring spiking neural networks for deep reinforcement learning in robotic tasks](https://www.nature.com/articles/s41598-024-77779-8.pdf)"
  },
  {
    "id": 12,
    "question": "When conducting instruction fine-tuning for large models, how can the diversity of the fine-tuning dataset be balanced with task-specific relevance to ensure that the model maintains generalization ability while excelling in specific tasks? For example, if a large amount of SQL-generated data is included, will it affect the model's performance in general question-answering scenarios? How can such issues be addressed?",
    "response": "**Research Report: Balancing Dataset Diversity and Task-Specific Relevance in Instruction Fine-Tuning for Large Language Models**\n\n**1\\. Introduction**\n\nInstruction fine-tuning (IFT) adapts pre-trained large language models (LLMs) to specific tasks by training on datasets comprising task-specific instructions and responses. A critical challenge lies in balancing **dataset diversity** (ensuring broad generalization) with **task-specific relevance** (e.g., SQL generation) to avoid overfitting while excelling in target tasks. This report synthesizes empirical evidence on how excessive domain-specific data (e.g., SQL) impacts generalization, evaluates mitigation strategies, and provides actionable recommendations.\n\n**2\\. Trade-offs Between Task-Specific Data and Generalization**\n\n**2.1 Empirical Impact of SQL Data on General QA Performance**\n\nIncluding excessive SQL data in IFT datasets risks **catastrophic forgetting**—degrading performance in general question answering (QA):\n\n**Thresholds for Degradation**: Though no explicit SQL-proportion thresholds are quantified, studies show domain-skewed IFT causes measurable declines. For instance, when instruction-tuning data with domain imbalances increased, general QA metrics like TruthfulQA accuracy dropped from **56% to 50%** \\[116\\]. Similarly, fine-tuning on table-based tasks (analogous to SQL) led to **significant drops in TabFact accuracy** (fact-checking QA), demonstrating vulnerability in knowledge-intensive tasks \\[63\\]\\[118\\].\n\n**Mechanisms of Degradation**:\n\n**Data Imbalance**: Models overfit to syntactic patterns (e.g., SQL keywords), reducing adaptability to diverse QA formats \\[13\\]\\[116\\].\n\n**Loss of Parametric Knowledge**: Task-specific tuning can overwrite world knowledge stored in LLM parameters, hurting benchmarks like MMLU (Massive Multitask Language Understanding) \\[51\\]\\[143\\].\n\n**2.2 SQL-Specific Overfitting and Its Manifestations**\n\nFine-tuning heavily on SQL data introduces unique risks:\n\n**Syntactic Overfitting**: Models prioritize SQL-like structures in responses, misinterpreting natural language questions \\[8\\]\\[117\\].\n\n**Resource-Intensive Adaptation**: Large SQL context windows (e.g., schemas) strain GPU memory, indirectly reducing capacity for other tasks \\[12\\].\n\n**3\\. Mitigation Strategies: Preserving Generalization**\n\n**3.1 Regularization Techniques**\n\nRegularization counters overfitting by constraining model updates during fine-tuning:\n\n**Weight Decay (L2)**: Penalizes large weight updates, preserving pre-trained knowledge. Optimal decay values (e.g., **0.1**) maintain stability in QA tasks \\[245\\]\\[297\\].\n\n**Dropout**: Randomly deactivates neurons during training, forcing redundancy. **Layer-specific dropout rates** (0.1–0.5 for embeddings; 0.2–0.8 for others) mitigate SQL overfitting better than weight decay in some cases \\[244\\]\\[351\\].\n\n**Adaptive Layer-wise Regularization**: Dynamically adjusts penalties per layer, protecting critical generalization parameters while tuning domain-specific layers \\[21\\].\n\n**Parameter-Efficient Fine-Tuning (PEFT)**:\n\n**LoRA/QLoRA**: Low-rank adaptation reduces trainable parameters, inherently regularizing updates. Models retain >**95% of original MMLU accuracy** after SQL-tuning \\[73\\]\\[77\\].\n\n**3.2 Data-Centric Approaches**\n\n**Data Augmentation**: Generate SQL variants (e.g., paraphrased queries) to improve task diversity within the domain \\[18\\]\\[25\\].\n\n**Balanced Multi-Task Mixing**: Blend SQL data with diverse tasks (e.g., commonsense QA, summarization). Balanced datasets improve **robustness on MMLU by +5–15%** versus domain-skewed sets \\[39\\]\\[264\\].\n\n**Quality Filtering**: Prioritize high-confidence, diverse examples over large volumes. Smaller, curated SQL datasets outperform noisy ones in generalizability \\[39\\]\\[228\\].\n\n**3.3 Architectural and Training Adjustments**\n\n**Early Stopping**: Halts training once validation QA accuracy plateaus, preventing overfitting \\[18\\]\\[127\\].\n\n**Knowledge Distillation**: Distill general-knowledge signals from base models into SQL-tuned models, preserving QA capabilities \\[141\\]\\[321\\].\n\n**4\\. Performance on Standardized Benchmarks**\n\n**4.1 SQL-Augmented Models on MMLU**\n\nControlled experiments reveal trade-offs:\n\n**Improvements in Task-Specific Metrics**: SQL-augmented models achieve **~80% accuracy** in Text-to-SQL tasks \\[268\\].\n\n**General QA Trade-offs**: MMLU accuracy fluctuates based on tuning balance:\n\n_Positive Cases_: Flan-PaLM (540B) tuned on 1.8K tasks (including SQL) achieved **75.2%** on MMLU \\[148\\].\n\n_Degradation Cases_: Over-reliance on SQL data reduced Mistral-7B’s MMLU accuracy from **61.2% to 59.4%** \\[40\\].\n\n**Diversity as a Buffer**: Models trained on balanced multi-task data show **smaller drops (<4%)** in MMLU versus SQL-heavy counterparts \\[264\\]\\[314\\].\n\n**4.2 Factors Influencing Benchmark Resilience**\n\n**Base Model Strength**: Larger models (e.g., Llama 3.1 405B) tolerate SQL skew better, maintaining **\\>83% MMLU** \\[149\\].\n\n**Chain-of-Thought Prompting**: Offsets degradation by **+7–12%** on reasoning-heavy QA tasks \\[152\\].\n\n**5\\. Recommendations for Optimal Fine-Tuning**\n\n1.  **Staged Tuning**: Pre-tune on diverse tasks before SQL-specific refinement to anchor generalization.\n2.  **Hybrid Regularization**: Combine **dropout (0.3–0.5) + LoRA** for compute-efficient SQL tuning \\[73\\]\\[244\\].\n3.  **Proportional Data Allocation**: Limit SQL data to **≤30%** of IFT datasets based on degradation trends in QA/MMLU metrics \\[116\\]\\[118\\].\n4.  **Validation Protocols**: Track MMLU and TruthfulQA weekly during tuning to detect early degradation.\n\n**6\\. Future Research Directions**\n\n**Quantitative SQL Thresholds**: Empirical studies measuring QA degradation as SQL proportions vary (e.g., 10%–50%) are critically needed.\n\n**Cross-Domain Regularization Benchmarks**: Compare dropout vs. weight decay efficacy for SQL overfitting using controlled MMLU evaluations \\[351\\].\n\n**Dynamic Data Sampling**: Algorithms to auto-balance task-specific and diverse examples during training \\[225\\].\n\n**Final Synthesis**: Balancing diversity and specificity in IFT requires **strategic regularization**, **data curation**, and **continuous validation**. While SQL data enhances task performance, it risks QA degradation when exceeding ∼30% of datasets. Mitigation hinges on adaptive regularization (e.g., LoRA), multi-task mixing, and leveraging base model capabilities—ensuring LLMs excel in target tasks without sacrificing generalization.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Tao Yu, Rui Zhang et al. “Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task.” ArXiv](https://doi.org/10.18653/v1/D18-1425)\n\n[2\\. Victor Zhong, Caiming Xiong et al. “Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning.” ArXiv](https://arxiv.org/abs/1709.00103)\n\n[3\\. Cracking SQL Barriers: An LLM-based Dialect Translation System](https://dbgroup.cs.tsinghua.edu.cn/ligl/SIGMOD25-CrackSQL.pdf)\n\n[4\\. Viktor Leis, Andrey Gubichev et al. “How Good Are Query Optimizers, Really?.” Proc. VLDB Endow.](https://doi.org/10.14778/2850583.2850594)\n\n[5\\. S. Agrawal, S. Chaudhuri et al. “Automated Selection of Materialized Views and Indexes in SQL Databases.” Very Large Data Bases Conference](https://www.semanticscholar.org/paper/eab5b35c52177b4f20c80b346ac9b623264d910e)\n\n[6\\. G. Weikum, Axel Mönkeberg et al. “Self-tuning Database Technology and Information Services: from Wishful Thinking to Viable Engineering.” Very Large Data Bases Conference](https://doi.org/10.1016/B978-155860869-6/50011-1)\n\n[7\\. RLVF: Learning from Verbal Feedback without Overgeneralization](https://raw.githubusercontent.com/mlresearch/v235/main/assets/stephan24a/stephan24a.pdf)\n\n[8\\. SQLong: Enhanced NL2SQL for Longer Contexts with LLMs](https://arxiv.org/pdf/2502.16747)\n\n[9\\. DO LLMS REALLY UNDERSTAND SQL?](https://ir.lib.uwo.ca/cgi/viewcontent.cgi?article=13776&context=etd)\n\n[10\\. LLaSA: Large Language and Structured Data Assistant](http://arxiv.org/html/2411.14460v1)\n\n[11\\. Feature Guide for Primary+Standby Instances](https://doc.hcs.huawei.com/en-us/gaussdb/doc/download/pdf/gaussdb-fg-cent.pdf)\n\n[12\\. LR-SQL: A Supervised Fine-Tuning Method for Text2SQL Tasks under Low-Resource Scenarios](https://www.arxiv.org/pdf/2410.11457)\n\n[13\\. David Stap, Eva Hasler et al. “The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities.” ArXiv](https://doi.org/10.48550/arXiv.2405.20089)\n\n[14\\. Alleviating the Fear of Losing Alignment in LLM Fine-tuning](https://arxiv.org/pdf/2504.09757)\n\n[15\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[16\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[17\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[18\\. 中国图象图形学报](https://www.cjig.cn/rc-pub/front/front-article/download/55699756/lowqualitypdf/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E7%96%BE%E7%97%85%E8%AF%8A%E6%96%AD%E7%9A%84%E6%AE%8B%E5%B7%AE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95.pdf)\n\n[19\\. Pillar Page: LLM Fine-Tuning](https://labelyourdata.com/articles/llm-fine-tuning)\n\n[20\\. Practice and Optimization of Deep Learning Model Training](https://wepub.org/index.php/IJCSIT/article/download/5068/5610/10570)\n\n[21\\. How to Complete Domain Tuning while Keeping General Ability in LLM: Adaptive Layer-wise and Element-wise Regularization](http://arxiv.org/html/2501.13669v1)\n\n[22\\. 24 深度学习 | 小树不修不直溜：深度学习中的正则化](https://static.fabric.jansora.com/geektime2024/026-100003101-%E4%B8%93%E6%A0%8F%E8%AF%BE-%E7%8E%8B%E5%A4%A9%E4%B8%80-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80%E8%AF%BE%EF%BC%88%E5%AE%8C%E7%BB%93%EF%BC%89/05-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%287%E8%AE%B2%29/24%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%A8%E5%B0%8F%E6%A0%91%E4%B8%8D%E4%BF%AE%E4%B8%8D%E7%9B%B4%E6%BA%9C%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96.pdf)\n\n[23\\. API Guide](https://docs.oracle.com/en/database/oracle/machine-learning/oml4sql/23/dmapi/neural-network.html)\n\n[24\\. R. Shokri, M. Stronati et al. “Membership Inference Attacks Against Machine Learning Models.” 2017 IEEE Symposium on Security and Privacy (SP)](https://doi.org/10.1109/SP.2017.41)\n\n[25\\. Regularization Techniques to Prevent Overfitting in Neural Networks](https://analyticsindiamag.com/types-of-regularization-techniques-to-avoid-overfitting-in-learning-models/)\n\n[26\\. How to Efficiently Train Huge Transformer LLMs](https://www.e2enetworks.com/blog/how-to-efficiently-train-huge-transformer-llms)\n\n[27\\. Powered SQL education: Automating SQL/PLSQL question classification with LLMs and machine learning](https://www.ijirss.com/index.php/ijirss/article/download/5467/941/8762)\n\n[28\\. Overfitting and Regularization in Machine Learning](https://www.geeksforgeeks.org/machine-learning/overfitting-and-regularization-in-ml/)\n\n[29\\. Best Datasets for Training Semantic Segmentation Models](https://keymakr.com/blog/best-datasets-for-training-semantic-segmentation-models/)\n\n[30\\. What Is LLM Architecture? Basic LLM Model Architecture](https://sam-solutions.com/blog/llm-architecture/#:~:text=The%20core%20computational%20unit%20in,transformers%20analyze%20all%20tokens%20simultaneously.)\n\n[31\\. Regularization and Overfitting in Machine Learning](https://schneppat.com/regularization-overfitting-in-machine-learning.html)\n\n[32\\. Nitish Srivastava, Geoffrey E. Hinton et al. “Dropout: a simple way to prevent neural networks from overfitting.” J. Mach. Learn. Res.](https://doi.org/10.5555/2627435.2670313)\n\n[33\\. LLM Landscape Report](https://cdn.prod.website-files.com/668d66434307b08c724f8a81/67dc26e1400679d594e43df8_LLM%20Landscape%20Report.pdf)\n\n[34\\. Mathematical Foundations of Deep Learning](https://hal.science/hal-04928560v1/document)\n\n[35\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[36\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[37\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[38\\. Instruction Pretraining LLMs](https://magazine.sebastianraschka.com/p/instruction-pretraining-llms)\n\n[39\\. An Overview of Instruction Tuning Data](https://www.ruder.io/an-overview-of-instruction-tuning-data/)\n\n[40\\. Towards Better Understanding Table Instruction Tuning: Decoupling the Effects from Data versus Models](https://arxiv.org/pdf/2501.14717)\n\n[41\\. Estimating Large Language Model Capabilities without Labeled Test Data](https://openreview.net/pdf?id=Pb1DhkTVLZ)\n\n[42\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[43\\. Top AI Models July 2025](https://aispectrum.io/top-models)\n\n[44\\. Victor Sanh, Albert Webson et al. “Multitask Prompted Training Enables Zero-Shot Task Generalization.” ArXiv](https://arxiv.org/abs/2110.08207)\n\n[45\\. LawInstruct: A Resource for Studying Language Model Adaptation to the Legal Domain](https://arxiv.org/pdf/2404.02127)\n\n[46\\. Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)\n\n[47\\. Artificial Intelligence Index Report 2023](https://event-cdn.baai.ac.cn/file/file-browser/C7FA4aFhrT2Hrnm77AKZPww62Ywm7Pyk.pdf)\n\n[48\\. Training on the Test Task Confounds Evaluation and Emergence](https://openreview.net/pdf?id=xEsmZTdo1D)\n\n[49\\. Data augmentation for fairness-aware machine learning: Preventing algorithmic bias in law enforcement systems](https://www.darleneproject.eu/wp-content/uploads/2023/10/Data-augmentation-for-fairness-aware-machine-learning.pdf)\n\n[50\\. Some intuitions about large language models](https://ai4comm.media.mit.edu/slides/emergence.pdf)\n\n[51\\. An overview of instruction-following models](https://web.stanford.edu/class/cs224v/lectures_2023/Instruct%20talk.pdf)\n\n[52\\. QLoRA vs Standard Finetuning Experimental Setup Details](https://openreview.net/attachment?id=OUIFPHEgJU&name=supplementary_material)\n\n[55\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[56\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[57\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[58\\. Jason Wei, Maarten Bosma et al. “Finetuned Language Models Are Zero-Shot Learners.” ArXiv](https://arxiv.org/abs/2109.01652)\n\n[59\\. UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function](https://openreview.net/pdf?id=WL414RO8No)\n\n[60\\. Yizhong Wang, Yeganeh Kordi et al. “Self-Instruct: Aligning Language Models with Self-Generated Instructions.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.48550/arXiv.2212.10560)\n\n[61\\. The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities](https://assets.amazon.science/1a/c5/b61c82c44b0aac88bb8e9a46b9fd/the-fine-tuning-paradox-boosting-translation-quality-without-sacrificing-llm-abilities.pdf)\n\n[62\\. Yunjie Ji, Yong Deng et al. “Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases.” ArXiv](https://doi.org/10.48550/arXiv.2303.14742)\n\n[63\\. Rethinking Table Instruction Tuning](https://openreview.net/pdf/f960279a3a13fb93e617f10c1895b0870214161a.pdf)\n\n[64\\. Less is More: Data Value Estimation for Visual Instruction Tuning](https://arxiv.org/pdf/2403.09559)\n\n[65\\. 探讨教学数据缩放对大型语言模型的影响：真实世界使用案例的实证研究](https://zhuanlan.zhihu.com/p/620659842)\n\n[66\\. How Much Data is Enough Data? Fine-Tuning Large Language Models for In-House Translation: Performance Evaluation Across Multiple Dataset Sizes](http://arxiv.org/html/2409.03454)\n\n[67\\. AIR: Complex Instruction Generation via Automatic Iterative Refinement](https://arxiv.org/pdf/2502.17787)\n\n[70\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[71\\. AI-HUB 2.0 PROJECT REPORT: Application Of Large Language Models in Software Engineering](https://research.tuni.fi/app/uploads/sites/34/2024/02/ea2fa4ce-report-ai-hub-2.0.-application_of_llm_is_software_engineering__report2.pdf)\n\n[72\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[73\\. Large Language Model Enhanced Text-to-SQL Generation: A Survey](https://arxiv.org/pdf/2410.06011)\n\n[74\\. DIRECTLY FINE-TUNING DIFFUSION MODELS ON DIFFERENTIABLE REWARDS](https://openreview.net/pdf/4f6a4d187763cd647549b92a33f6f1c84f23bdec.pdf)\n\n[75\\. Fine-Tuning Large Language Models – Challenges and Best Practices](https://www.encora.com/insights/fine-tuning-large-language-models-challenges-and-best-practices)\n\n[76\\. Regularization and Overfitting in Machine Learning](https://schneppat.com/regularization-overfitting-in-machine-learning.html)\n\n[77\\. Revisiting Fine-Tuning: A Survey of Parameter-Efficient Techniques for Large AI Models](https://www.preprints.org/manuscript/202504.0743/v1/download)\n\n[78\\. Foundations of Large Language Models](https://readwise-assets.s3.amazonaws.com/media/wisereads/articles/foundations-of-large-language-/2501.09223v1.pdf)\n\n[79\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[80\\. Comparative Analysis of Preference-Informed Alignment Techniques for Language Model Alignment](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/final-projects/SooWeiKoh.pdf)\n\n[81\\. Performance of Recent Large Language Models for a Low-Resourced Language](https://arxiv.org/pdf/2407.21330)\n\n[82\\. LLM Parameters: Understanding the Building Blocks of Large Language Models](https://www.coursera.org/articles/llm-parameters)\n\n[83\\. On Protecting the Data Privacy of Large Language Models (LLMs): A Survey](http://arxiv.org/pdf/2403.05156v1.pdf?ref=applied-gai-in-security.ghost.io)\n\n[84\\. CS 224N: Default Final Project: Build GPT-2](https://web.stanford.edu/class/cs224n/project_w25/CS_224n__Default_Final_Project__Build_GPT_2.pdf)\n\n[85\\. Large Language Model Loss Function - LLM Built](https://llmbuilt.com/large-language-model-loss-function/)\n\n[86\\. Pre-training Vs. Fine-Tuning Large Language Models](https://www.ankursnewsletter.com/p/pre-training-vs-fine-tuning-large)\n\n[87\\. Sentence Embeddings for Massively Multilingual Speech and Text Processing](https://theses.hal.science/tel-04573934v1/file/144440_DUQUENNE_2024_archivage.pdf)\n\n[88\\. Regularization in Deep Learning and Generative Models](http://paperreading.club/category?cate=Regularization)\n\n[90\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[91\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[92\\. Control LLM: Controlled Evolution for Intelligence Retention in LLM](https://arxiv.org/html/2501.10979v1)\n\n[93\\. Towards Better Understanding Table Instruction Tuning: Decoupling the Effects from Data versus Models](https://arxiv.org/pdf/2501.14717)\n\n[94\\. IMPROVING DATA EFFICIENCY VIA CURATING LLM-DRIVEN RATING SYSTEMS](https://openreview.net/pdf/55cc7d1e03331f2c1db91d5f02efed31d57f8ad9.pdf)\n\n[95\\. PMLB: a large benchmark suite for machine learning evaluation and comparison](https://biodatamining.biomedcentral.com/articles/10.1186/s13040-017-0154-4)\n\n[96\\. Jason Wei, Maarten Bosma et al. “Finetuned Language Models Are Zero-Shot Learners.” ArXiv](https://arxiv.org/abs/2109.01652)\n\n[97\\. Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs](http://arxiv.org/html/2410.10739v1)\n\n[98\\. MMTEB: MASSIVE MULTILINGUAL TEXT EMBEDDING BENCHMARK](https://openreview.net/pdf/3fa1cbef08a29b978ec7089b81f28d51ed87b4c7.pdf)\n\n[99\\. MMInstruct: a high-quality multi-modal instruction tuning dataset with extensive diversity](http://scis.scichina.com/en/2024/220103.pdf)\n\n[100\\. SHED: Shapley-Based Automated Dataset Refinement for Instruction Fine-Tuning](https://arxiv.org/html/2405.00705v1)\n\n[101\\. Adds Open LLM Leaderboard Tasks (#2047)](https://github.com/EleutherAI/lm-evaluation-harness/commit/3c8db1bb7be5662e4fd5b48a26b6214f758e483f)\n\n[102\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[103\\. Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models](https://zhuanlan.zhihu.com/p/678317469)\n\n[104\\. Victor Sanh, Albert Webson et al. “Multitask Prompted Training Enables Zero-Shot Task Generalization.” ArXiv](https://arxiv.org/abs/2110.08207)\n\n[105\\. Better supervised fine-tuning of closed-source large models](https://vixra.org/pdf/2411.0154v1.pdf)\n\n[106\\. Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets](https://openreview.net/pdf/c20ef752be9f4073e42d79d5cfcc0cd29e6931bd.pdf)\n\n[110\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[111\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[112\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[113\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[114\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[115\\. Oded Ovadia, Meni Brief et al. “Knowledge-Instruct: Effective Continual Pre-training from Limited Data using Instructions.”](https://arxiv.org/abs/2504.05571)\n\n[116\\. UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function](https://openreview.net/pdf?id=WL414RO8No)\n\n[117\\. DO LLMS REALLY UNDERSTAND SQL?](https://ir.lib.uwo.ca/cgi/viewcontent.cgi?article=13776&context=etd)\n\n[118\\. SocraticLM: Exploring Socratic Personalized Teaching with Large Language Models](https://openreview.net/pdf?id=qkoZgJhxsA)\n\n[119\\. From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning](https://openreview.net/pdf/a263f7f649d9d686fc3f835f81e6123e435e6eea.pdf)\n\n[120\\. AI-HUB 2.0 PROJECT REPORT: Application Of Large Language Models in Software Engineering](https://research.tuni.fi/app/uploads/sites/34/2024/02/ea2fa4ce-report-ai-hub-2.0.-application_of_llm_is_software_engineering__report2.pdf)\n\n[121\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[122\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[123\\. Nitish Srivastava, Geoffrey E. Hinton et al. “Dropout: a simple way to prevent neural networks from overfitting.” J. Mach. Learn. Res.](https://doi.org/10.5555/2627435.2670313)\n\n[124\\. Large Language Model Enhanced Text-to-SQL Generation: A Survey](https://arxiv.org/pdf/2410.06011)\n\n[125\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[126\\. STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models](https://openreview.net/pdf?id=ikucyr7shBv)\n\n[127\\. DIRECTLY FINE-TUNING DIFFUSION MODELS ON DIFFERENTIABLE REWARDS](https://openreview.net/pdf/4f6a4d187763cd647549b92a33f6f1c84f23bdec.pdf)\n\n[128\\. On Protecting the Data Privacy of Large Language Models (LLMs): A Survey](http://arxiv.org/pdf/2403.05156v1.pdf?ref=applied-gai-in-security.ghost.io)\n\n[129\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[130\\. Practice and Optimization of Deep Learning Model Training](https://wepub.org/index.php/IJCSIT/article/download/5068/5610/10570)\n\n[131\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[132\\. Oracle Database Machine Learning for SQL Use Cases](https://docs.oracle.com/en/database/oracle/machine-learning/oml4sql/23/mlsql/oracle-machine-learning-sql-use-cases.pdf)\n\n[133\\. Underfitting and Overfitting in Machine Learning](https://pareto.ai/blog/underfitting-and-overfitting)\n\n[134\\. CS 224N: Default Final Project: Build GPT-2](https://web.stanford.edu/class/cs224n/project_w25/CS_224n__Default_Final_Project__Build_GPT_2.pdf)\n\n[135\\. Regularization and Overfitting in Machine Learning](https://schneppat.com/regularization-overfitting-in-machine-learning.html)\n\n[136\\. Linear Regression](https://www3.nd.edu/~dchiang/teaching/nlp/2015/notes/chapter9v1.pdf)\n\n[137\\. Fine-Tuning Large Language Models – Challenges and Best Practices](https://www.encora.com/insights/fine-tuning-large-language-models-challenges-and-best-practices)\n\n[138\\. Building Complex Machine Learning Models](https://dzone.com/articles/mastering-the-art-of-building-complex-machine-lear)\n\n[140\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[141\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[142\\. Qwen2-7B-Instruct – Replicate](https://replicate.com/zsxkib/qwen2-7b-instruct)\n\n[143\\. Control LLM: Controlled Evolution for Intelligence Retention in LLM](https://arxiv.org/html/2501.10979v1)\n\n[144\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[145\\. Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets](https://openreview.net/pdf/c20ef752be9f4073e42d79d5cfcc0cd29e6931bd.pdf)\n\n[146\\. Towards Better Understanding Table Instruction Tuning: Decoupling the Effects from Data versus Models](https://arxiv.org/pdf/2501.14717)\n\n[147\\. Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models](https://openreview.net/pdf/c54789f2d5dd00c34f30dd23305bbdb44b35580f.pdf)\n\n[148\\. Hyung Won Chung, Le Hou et al. “Scaling Instruction-Finetuned Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2210.11416)\n\n[149\\. Llama 3.1 405B vs 70B vs 8B: What's the Difference?](http://anakin.ai/blog/llama-3-1-405b-vs-70b-vs-8bdifference/)\n\n[150\\. One-Shot Learning as Instruction Data Prospector for Large Language Models](https://openreview.net/pdf/fc474a1a1dab80a3d6e9d1b304bcd3dadebca0af.pdf)\n\n[151\\. MAGPIE: ALIGNMENT DATA SYNTHESIS FROM SCRATCH BY PROMPTING ALIGNED LLMs WITH NOTHING](https://openreview.net/pdf/50a05182d0e7d75436c8ae66195c7473443b2dd2.pdf)\n\n[152\\. WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More Challenging](https://arxiv.org/pdf/2502.18316)\n\n[153\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[154\\. Victor Sanh, Albert Webson et al. “Multitask Prompted Training Enables Zero-Shot Task Generalization.” ArXiv](https://arxiv.org/abs/2110.08207)\n\n[155\\. Training on the Test Task Confounds Evaluation and Emergence](https://openreview.net/pdf?id=xEsmZTdo1D)\n\n[156\\. Calibrating Language Models via Augmented Prompt Ensembles](https://openreview.net/pdf?id=L0dc4wqbNs)\n\n[157\\. An overview of instruction-following models](https://web.stanford.edu/class/cs224v/lectures_2023/Instruct%20talk.pdf)\n\n[160\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[161\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[162\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[163\\. Jason Wei, Maarten Bosma et al. “Finetuned Language Models Are Zero-Shot Learners.” ArXiv](https://arxiv.org/abs/2109.01652)\n\n[164\\. Yizhong Wang, Swaroop Mishra et al. “Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/2022.emnlp-main.340)\n\n[165\\. 60 Data Points are Sufficient to Fine-Tune LLMs for Question-Answering](https://arxiv.org/pdf/2409.15825)\n\n[166\\. Pranav Rajpurkar, Jian Zhang et al. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D16-1264)\n\n[167\\. Datasets for Large Language Models: A Comprehensive Survey](https://dispatch.iotexec.com/wp-content/uploads/2024/08/Datasets_for_LLMs_A_Comprehensive_Survey_1724808443.pdf)\n\n[168\\. Toward expert-level medical question answering with large language models](https://www.nature.com/articles/s41591-024-03423-7.pdf)\n\n[169\\. Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning](https://openreview.net/pdf?id=fUHVyYBnVG)\n\n[170\\. OmniScience: A Domain-Specialized LLM for Scientific Reasoning and Discovery](https://arxiv.org/pdf/2503.17604v4)\n\n[171\\. DO LLMS REALLY UNDERSTAND SQL?](https://ir.lib.uwo.ca/cgi/viewcontent.cgi?article=13776&context=etd)\n\n[172\\. Instruction Tuning Large Language Models to Understand Electronic Health Records](https://proceedings.neurips.cc/paper_files/paper/2024/file/62986e0a78780fe5f17b495aeded5bab-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[173\\. INSTRUCTIONGPT-4: A 200-INSTRUCTION PARADIGM FOR FINE-TUNING MINIGPT-4](https://openreview.net/pdf?id=DNvzCsQG1D)\n\n[174\\. MECHANISTICALLY ANALYZING THE EFFECTS OF FINE-TUNING ON PROCEDURALLY DEFINED TASKS](https://openreview.net/pdf?id=A0HKeKl4Nl)\n\n[175\\. Instruction Tuning With Loss Over Instructions](https://proceedings.neurips.cc/paper_files/paper/2024/file/7ffb43adf37b3eeaba559098bc084cc6-Paper-Conference.pdf)\n\n[176\\. Instruction Tuning for Domain Adaptation of Large Language Models: A Case Study in the Field of Education](https://repository.tudelft.nl/file/File_e2064dc9-97f1-4e9d-ac32-1bebebf05c3d?preview=1)\n\n[177\\. On Training Data Influence of GPT Models](https://arxiv.org/pdf/2404.07840)\n\n[180\\. A survey of regularization strategies for deep models](https://link.springer.com/article/10.1007/s10462-019-09784-7)\n\n[181\\. Regularization and Overfitting in Machine Learning](https://schneppat.com/regularization-overfitting-in-machine-learning.html)\n\n[182\\. Fine-Tuning Large Language Models – Challenges and Best Practices](https://www.encora.com/insights/fine-tuning-large-language-models-challenges-and-best-practices)\n\n[183\\. Large Language Model Loss Function - LLM Built](https://llmbuilt.com/large-language-model-loss-function/)\n\n[184\\. Regularization in Deep Learning and Generative Models](http://paperreading.club/category?cate=Regularization)\n\n[185\\. Selective Self-Rehearsal: Enhancing Generalization in Fine-Tuning Large Language Models](https://www.catalyzex.com/author/Sonam%20Gupta)\n\n[186\\. Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution Data](https://aclanthology.org/2020.emnlp-main.102/)\n\n[187\\. Overfitting In AI Case Studies](https://www.meegle.com/en_us/topics/overfitting/overfitting-in-ai-case-studies)\n\n[188\\. Underfitting and Overfitting in Machine Learning](https://pareto.ai/blog/underfitting-and-overfitting)\n\n[189\\. Advanced Techniques for Training Language Models](https://botpenguin.com/blogs/advanced-techniques-for-training-language-models)\n\n[190\\. How to Guide: Overcoming overfitting in your ML models](https://predibase.com/blog/how-to-guide-overcoming-overfitting-in-your-ml-models)\n\n[191\\. Best Datasets for Training Semantic Segmentation Models](https://keymakr.com/blog/best-datasets-for-training-semantic-segmentation-models/)\n\n[192\\. Model Training in AI/ML: Process, Challenges, and Best Practices](https://www.kolena.com/blog/model-training-in-ai-ml-process-challenges-and-best-practices)\n\n[193\\. Fine-tuning Large Language Models: Complete Optimization Guide](https://www.scribbledata.io/blog/fine-tuning-large-language-models/)\n\n[194\\. Regularization in Machine Learning](https://machinelearningmodels.org/regularization-in-machine-learning/)\n\n[195\\. How To Implement Intent Classification In NLP \\[7 ML & DL Models\\] With Python Example](https://spotintelligence.com/2023/11/03/intent-classification-nlp/)\n\n[196\\. Conference on Empirical Methods in Natural Language Processing (2016)](https://aclanthology.org/events/emnlp-2016/)\n\n[197\\. Mastering Deep Learning: Techniques for Enhancing Accuracy](https://www.digitalprogolf.com/mastering-deep-learning-techniques-for-enhancing-accuracy/)\n\n[198\\. Regularization](https://iq.opengenus.org/regularization/)\n\n[199\\. Unleashing the Power of Machine Learning Algorithms: Revolutionizing Industries and Transforming Lives](https://behaveannual.org/machine-learning/machine-learning-algorithms/)\n\n[200\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[201\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[202\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[203\\. Hyung Won Chung, Le Hou et al. “Scaling Instruction-Finetuned Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2210.11416)\n\n[204\\. Instruction Tuning for Large Language Models](https://www.geeksforgeeks.org/instruction-tuning-for-large-language-models/)\n\n[205\\. Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets](https://openreview.net/pdf/c20ef752be9f4073e42d79d5cfcc0cd29e6931bd.pdf)\n\n[206\\. Towards Better Understanding Table Instruction Tuning: Decoupling the Effects from Data versus Models](https://arxiv.org/pdf/2501.14717)\n\n[207\\. Fine-tuning Llama For Better Performance With the MMLU Benchmark](https://osf.io/e3v5x/download)\n\n[208\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[209\\. Victor Sanh, Albert Webson et al. “Multitask Prompted Training Enables Zero-Shot Task Generalization.” ArXiv](https://arxiv.org/abs/2110.08207)\n\n[210\\. Control LLM: Controlled Evolution for Intelligence Retention in LLM](https://arxiv.org/html/2501.10979v1)\n\n[211\\. Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs](http://arxiv.org/html/2410.10739v1)\n\n[212\\. Sheng Shen, Le Hou et al. “Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts.” ArXiv](https://doi.org/10.48550/arXiv.2305.14705)\n\n[213\\. Instruction Fine-Tuning for Language Models](http://engure.fun/article/120)\n\n[214\\. MMInstruct: a high-quality multi-modal instruction tuning dataset with extensive diversity](http://scis.scichina.com/en/2024/220103.pdf)\n\n[220\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[221\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[222\\. Jason Wei, Maarten Bosma et al. “Finetuned Language Models Are Zero-Shot Learners.” ArXiv](https://arxiv.org/abs/2109.01652)\n\n[223\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[224\\. Yizhong Wang, Yeganeh Kordi et al. “Self-Instruct: Aligning Language Models with Self-Generated Instructions.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.48550/arXiv.2212.10560)\n\n[225\\. Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models](https://www.arxiv.org/pdf/2408.02085v1)\n\n[226\\. UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function](https://openreview.net/pdf?id=WL414RO8No)\n\n[227\\. Dataset Quantization](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Dataset_Quantization_ICCV_2023_paper.pdf)\n\n[228\\. From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning](https://arxiv.org/pdf/2308.12032v2)\n\n[229\\. Instruction Tuning Text-to-SQL with Large Language Models in the Power Grid Domain](https://dl.acm.org/doi/abs/10.1145/3622896.3622906)\n\n[230\\. Datasets for Large Language Models: A Comprehensive Survey](https://dispatch.iotexec.com/wp-content/uploads/2024/08/Datasets_for_LLMs_A_Comprehensive_Survey_1724808443.pdf)\n\n[231\\. Yihan Cao, Yanbin Kang et al. “Instruction Mining: When Data Mining Meets Large Language Model Finetuning.”](https://arxiv.org/abs/2307.06290)\n\n[232\\. The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities](https://assets.amazon.science/1a/c5/b61c82c44b0aac88bb8e9a46b9fd/the-fine-tuning-paradox-boosting-translation-quality-without-sacrificing-llm-abilities.pdf)\n\n[233\\. Optimizing Database Query Learning: A Generative AI Approach for Semantic Error Feedback](https://peer.asee.org/optimizing-database-query-learning-a-generative-ai-approach-for-semantic-error-feedback.pdf)\n\n[238\\. Nitish Srivastava, Geoffrey E. Hinton et al. “Dropout: a simple way to prevent neural networks from overfitting.” J. Mach. Learn. Res.](https://doi.org/10.5555/2627435.2670313)\n\n[239\\. Sergey Ioffe, Christian Szegedy. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” ArXiv](https://arxiv.org/abs/1502.03167)\n\n[240\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[241\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[242\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[243\\. A. Krizhevsky. “Learning Multiple Layers of Features from Tiny Images.”](https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086)\n\n[244\\. Demystifying Large Language Model Fine-Tuning](https://blogs.dimensionless.ai/demystifying-large-language-model-fine-tuning)\n\n[245\\. Rethinking Optimization and Architecture for Tiny Language Models](https://raw.githubusercontent.com/mlresearch/v235/main/assets/tang24c/tang24c.pdf)\n\n[246\\. Practice and Optimization of Deep Learning Model Training](https://wepub.org/index.php/IJCSIT/article/download/5068/5610/10570)\n\n[247\\. Weight Decay | Continuum Labs](https://training.continuumlabs.ai/training/the-fine-tuning-process/hyperparameters/weight-decay#:~:text=Start%20with%20a%20small%20value,accuracy%20on%20the%20training%20data.)\n\n[248\\. Dropout vs Weight Decay](https://www.geeksforgeeks.org/dropout-vs-weight-decay/)\n\n[249\\. Exploiting Multilingualism and Transfer Learning for Low Resource Machine Translation](https://repository.kulib.kyoto-u.ac.jp/dspace/bitstream/2433/232411/2/djohk00663.pdf)\n\n[250\\. Cheolhyoung Lee, Kyunghyun Cho et al. “Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models.” ArXiv](https://arxiv.org/abs/1909.11299)\n\n[251\\. Fine-Tuning Large Language Models – Challenges and Best Practices](https://www.encora.com/insights/fine-tuning-large-language-models-challenges-and-best-practices)\n\n[252\\. What is Dropout: Artificial Intelligence Explained](https://www.chatgptguide.ai/2024/02/24/what-is-dropout/)\n\n[253\\. Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n\n[254\\. CADAid: Validation of Architectural Drawings Powered by Machine Learning](https://kartai.no/wp-content/uploads/2024/06/Bachelor_s_Thesis.pdf)\n\n[255\\. Mining of Massive Datasets](https://mrce.in/ebooks/Mining%20of%20Massive%20Datasets%203rd%20Ed.pdf)\n\n[256\\. Contributions to Explainable Deep Learning Models](https://openaccess.uoc.edu/bitstream/10609/151211/1/PhD_Thesis.pdf)\n\n[257\\. Dropout Reduces Underfitting](https://arxiv.org/pdf/2303.01500v1)\n\n[258\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[259\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[260\\. Fine-tuning Llama For Better Performance With the MMLU Benchmark](https://osf.io/e3v5x/download)\n\n[261\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[262\\. Control LLM: Controlled Evolution for Intelligence Retention in LLM](http://arxiv.org/html/2501.10979v2)\n\n[263\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[264\\. Relation_Extraction](http://paperreading.club/category?cate=Relation_Extraction&page=0)\n\n[265\\. MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning](https://github.com/codefuse-ai/MFTCoder)\n\n[266\\. Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs](http://arxiv.org/html/2410.10739v1)\n\n[267\\. Jason Wei, Maarten Bosma et al. “Finetuned Language Models Are Zero-Shot Learners.” ArXiv](https://arxiv.org/abs/2109.01652)\n\n[268\\. Alleviating the Fear of Losing Alignment in LLM Fine-tuning](https://arxiv.org/pdf/2504.09757)\n\n[269\\. Mixing It Up: The Cocktail Effect of Multi-Task Fine-Tuning on LLM Performance -- A Case Study in Finance](https://www.promptlayer.com/research-papers/mixing-it-up-the-cocktail-effect-of-multi-task-fine-tuning-on-llm-performance-a-case-study-in-finance)\n\n[270\\. DB-Explore: Automated Database Exploration and Instruction Synthesis for Text-to-SQL](https://arxiv.org/pdf/2503.04959v2)\n\n[271\\. Nebuly | User-Experience for LLMs](https://www.nebuly.com/blog/metas-llama-a-small-language-model-beating-giants)\n\n[272\\. RAMIE：使用大语言模型的检索增强型多任务信息提取用于膳食补充剂](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_11_26/2411.15700.pdf)\n\n[273\\. Enhancing LLM Fine-Tuning via Selective Parameter Merging](https://openreview.net/pdf?id=eta9LFgEY1w)\n\n[278\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[279\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[280\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[281\\. Jason Wei, Maarten Bosma et al. “Finetuned Language Models Are Zero-Shot Learners.” ArXiv](https://arxiv.org/abs/2109.01652)\n\n[282\\. Mandar Joshi, Eunsol Choi et al. “TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.” ArXiv](https://doi.org/10.18653/v1/P17-1147)\n\n[283\\. Instruction Tuning Text-to-SQL with Large Language Models in the Power Grid Domain](https://dl.acm.org/doi/abs/10.1145/3622896.3622906)\n\n[284\\. UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function](https://arxiv.org/pdf/2410.21438)\n\n[285\\. Advancing Mathematical Reasoning with Language Models: A Multimodal and Knowledge-Intensive Perspective](https://escholarship.org/content/qt678864d8/qt678864d8.pdf)\n\n[286\\. Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries](https://arxiv.org/pdf/2402.08349)\n\n[287\\. From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning](https://arxiv.org/pdf/2308.12032v2)\n\n[288\\. MECHANISTICALLY ANALYZING THE EFFECTS OF FINE-TUNING ON PROCEDURALLY DEFINED TASKS](https://openreview.net/pdf?id=A0HKeKl4Nl)\n\n[289\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[290\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[291\\. Sergey Ioffe, Christian Szegedy. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” ArXiv](https://arxiv.org/abs/1502.03167)\n\n[292\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[293\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[294\\. Nitish Srivastava, Geoffrey E. Hinton et al. “Dropout: a simple way to prevent neural networks from overfitting.” J. Mach. Learn. Res.](https://doi.org/10.5555/2627435.2670313)\n\n[295\\. Rethinking Optimization and Architecture for Tiny Language Models](https://raw.githubusercontent.com/mlresearch/v235/main/assets/tang24c/tang24c.pdf)\n\n[296\\. Fine-Tuning Large Language Models – Challenges and Best Practices](https://www.encora.com/insights/fine-tuning-large-language-models-challenges-and-best-practices)\n\n[297\\. Weight Decay | Continuum Labs](https://training.continuumlabs.ai/training/the-fine-tuning-process/hyperparameters/weight-decay#:~:text=Start%20with%20a%20small%20value,accuracy%20on%20the%20training%20data.)\n\n[298\\. Cheolhyoung Lee, Kyunghyun Cho et al. “Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models.” ArXiv](https://arxiv.org/abs/1909.11299)\n\n[299\\. Preserving Diversity in Supervised Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2408.16673)\n\n[300\\. LoRA Dropout as a Sparsity Regularizer for Overfitting Reduction](https://openreview.net/forum?id=c4498OydLP)\n\n[301\\. Dropout vs Weight Decay](https://www.geeksforgeeks.org/dropout-vs-weight-decay/)\n\n[302\\. Dropout Reduces Underfitting](https://arxiv.org/pdf/2303.01500v1)\n\n[303\\. Exploiting Multilingualism and Transfer Learning for Low Resource Machine Translation](https://repository.kulib.kyoto-u.ac.jp/dspace/bitstream/2433/232411/2/djohk00663.pdf)\n\n[304\\. Deep Learning](http://alvarestech.com/temp/deep/Deep%20Learning%20by%20Ian%20Goodfellow,%20Yoshua%20Bengio,%20Aaron%20Courville%20%28z-lib.org%29.pdf)\n\n[305\\. CADAid: Validation of Architectural Drawings Powered by Machine Learning](https://kartai.no/wp-content/uploads/2024/06/Bachelor_s_Thesis.pdf)\n\n[306\\. A survey on LoRA of large language models](https://journal.hep.com.cn/fcs/EN/10.1007/s11704-024-40663-9)\n\n[307\\. Comparing BERT Fine-Tuning Methods](https://web.stanford.edu/class/cs224n/final-reports/256736657.pdf)\n\n[308\\. LoRA Dropout as a Sparsity Regularizer for Overfitting Control](http://export.arxiv.org/pdf/2404.09610v1)\n\n[309\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[310\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[311\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[312\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[313\\. Control LLM: Controlled Evolution for Intelligence Retention in LLM](https://arxiv.org/html/2501.10979v1)\n\n[314\\. Fine-tuning Llama For Better Performance With the MMLU Benchmark](https://osf.io/e3v5x/download)\n\n[315\\. Jason Wei, Maarten Bosma et al. “Finetuned Language Models Are Zero-Shot Learners.” ArXiv](https://arxiv.org/abs/2109.01652)\n\n[316\\. HMORA: MAKING LLMs MORE EFFECTIVE WITH HIERARCHICAL MIXTURE OF LoRA EXPERTS](https://openreview.net/pdf?id=lTkHiXeuDl)\n\n[317\\. RouteLLM: Balancing Cost and Quality in LLM Deployments](https://zilliz.com/learn/routellm-open-source-framework-for-navigate-cost-quality-trade-offs-in-llm-deployment)\n\n[318\\. Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation](https://openreview.net/pdf?id=BkwCrIsTbR)\n\n[319\\. Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs](http://arxiv.org/html/2410.10739v1)\n\n[320\\. ROUTE: ROBUST MULTITASK TUNING AND COLLABORATION FOR TEXT-TO-SQL](https://openreview.net/pdf?id=BAglD6NGy0)\n\n[321\\. Nebuly | User-Experience for LLMs](https://www.nebuly.com/blog/metas-llama-a-small-language-model-beating-giants)\n\n[322\\. Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets](https://openreview.net/pdf/c20ef752be9f4073e42d79d5cfcc0cd29e6931bd.pdf)\n\n[323\\. TINYBENCHMARKS: EVALUATING LLMs WITH FEWER EXAMPLES](https://openreview.net/pdf?id=CN4xL9IYRO)\n\n[324\\. MMLU Benchmark of LLM Eval](https://www.bracai.eu/post/mmlu-benchmark)\n\n[325\\. INSTRUCTION-FINETUNED FOUNDATION MODELS FOR MULTIMODAL WEB NAVIGATION](https://openreview.net/pdf?id=oLc9sGOBbc)\n\n[329\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[330\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[331\\. Jason Wei, Maarten Bosma et al. “Finetuned Language Models Are Zero-Shot Learners.” ArXiv](https://arxiv.org/abs/2109.01652)\n\n[332\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[333\\. Peter Clark, Isaac Cowhey et al. “Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge.” ArXiv](https://arxiv.org/abs/1803.05457)\n\n[334\\. DO LLMS REALLY UNDERSTAND SQL?](https://ir.lib.uwo.ca/cgi/viewcontent.cgi?article=13776&context=etd)\n\n[335\\. Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models](https://www.arxiv.org/pdf/2408.02085v1)\n\n[336\\. Fine-Tuning Medical Language Models for Enhanced Long-Contextual Understanding and Domain Expertise](https://openreview.net/pdf?id=dSNnyFYZwB)\n\n[337\\. Analysis of Questionnaire Results Using Metric Methods](https://www.naturalspublishing.com/files/published/f1xs21197697y3.pdf)\n\n[338\\. From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning](https://arxiv.org/pdf/2308.12032v2)\n\n[339\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[340\\. Sergey Ioffe, Christian Szegedy. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” ArXiv](https://arxiv.org/abs/1502.03167)\n\n[341\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[342\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[343\\. Yann LeCun, L. Bottou et al. “Gradient-based learning applied to document recognition.” Proc. IEEE](https://doi.org/10.1109/5.726791)\n\n[344\\. Nitish Srivastava, Geoffrey E. Hinton et al. “Dropout: a simple way to prevent neural networks from overfitting.” J. Mach. Learn. Res.](https://doi.org/10.5555/2627435.2670313)\n\n[345\\. Demystifying Large Language Model Fine-Tuning](https://blogs.dimensionless.ai/demystifying-large-language-model-fine-tuning)\n\n[346\\. Rethinking Optimization and Architecture for Tiny Language Models](https://raw.githubusercontent.com/mlresearch/v235/main/assets/tang24c/tang24c.pdf)\n\n[347\\. LoRA Dropout as a Sparsity Regularizer for Overfitting Reduction](https://openreview.net/forum?id=c4498OydLP)\n\n[348\\. Fine-Tuning Large Language Models – Challenges and Best Practices](https://www.encora.com/insights/fine-tuning-large-language-models-challenges-and-best-practices)\n\n[349\\. Weight Decay | Continuum Labs](https://training.continuumlabs.ai/training/the-fine-tuning-process/hyperparameters/weight-decay#:~:text=Start%20with%20a%20small%20value,accuracy%20on%20the%20training%20data.)\n\n[350\\. Dropout Reduces Underfitting](https://arxiv.org/pdf/2303.01500v1)\n\n[351\\. Exploiting Multilingualism and Transfer Learning for Low Resource Machine Translation](https://repository.kulib.kyoto-u.ac.jp/dspace/bitstream/2433/232411/2/djohk00663.pdf)\n\n[352\\. Cheolhyoung Lee, Kyunghyun Cho et al. “Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models.” ArXiv](https://arxiv.org/abs/1909.11299)\n\n[353\\. Dropout vs Weight Decay](https://www.geeksforgeeks.org/dropout-vs-weight-decay/)\n\n[354\\. Mastering Dropout: The Ultimate Strategy to Prevent Overfitting in Neural Networks](https://www.lunartech.ai/blog/mastering-dropout-the-ultimate-strategy-to-prevent-overfitting-in-neural-networks)\n\n[355\\. CADAid: Validation of Architectural Drawings Powered by Machine Learning](https://kartai.no/wp-content/uploads/2024/06/Bachelor_s_Thesis.pdf)\n\n[356\\. A comparison of dropout and weight decay for regularizing deep neural networks](https://scholarworks.uark.edu/cgi/viewcontent.cgi?article=1028&context=csceuht)\n\n[357\\. Preserving Diversity in Supervised Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2408.16673)\n\n[358\\. Extracting protein flexibility features from a pre-trained protein language model](https://lup.lub.lu.se/student-papers/record/9146229/file/9146235.pdf)\n\n[359\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[360\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[361\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[362\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[363\\. Control LLM: Controlled Evolution for Intelligence Retention in LLM](https://arxiv.org/html/2501.10979v1)\n\n[364\\. Jason Wei, Maarten Bosma et al. “Finetuned Language Models Are Zero-Shot Learners.” ArXiv](https://arxiv.org/abs/2109.01652)\n\n[365\\. Fine-tuning Llama For Better Performance With the MMLU Benchmark](https://osf.io/e3v5x/download)\n\n[366\\. RouteLLM: Balancing Cost and Quality in LLM Deployments](https://zilliz.com/learn/routellm-open-source-framework-for-navigate-cost-quality-trade-offs-in-llm-deployment)\n\n[367\\. Training-free LLM Merging for Multi-task Learning](https://www.arxiv.org/pdf/2506.12379)\n\n[368\\. ROUTE: ROBUST MULTITASK TUNING AND COLLABORATION FOR TEXT-TO-SQL](https://openreview.net/pdf?id=BAglD6NGy0)\n\n[369\\. HMORA: MAKING LLMs MORE EFFECTIVE WITH HIERARCHICAL MIXTURE OF LoRA EXPERTS](https://openreview.net/pdf?id=lTkHiXeuDl)\n\n[370\\. Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs](http://arxiv.org/html/2410.10739v1)\n\n[371\\. Nebuly | User-Experience for LLMs](https://www.nebuly.com/blog/metas-llama-a-small-language-model-beating-giants)\n\n[372\\. Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark](https://arxiv.org/pdf/2504.14693)\n\n[373\\. MMInstruct: a high-quality multi-modal instruction tuning dataset with extensive diversity](http://scis.scichina.com/en/2024/220103.pdf)\n\n[374\\. Exploring Text-to-SQL with Large Language Models: A Comparative Study of Claude Opus and a fine-tuned smaller-sized LLM](https://kth.diva-portal.org/smash/get/diva2:1887425/FULLTEXT01.pdf)\n\n[375\\. MMLU Benchmark of LLM Eval](https://www.bracai.eu/post/mmlu-benchmark)\n\n[376\\. Alleviating the Fear of Losing Alignment in LLM Fine-tuning](https://arxiv.org/pdf/2504.09757)\n\n[377\\. Base LLM vs. Instruction-tuned LLM](https://toloka.ai/blog/base-llm-vs-instruction-tuned-llm/#:~:text=Instruction-tuned%20LLMs,-An%20Instruction-Tuned&text=Instruction%20tuning%20involves%20supervised%20fine,conversational%20or%20task-oriented%20way.)"
  },
  {
    "id": 13,
    "question": "Why doesn't ChatGPT directly fine-tune using Reward-Model data, but instead use RLHF? Give me a more deep technical report, and focus on references to recent research papers on this topic.",
    "response": "**Research Report: RLHF vs. Direct Reward Fine-Tuning in ChatGPT**\n\n**1\\. Introduction**\n\nLarge language models (LLMs) like ChatGPT require sophisticated alignment techniques to ensure their outputs align with human preferences. While approaches like supervised fine-tuning (SFT) with reward model data appear straightforward, OpenAI employs Reinforcement Learning from Human Feedback (RLHF) as its primary alignment methodology for ChatGPT implementations post-2023 \\[67\\]\\[71\\]. This report examines the technical rationale for this choice, analyzes emerging alternatives like Direct Preference Optimization (DPO), and synthesizes findings from 2024-2025 research on their comparative performance.\n\n**2\\. Technical Limitations of Direct Reward Fine-Tuning**\n\nDirect fine-tuning using reward model data faces several fundamental limitations:\n\n**Sequence-Level Optimization Failure**: SFT operates in a \"scaling-centric\" regime optimizing token-level likelihood, failing to optimize complex sequence-level rewards inherent in human preferences \\[1\\]. This creates misalignment between model objectives and actual human judgments.\n\n**Hallucination Amplification**: Models fine-tuned directly on reward data generate hallucinated content when demonstrations exceed capabilities, especially with ambiguous prompts \\[2\\]. Unlike RLHF, SFT lacks mechanisms to penalize nuanced token-level errors proportionally \\[5\\].\n\n**Exploration Deficiency**: Behavior cloning approaches struggle with out-of-distribution prompts due to inadequate exploration capacity. They cannot incorporate negative feedback effectively, perpetuating harmful behaviors \\[12\\].\n\n**Data Quality Dependency**: Consistent high-quality demonstration data is essential but difficult to scale cost-effectively. Reward modeling inconsistencies propagate directly to model outputs without RL's regularization effects \\[5\\]\\[8\\].\n\n**3\\. RLHF Technical Architecture and Implementation Challenges**\n\nRLHF addresses these limitations through a multi-stage optimization framework:\n\n**3.1 Core Methodology**\n\n**Reward Modeling**: Human preferences train a differentiable reward function that scores output quality\n\n**Policy Optimization**: Reinforcement learning (typically PPO) fine-tunes the LLM to maximize reward scores while constrained by KL-divergence from the base model \\[22\\]\\[63\\]\n\n**3.2 Implementation Challenges (Post-2024)**\n\n**Computational Overhead**: PPO requires maintaining 4 model copies concurrently (policy, reference, reward, value models), demanding exceptional GPU memory \\[43\\]\\[54\\]. At 1B+ parameters, RLHF requires 50-70% more vRAM than DPO-equivalent implementations \\[165\\].\n\n**Training Instability**: PPO exhibits hyperparameter sensitivity and convergence issues \\[44\\]. Modern implementations use advanced stabilization techniques:\n\nReward prediction only at sequence termini\n\nToken-level reinforcement learning\n\nDistributed advantage normalization \\[57\\]\n\n**Reward Hacking**: Models exploit reward function flaws, producing exaggeratedly lengthy or formulaic responses scoring highly but aligning poorly with true preferences \\[5\\]\\[77\\]. Mitigation requires careful KL-regularization scheduling \\[343\\].\n\n**4\\. DPO Emergence and Comparative Analysis**\n\nDirect Preference Optimization (DPO) emerged as a promising alternative, directly optimizing policies from preference data without explicit reward modeling:\n\n**4.1 Technical Approach**\n\nReplaces reward modeling + RL pipeline with a single supervised objective\n\nParameterizes the optimal policy in closed form via implicit reward modeling \\[28\\]\n\nRequires only binary preference data (chosen vs. rejected responses) \\[110\\]\n\n**4.2 Quantitative Comparisons**\n\nRecent experiments under controlled conditions (identical 10B+ architectures, 2025 hardware) show:\n\n**Training Efficiency**: DPO reduces GPU-hours by 50-75% vs. PPO-based RLHF:\n\nDPO: ~6.93 hours (16x V100) vs PPO: 14.4+ hours (4x A100) for equivalent tasks \\[204\\]\\[321\\]\n\nDPO's offline preference utilization avoids costly online sampling \\[180\\]\n\n**Performance Dynamics**:\n\nMatches RLHF in sentiment control and single-turn dialogue \\[147\\]\n\nShows superior factual accuracy (58% error reduction in Llama-2) \\[29\\]\n\nDegrades sharply after overtraining due to reward overoptimization \\[265\\]\n\n**Alignment Robustness**:\n\nMore susceptible to out-of-distribution data than RLHF \\[164\\]\\[271\\]\n\nComparable reward hacking vulnerability via response pattern exploitation \\[228\\]\\[354\\]\n\n**5\\. Operational and Deployment Considerations**\n\nProduction system constraints further explain RLHF preference:\n\n**Inference Scalability**: RLHF-aligned models show more predictable inference latency profiles despite higher training costs. System entropy regularization enables better dynamic batching efficiency \\[1\\].\n\n**Hybrid Architectures**: Recent ChatGPT implementations likely combine RLHF foundations with DPO components. Asynchronous frameworks like UNA achieve 68% faster training while maintaining RLHF-quality outputs \\[384\\].\n\n**Safety Certification**: RLHF's explicit reward modeling provides auditable safety guardrails critical for enterprise deployment, despite DPO's computational advantages \\[113\\].\n\n**6\\. OpenAI's Evolving Technical Rationale**\n\nPost-2024 documentation confirms a strategic transition:\n\n**RLHF Dominance (2023-2024)**: Technical reports emphasize RLHF's necessity for aligning base models like GPT-3/4, particularly for complex multi-turn interaction \\[75\\]\\[78\\]. Production systems prioritized its sequence-level optimization capabilities despite costs.\n\n**DPO Integration (2025)**: OpenAI now employs DPO for GPT-4.1 alignment, citing \"much lower computing needs\" while maintaining quality \\[150\\]\\[184\\]. Technical whitepapers confirm DPO incorporation into ChatGPT infrastructure \\[370\\].\n\n**Hybrid Future**: Reinforcement through Optimized Preference (RTO) architectures combine DPO's efficiency with RL exploration mechanisms, signaling next-generation alignment frameworks \\[107\\].\n\n**7\\. Research Frontiers and Emerging Solutions**\n\nCurrent studies address core limitations of both approaches:\n\n**Reward Hacking Mitigation**:\n\nKL-controlled DPO \\[348\\]\n\nInformation-theoretic reward modeling \\[342\\]\n\nCausal reward structures \\[287\\]\n\n**Hybrid Architectures**:\n\nRTO: Combines DPO token optimization with PPO exploration \\[107\\]\n\nUNA: Asynchronous preference optimization \\[384\\]\n\nWPO: Stabilizes DPO against overoptimization \\[265\\]\n\n**Computational Optimization**:\n\nQuantized RLHF training for 70B+ models \\[212\\]\n\nSparse reward modeling techniques \\[281\\]\n\n**8\\. Conclusion**\n\nChatGPT relies on RLHF instead of direct reward fine-tuning due to fundamental limitations in SFT's sequence-level optimization capacity. While DPO offers compelling efficiency advantages demonstrated in 2024-2025 research, RLHF's explicit reward modeling provides critical safety alignment and auditability for production systems. OpenAI now strategically integrates DPO into newer implementations like GPT-4.1, but maintains RLHF foundations where sequence-level interaction complexity demands robust preference optimization. Hybrid frameworks emerging from recent research promise to resolve the core efficiency/alignment tradeoffs within the next generation of LLM alignment techniques.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Preserving Diversity in Supervised Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2408.16673)\n\n[2\\. Wayne Xin Zhao, Kun Zhou et al. “A Survey of Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2303.18223)\n\n[3\\. SuperHF: Supervised Iterative Learning from Human Feedback](https://openreview.net/attachment?id=Io71X_zEce&name=pdf)\n\n[4\\. A Survey on Hallucination in Large Language and Foundation Models](https://www.preprints.org/frontend/manuscript/7b49e7437a3bd104a0b6008f20667fba/download_pub)\n\n[5\\. Lecture 4: Learning from Human Feedback](http://web.stanford.edu/class/cs329x/slides/scribe_human_feedback.pdf)\n\n[6\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[7\\. RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://abdullah-mamun.com/talk/rlaif-vs.-rlhf-scaling-reinforcement-learning-from-human-feedback-with-ai-feedback/rlaif_mamun.pdf)\n\n[8\\. Policy Filtration in RLHF to Fine-Tune LLM for Code Generation](https://arxiv.org/pdf/2409.06957)\n\n[9\\. INF's Open-Source Large Language Models](https://s.infly.cn/f/img/pdf/inf_34b_tech_report.pdf)\n\n[10\\. A Self-Supervised Reinforcement Learning Approach for Fine-Tuning Large Language Models Using Cross-Attention Signals](https://arxiv.org/pdf/2502.10482)\n\n[11\\. The Accuracy Paradox in RLHF: When Better Reward Models Don’t Yield Better Language Models](https://arxiv.org/html/2410.06554v1)\n\n[12\\. Shreyas Chaudhari, Pranjal Aggarwal et al. “RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs.” ArXiv](https://doi.org/10.48550/arXiv.2404.08555)\n\n[13\\. AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks](https://openreview.net/notes/edits/attachment?id=ZK4apFYeJG&name=pdf)\n\n[14\\. Scalable Oversight for Superhuman AI via Recursive Self-Critiquing](https://arxiv.org/pdf/2502.04675)\n\n[15\\. A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities](https://journals.indexcopernicus.com/api/file/viewByFileId/2330852)\n\n[16\\. Supervised Fine-Tuning vs. RLHF: How to Choose the Right Approach to Train Your LLM](https://www.invisible.co/blog/supervised-fine-tuning-vs-rlhf-how-to-choose-the-right-approach-to-train-your-llm)\n\n[17\\. Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment](https://arxiv.org/pdf/2501.09620)\n\n[18\\. 2023年神秘而难以理解的大模型强化学习技术：RLHF PPO，DPO，以及InstructGPT，DeepSpeed-Chat，LLama2，Baichuan2的RLHF](https://zhuanlan.zhihu.com/p/662753985)\n\n[19\\. journal_club/README.md at main · hrlblab/journal_club](https://github.com/hrlblab/journal_club/blob/main/README.md)\n\n[21\\. RLHF: Reinforcement Learning using Human Feedback for Optimization of ChatGPT](https://thegrenze.com/pages/servej.php?fn=92.pdf&name=RLHF:%20Reinforcement%20Learning%20using%20Human%20Feedbackfor%20Optimization%20of%20ChatGPT&id=3076&association=GRENZE&journal=GIJET&year=2024&volume=10&issue=2)\n\n[22\\. A Comparison of LLM Fine-tuning Methods and Evaluation Metrics with Travel Chatbot Use Case](https://arxiv.org/pdf/2408.03562)\n\n[23\\. MALLOWS-DPO: FINE-TUNE YOUR LLM WITH PREFERENCE DISPERSIONS](http://www.columbia.edu/~wt2319/Mallows_DPO.pdf)\n\n[24\\. A Statistical Framework for Ranking LLM-Based Chatbots](https://openreview.net/pdf/2033d61f7ce263408060508f536df4b7b6897fad.pdf)\n\n[25\\. A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel Chatbot Use Case](https://paperreading.club/page?id=245052)\n\n[26\\. HOW TO EVALUATE REWARD MODELS FOR RLHF](https://arxiv.org/pdf/2410.14872)\n\n[27\\. EARLIER TOKENS CONTRIBUTE MORE: LEARNING DIRECT PREFERENCE OPTIMIZATION FROM TEMPORAL DECAY PERSPECTIVE](https://arxiv.org/pdf/2502.14340)\n\n[28\\. Rafael Rafailov, Archit Sharma et al. “Direct Preference Optimization: Your Language Model is Secretly a Reward Model.” ArXiv](https://doi.org/10.48550/arXiv.2305.18290)\n\n[29\\. Reducing LLM Hallucinations: A Developer's Guide](https://www.getzep.com/ai-agents/reducing-llm-hallucinations/)\n\n[30\\. 小模型当老师效果更好：借助RLTs方法7B参数击败671B，训练成本暴降99%](https://developer.aliyun.com/article/1669047)\n\n[41\\. RLHF Workflow: From Reward Modeling to Online RLHF](https://arxiv.org/pdf/2405.07863)\n\n[42\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[43\\. Putting RL back in RLHF with RLOO](https://github.com/danielkorat/blog/blob/d95ddfdb68221ea31246858e1b3f63a61a4cb1ba/putting_rl_back_in_rlhf_with_rloo.md)\n\n[44\\. DPO Meets PPO: Reinforced Token Optimization for RLHF](https://openreview.net/pdf?id=gtFG2tBREa)\n\n[45\\. REINFORCE++: A SIMPLE AND EFFICIENT APPROACH FOR ALIGNING LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2501.03262)\n\n[46\\. UNA: UNIFYING ALIGNMENTS OF RLHF/PPO, DPO AND KTO BY A GENERALIZED IMPlicit REWARD FUNCTION](https://arxiv.org/pdf/2408.15339)\n\n[47\\. RLHF 工作流程：从奖励模型到在线 RLHF](https://yiyibooks.cn/__trs__/arxiv/2405.07863v1/index.html)\n\n[48\\. REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models](https://arxiv.org/html/2501.03262v1)\n\n[49\\. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](https://arxiv.org/pdf/2405.11143)\n\n[50\\. Deep Reinforcement Learning with Python: RLHF for Chatbots and Large Language Models, Second Edition](https://download.bibis.ir/Books/Artificial-Intelligence/Deep-Learning/2024/Deep%20Reinforcement%20Learning%20with%20Python_bibis.ir.pdf)\n\n[51\\. Interpreting Learned Feedback Patterns in Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/403bf224290de69c7d5dc856f5a99d9e-Paper-Conference.pdf)\n\n[52\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[53\\. Shengyi Huang, Michael Noukhovitch et al. “The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization.” ArXiv](https://doi.org/10.48550/arXiv.2403.17031)\n\n[54\\. 将强化学习重新引入 RLHF](https://github.com/huggingface/blog/blob/1cab471b5d0a7e432c4b133a647a4962a6976848/zh/putting_rl_back_in_rlhf_with_rloo.md)\n\n[55\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[56\\. RL 是 LLM 的新范式](https://www.explinks.com/blog/wx-rl-is-the-new-paradigm-of-llm/)\n\n[57\\. OpenRLHF：易于使用、可扩展且高性能的 RLHF 框架](https://yiyibooks.cn/__trs__/arxiv/2405.11143v1/index.html)\n\n[61\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[62\\. RLHF: Reinforcement Learning using Human Feedback for Optimization of ChatGPT](https://thegrenze.com/pages/servej.php?fn=92.pdf&name=RLHF:%20Reinforcement%20Learning%20using%20Human%20Feedbackfor%20Optimization%20of%20ChatGPT&id=3076&association=GRENZE&journal=GIJET&year=2024&volume=10&issue=2)\n\n[63\\. CHATGPT, AI LARGE LANGUAGE MODELS, AND LAW](https://fordhamlawreview.org/wp-content/uploads/2024/03/Vol.-92_Surden-1941-1972.pdf)\n\n[64\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[65\\. ChatGPT’s One-year Anniversary: Are Open-Source Large Language Models Catching up?](https://ai.radensa.ru/wp-content/uploads/2023/12/2311.16989.pdf)\n\n[66\\. ChatGPT and Generative AI Tools](https://www.tgs.com/hubfs/Technical%20Library/Technical%20Library%20Files/industry_insights2023_01_chatgpt_ai_final.pdf)\n\n[67\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[68\\. The Path to AGI: Technical Milestones, Philosophical Debates, and Societal Implications](https://cdn.prod.website-files.com/67343452a4bb784798d28a97/67a2dc57414396e94ea504d8_The%20Path%20to%20AGI.pdf)\n\n[69\\. Implement Generative AI Tools in Analytics](https://archive.nyu.edu/bitstream/2451/69533/2/Implement%20Generative%20AI%20Tools%20in%20Analytics%20-%20Sike%20Shou.pdf)\n\n[70\\. Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196v3)\n\n[71\\. ChatGPT研究框架(2023)](http://www.ecconsortium.org/Uploads/file/20230213/1676282819756261.pdf)\n\n[72\\. Life after DPO](https://web.stanford.edu/class/cs224n/slides/cs224n-spr2024-lecture15-life-after-dpo-lambert.pdf)\n\n[73\\. A Survey on Hallucination in Large Language and Foundation Models](https://www.preprints.org/frontend/manuscript/7b49e7437a3bd104a0b6008f20667fba/download_pub)\n\n[74\\. Iterative improvements from feedback for language models](https://www.scienceopen.com/document_file/c8025097-c778-459a-95fa-1f1b62b86464/ScienceOpenPreprint/RLLMs.pdf)\n\n[75\\. OPENCHAT: ADVANCING OPEN-SOURCE LANGUAGE MODELS WITH MIXED-QUALITY DATA](https://openreview.net/pdf?id=AOJyfhWYHf)\n\n[76\\. 【银河计算机】计算机行业2023年中期策略报告：跨越奇点，人工智能全景投资框架](https://bigdata-s3.wmcloud.com/researchreport/2023-07/72ff3d02d60a875a553505da4b45389d.pdf)\n\n[77\\. OpenAI的秘密武器、ChatGPT背后功臣RLHF，被开源了](https://zhuanlan.zhihu.com/p/691185761)\n\n[78\\. RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://abdullah-mamun.com/talk/rlaif-vs.-rlhf-scaling-reinforcement-learning-from-human-feedback-with-ai-feedback/rlaif_mamun.pdf)\n\n[79\\. Exploring the Potential of Artificial Intelligence for Instruction in Pragmatics](https://www.castledown.com/journals/tltl/article/view/tltl.v6n3.1521/688)\n\n[80\\. OpenAI's ChatGPT Vs Google's LaMDA](https://www.theinsaneapp.com/2022/12/chatgpt-vs-lamda.html)\n\n[81\\. CS 696 Applied Large Language Models](https://eli.sdsu.edu/courses/spring25/cs696/notes/D15%20Hooks,%20Continually%20Pre-train,%20DPO.pdf)\n\n[82\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[83\\. DPO Meets PPO: Reinforced Token Optimization for RLHF](https://arxiv.org/pdf/2404.18922)\n\n[84\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[85\\. WEAK-TO-STRONG PREFERENCE OPTIMIZATION: STEALING REWARD FROM WEAK ALIGNED MODEL](https://openreview.net/pdf/f6c72302b5985b222f1213927b75bedfe2d64f05.pdf)\n\n[86\\. WPO: Enhancing RLHF with Weighted Preference Optimization](https://arxiv.org/pdf/2406.11827)\n\n[87\\. Can RLHF be More Efficient with Imperfect Reward Models? A Policy Coverage Perspective](https://arxiv.org/pdf/2502.19255)\n\n[88\\. Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF](https://users.ece.cmu.edu/~yuejiec/papers/VPO.pdf)\n\n[89\\. RLHF in 2024 with DPO & Hugging Face](https://www.philschmid.de/dpo-align-llms-in-2024-with-trl)\n\n[90\\. Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer](https://arxiv.org/pdf/2405.16436)\n\n[91\\. DPO-Shift: Shifting the Distribution of Direct Preference Optimization](https://arxiv.org/pdf/2502.07599)\n\n[92\\. Life after DPO](https://web.stanford.edu/class/cs224n/slides/cs224n-spr2024-lecture15-life-after-dpo-lambert.pdf)\n\n[93\\. Multi-modal preference alignment remedies regression of visual instruction tuning on language model](http://arxiv.org/html/2402.10884v1)\n\n[94\\. Robust Preference Optimization through Reward Model Distillation](https://arxiv.org/pdf/2405.19316)\n\n[95\\. MODEL EDITING AS A ROBUST AND DENOISED VARIANT OF DPO: A CASE STUDY ON TOXICITY](https://openreview.net/pdf/028aaa9270c6160e41479d39a2d6a02eaf95964d.pdf)\n\n[96\\. New Desiderata for Direct Preference Optimization](https://openreview.net/pdf?id=Fgf0iAOb22)\n\n[97\\. 一些RLHF的平替汇总](https://www.ainavpro.com/6788.html)\n\n[98\\. Where 2024’s “open GPT4” can’t match OpenAI’s](https://www.interconnects.ai/p/open-gpt4-limitations)\n\n[99\\. Yuntao Bai, Andy Jones et al. “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2204.05862)\n\n[101\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[102\\. RLHF与DPO技术在大型语言模型中的应用与挑战](https://www.yicaiai.com/news/article/677f2d514ddd79f11a1a7cc6)\n\n[103\\. How To Build A Large Language Model](https://apxml.com/courses/how-to-build-a-large-language-model/chapter-27-model-compression-techniques/weight-quantization-int8-int4)\n\n[104\\. GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization](https://openreview.net/pdf/ca536d5d71ad6750460557d4f26f42368e272282.pdf)\n\n[105\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[106\\. 3D-PROPERTIES: IDENTIFYING CHALLENGES IN DPO AND CHARTING A PATH FORWARD](https://openreview.net/notes/edits/attachment?id=JijWkFGLHn&name=pdf)\n\n[107\\. DPO Meets PPO: Reinforced Token Optimization for RLHF](https://arxiv.org/pdf/2404.18922)\n\n[108\\. DPO 符合 PPO：针对 RLHF 的强化令牌优化](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_4_30/2404.18922.pdf)\n\n[109\\. LLaMafia/llamafia.github：关于DPO、RLHF、Embedding共享及多任务训练的讨论](https://github.com/LLaMafia/llamafia.github/blob/main/Log/20231220.md)\n\n[110\\. RLHF vs. DPO: Comparing LLM Feedback Methods](https://llmmodels.org/blog/rlhf-vs-dpo-comparing-llm-feedback-methods/)\n\n[111\\. Natural Language Processing: Neural Networks and Large Language Models](https://opensource.niutrans.com/nlpbook/chapters/nlp-book-chapter10.pdf)\n\n[112\\. Deep Reinforcement Learning with Python: RLHF for Chatbots and Large Language Models, Second Edition](https://download.bibis.ir/Books/Artificial-Intelligence/Deep-Learning/2024/Deep%20Reinforcement%20Learning%20with%20Python_bibis.ir.pdf)\n\n[113\\. Reinforcement Learning for LLMs: RLHF, DPO, and the Future of Aligned Generative AI](https://www.inferless.com/learn/a-deep-dive-into-reinforcement-learning)\n\n[114\\. Rafael Rafailov, Archit Sharma et al. “Direct Preference Optimization: Your Language Model is Secretly a Reward Model.” ArXiv](https://doi.org/10.48550/arXiv.2305.18290)\n\n[115\\. Jean Kaddour, J. Harris et al. “Challenges and Applications of Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2307.10169)\n\n[116\\. Current and future state of evaluation of large language models for medical summarization tasks](https://www.nature.com/articles/s44401-024-00011-2.pdf)\n\n[117\\. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](https://arxiv.org/pdf/2405.11143v2)\n\n[121\\. DPO Meets PPO: Reinforced Token Optimization for RLHF](https://arxiv.org/pdf/2404.18922)\n\n[122\\. 3D-PROPERTIES: IDENTIFYING CHALLENGES IN DPO AND CHARTING A PATH FORWARD](https://openreview.net/notes/edits/attachment?id=JijWkFGLHn&name=pdf)\n\n[123\\. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](https://arxiv.org/pdf/2405.11143)\n\n[124\\. RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness](https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness_CVPR_2025_paper.pdf)\n\n[125\\. Policy Filtration in RLHF to Fine-Tune LLM for Code Generation](https://arxiv.org/pdf/2409.06957)\n\n[126\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[127\\. The Basics of Reinforcement Learning from Human Feedback](https://rlhfbook.com/book.pdf)\n\n[128\\. International Scientific Report on the Safety of Advanced AI: Interim Report](https://www.developmentaid.org/api/frontend/cms/file/2024/05/international_scientific_report_on_the_safety_of_advanced_ai_interim_report.pdf)\n\n[129\\. Understanding the Effects of RLHF on LLM Generalisation and Diversity](https://openreview.net/pdf?id=Bc3S2G1PxH)\n\n[130\\. ChatGPT发展历程、原理、技术架构详解和产业未来](https://yi.tips/wp-content/uploads/2023/06/1e22397bc9174737.pdf)\n\n[131\\. TOWARDS UNDERSTANDING SAFETY ALIGNMENT: A MECHANISTIC PERSPECTIVE FROM SAFETY NEURONS](https://openreview.net/pdf?id=1NkrxqY4jK)\n\n[132\\. RLHF Workflow: From Reward Modeling to Online RLHF](https://arxiv.org/pdf/2405.07863)\n\n[133\\. A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications](https://arxiv.org/pdf/2410.15595)\n\n[134\\. Programmatically scaling human preferences and alignment in GenAI](https://go.snorkel.ai/rs/979-SZB-034/images/hoang-tran-programmatically-scale-human-preferences-and-alignment-in-genai_slides.pdf)\n\n[135\\. Inside GPT-4.1: AI Breakthroughs Unveiled](https://content.trickle.so/blog/inside-gpt-4-1-technical-analysis)\n\n[136\\. Awesome-LLM-Agents: A Collection of High Quality research papers and open-source projects about LLM-agents](https://github.com/junhua/awesome-llm-agents)\n\n[137\\. Where 2024’s “open GPT4” can’t match OpenAI’s](https://www.interconnects.ai/p/open-gpt4-limitations)\n\n[138\\. RLHF与PPO实现细节的复现与分析](https://github.com/ridhachahed/blog/blob/9661aad011f68b3890fda3d59f2ccebbd5e277d0/the_n_implementation_details_of_rlhf_with_ppo.md)\n\n[139\\. Jinghan Zhang, Xiting Wang et al. “Prototypical Reward Network for Data-Efficient RLHF.” ArXiv](https://doi.org/10.48550/arXiv.2406.06606)\n\n[140\\. P. Christiano, J. Leike et al. “Deep Reinforcement Learning from Human Preferences.” ArXiv](https://arxiv.org/abs/1706.03741)\n\n[141\\. Lecture 9: RLHF and Guest Lecture on DPO](https://web.stanford.edu/class/cs234/slides/lecture9pre.pdf)\n\n[142\\. DPO技术在RLHF中的应用与开源实现](https://github.com/mcdmag/airllm/blob/main/rlhf/README.md)\n\n[143\\. Contrastive Post-training Large Language Models on Data Curriculum](https://openreview.net/pdf?id=mmSmQ0gNyZ)\n\n[144\\. DPO Meets PPO: Reinforced Token Optimization for RLHF](https://openreview.net/pdf?id=gtFG2tBREa)\n\n[145\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[146\\. Programmatically scaling human preferences and alignment in GenAI](https://go.snorkel.ai/rs/979-SZB-034/images/hoang-tran-programmatically-scale-human-preferences-and-alignment-in-genai_slides.pdf)\n\n[147\\. Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://openreview.net/pdf?id=HPuSIXJaa9)\n\n[148\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[149\\. 一些RLHF的平替汇总](https://www.ainavpro.com/6788.html)\n\n[150\\. Inside GPT-4.1: AI Breakthroughs Unveiled](https://content.trickle.so/blog/inside-gpt-4-1-technical-analysis)\n\n[151\\. DPO 符合 PPO：针对 RLHF 的强化令牌优化](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_4_30/2404.18922.pdf)\n\n[152\\. Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](https://arxiv.org/pdf/2404.10719)\n\n[153\\. 使用PyTorch实现GPT-2直接偏好优化训练：DPO方法改进及其与监督微调的效果对比](https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng%3D%3D&mid=2247650788&idx=2&sn=345cd67770c27f65be1a0395e8392b65&chksm=e9ef846fde980d79f22db8c19c788d0ef438df7386065d4bf216bbcaca23cc01b995b796c673&scene=27)\n\n[154\\. Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer](https://openreview.net/pdf?id=PBff7aUytY)\n\n[155\\. ASYNCHRONOUS RLHF: FASTER AND MORE EFFICIENT OFF-POLICY RL FOR LANGUAGE MODELS](https://openreview.net/pdf?id=FhTAG591Ve)\n\n[156\\. 微调实操四:直接偏好优化方法-DPO](https://zhuanlan.zhihu.com/p/684050787)\n\n[157\\. FASTER, MORE EFFICIENT RLHF THROUGH OFF-POLICY ASYNCHRONOUS LEARNING](https://openreview.net/pdf?id=ND3io3eses)\n\n[158\\. WPO: Enhancing RLHF with Weighted Preference Optimization](https://arxiv.org/pdf/2406.11827)\n\n[161\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[162\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[163\\. FASTER, MORE EFFICIENT RLHF THROUGH OFF-POLICY ASYNCHRONOUS LEARNING](https://openreview.net/pdf?id=ND3io3eses)\n\n[164\\. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](https://arxiv.org/pdf/2405.11143v2)\n\n[165\\. Putting RL back in RLHF with RLOO](https://github.com/danielkorat/blog/blob/d95ddfdb68221ea31246858e1b3f63a61a4cb1ba/putting_rl_back_in_rlhf_with_rloo.md)\n\n[166\\. One-Shot Safety Alignment for Large Language Models via Optimal Dualization](https://arxiv.org/pdf/2405.19544)\n\n[167\\. CODEFUSION: A Pre-trained Diffusion Model for Code Generation](https://www.microsoft.com/en-us/research/uploads/prod/2023/11/CodeFusion-Revised-CameraReady.pdf)\n\n[168\\. ASYNCHRONOUS RLHF: FASTER AND MORE EFFICIENT OFF-POLICY RL FOR LANGUAGE MODELS](https://openreview.net/pdf?id=FhTAG591Ve)\n\n[169\\. UNA: UNIFYING ALIGNMENTS OF RLHF/PPO, DPO AND KTO BY A GENERALIZED IMPlicit REWARD FUNCTION](https://arxiv.org/pdf/2408.15339)\n\n[170\\. P. Christiano, J. Leike et al. “Deep Reinforcement Learning from Human Preferences.” ArXiv](https://arxiv.org/abs/1706.03741)\n\n[171\\. Yuntao Bai, Andy Jones et al. “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2204.05862)\n\n[172\\. Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU](https://github.com/merico34/Huggingface-blog/blob/2cd3a4ac20fa6e03b464118c22e61bc965b8465c/trl-peft.md)\n\n[173\\. Michael Santacroce, Yadong Lu et al. “Efficient RLHF: Reducing the Memory Usage of PPO.” ArXiv](https://doi.org/10.48550/arXiv.2309.00754)\n\n[174\\. Nisan Stiennon, Long Ouyang et al. “Learning to summarize from human feedback.” ArXiv](https://arxiv.org/abs/2009.01325)\n\n[175\\. DPO Meets PPO: Reinforced Token Optimization for RLHF](https://arxiv.org/pdf/2404.18922)\n\n[176\\. FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data](https://arxiv.org/pdf/2408.06273)\n\n[177\\. Optimizing Systems for Deep Learning Applications](https://vtechworks.lib.vt.edu/bitstreams/cff9c2b6-cb4f-44b1-ac48-f20084ed746b/download)\n\n[178\\. Deep Reinforcement Learning with Python: RLHF for Chatbots and Large Language Models, Second Edition](https://download.bibis.ir/Books/Artificial-Intelligence/Deep-Learning/2024/Deep%20Reinforcement%20Learning%20with%20Python_bibis.ir.pdf)\n\n[179\\. RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models](https://openreview.net/pdf/dadceb4d5d940e6b53d49e1c1329ae15c09cd064.pdf)\n\n[180\\. Robust RLHF with Noisy Rewards](https://openreview.net/pdf/bc3b7d016bd220ff4589b4eac1c9bfd5f4f01e01.pdf)\n\n[181\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[182\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[183\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[184\\. Inside GPT-4.1: AI Breakthroughs Unveiled](https://content.trickle.so/blog/inside-gpt-4-1-technical-analysis)\n\n[185\\. Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment](https://openreview.net/pdf?id=xmFHPCU1rY)\n\n[186\\. P. Christiano, J. Leike et al. “Deep Reinforcement Learning from Human Preferences.” ArXiv](https://arxiv.org/abs/1706.03741)\n\n[187\\. The Basics of Reinforcement Learning from Human Feedback](https://rlhfbook.com/book.pdf)\n\n[188\\. Policy Filtration in RLHF to Fine-Tune LLM for Code Generation](https://arxiv.org/pdf/2409.06957)\n\n[189\\. Yuntao Bai, Andy Jones et al. “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2204.05862)\n\n[190\\. ChatGPT’s One-year Anniversary: Are Open-Source Large Language Models Catching up?](https://ai.radensa.ru/wp-content/uploads/2023/12/2311.16989.pdf)\n\n[191\\. UNA: UNIFYING ALIGNMENTS OF RLHF/PPO, DPO AND KTO BY A GENERALIZED IMPlicit REWARD FUNCTION](https://arxiv.org/pdf/2408.15339)\n\n[192\\. 3D-PROPERTIES: IDENTIFYING CHALLENGES IN DPO AND CHARTING A PATH FORWARD](https://openreview.net/notes/edits/attachment?id=JijWkFGLHn&name=pdf)\n\n[193\\. TOWARDS UNDERSTANDING SAFETY ALIGNMENT: A MECHANISTIC PERSPECTIVE FROM SAFETY NEURONS](https://openreview.net/pdf?id=1NkrxqY4jK)\n\n[194\\. Generative Artificial Intelligence in the Energy Sector](https://epub.uni-bayreuth.de/7674/1/GenAI-in-the-Energy-Sector.pdf)\n\n[195\\. ChatGPT：开启AI新纪元](https://file.iyanbao.com/pdf/673a6-d61e0eca-b442-462c-9e85-2adb2249983d.pdf)\n\n[196\\. RLHF progress: Scaling DPO to 70B, DPO vs PPO update, Tülu 2, Zephyr-β, meaningful evaluation, data contamination](https://www.interconnects.ai/p/rlhf-progress-scaling-dpo-to-70b)\n\n[197\\. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](https://arxiv.org/pdf/2405.11143v2)\n\n[198\\. 大语言模型对齐方法的比较研究](https://yiyibooks.cn/__trs__/arxiv/2404.10719v1/index.html)\n\n[199\\. Han Zhong, Guhao Feng et al. “DPO Meets PPO: Reinforced Token Optimization for RLHF.” ArXiv](https://doi.org/10.48550/arXiv.2404.18922)\n\n[200\\. Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](https://openreview.net/pdf?id=6XH8R7YrSk)\n\n[201\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[202\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[203\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[204\\. Reward Difference Optimization For Sample Reweighting In Offline RLHF](https://arxiv.org/pdf/2408.09385)\n\n[205\\. Rafael Rafailov, Archit Sharma et al. “Direct Preference Optimization: Your Language Model is Secretly a Reward Model.” ArXiv](https://doi.org/10.48550/arXiv.2305.18290)\n\n[206\\. A LONG WAY TO GO: INVESTIGATING LENGTH CORRELATIONS IN RLHF](https://openreview.net/pdf?id=sNtDKdcI1f)\n\n[207\\. Nisan Stiennon, Long Ouyang et al. “Learning to summarize from human feedback.” ArXiv](https://arxiv.org/abs/2009.01325)\n\n[208\\. GPT-4引领认知革命 Deep Speed加速行业发展——GPT系列专题之二](https://pdf.dfcfw.com/pdf/H301_AP202304281586021136_1.pdf)\n\n[209\\. Delve into PPO: Implementation Matters for Stable RLHF](https://openreview.net/pdf?id=rxEmiOEIFL)\n\n[210\\. GitHub - K-Wu/llm-analysis: Latency and Memory Analysis of Transformer Models for Training and Inference](https://github.com/K-Wu/llm-analysis)\n\n[211\\. FASTER, MORE EFFICIENT RLHF THROUGH OFF-POLICY ASYNCHRONOUS LEARNING](https://openreview.net/pdf?id=ND3io3eses)\n\n[212\\. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](https://arxiv.org/pdf/2405.11143)\n\n[213\\. The Importance of Online Data: Understanding Preference Fine-tuning via Coverage](https://proceedings.neurips.cc/paper_files/paper/2024/file/16c628ab12dc4caca8e7712affa6c767-Paper-Conference.pdf)\n\n[214\\. Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer](https://arxiv.org/pdf/2405.16436)\n\n[215\\. RLHF in 2024 with DPO & Hugging Face](https://www.philschmid.de/dpo-align-llms-in-2024-with-trl)\n\n[216\\. LIONS: An Empirically Optimized Approach to Align Language Models](https://arxiv.org/pdf/2407.06542)\n\n[217\\. Optimizing Systems for Deep Learning Applications](https://vtechworks.lib.vt.edu/bitstreams/cff9c2b6-cb4f-44b1-ac48-f20084ed746b/download)\n\n[221\\. Reward Shaping to Mitigate Reward Hacking in RLHF](https://www.arxiv.org/pdf/2502.18770)\n\n[222\\. Natural Language Processing: Neural Networks and Large Language Models](https://opensource.niutrans.com/nlpbook/chapters/nlp-book-chapter10.pdf)\n\n[223\\. 大语言模型对齐方面DPO优于PPO吗？ 综合研究](https://yiyibooks.cn/__trs__/arxiv/2404.10719v2/index.html)\n\n[224\\. 大语言模型对齐方法的比较研究](https://yiyibooks.cn/__trs__/arxiv/2404.10719v1/index.html)\n\n[225\\. Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment](https://arxiv.org/pdf/2501.09620)\n\n[226\\. Diversity-Oriented Data Augmentation with Large Language Models](https://www.arxiv.org/pdf/2502.11671)\n\n[227\\. IMPROVING CODE QUALITY USING FINE-TUNING LARGE LANGUAGE MODELS](https://trepo.tuni.fi/bitstream/handle/10024/161629/NguyenDuc.pdf?sequence=2&isAllowed=y)\n\n[228\\. A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications](https://arxiv.org/pdf/2410.15595)\n\n[229\\. PerPO: Perceptual Preference Optimization via Discriminative Rewarding](https://arxiv.org/pdf/2502.04371)\n\n[230\\. DPO Meets PPO: Reinforced Token Optimization for RLHF](https://arxiv.org/pdf/2404.18922)\n\n[231\\. RLHF vs. DPO: Comparing LLM Feedback Methods](https://llmmodels.org/blog/rlhf-vs-dpo-comparing-llm-feedback-methods/)\n\n[232\\. 通过大型语言模型的视角探索自主代理：综述](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_4_9/2404.04442.pdf)\n\n[233\\. ROBUST RLHF WITH NOISY REWARDS](https://openreview.net/pdf?id=Cfbr56K4gp)\n\n[234\\. ODIN: Disentangled Reward Mitigates Hacking in RLHF](https://openreview.net/pdf?id=zcIV8OQFVF)\n\n[235\\. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees](https://proceedings.neurips.cc/paper_files/paper/2024/file/c0f7ee1901fef1da4dae2b88dfd43195-Paper-Conference.pdf)\n\n[236\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[237\\. RLHF and DPO Compared](https://crowdworks.blog/en/rlhf-and-dpo-compared/)\n\n[241\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[242\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[243\\. ChatGPT：开启AI新纪元](https://file.iyanbao.com/pdf/673a6-d61e0eca-b442-462c-9e85-2adb2249983d.pdf)\n\n[244\\. Artificial Intelligence: Legal Reasoning, Legal Research and Legal Writing](https://scholarship.law.umn.edu/cgi/viewcontent.cgi?article=1566&context=mjlst)\n\n[245\\. International AI Safety Report](http://maruyama-mitsuhiko.cocolog-nifty.com/security/files/international_ai_safety_report_2025_accessible_f.docx)\n\n[246\\. OpenAI's Revenue Triples to $12.7 Billion in 2025](https://www.ainvest.com/news/openai-revenue-triples-12-7-billion-2025-2507/)\n\n[247\\. 2025年业界AI智能体技术进展与趋势报告](https://www.51cto.com/aigc/6349.html)\n\n[248\\. AL2025_03 Time Bandit ChatGPT Jailbreak: A New AI Vulnerability Bypasses Safeguards (30th January 2025)](https://cirt.gy/static/e379c8ed28b774db9f81bc544117b60c/AL2025_03-Time-Bandit-ChatGPT-Jailbreak-A-New-AI-Vulnerability-Bypasses-Safeguards.pdf)\n\n[249\\. Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning](https://arxiv.org/pdf/2310.09430.pdf)\n\n[250\\. Ovis：多模态大语言模型的结构嵌入对齐](https://www.yiyibooks.cn/__trs__/arxiv/2405.20797v2/index.html)\n\n[251\\. A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223v12)\n\n[252\\. Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision](https://yiyibooks.cn/__trs__/arxiv/2403.09472v1/index.html)\n\n[253\\. Multiscale Positive-Unlabeled Detection of AI-Generated Texts](https://arxiv.org/pdf/2305.18149v3.pdf)\n\n[254\\. OpenAI tailors version of ChatGPT for US government](https://techxplore.com/news/2025-01-openai-tailors-version-chatgpt.pdf)\n\n[255\\. ChatGPT的演变与模型比较](https://www.scrumlaunch.com/blog/openai-gpt-models-differences#:~:text=OpenAI%20claims%20GPT-4.1%20outperforms,at%20writing%20clean,%20accurate%20code.)\n\n[256\\. FLIP 推理挑战](https://www.xueshuxiangzi.com/downloads/2025_4_17/2504.12256.pdf)\n\n[257\\. AI Content Writer – Generate Content with AI](https://cn.wordpress.org/plugins/?p=220005)\n\n[258\\. AI Trends Report 2025](http://www.hkdca.com/wp-content/uploads/2025/05/ai-trends-report-2025-statworx.pdf)\n\n[259\\. Hardening Azure OpenAI Service for Enterprise Usage](https://www.theseus.fi/bitstream/10024/818205/2/Ots_Karl.pdf)\n\n[260\\. OpenAI’s ‘o3-mini’ is free for all users — what you need to know](https://www.tomsguide.com/ai/chatgpt/openais-o3-mini-is-free-for-all-users-what-you-need-to-know)\n\n[261\\. ASYNCHRONOUS RLHF: FASTER AND MORE EFFICIENT OFF-POLICY RL FOR LANGUAGE MODELS](https://openreview.net/pdf/d0759a26adbc702480d4dafff3b0bc31aa5c6240.pdf)\n\n[262\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[263\\. Programmatically scaling human preferences and alignment in GenAI](https://go.snorkel.ai/rs/979-SZB-034/images/hoang-tran-programmatically-scale-human-preferences-and-alignment-in-genai_slides.pdf)\n\n[264\\. Papers](https://lamda-rl.nju.edu.cn/papers.html)\n\n[265\\. WPO: Enhancing RLHF with Weighted Preference Optimization](https://arxiv.org/pdf/2406.11827)\n\n[266\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[267\\. DPO Meets PPO: Reinforced Token Optimization for RLHF](https://arxiv.org/pdf/2404.18922)\n\n[268\\. 3D-PROPERTIES: IDENTIFYING CHALLENGES IN DPO AND CHARTING A PATH FORWARD](https://openreview.net/pdf/d8fc75d6a30dce23f6009692599728270c04da37.pdf)\n\n[269\\. RAINBOW PO: A UNIFIED FRAMEWORK FOR COMBINING IMPROVEMENTS IN PREFERENCE OPTIMIZATION](http://www.columbia.edu/~wt2319/RainbowPO.pdf)\n\n[270\\. Software Engineering and its Automation with Large Language Models](https://www.mn.uio.no/ifi/forskning/aktuelt/arrangementer/disputaser/2024/grishina---software-engineering-and-its-automation-with-large-language-models---phd-thesis-2024-published.pdf)\n\n[271\\. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](https://arxiv.org/pdf/2405.11143v2)\n\n[272\\. Can RLHF be More Efficient with Imperfect Reward Models? A Policy Coverage Perspective](https://arxiv.org/pdf/2502.19255)\n\n[273\\. FASTER, MORE EFFICIENT RLHF THROUGH OFF-POLICY ASYNCHRONOUS LEARNING](https://openreview.net/pdf?id=ND3io3eses)\n\n[274\\. Training Large Language Models: Evolution from RLHF to DPO](https://themissingprompt.com/how-to-train-a-large-language-model-llm/)\n\n[275\\. Does RLHF Scale? Exploring the Impacts from Data, Model, and Method](http://arxiv.org/html/2412.06000v1)\n\n[276\\. P. Christiano, J. Leike et al. “Deep Reinforcement Learning from Human Preferences.” ArXiv](https://arxiv.org/abs/1706.03741)\n\n[281\\. Reward Shaping to Mitigate Reward Hacking in RLHF](https://www.arxiv.org/pdf/2502.18770)\n\n[282\\. Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking](https://arxiv.org/html/2403.03185v4)\n\n[283\\. DPO Meets PPO: Reinforced Token Optimization for RLHF](https://arxiv.org/pdf/2404.18922)\n\n[284\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[285\\. A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications](https://arxiv.org/pdf/2410.15595)\n\n[286\\. The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking](http://arxiv.org/pdf/2501.19358v1.pdf?ref=applied-gai-in-security.ghost.io)\n\n[287\\. Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment](https://arxiv.org/pdf/2501.09620)\n\n[288\\. Diversity-Oriented Data Augmentation with Large Language Models](https://www.arxiv.org/pdf/2502.11671)\n\n[289\\. ONLINE PREFERENCE ALIGNMENT FOR LANGUAGE MODELS VIA COUNT-BASED EXPLORATION](https://openreview.net/pdf?id=cfKZ5VrhXt)\n\n[290\\. Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against Reward Hacking](https://arxiv.org/pdf/2412.09544)\n\n[291\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[292\\. Reinforcement Learning for LLMs: RLHF, DPO, and the Future of Aligned Generative AI](https://www.inferless.com/learn/a-deep-dive-into-reinforcement-learning)\n\n[293\\. GitHub - zkshan2002/RTO](https://github.com/zkshan2002/RTO)\n\n[301\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[302\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[303\\. THE ALIGNMENT PROBLEM FROM A DEEP LEARNING PERSPECTIVE](https://openreview.net/notes/edits/attachment?id=AJsyxlvQDy&name=pdf)\n\n[304\\. DeepSeek-R1 vs. ChatGPT: Assessing the Titans of Next-Generation AI Linguistic Models](https://www.jneonatalsurg.com/index.php/jns/article/download/4153/3571/16455)\n\n[305\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[306\\. A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223v12)\n\n[307\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[308\\. 100 ChatGPT Statistics to Know in 2025 & Its Future Trends](https://intelliarts.com/blog/chatgpt-statistics/)\n\n[309\\. OpenAI: The ChatGPT creator leading large AI commercialization](https://assets.ctfassets.net/f1df9zr7wr1a/3nuzCi7kvmsNO3dfZ7Q3qo/6e42223b763ed28e4314d4c370ab8351/OpenAI__The_ChatGPT_creator_leading_large_AI_commercialization.docx)\n\n[310\\. Aakanksha Chowdhery, Sharan Narang et al. “PaLM: Scaling Language Modeling with Pathways.” J. Mach. Learn. Res.](https://arxiv.org/abs/2204.02311)\n\n[311\\. ALEXANDER SCIABIN'S ROLU IN THE DEVELOPMENT OF THE RUSSIAN PIANISM SCHOOL](https://zenodo.org/records/15115723/files/Madrid.Spain-18.pdf?download=1)\n\n[312\\. ChatGPT：开启AI新纪元](https://file.iyanbao.com/pdf/673a6-d61e0eca-b442-462c-9e85-2adb2249983d.pdf)\n\n[313\\. The Road to ChatGPT](https://u3astilbaai.org/ai/documents/Presentation.pdf)\n\n[314\\. All The ChatGPT Updates & Timeline](https://autogpt.net/chatgpt-updates/)\n\n[315\\. ChatGPT Online - The Best AI ChatBot Powered by OpenAI](https://cgptonline.tech/)\n\n[316\\. ChatGPT Updates](https://www.dhiwise.com/post/chatgpt-updates-timeline-features-and-impact)\n\n[317\\. 第41回日本麻醉・集中治療テクノロジー学会](https://jsta.net/pdf/2024.pdf)\n\n[318\\. ChatGPT vs Google Assistant: Productivity, Content & Smart Device Comparison (2025)](https://www.ranktracker.com/blog/chatgpt-vs-google-assistant/#:~:text=Conversational%20AI%20isn't%20just,,%20and%20search-driven%20tasks.)\n\n[319\\. The Role of Artificial Intelligence in Learning & Development: Understanding ChatGPT A Quick Reference](https://cete.osu.edu/wp-content/uploads/ChatGPT-Handout.pdf)\n\n[320\\. The Journal of Computing Sciences in Colleges](https://www.ccsc.org/publications/journals/EA2023.pdf)\n\n[321\\. ASYNCHRONOUS RLHF: FASTER AND MORE EFFICIENT OFF-POLICY RL FOR LANGUAGE MODELS](https://openreview.net/pdf/d0759a26adbc702480d4dafff3b0bc31aa5c6240.pdf)\n\n[322\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[323\\. UNA: UNIFYING ALIGNMENTS OF RLHF/PPO, DPO AND KTO BY A GENERALIZED IMPlicit REWARD FUNCTION](https://arxiv.org/pdf/2408.15339)\n\n[324\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[325\\. Towards Cost-Effective Reward Guided Text Generation](https://cs.uwaterloo.ca/~ppoupart/publications/nlp/Towards-Cost-Effective-Reward-Guided-Text-Generation.pdf)\n\n[326\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[327\\. 大语言模型对齐方面DPO优于PPO吗？ 综合研究](https://yiyibooks.cn/__trs__/arxiv/2404.10719v2/index.html)\n\n[328\\. Programmatically scaling human preferences and alignment in GenAI](https://go.snorkel.ai/rs/979-SZB-034/images/hoang-tran-programmatically-scale-human-preferences-and-alignment-in-genai_slides.pdf)\n\n[329\\. REAL: Efficient RLHF Training of Large Language Models with Parameter Reallocation](https://openreview.net/pdf/b7d60005a41ab17ed477e8cbcb98347b18b90d8c.pdf)\n\n[330\\. One-Shot Safety Alignment for Large Language Models via Optimal Dualization](https://arxiv.org/pdf/2405.19544)\n\n[331\\. Reward Difference Optimization For Sample Reweighting In Offline RLHF](https://arxiv.org/pdf/2408.09385)\n\n[332\\. RAINBOW PO: A UNIFIED FRAMEWORK FOR COMBINING IMPROVEMENTS IN PREFERENCE OPTIMIZATION](http://www.columbia.edu/~wt2319/RainbowPO.pdf)\n\n[333\\. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](https://arxiv.org/pdf/2405.11143v2)\n\n[334\\. P. Christiano, J. Leike et al. “Deep Reinforcement Learning from Human Preferences.” ArXiv](https://arxiv.org/abs/1706.03741)\n\n[335\\. Nisan Stiennon, Long Ouyang et al. “Learning to summarize from human feedback.” ArXiv](https://arxiv.org/abs/2009.01325)\n\n[341\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[342\\. Reward Shaping to Mitigate Reward Hacking in RLHF](https://www.arxiv.org/pdf/2502.18770)\n\n[343\\. ODIN: Disentangled Reward Mitigates Hacking in RLHF](https://openreview.net/pdf?id=zcIV8OQFVF)\n\n[344\\. Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective](https://arxiv.org/pdf/2506.02553v1)\n\n[345\\. Calibrating Translation Decoding with Quality Estimation on LLMs](https://arxiv.org/pdf/2504.19044)\n\n[346\\. Can RLHF be More Efficient with Imperfect Reward Models? A Policy Coverage Perspective](https://arxiv.org/pdf/2502.19255)\n\n[347\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[348\\. Robust LLM Alignment via Distributionally Robust Direct Preference Optimization](https://arxiv.org/pdf/2502.01930v2)\n\n[349\\. Programmatically scaling human preferences and alignment in GenAI](https://go.snorkel.ai/rs/979-SZB-034/images/hoang-tran-programmatically-scale-human-preferences-and-alignment-in-genai_slides.pdf)\n\n[350\\. The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking](http://arxiv.org/pdf/2501.19358v1.pdf?ref=applied-gai-in-security.ghost.io)\n\n[351\\. DPO Meets PPO: Reinforced Token Optimization for RLHF](https://arxiv.org/pdf/2404.18922)\n\n[352\\. Taming Overconfidence in LLMs: Reward Calibration in RLHF](https://openreview.net/pdf/936d270426776f9b8c01d10c3bfb30d6668e5ab6.pdf)\n\n[353\\. Diversity-Oriented Data Augmentation with Large Language Models](https://www.arxiv.org/pdf/2502.11671)\n\n[354\\. A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications](https://arxiv.org/pdf/2410.15595)\n\n[355\\. LLM-Generated Code in 2025: Trends and Predictions](https://www.revelo.com/blog/llm-code-generation-2025-trends-predictions-human-data#:~:text=Predictions%20for%20LLM-Generated%20Code%20in%202025,-Looking%20ahead%20to&text=Increased%20Accuracy%20and%20Reduced%20Hallucinations,generating%20incorrect%20or%20nonsensical%20code).)\n\n[356\\. Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against Reward Hacking](https://arxiv.org/pdf/2412.09544)\n\n[357\\. RLHF vs. DPO: Comparing LLM Feedback Methods](https://llmmodels.org/blog/rlhf-vs-dpo-comparing-llm-feedback-methods/)\n\n[358\\. Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking](https://arxiv.org/html/2403.03185v4)\n\n[361\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[362\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[363\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[364\\. DeepSeek-R1 vs. ChatGPT: Assessing the Titans of Next-Generation AI Linguistic Models](https://www.jneonatalsurg.com/index.php/jns/article/download/4153/3571/16455)\n\n[365\\. Production Plant Layout Planning Supported by Selected CAx Tools and Artificial Intelligence](https://journals.prz.edu.pl/amme/article/download/1861/1434)\n\n[366\\. THE ALIGNMENT PROBLEM FROM A DEEP LEARNING PERSPECTIVE](https://openreview.net/notes/edits/attachment?id=AJsyxlvQDy&name=pdf)\n\n[367\\. AI DAO White Paper](http://aidao.finance/assets/files/aidao_wp.pdf)\n\n[368\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[369\\. ICTO2025](https://www.iae-paris-est.fr/sites/default/files/2025-01/cfp%20ICTO%202025.pdf)\n\n[370\\. 2025年3月OpenAI发布ChatGPT技术白皮书](https://modou.liehuxinghuo.com/chatgpt/2078.html)\n\n[371\\. ChatGPT：开启AI新纪元](https://file.iyanbao.com/pdf/673a6-d61e0eca-b442-462c-9e85-2adb2249983d.pdf)\n\n[372\\. White Paper: Embracing Responsible Use of ChatGPT in Education](https://red.mnstate.edu/cgi/viewcontent.cgi?article=1049&context=ijgll)\n\n[373\\. Alec Radford, Karthik Narasimhan. “Improving Language Understanding by Generative Pre-Training.”](https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n[374\\. The Journal of Computing Sciences in Colleges](https://www.ccsc.org/publications/journals/EA2023.pdf)\n\n[375\\. ChatGPT发展历程、原理、技术架构详解和产业未来](https://yi.tips/wp-content/uploads/2023/06/1e22397bc9174737.pdf)\n\n[376\\. How OpenAI uses Apache Kafka and Flink for GenAI Data Pipelines](https://www.kai-waehner.de/blog/2025/06/09/how-openai-uses-apache-kafka-and-flink-for-genai/)\n\n[377\\. Hardening Azure OpenAI Service for Enterprise Usage](https://www.theseus.fi/bitstream/10024/818205/2/Ots_Karl.pdf)\n\n[378\\. 100 ChatGPT Statistics to Know in 2025 & Its Future Trends](https://intelliarts.com/blog/chatgpt-statistics/)\n\n[379\\. Security Implications of ChatGPT](https://community.isc2.org/ijoyk78323/attachments/ijoyk78323/Threats/801/1/1685300935939.pdf)\n\n[380\\. An educational researcher's guide to ChatGPT: How it works and how to use it](https://scipg.com/index.php/101/article/download/890/680)\n\n[381\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[382\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[383\\. Reward Difference Optimization For Sample Reweighting In Offline RLHF](https://arxiv.org/pdf/2408.09385)\n\n[384\\. ASYNCHRONOUS RLHF: FASTER AND MORE EFFICIENT OFF-POLICY RL FOR LANGUAGE MODELS](https://openreview.net/pdf/d0759a26adbc702480d4dafff3b0bc31aa5c6240.pdf)\n\n[385\\. P. Christiano, J. Leike et al. “Deep Reinforcement Learning from Human Preferences.” ArXiv](https://arxiv.org/abs/1706.03741)\n\n[386\\. Yuntao Bai, Andy Jones et al. “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2204.05862)\n\n[387\\. Nisan Stiennon, Long Ouyang et al. “Learning to summarize from human feedback.” ArXiv](https://arxiv.org/abs/2009.01325)\n\n[388\\. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](https://arxiv.org/pdf/2405.11143)\n\n[389\\. Avoiding exp(R_max) scaling in RLHF through Preference-based Exploration](http://www.arxiv.org/pdf/2502.00666)\n\n[390\\. RLHF in 2024 with DPO & Hugging Face](https://www.philschmid.de/dpo-align-llms-in-2024-with-trl)\n\n[391\\. CS 696 Applied Large Language Models](https://eli.sdsu.edu/courses/spring25/cs696/notes/D15%20Hooks,%20Continually%20Pre-train,%20DPO.pdf)\n\n[392\\. FASTER, MORE EFFICIENT RLHF THROUGH OFF-POLICY ASYNCHRONOUS LEARNING](https://openreview.net/pdf?id=ND3io3eses)\n\n[393\\. Forecasting GPU Performance for Deep Learning Training and Inference](https://arxiv.org/pdf/2407.13853)\n\n[394\\. One-Shot Safety Alignment for Large Language Models via Optimal Dualization](https://arxiv.org/pdf/2405.19544)\n\n[395\\. Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach](https://arxiv.org/pdf/2505.01997)\n\n[396\\. Искусственный Интеллект Индексный отчет 2024 г.](https://media.rbcdn.ru/media/reports/AI-Index-Report-2024_%D0%A0%D1%83%D1%81_NapoleonIT.pdf)\n\n[397\\. UNA: UNIFYING ALIGNMENTS OF RLHF/PPO, DPO AND KTO BY A GENERALIZED IMPlicit REWARD FUNCTION](https://arxiv.org/pdf/2408.15339)\n\n[398\\. RAINBOW PO: A UNIFIED FRAMEWORK FOR COMBINING IMPROVEMENTS IN PREFERENCE OPTIMIZATION](http://www.columbia.edu/~wt2319/RainbowPO.pdf)\n\n[399\\. HybridFlow: A Flexible and Efficient RLHF Framework](https://i.cs.hku.hk/~cwu/papers/gmsheng-eurosys25.pdf)\n\n[401\\. Reward Shaping to Mitigate Reward Hacking in RLHF](https://www.arxiv.org/pdf/2502.18770)\n\n[402\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[403\\. A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications](https://arxiv.org/pdf/2410.15595)\n\n[404\\. RRM: ROBUST REWARD MODEL TRAINING MITIGATES REWARD HACKING](https://openreview.net/pdf?id=88AS5MQnmC)\n\n[405\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[406\\. The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking](http://arxiv.org/pdf/2501.19358v1.pdf?ref=applied-gai-in-security.ghost.io)\n\n[407\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[408\\. Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against Reward Hacking](https://arxiv.org/pdf/2412.09544)\n\n[409\\. Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking](https://arxiv.org/html/2403.03185v4)\n\n[410\\. DPO Meets PPO: Reinforced Token Optimization for RLHF](https://arxiv.org/pdf/2404.18922)\n\n[411\\. BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF](https://www.themoonlight.io/en/review/badreward-clean-label-poisoning-of-reward-models-in-text-to-image-rlhf)"
  },
  {
    "id": 14,
    "question": "How can we improve large language models' effectiveness on long text reasoning tasks (such as fact extraction and summarization) and avoid the phenomenon where key information is easily overlooked in long contexts? Answer from the perspectives of model architecture, training methods, inference strategies, and model evaluation.",
    "response": "**Research Report: Enhancing Long Text Reasoning in Large Language Models**\n\n**1\\. Introduction**\n\nLarge language models (LLMs) struggle with long-context tasks (e.g., fact extraction, summarization) due to computational constraints, positional encoding limitations, and the \"mid-chunk information loss\" phenomenon, where critical details are overlooked in lengthy inputs. This report synthesizes solutions across model architecture, training, inference, and evaluation, addressing these gaps using empirical evidence.\n\n**2\\. Model Architecture Improvements**\n\n**2.1 Positional Encoding & Attention Mechanisms**\n\n**Extrapolation Techniques**: ALiBi, RoPE-PI, and NTK interpolation extend context windows beyond pre-training limits by modifying position indices. These reduce computational overhead while maintaining coherence in 100k+ token sequences. Models like QWENLONG-L1 use progressive context scaling to adapt short-context models to long inputs \\[41\\].\n\n**Sparse Attention**: LinFormer and LongLoRA approximate attention matrices with low-rank adaptations, cutting memory usage by 40% for 100k-token processing. Performer replaces softmax with random feature mappings, enabling near-linear complexity \\[41\\].\n\n**State Space Models (SSMs)**: Architectures like S4 and H3 achieve linear computational scaling, outperforming transformers on tasks requiring multi-hop reasoning (e.g., legal contract analysis) \\[41\\].\n\n**2.2 Memory-Augmented Designs**\n\n**Recurrent Structures**: Retentive Network (RetNet) balances efficiency and accuracy via chunkwise recurrence. It processes sequences in parallel chunks during training but switches to O(1) recurrent inference, reducing GPU memory by 70% versus transformers for 100k-token documents \\[87\\]\\[89\\].\n\n**External Memory Banks**: Memorizing Transformer and Landmark Attention store key information in external modules, enabling recall of distant context with minimal latency \\[41\\].\n\n**2.3 Hybrid Architectures**\n\n**RAG vs. Memory-Augmented Systems**:\n\n**RAG**: Retrieves relevant snippets using vector databases (e.g., FAISS), ideal for dynamic knowledge (e.g., medical guidelines). However, CPU-based retrieval adds ~56% latency \\[206\\].\n\n**Cache-Augmented Generation (CAG)**: Embeds full context (up to 128k tokens) directly, avoiding retrieval bottlenecks. CAG achieves 49% faster response times than RAG but requires 4× more VRAM \\[104\\].\n\n**MemoRAG**: Hybrid approach handling 1M tokens via global memory indexing, outperforming RAG in complex QA \\[111\\].\n\n**3\\. Training Methods**\n\n**3.1 Pre-training & Data Optimization**\n\n**High-Quality Corpora**: Filtering \"textbook-like\" data improves fact retention. RETRO-style retrieval-augmented pre-training reduces hallucinations by 30% in biomedical texts \\[49\\].\n\n**Multi-Task Joint Training**: JMLR synchronously trains retrievers and LLMs for medical tasks, making retrievers aware of LLM needs. This halves omission rates in clinical report summaries \\[169\\].\n\n**3.2 Supervised Fine-Tuning (SFT)**\n\n**Medical Summarization**: Pointer-generator networks + coverage mechanisms reduce omissions by copying source text. MED-OMIT metric quantifies diagnostic impact of missing facts \\[293\\]\\[358\\].\n\n**Legal Extraction**: Query-focused SFT trains models to extract sentences relevant to predefined queries (e.g., \"termination clauses\"), overcoming token limits. Achieves 92% precision in 100k-token contracts \\[288\\]\\[350\\].\n\n**Domain Adaptation**: LegalPegasus (legal) and BioBERT (medical) outperform general models after task-specific tuning \\[295\\].\n\n**3.3 Reinforcement Learning**\n\n**RLHF**: InstructGPT-style human feedback improves truthfulness by 40% in summarization \\[49\\].\n\n**Direct Preference Optimization (DPO)**: Corrects factual errors without costly reward modeling, improving LV-Eval scores by 15% \\[49\\].\n\n**4\\. Inference Strategies**\n\n**4.1 Efficient Decoding**\n\n**KV Cache Compression**: Matryoshka and Scissorhands prune redundant key-value pairs, slashing VRAM usage by 60% for 100k-token inference \\[41\\].\n\n**Dynamic Activation**: RetNet’s recurrent inference mode enables 170 tokens/sec on RTX 4090 GPUs, outperforming transformer-based RAG (110 tokens/sec) \\[269\\]\\[330\\].\n\n**4.2 Retrieval & Context Optimization**\n\n**Selective Context**: Prunes redundant input, cutting 100k-token processing latency by 35% \\[70\\].\n\n**HybridRAG**: Combines cloud memory with client-side models for real-time legal extraction, adding only 0.5–1% overhead \\[164\\].\n\n**4.3 Summarization-Specific Techniques**\n\n**KEITSum**: Identifies key entities and conclusion sentences, guiding smaller LLMs to reduce hallucinations in long-document summaries \\[71\\].\n\n**Iterative Refinement (LLMRefine)**: Pinpoints defects in drafts using fine-grained feedback, improving ROUGE-L by 12% \\[69\\].\n\n**5\\. Model Evaluation**\n\n**5.1 Benchmarks**\n\n**Needle-in-Haystack (NIAH)**: Tests retrieval of facts in synthetic 100k–2M token corpora. RetNet achieves 98% accuracy at 11M tokens, while GPT-4 fails beyond 128k \\[247\\]\\[373\\].\n\n**LV-Eval**: Multi-hop QA benchmark (up to 256k tokens) with \"confusing facts\" insertion. Moonshot-v1 leads with 85% F1 at 64k tokens \\[245\\]\\[364\\].\n\n**CLongEval & LongIns**: Chinese and keyword-density benchmarks highlighting positional sensitivity \\[41\\]\\[60\\].\n\n**5.2 Metrics**\n\n**Information Coverage (IC)**: Measures essential context blocks for accurate responses \\[49\\].\n\n**MED-OMIT**: Extrinsic medical metric evaluating diagnostic harm of omissions \\[358\\].\n\n**F1 for Structured Extraction**: Critical for legal/medical tasks with precise entity requirements \\[245\\].\n\n**6\\. Hardware & Deployment Tradeoffs**\n\n**Consumer GPUs (RTX 4090)**: RetNet processes 100k tokens at 170 tokens/sec (6.7B model), while RAG uses ~24GB VRAM for retrieval + 7B LLMs. HybridRAG reduces latency to 2.3 seconds \\[325\\]\\[330\\].\n\n**Scalability**: RAG suits dynamic knowledge (e.g., updated regulations) but lags in multi-hop reasoning. Memory-augmented transformers (e.g., MemoRAG) dominate static corpora but require 400GB+ VRAM for 1M tokens \\[155\\]\\[193\\].\n\n**Cost**: RAG adds <1% overhead; long-context LLMs cost 5–8× more but outperform in summarization \\[192\\].\n\n**7\\. Task-Specific Optimization**\n\n**Medical Summarization**: JMLR + PICO-masking reduces omissions by 40% via joint retriever-LLM tuning \\[169\\].\n\n**Legal Extraction**: Query-based SFT + GPT-3.5 prompt engineering achieves 95% recall in contracts \\[350\\].\n\n**Hybrid Approaches**: AMRAG’s self-memory storage minimizes redundant searches in EHR analysis \\[162\\].\n\n**8\\. Conclusion**\n\nEnhancing long-context reasoning requires:\n\n1.  **Architectural Innovations**: RetNet for efficiency, SSMs for linear scaling.\n2.  **Training Hybridization**: Joint retrieval-LLM tuning (JMLR) for domain-specific accuracy.\n3.  **Inference Optimization**: KV cache compression and selective context for speed.\n4.  **Evaluation Rigor**: LV-Eval/NIAH benchmarks and task-specific metrics (MED-OMIT).\n\nFor 100k+ tokens, memory-augmented transformers lead in summarization, while RAG excels in dynamic fact extraction. RetNet offers a balanced solution for consumer hardware, but VRAM constraints remain for trillion-token corpora. Future work should optimize hardware-software co-design and task-aware evaluation frameworks.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Efficient Large Language Models: A Survey](https://openreview.net/notes/edits/attachment?id=pKPJpBLaoN&name=pdf)\n\n[2\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[3\\. A Survey of Context Engineering for Large Language Models](https://hub.baai.ac.cn/paper/1a3763b7-6e29-4e77-ad27-5a1337b23b24)\n\n[4\\. QWENLONG-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning](https://arxiv.org/pdf/2505.17667)\n\n[5\\. GitHub - DavidZWZ/Awesome-RAG-Reasoning: \\[Up-to-date\\] Awesome RAG Reasoning Resources](https://github.com/DavidZWZ/Awesome-RAG-Reasoning)\n\n[6\\. 大型语言模型可以在长上下文推理中自我改进](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_11_14/2411.08147.pdf)\n\n[7\\. Gemini 1.5 long context 大模型上下文扩展](https://zhuanlan.zhihu.com/p/684883050)\n\n[8\\. New methods boost reasoning in large language models](https://www.microsoft.com/en-us/research/blog/new-methods-boost-reasoning-in-small-and-large-language-models/)\n\n[9\\. INFTYTHINK: Breaking the Length Limits of Long-Context Reasoning in Large Language Models](https://www.arxiv.org/pdf/2503.06692)\n\n[10\\. Yushi Bai, Xin Lv et al. “LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding.” ArXiv](https://doi.org/10.48550/arXiv.2308.14508)\n\n[11\\. 大模型关键技术与应用 Key Technologies and Applications of Large Models](https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-cn/mediares/magazine/publication/com_cn/article/202402/12.pdf)\n\n[12\\. Inference - Paper Reading](http://paperreading.club/category?cate=Inference)\n\n[13\\. MiniMax AI发布MiniMax-M1：适用于长上下文和强化学习任务的456B参数混合模型](https://www.marktechpost.com/2025/06/19/minimax-ai-releases-minimax-m1-a-456b-parameter-hybrid-model-for-long-context-and-reinforcement-learning-rl-tasks/)\n\n[14\\. Advancing Reasoning in Large Language Models: Promising Methods and Approaches](https://arxiv.org/html/2502.03671v1)\n\n[15\\. Towards Enhanced Reasoning in Large Language Models](https://escholarship.org/content/qt7t63t1z9/qt7t63t1z9_noSplash_9784b4ce223ac9aea6af5ab44c275049.pdf?t=sr5q13)\n\n[16\\. LLM Research Papers: The 2025 List (January to June)](https://magazine.sebastianraschka.com/p/llm-research-papers-2025-list-one)\n\n[17\\. Chain-of-Thought Matters: Improving Long-Context Language Models with Reasoning Path Supervision](https://arxiv.org/pdf/2502.20790)\n\n[18\\. Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression](https://arxiv.org/pdf/2408.15491)\n\n[19\\. Thus Spake Long-Context Large Language Model](https://arxiv.org/pdf/2502.17129)\n\n[21\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[22\\. AI-Assisted Data Extraction with a Large Language Model: A Study Within Reviews](https://www.medrxiv.org/content/10.1101/2025.03.20.25324350v1.full.pdf)\n\n[23\\. Yuxia Wang, Minghan Wang et al. “Factuality of Large Language Models in the Year 2024.” ArXiv](https://doi.org/10.48550/arXiv.2402.02420)\n\n[24\\. Nikhil Kandpal, H. Deng et al. “Large Language Models Struggle to Learn Long-Tail Knowledge.” International Conference on Machine Learning](https://doi.org/10.48550/arXiv.2211.08411)\n\n[25\\. GVDIE: A Zero-Shot Generative Information Extraction Method for Visual Documents Based on Large Language Models](http://www.apsipa2024.org/files/papers/448.pdf)\n\n[26\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[27\\. VERIFACT: 通过精炼事实提取和参考事实增强长篇文章真实性评估](https://www.xueshuxiangzi.com/downloads/2025_5_16/2505.09701.pdf)\n\n[28\\. Google DeepMind’s fact quest: Improving long-form accuracy in LLMs with SAFE](https://dataconomy.com/2024/04/01/google-deepmind-safe-llm-checker/)\n\n[29\\. 利用内部和外部知识预训练大型记忆语言模型](https://www.xueshuxiangzi.com/downloads/2025_5_23/2505.15962.pdf)\n\n[30\\. 大语言模型在长上下文中的信息检索应用](https://baoyu.io/translations/ai-paper/2310.03025-retrieval-meets-long-context-large-language-models)\n\n[31\\. An Empirical Study on Information Extraction using Large Language Models](https://arxiv.org/pdf/2305.14450)\n\n[32\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[33\\. Large Language Models for Generative Information Extraction: A Survey](https://openreview.net/pdf/f43b52efa860ab47fd73ce13ebc5406b0838b585.pdf)\n\n[34\\. Information Extraction from Contracts Using Large Language Models](https://fse.studenttheses.ub.rug.nl/34210/1/mAI2024CornelisG.pdf)\n\n[35\\. Long-form factuality in large language models](https://openreview.net/pdf?id=4M9f8VMt2C)\n\n[36\\. 大模型日报 2024-04-14](https://zhuanlan.zhihu.com/p/692425803)\n\n[37\\. Augmenting Large Language Model's Knowledge Using External Sources Without Finetuning](https://elib.dlr.de/210070/1/Augmenting_Large_Language_Model_s_Knowledge_Using_External_Sources_Without_Finetuning.pdf)\n\n[38\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[39\\. Leveraging Large Language Models for Event Extraction](https://lup.lub.lu.se/student-papers/record/9175548/file/9175549.pdf)\n\n[41\\. CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models](https://arxiv.org/pdf/2403.03514)\n\n[42\\. LV-EVAL: A BALANCED LONG-CONTEXT BENCHMARK WITH 5 LENGTH LEVELS UP TO 256K](http://arxiv.org/pdf/2402.05136)\n\n[43\\. 大型语言模型的数据集：综合调查](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_2_29/2402.18041.pdf)\n\n[44\\. Evaluating Long-Context Understanding via Latent and Positional Structure Queries in Large Language Models](https://www.techrxiv.org/users/834564/articles/1227194/master/file/data/long%20context/long%20context.pdf)\n\n[45\\. Mitigating Catastrophic Forgetting in Task-Incremental Learning for Large Language Models](https://doria.fi/bitstream/handle/10024/189802/shakya_mudita.pdf?sequence=3&isAllowed=y)\n\n[46\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[47\\. Holistic Reasoning with Long-Context LMs: A Benchmark for Database Operations on Massive Textual Data](https://openreview.net/pdf/13dc9500e4f3140f0ffb5fc635d103be5e41d1ab.pdf)\n\n[48\\. LR2Bench: Evaluating Long-chain Reflective Reasoning Capabilities of Large Language Models via Constraint Satisfaction Problems](https://arxiv.org/pdf/2502.17848)\n\n[49\\. ETHIC：在高信息覆盖率的长上下文任务中评估大型语言模型](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_23/2410.16848.pdf)\n\n[50\\. LV-Eval：平衡的长上下文基准，5 个长度级别高达 256K](https://yiyibooks.cn/__trs__/arxiv/2402.05136v1/index.html)\n\n[51\\. Large Language Models in Cybersecurity \\[2025\\]](https://research.aimultiple.com/llms-in-cybersecurity/)\n\n[52\\. A Survey on Large Language Model Acceleration based on KV Cache Management](https://openreview.net/pdf/31d6ab447981f515ddb7d2f06b133bd50bc153ef.pdf)\n\n[53\\. FSTP Project Report AID 2030 - Artificial Intelligence Data Kit 2030](https://european-language-equality.eu/wp-content/uploads/2023/06/ELE2_Project_Report_AID_2030.pdf)\n\n[54\\. Revisiting Catastrophic Forgetting in Large Language Model Tuning](https://openreview.net/pdf/cadc5df9def84f74c2c0ead3692321d8c6cb606c.pdf)\n\n[55\\. Evaluating Large Language Models on Spatial Tasks: A Multi-Task Benchmarking Study](https://arxiv.org/pdf/2408.14438)\n\n[56\\. A survey on augmenting knowledge graphs (KGs) with large language models (LLMs): models, evaluation metrics, benchmarks, and challenges](https://d-nb.info/1354098625/34)\n\n[57\\. A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks](https://arxiv.org/pdf/2310.09430v1)\n\n[58\\. Large Language Models Evaluation Benchmarks Collection](https://www.pudn.com/Download/item/id/1708924291168224.html)\n\n[59\\. StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with Multi Turns](https://www.themoonlight.io/zh/review/storybench-a-dynamic-benchmark-for-evaluating-long-term-memory-with-multi-turns)\n\n[60\\. LongIns: A Challenging Long-context Instruction-based Exam for Large Language Models](http://arxiv.org/html/2406.17588v2)\n\n[61\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[62\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[63\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[64\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[65\\. 大语言模型长文本推断优化技术综述](https://www.j-bigdataresearch.com.cn/rc-pub/front/front-article/download/104225243/lowqualitypdf/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E9%95%BF%E6%96%87%E6%9C%AC%E6%8E%A8%E6%96%AD%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0.pdf)\n\n[66\\. 马尔可夫增强聚类用于长文档摘要：使用大型语言模型解决“中途迷失”挑战](https://www.xueshuxiangzi.com/downloads/2025_6_24/2506.18036.pdf)\n\n[67\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[68\\. Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization](https://arxiv.org/pdf/2403.00067)\n\n[69\\. LLMRefine: 一种推理时间优化方法](https://linnk.ai/insight/text-generation/iterative-refinement-of-large-language-model-outputs-using-fine-grained-actionable-feedback-cIygBKJ5/)\n\n[70\\. Compressing Context to Enhance Inference Efficiency of Large Language Models](https://aclanthology.org/2023.emnlp-main.391/)\n\n[71\\. Key-Element-Informed sLLM Tuning for Document Summarization](https://www.isca-archive.org/interspeech_2024/ryu24_interspeech.pdf)\n\n[72\\. ARCHON: An Architecture Search Framework for Inference-Time Techniques](https://scalingintelligence.stanford.edu/pubs/archon.pdf)\n\n[73\\. Proceedings 6th International Open Search Symposium #ossym2024](https://elib.dlr.de/210749/1/172-176-PB.pdf)\n\n[74\\. Evaluating Long-Context Understanding via Latent and Positional Structure Queries in Large Language Models](https://www.techrxiv.org/users/834564/articles/1227194/master/file/data/long%20context/long%20context.pdf)\n\n[75\\. ARCHON: AN ARCHITECTURE SEARCH FRAMEWORK FOR INFERENCE-TIME TECHNIQUES](https://openreview.net/pdf?id=5wuZyG1ACs)\n\n[76\\. H₂O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2023/file/6ceefa7b15572587b78ecfcebb2827f8-Paper-Conference.pdf)\n\n[77\\. Optimizing Large Language Models: Top 5 Techniques Unveiled](https://attri.ai/blog/mastering-llm-optimization-with-these-5-essential-techniques)\n\n[78\\. Summarization of Large Text Volumes Using Large Language Models](https://kdd2024.kdd.org/wp-content/uploads/2024/08/15-KDD-UC-Betrian.pdf)\n\n[79\\. A Survey of Storage Optimization Techniques in Large Language Model Inference](https://crad.ict.ac.cn/en/article/doi/10.7544/issn1000-1239.202440628)\n\n[81\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[82\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[83\\. Sepp Hochreiter, J. Schmidhuber. “Long Short-Term Memory.” Neural Computation](https://doi.org/10.1162/neco.1997.9.8.1735)\n\n[84\\. I. Sutskever, O. Vinyals et al. “Sequence to Sequence Learning with Neural Networks.” ArXiv](https://arxiv.org/abs/1409.3215)\n\n[85\\. Proceedings of ICONI 2024](https://journal-home.s3.ap-northeast-2.amazonaws.com/site/iconi2024/proceeding/Proceedings_of_ICONI%2B2024.pdf)\n\n[86\\. Yoshua Bengio, Patrice Y. Simard et al. “Learning long-term dependencies with gradient descent is difficult.” IEEE transactions on neural networks](https://doi.org/10.1109/72.279181)\n\n[87\\. Retentive Network](https://openreview.net/pdf?id=sxZlp9ZoHD)\n\n[88\\. Retentive Network: A Successor to Transformer for Large Language Models](https://openreview.net/pdf?id=UU9Icwbhin)\n\n[89\\. Whitepaper - Breakdown of Retentive Networks (RetNet)](https://frenos.io/hubfs/theme-frenos/whitepaper/Frenos%20Scaling%20Retentive%20Networks.pdf?hsLang=en)\n\n[90\\. Retentive Network: A Novel Neural Network Architecture](https://www.e2enetworks.com/blog/retentive-network-a-novel-neural-network-architecture)\n\n[91\\. Recurrent networks can recycle neural resources to flexibly trade speed for accuracy in visual recognition](https://www.biorxiv.org/content/10.1101/677237v4.full.pdf)\n\n[92\\. Shaojie Bai, J. Z. Kolter et al. “An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.” ArXiv](https://arxiv.org/abs/1803.01271)\n\n[93\\. Predicting and Explaining Cognitive Load, Attention, and Working Memory in Virtual Multitasking](https://attentionlab.psych.ucsb.edu/sites/default/files/images/publications/Predicting_and_Explaining_Cognitive_Load_Attention_and_Working_Memory_in_Virtual_Multitasking.pdf)\n\n[94\\. GitHub - myscience/retnet-pytorch: Implementation of Retention-Network in PyTorch](https://github.com/myscience/retnet-pytorch)\n\n[95\\. A Comparative Study of Deep Learning Models for Human Activity Recognition](https://ojs.wiserpub.com/index.php/CCDS/article/download/6264/2934/56997)\n\n[101\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[102\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[103\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[104\\. Cache-Augmented Generation in RAG Pipelines: Fast and Memory-Efficient Approach to Multi-Agent Knowledge Query Systems](https://aquila.usm.edu/cgi/viewcontent.cgi?article=2011&context=honors_theses)\n\n[105\\. Iz Beltagy, Matthew E. Peters et al. “Longformer: The Long-Document Transformer.” ArXiv](https://arxiv.org/abs/2004.05150)\n\n[106\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[107\\. Memory in Agents: What, Why and How](https://mem0.ai/blog/memory-in-agents-what-why-and-how/)\n\n[108\\. Dartboard: Better RAG using Relevant Information Gain](https://arxiv.org/pdf/2407.12101)\n\n[109\\. Memory-augmented generative adversarial transformers](https://scholarlypublications.universiteitleiden.nl/access/item%3A3754747/view)\n\n[110\\. LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs](https://arxiv.org/pdf/2406.15319)\n\n[111\\. GitHub - qhjqhj00/MemoRAG: Empowering RAG with a memory-based data interface for all-purpose applications!](https://github.com/qhjqhj00/MemoRAG)\n\n[112\\. Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach](https://arxiv.org/pdf/2407.16833)\n\n[121\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[122\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[123\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[124\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[125\\. A. See, Peter J. Liu et al. “Get To The Point: Summarization with Pointer-Generator Networks.” ArXiv](https://doi.org/10.18653/v1/P17-1099)\n\n[126\\. Challenges In Supervised Fine-Tuning](https://www.meegle.com/en_us/topics/supervised-fine-tuning/challenges-in-supervised-fine-tuning)\n\n[127\\. Daniel M. Ziegler, Nisan Stiennon et al. “Fine-Tuning Language Models from Human Preferences.” ArXiv](https://arxiv.org/abs/1909.08593)\n\n[128\\. International Journal of Advanced Computer Science and Applications](https://thesai.org/Downloads/IJACSA_Volume3No1.pdf)\n\n[129\\. Information Extraction from Contracts Using Large Language Models](https://fse.studenttheses.ub.rug.nl/34210/1/mAI2024CornelisG.pdf)\n\n[130\\. Supervised Fine-Tuning: Techniques and Applications](https://www.sapien.io/zh/blog/what-is-supervised-fine-tuning-overview-and-techniques)\n\n[131\\. Query-based document summarization - Azure Architecture Center](https://learn.microsoft.com/zh-cn/azure/architecture/ai-ml/guide/query-based-summarization)\n\n[132\\. Fine-Tuning Small Language Models on Summarization Tasks](https://escholarship.org/content/qt1n06g01k/qt1n06g01k.pdf)\n\n[133\\. Automatic Summarization](https://owiki.org/wiki/Automatic_summarization)\n\n[134\\. Supervised Fine-Tuning: Overview and Techniques](https://www.sapien.io/blog/what-is-supervised-fine-tuning-overview-and-techniques)\n\n[135\\. Supervised Fine-Tuning For Model Training](https://www.meegle.com/en_us/topics/supervised-fine-tuning/supervised-fine-tuning-for-model-training)\n\n[136\\. Comparative Summarization of Document Collections](https://openresearch-repository.anu.edu.au/bitstream/1885/261562/1/thesis_u6155790_corrected.pdf)\n\n[137\\. Supervised Fine-tuning: customizing LLMs](https://mantisnlp.com/blog/supervised-fine-tuning-customizing-llms/)\n\n[138\\. KERMIT: Knowledge Extractive and Reasoning Model using Transformers](http://www.diva-portal.org/smash/get/diva2:1872915/FULLTEXT01.pdf)\n\n[139\\. Suptech tools for prudential supervision and their use during the pandemic](https://www.bis.org/fsi/publ/insights37.pdf)\n\n[140\\. Supervised Fine Tuning](https://global-integration.larksuite.com/en_us/topics/ai-glossary/supervised-fine-tuning)\n\n[141\\. Sepp Hochreiter, J. Schmidhuber. “Long Short-Term Memory.” Neural Computation](https://doi.org/10.1162/neco.1997.9.8.1735)\n\n[142\\. Vladimir Karpukhin, Barlas Oğuz et al. “Dense Passage Retrieval for Open-Domain Question Answering.” ArXiv](https://doi.org/10.18653/v1/2020.emnlp-main.550)\n\n[143\\. Towards Understanding Systems Trade-offs in Retrieval-Augmented Generation Model Inference](http://arxiv.org/abs/2412.11854)\n\n[144\\. Kelvin Guu, Kenton Lee et al. “REALM: Retrieval-Augmented Language Model Pre-Training.” ArXiv](https://arxiv.org/abs/2002.08909)\n\n[145\\. Sebastian Borgeaud, A. Mensch et al. “Improving language models by retrieving from trillions of tokens.” International Conference on Machine Learning](https://arxiv.org/abs/2112.04426)\n\n[146\\. Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach](https://arxiv.org/pdf/2407.16833)\n\n[147\\. Urvashi Khandelwal, Omer Levy et al. “Generalization through Memorization: Nearest Neighbor Language Models.” ArXiv](https://arxiv.org/abs/1911.00172)\n\n[148\\. Long-Term Memory for Cognitive Architectures: A Hardware Approach Using Resistive Devices](https://digital.library.adelaide.edu.au/dspace/bitstream/2440/117927/2/Wang2018_PhD.pdf)\n\n[149\\. UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis](https://proceedings.neurips.cc/paper_files/paper/2024/file/7c06759d1a8567f087b02e8589454917-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[150\\. Retrieval Augmented Generation in the Wild: A System 2 Perspective](http://sites.computer.org/debull/A24dec/p47.pdf)\n\n[151\\. Memory Never Fades: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation](https://openreview.net/pdf?id=8Cggwrvkho)\n\n[152\\. RETRIEVAL-AUGMENTED GENERATION AND LONG CONTEXT MODELS: A COMPARATIVE ANALYSIS OF ADVANCED GENERATIVE AI APPROACHES](https://www.irjet.net/archives/V11/i5/IRJET-V11I5158.pdf)\n\n[153\\. Retrieval-Augmented-Generation (RAG) vs Long-context LLMs: Which to Choose?](https://www.ainewsletter.today/p/retrieval-augmented-generation-rag)\n\n[154\\. LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs](https://openreview.net/pdf/82f337f34844c2928d723485da38dae553da230e.pdf)\n\n[155\\. MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation](https://arxiv.org/pdf/2409.05591v3)\n\n[156\\. Comparative Analysis of Retrieval Augmented Generator and Traditional Large Language Models](https://repositum.tuwien.at/bitstream/20.500.12708/202324/1/Oroz%20Tin%20-%202024%20-%20Comparative%20Analysis%20of%20Retrieval%20Augmented%20Generator%20and...pdf)\n\n[157\\. Associative Recurrent Memory Transformer](http://arxiv.org/pdf/2407.04841)\n\n[161\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[162\\. Adaptive Memory Enhancement: Augmentation and Self-Check Mechanisms in Retrieval Processes (AMRAG)](https://faranzz.com/pdf/amrag.pdf)\n\n[163\\. Hybrid-RACA: Hybrid Retrieval-Augmented Composition Assistance for Real-time Text Prediction](http://arxiv.org/html/2308.04215v3)\n\n[164\\. HYBRID RETRIEVAL-AUGMENTED GENERATION FOR REAL-TIME COMPOSITION ASSISTANCE](https://openreview.net/pdf?id=LajkZlgD83)\n\n[165\\. REMEMBER: Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning in Zero- and Few-shot Neurodegenerative Diagnosis](https://arxiv.org/pdf/2504.09354)\n\n[166\\. Information Retrieval: Recent Advances and Beyond](https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?arnumber=10184013)\n\n[167\\. JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability](https://openreview.net/pdf?id=pxhbRymrkp)\n\n[168\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[169\\. Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability](https://www.themoonlight.io/en/review/jmlr-joint-medical-llm-and-retrieval-training-for-enhancing-reasoning-and-professional-question-answering-capability)\n\n[170\\. AlzheimerRAG: Multimodal Retrieval Augmented Generation for PubMed articles](http://arxiv.org/html/2412.16701v1)\n\n[171\\. Vladimir Karpukhin, Barlas Oğuz et al. “Dense Passage Retrieval for Open-Domain Question Answering.” ArXiv](https://doi.org/10.18653/v1/2020.emnlp-main.550)\n\n[172\\. Generative Large Language Models Augmented Hybrid Retrieval System for Biomedical Question Answering](https://ceur-ws.org/Vol-3740/paper-12.pdf)\n\n[173\\. Kelvin Guu, Kenton Lee et al. “REALM: Retrieval-Augmented Language Model Pre-Training.” ArXiv](https://arxiv.org/abs/2002.08909)\n\n[181\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[182\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[183\\. MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation](https://arxiv.org/pdf/2409.05591v3)\n\n[184\\. RETRIEVAL-AUGMENTED GENERATION AND LONG CONTEXT MODELS: A COMPARATIVE ANALYSIS OF ADVANCED GENERATIVE AI APPROACHES](https://www.irjet.net/archives/V11/i5/IRJET-V11I5158.pdf)\n\n[185\\. Holistic Reasoning with Long-Context LMs: A Benchmark for Database Operations on Massive Textual Data](https://openreview.net/pdf/ff0c8698acd9c90a48a3b51f371c8382ebb8744b.pdf)\n\n[186\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[187\\. UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis](https://people.iiis.tsinghua.edu.cn/~huanchen/publications/uda-neurips24.pdf)\n\n[188\\. TEST-TIME RAG (TTRAG): ENHANCING LONG CONTEXT UNDERSTANDING IN LLMs WITH RETRIEVAL-AUGMENTED MECHANISMS](https://openreview.net/pdf?id=zgs450VzkU)\n\n[189\\. Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach](https://arxiv.org/pdf/2407.16833)\n\n[190\\. Gemini 2.0 vs. Agentic RAG: Who wins at Structured Information Extraction?](https://unstructured.io/blog/gemini-2-0-vs-agentic-rag-who-wins-at-structured-information-extraction)\n\n[191\\. M. Lewis, Yinhan Liu et al. “BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/2020.acl-main.703)\n\n[192\\. RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving](https://arxiv.org/pdf/2503.14649)\n\n[193\\. RAG vs Long-Context LLMs: Approaches for Real-World Applications](https://blog.premai.io/rag-vs-long-context-llms-which-approach-excels-in-real-world-applications/)\n\n[194\\. Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs](https://openreview.net/pdf?id=6LKmaC4cO0)\n\n[195\\. LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs](https://github.com/Alibaba-NLP/LaRA)\n\n[196\\. NLP • LLM Context Length Extension](https://aman.ai/primers/ai/context-length-extension/)\n\n[197\\. Comparison: RAG vs. Long Context Window models](https://www.legionintel.com/blog/rag-systems-vs-lcw-performance-and-cost-trade-offs)\n\n[201\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[202\\. RECOMP: IMPROVING RETRIEVAL-AUGMENTED LMs WITH COMPRESSION AND SELECTIVE AUGMENTATION](https://openreview.net/pdf?id=mlJLVigNHp)\n\n[203\\. HYBRID RETRIEVAL-AUGMENTED GENERATION FOR REAL-TIME COMPOSITION ASSISTANCE](https://openreview.net/pdf?id=LajkZlgD83)\n\n[204\\. Retrieval Augmented Generation with Huggingface Transformers and Ray](https://github.com/maghwa/blog/blob/8a65b5acb5a71c2838dd02ff3f5daaefda518dc3/ray-rag.md)\n\n[205\\. Systematic Performance Optimization for Retrieval-Augmented Generation Serving](https://openreview.net/pdf/5565313f201a70f2ee8c2be077ea0098a271bae6.pdf)\n\n[206\\. TELERAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval](https://arxiv.org/pdf/2502.20969)\n\n[207\\. Vladimir Karpukhin, Barlas Oğuz et al. “Dense Passage Retrieval for Open-Domain Question Answering.” ArXiv](https://doi.org/10.18653/v1/2020.emnlp-main.550)\n\n[208\\. Adapting Deep Neural Information Retrieval Models to Long Documents and New Domains](https://theses.hal.science/tel-04344209/file/LI_2023_archivage.pdf)\n\n[209\\. RetrievalAttention: ACCELERATING LONG-CONTEXT LLM INFERENCE VIA VECTOR RETRIEVAL](https://openreview.net/pdf/ae9689c7f1c60a148e3dcb476567cde81f21f8d4.pdf)\n\n[210\\. External Knowledge Augmented Language Models for Code Generation and Agents](https://frankxfz.me/frank_thesis_proposal.pdf)\n\n[211\\. Jeff Johnson, Matthijs Douze et al. “Billion-Scale Similarity Search with GPUs.” IEEE Transactions on Big Data](https://doi.org/10.1109/tbdata.2019.2921572)\n\n[212\\. Retrieval-Augmented Generation (RAG) Benchmarks in Generative AI](https://docs.oracle.com/en-us/iaas/Content/generative-ai/scenario-2.htm)\n\n[213\\. Retentive Network](https://openreview.net/pdf?id=sxZlp9ZoHD)\n\n[214\\. Accelerating Retrieval-Augmented Generation](https://vlsi.cornell.edu/papers/iks-asplos2025.pdf)\n\n[215\\. Daniel Fernando Campos, Tri Nguyen et al. “MS MARCO: A Human Generated MAchine Reading COmprehension Dataset.” ArXiv](https://arxiv.org/abs/1611.09268)\n\n[216\\. Retrieval Augmented Generation in Practice](https://shipitcon.com/wp-content/uploads/2023/09/3-Mihai-Criveti-Retrieval-Augmented-Generation-v3.pdf)\n\n[217\\. Moshe Berchansky, Peter Izsak et al. “Optimizing Retrieval-augmented Reader Models via Token Elimination.” ArXiv](https://doi.org/10.48550/arXiv.2310.13682)\n\n[221\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[222\\. Information Extraction from Contracts Using Large Language Models](https://fse.studenttheses.ub.rug.nl/34210/1/mAI2024CornelisG.pdf)\n\n[223\\. A. See, Peter J. Liu et al. “Get To The Point: Summarization with Pointer-Generator Networks.” ArXiv](https://doi.org/10.18653/v1/P17-1099)\n\n[224\\. May Myo Zin, Ha-Thanh Nguyen et al. “Information Extraction from Lengthy Legal Contracts: Leveraging Query-Based Summarization and GPT-3.5.” International Conference on Legal Knowledge and Information Systems](https://doi.org/10.3233/FAIA230963)\n\n[225\\. Rada Mihalcea, P. Tarau. “TextRank: Bringing Order into Text.” Conference on Empirical Methods in Natural Language Processing](https://www.semanticscholar.org/paper/7b95d389bc6affe6a127d53b04bcfd68138f1a1a)\n\n[226\\. Proceedings of the 9th SwissText Conference](https://www.swisstext.org/wp-content/uploads/2024/06/Proceedings_Preprint.pdf)\n\n[227\\. Günes Erkan, Dragomir R. Radev. “LexRank: Graph-based Lexical Centrality as Salience in Text Summarization.” ArXiv](https://doi.org/10.1613/jair.1523)\n\n[228\\. Multi-document Summarization in Medical Literature using PICO-Masking Approach](https://openreview.net/pdf/fa04899e439afb7273744485bb4b1c0fa33d4025.pdf)\n\n[229\\. Legal Texts Semantic Retrieval and Contract Discovery](https://aclanthology.org/people/a/agnieszka-kaliska/)\n\n[230\\. Large Language Model in Medical Information Extraction from Titles and Abstracts with Prompt Engineering Strategies: A Comparative Study of GPT-3.5 and GPT-4](http://medrxiv.org/cgi/reprint/2024.03.20.24304572v1)\n\n[231\\. Artificial Intelligence in Business Management](http://edl.emi.gov.et/jspui/bitstream/123456789/1721/1/_OceanofPDF.com_Artificial_Intelligence_in_Business_Management_-_Teik_Toe_Teoh.pdf)\n\n[232\\. A. Shukla, Paheli Bhattacharya et al. “Legal Case Document Summarization: Extractive and Abstractive Methods and their Evaluation.” AACL](https://doi.org/10.48550/arXiv.2210.07544)\n\n[233\\. Mohamed Yassine Landolsi, L. Hlaoua et al. “Information extraction from electronic medical documents: state of the art and future research directions.” Knowledge and Information Systems](https://doi.org/10.1007/s10115-022-01779-1)\n\n[234\\. Document Summarization](https://www.ibm.com/architectures/patterns/genai-document-summarization)\n\n[235\\. Oct 2024 Benchmark of OpenAI, Anthropic, and Google LLMs for Legal Tasks on SpotDraft](https://www.spotdraft.com/blog/benchmark-of-llms-oct-2024)\n\n[241\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[242\\. Dzmitry Bahdanau, Kyunghyun Cho et al. “Neural Machine Translation by Jointly Learning to Align and Translate.” CoRR](https://arxiv.org/abs/1409.0473)\n\n[243\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[244\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[245\\. LV-EVAL: A BALANCED LONG-CONTEXT BENCHMARK WITH 5 LENGTH LEVELS UP TO 256K](https://openreview.net/pdf/4528a7c2dcaa7fd2afdcc62e4a10f32b4f5bd020.pdf)\n\n[246\\. Karl Moritz Hermann, Tomás Kociský et al. “Teaching Machines to Read and Comprehend.” ArXiv](https://arxiv.org/abs/1506.03340)\n\n[247\\. Comparison: RAG vs. Long Context Window models](https://www.legionintel.com/blog/rag-systems-vs-lcw-performance-and-cost-trade-offs)\n\n[248\\. Yuri Kuratov, Aydar Bulatov et al. “In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss.” ArXiv](https://doi.org/10.48550/arXiv.2402.10790)\n\n[249\\. RAG-DDR: OPTIMIZING RETRIEVAL-AUGMENTED GENERATION USING DIFFERENTIABLE DATA REWARDS](https://openreview.net/pdf/0bd1e9043e05053540319ae610077d2a586cfb70.pdf)\n\n[250\\. Multi Needle in a Haystack: Exploring Long Context LLM Retrieval and Reasoning](https://blog.langchain.dev/multi-needle-in-a-haystack/)\n\n[251\\. Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems](https://paperreading.club/page?id=237671)\n\n[252\\. Retrieval-augmented Generation: Enhancing Factual Consistency and Reducing Hallucinations in NLP](https://www.catalyzex.com/s/Retrieval-augmented%20Generation)\n\n[253\\. The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models](https://cs.uwaterloo.ca/~jimmylin/publications/3726302.3730090.pdf)\n\n[254\\. GitHub - dev-abuke/RAG-System-Optimization: The goal of this project is to develop a Retrieval-Augmented Generation (RAG) system, benchmark its performance, and implement an optimization to improve the results.](https://github.com/dev-abuke/RAG-System-Optimization)\n\n[255\\. Benchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation Metrics and Datasets](https://www.scitepress.org/Papers/2024/130657/130657.pdf)\n\n[256\\. Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents](http://www.arxiv.org/pdf/2411.16740)\n\n[257\\. A Deep Dive into Retrieval Augmented Generation (RAG) vs. Fine-tuning](https://www.strong.io/blog/a-deep-dive-into-retrieval-augmented-generation-rag-vs-fine-tuning)\n\n[261\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[262\\. Why more developers are choosing RTX 4090 over A100 for AI workloads](https://compute.hivenet.com/post/why-more-developers-are-choosing-rtx-4090-over-a100#:~:text=On%20paper,%20it%20delivers%20around,the%20A100%20in%20raw%20throughput.)\n\n[263\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[264\\. I. Loshchilov, F. Hutter. “Decoupled Weight Decay Regularization.” International Conference on Learning Representations](https://www.semanticscholar.org/paper/d07284a6811f1b2745d91bdb06b040b57f226882)\n\n[265\\. PUZZLE: DISTILLATION-BASED NAS FOR INference-Optimized LLMs](https://arxiv.org/pdf/2411.19146)\n\n[266\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[267\\. Generative Representational Instruction Tuning](https://arxiv.org/pdf/2402.09906)\n\n[268\\. RTX 5090 in AI Tasks: Evaluating NVIDIA's New GPU](https://hostkey.com/blog/81-jensen-huang-seriously-testing-the-nvidia-rtx-5090-in-ai-workflows/)\n\n[269\\. MAGICPIG: LSH Sampling for Efficient LLM Generation](https://www.arxiv.org/pdf/2410.16179v1)\n\n[270\\. GOLF: Unleashing GPU-Driven Acceleration for FALCON Post-Quantum Cryptography](https://eprint.iacr.org/2025/749.pdf)\n\n[271\\. Сравнительный анализ графических процессоров NVIDIA GeForce RTX 5070 и RTX 4090](https://vsegpt.ru/pdfs/research_5070_vs_4090_deep_10.pdf)\n\n[272\\. NVIDIA RTX 5090 vs. RTX 4090 – Comparison, benchmarks for AI, LLM Workloads](https://bizon-tech.com/blog/nvidia-rtx-5090-comparison-gpu-benchmarks-for-ai?srsltid=AfmBOookrIWVbTpGC-dRCv7Xlmm_We0G3P6vSbeq83qblqjPN7Y4vPYR)\n\n[273\\. Woosuk Kwon, Zhuohan Li et al. “Efficient Memory Management for Large Language Model Serving with PagedAttention.” Proceedings of the 29th Symposium on Operating Systems Principles](https://doi.org/10.1145/3600006.3613165)\n\n[274\\. TELERAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval](https://arxiv.org/pdf/2502.20969)\n\n[275\\. 通过上下文检索优化RAG的语境理解](https://www.explinks.com/blog/better-context-for-your-rag-with-contextual-retrieval/)\n\n[276\\. Low P2P GPU bandwidth performance between GeForce GPUs](https://forums.developer.nvidia.com/t/low-p2p-gpu-bandwidth-performance-between-geforce-gpus/308334?page=2)\n\n[277\\. High-Throughput GPU Implementation of Dilithium Post-Quantum Digital Signature](https://eprint.iacr.org/2024/1365.pdf)\n\n[278\\. Two optimizations to GPU code generation in the Futhark compiler](https://futhark-lang.org/student-projects/christian-anders-scan-reduce-msc-project.pdf)\n\n[279\\. Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation](https://arxiv.org/pdf/2502.13145)\n\n[281\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[282\\. Information Extraction from Contracts Using Large Language Models](https://fse.studenttheses.ub.rug.nl/34210/1/mAI2024CornelisG.pdf)\n\n[283\\. A. See, Peter J. Liu et al. “Get To The Point: Summarization with Pointer-Generator Networks.” ArXiv](https://doi.org/10.18653/v1/P17-1099)\n\n[284\\. Günes Erkan, Dragomir R. Radev. “LexRank: Graph-based Lexical Centrality as Salience in Text Summarization.” ArXiv](https://doi.org/10.1613/jair.1523)\n\n[285\\. Closing the gap between open source and commercial large language models for medical evidence summarization](https://pmc.ncbi.nlm.nih.gov/articles/PMC11383939/)\n\n[286\\. Alistair E. W. Johnson, T. Pollard et al. “MIMIC-III, a freely accessible critical care database.” Scientific Data](https://doi.org/10.1038/sdata.2016.35)\n\n[287\\. Yang Liu, Mirella Lapata. “Text Summarization with Pretrained Encoders.” ArXiv](https://doi.org/10.18653/v1/D19-1387)\n\n[288\\. May Myo Zin, Ha-Thanh Nguyen et al. “Information Extraction from Lengthy Legal Contracts: Leveraging Query-Based Summarization and GPT-3.5.” International Conference on Legal Knowledge and Information Systems](https://doi.org/10.3233/FAIA230963)\n\n[289\\. Proceedings of the 9th SwissText Conference](https://www.swisstext.org/wp-content/uploads/2024/06/Proceedings_Preprint.pdf)\n\n[290\\. Large Language Model in Medical Information Extraction from Titles and Abstracts with Prompt Engineering Strategies: A Comparative Study of GPT-3.5 and GPT-4](http://medrxiv.org/cgi/reprint/2024.03.20.24304572v1)\n\n[291\\. MEDVOC: Vocabulary Adaptation for Fine-tuning Pre-trained Language Models on Medical Text Summarization](https://www.ijcai.org/proceedings/2024/0683.pdf)\n\n[292\\. Closing the Gap Between Open Source and Commercial Large Language Models for Medical Evidence Summarization](https://www.nature.com/articles/s41746-024-01239-w)\n\n[293\\. MED-OMIT: Extrinsic-Focused Evaluation Metric for Omissions in Medical Summarization](https://arxiv.org/pdf/2311.08303.pdf)\n\n[294\\. Automated Semantic Analysis, Legal Assessment, and Summarization of Standard Form Contracts](https://mediatum.ub.tum.de/doc/1581193/h6380wz6h5srahb6llt2d1jd7.Dissertation-Daniel_Braun.pdf)\n\n[295\\. A. Shukla, Paheli Bhattacharya et al. “Legal Case Document Summarization: Extractive and Abstractive Methods and their Evaluation.” AACL](https://doi.org/10.48550/arXiv.2210.07544)\n\n[296\\. A Framework to Assess Clinical Safety and Hallucination Rates of LLMs for Medical Text Summarisation](https://www.medrxiv.org/content/10.1101/2024.09.12.24313556v2.full.pdf)\n\n[301\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[302\\. Iz Beltagy, Matthew E. Peters et al. “Longformer: The Long-Document Transformer.” ArXiv](https://arxiv.org/abs/2004.05150)\n\n[303\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[304\\. Alex Graves, Greg Wayne et al. “Neural Turing Machines.” ArXiv](https://arxiv.org/abs/1410.5401)\n\n[305\\. Comparison: RAG vs. Long Context Window models](https://www.legionintel.com/blog/rag-systems-vs-lcw-performance-and-cost-trade-offs)\n\n[306\\. Jack W. Rae, Anna Potapenko et al. “Compressive Transformers for Long-Range Sequence Modelling.” ArXiv](https://arxiv.org/abs/1911.05507)\n\n[307\\. RAG-DDR: OPTIMIZING RETRIEVAL-AUGMENTED GENERATION USING DIFFERENTIABLE DATA REWARDS](https://openreview.net/pdf/0bd1e9043e05053540319ae610077d2a586cfb70.pdf)\n\n[308\\. Improving Retrieval Augmented Generation accuracy with GraphRAG](https://aws.amazon.com/th/blogs/machine-learning/improving-retrieval-augmented-generation-accuracy-with-graphrag/)\n\n[309\\. Retrieval-augmented Generation: Enhancing Factual Consistency and Reducing Hallucinations in NLP](https://www.catalyzex.com/s/Retrieval-augmented%20Generation)\n\n[310\\. EpMAN: 用于泛化到更长上下文的情景记忆注意力](https://www.xueshuxiangzi.com/downloads/2025_2_21/2502.14280.pdf)\n\n[311\\. Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems](https://paperreading.club/page?id=237671)\n\n[312\\. Yuri Kuratov, Aydar Bulatov et al. “In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss.” ArXiv](https://doi.org/10.48550/arXiv.2402.10790)\n\n[313\\. Retrieval Augmented Generation of Summarized Answers on Visually-Rich Documents for Trend and Risk Analysis](https://ceur-ws.org/Vol-3946/DARLI-AP-6.pdf)\n\n[314\\. Multi Needle in a Haystack: Exploring Long Context LLM Retrieval and Reasoning](https://blog.langchain.dev/multi-needle-in-a-haystack/)\n\n[315\\. The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models](https://cs.uwaterloo.ca/~jimmylin/publications/3726302.3730090.pdf)\n\n[316\\. GitHub - dev-abuke/RAG-System-Optimization: The goal of this project is to develop a Retrieval-Augmented Generation (RAG) system, benchmark its performance, and implement an optimization to improve the results.](https://github.com/dev-abuke/RAG-System-Optimization)\n\n[317\\. Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents](http://www.arxiv.org/pdf/2411.16740)\n\n[318\\. Memory-augmented generative adversarial transformers](https://scholarlypublications.universiteitleiden.nl/access/item%3A3754747/view)\n\n[321\\. Adam Paszke, Sam Gross et al. “PyTorch: An Imperative Style, High-Performance Deep Learning Library.” ArXiv](https://arxiv.org/abs/1912.01703)\n\n[322\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[323\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[324\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[325\\. Why more developers are choosing RTX 4090 over A100 for AI workloads](https://compute.hivenet.com/post/why-more-developers-are-choosing-rtx-4090-over-a100#:~:text=On%20paper,%20it%20delivers%20around,the%20A100%20in%20raw%20throughput.)\n\n[326\\. I. Loshchilov, F. Hutter. “Decoupled Weight Decay Regularization.” International Conference on Learning Representations](https://www.semanticscholar.org/paper/d07284a6811f1b2745d91bdb06b040b57f226882)\n\n[327\\. Retentive Network](https://openreview.net/pdf?id=sxZlp9ZoHD)\n\n[328\\. Generative Representational Instruction Tuning](https://openreview.net/pdf?id=BC4lIvfSzv)\n\n[329\\. Llama 3.2 全栈优化释放 NVIDIA GPU 的高性能](https://developer.nvidia.com/zh-cn/blog/?p=12199)\n\n[330\\. Сравнительный анализ графических процессоров NVIDIA GeForce RTX 5070 и RTX 4090](https://vsegpt.ru/pdfs/research_5070_vs_4090_deep_10.pdf)\n\n[331\\. NVIDIA RTX 5090 vs 4090: What’s New & How Much Faster Is It?](https://gamemaxpc.com/pc-case-news/6044.html)\n\n[332\\. Pruning Large Language Models with Semi-Structural Adaptive Sparse Training](https://arxiv.org/pdf/2407.20584)\n\n[333\\. NVIDIA GeForce RTX 5080 Founders Edition Review](https://www.techpowerup.com/review/nvidia-geforce-rtx-5080-founders-edition/40.html)\n\n[334\\. Nvidia GeForce RTX 4090 Review: Queen of the Castle](https://www.tomshardware.com/reviews/nvidia-geforce-rtx-4090-review/5)\n\n[335\\. Flexible Operator Fusion for Fast Sparse Transformer with Diverse Masking on GPU](https://arxiv.org/pdf/2506.06095)\n\n[336\\. NVIDIA RTX 5090 vs. RTX 4090 – Comparison, benchmarks for AI, LLM Workloads](https://bizon-tech.com/blog/nvidia-rtx-5090-comparison-gpu-benchmarks-for-ai?srsltid=AfmBOookrIWVbTpGC-dRCv7Xlmm_We0G3P6vSbeq83qblqjPN7Y4vPYR)\n\n[337\\. NVIDIA ADA GPU ARCHITECTURE](https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf)\n\n[338\\. Efficient Accelerator-Rich Computers for Future Applications](https://escholarship.org/content/qt68w3z4vq/qt68w3z4vq.pdf)\n\n[339\\. Nvidia RTX 4090 vs RTX 3090 Ti](https://www.pcguide.com/gpu/nvidia-rtx-4090-vs-rtx-3090-ti/)\n\n[340\\. NVIDIA GeForce RTX 4090 GPU提供比筆記型電腦CPU高達15倍的AI吞吐量](https://www.xfastest.com/thread-289001-1-1.html)\n\n[341\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[342\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[343\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[344\\. Information Extraction from Contracts Using Large Language Models](https://fse.studenttheses.ub.rug.nl/34210/1/mAI2024CornelisG.pdf)\n\n[345\\. Few-Shot Prompting vs Fine-Tuning LLM for Generative AI Solutions](https://skimai.com/few-shot-prompting-vs-fine-tuning-llm-for-generative-ai-solutions/)\n\n[346\\. Rada Mihalcea, P. Tarau. “TextRank: Bringing Order into Text.” Conference on Empirical Methods in Natural Language Processing](https://www.semanticscholar.org/paper/7b95d389bc6affe6a127d53b04bcfd68138f1a1a)\n\n[347\\. Günes Erkan, Dragomir R. Radev. “LexRank: Graph-based Lexical Centrality as Salience in Text Summarization.” ArXiv](https://doi.org/10.1613/jair.1523)\n\n[348\\. Closing the gap between open source and commercial large language models for medical evidence summarization](https://pmc.ncbi.nlm.nih.gov/articles/PMC11383939/)\n\n[349\\. Proceedings of the 9th SwissText Conference](https://www.swisstext.org/wp-content/uploads/2024/06/Proceedings_Preprint.pdf)\n\n[350\\. May Myo Zin, Ha-Thanh Nguyen et al. “Information Extraction from Lengthy Legal Contracts: Leveraging Query-Based Summarization and GPT-3.5.” International Conference on Legal Knowledge and Information Systems](https://doi.org/10.3233/FAIA230963)\n\n[351\\. Large Language Model in Medical Information Extraction from Titles and Abstracts with Prompt Engineering Strategies: A Comparative Study of GPT-3.5 and GPT-4](http://medrxiv.org/cgi/reprint/2024.03.20.24304572v1)\n\n[352\\. How to Choose Between RAG and Fine Tuning?](https://www.mindee.com/blog/rag-vs-fine-tuning)\n\n[353\\. Automated Semantic Analysis, Legal Assessment, and Summarization of Standard Form Contracts](https://mediatum.ub.tum.de/doc/1581193/h6380wz6h5srahb6llt2d1jd7.Dissertation-Daniel_Braun.pdf)\n\n[354\\. Comparative Analysis of Open-Source Language Models in Summarizing Medical Text Data](https://arxiv.org/html/2405.16295v2)\n\n[355\\. MEDVOC: Vocabulary Adaptation for Fine-tuning Pre-trained Language Models on Medical Text Summarization](https://www.ijcai.org/proceedings/2024/0683.pdf)\n\n[356\\. Yajnaseni Dash, Abhay Kumar et al. “Advances in Medical Text Summarization: Comparative Performance Analysis of PEGASUS and T5.” 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)](https://doi.org/10.1109/ICCCNT61001.2024.10724845)\n\n[357\\. MARS: MedicAl thRead Summarization Dataset based on IIYI with Comparative Analysis of Large Language Models](https://digital.lib.washington.edu/researchworks/items/b4f60f21-f2c7-452e-bee4-2948aab9ec48)\n\n[358\\. MED-OMIT: Extrinsic-Focused Evaluation Metric for Omissions in Medical Summarization](https://arxiv.org/pdf/2311.08303.pdf)\n\n[361\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[362\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[363\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[364\\. LV-EVAL: A BALANCED LONG-CONTEXT BENCHMARK WITH 5 LENGTH LEVELS UP TO 256K](https://openreview.net/pdf/4528a7c2dcaa7fd2afdcc62e4a10f32b4f5bd020.pdf)\n\n[365\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[366\\. Kelvin Guu, Kenton Lee et al. “REALM: Retrieval-Augmented Language Model Pre-Training.” ArXiv](https://arxiv.org/abs/2002.08909)\n\n[367\\. Comparison: RAG vs. Long Context Window models](https://www.legionintel.com/blog/rag-systems-vs-lcw-performance-and-cost-trade-offs)\n\n[368\\. Mem0：构建具备可扩展长期记忆的生产级AI代理](https://www.chatpaper.ai/zh/dashboard/paper/f14ab931-4948-44d0-ae1f-e04f773ade61)\n\n[369\\. RAG-DDR: OPTIMIZING RETRIEVAL-AUGMENTED GENERATION USING DIFFERENTIABLE DATA REWARDS](https://openreview.net/pdf/0bd1e9043e05053540319ae610077d2a586cfb70.pdf)\n\n[370\\. Improving Retrieval Augmented Generation accuracy with GraphRAG](https://aws.amazon.com/th/blogs/machine-learning/improving-retrieval-augmented-generation-accuracy-with-graphrag/)\n\n[371\\. Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems](https://paperreading.club/page?id=237671)\n\n[372\\. The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models](https://cs.uwaterloo.ca/~jimmylin/publications/3726302.3730090.pdf)\n\n[373\\. Yuri Kuratov, Aydar Bulatov et al. “In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss.” ArXiv](https://doi.org/10.48550/arXiv.2402.10790)\n\n[374\\. Multi Needle in a Haystack: Exploring Long Context LLM Retrieval and Reasoning](https://blog.langchain.dev/multi-needle-in-a-haystack/)\n\n[375\\. Retrieval-augmented Generation: Enhancing Factual Consistency and Reducing Hallucinations in NLP](https://www.catalyzex.com/s/Retrieval-augmented%20Generation)\n\n[376\\. Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents](http://www.arxiv.org/pdf/2411.16740)"
  },
  {
    "id": 15,
    "question": "What are the differences and connections between the supervised fine-tuning, value alignment of Large Multi-Modal Models (LMMs), and pure text-based Large Language Models (LLMs)?",
    "response": "**Research Report: Differences and Connections Between Supervised Fine-Tuning, Value Alignment, and Large Multi-Modal Models**\n\n**1\\. Introduction: Conceptual Foundations**\n\n**Supervised Fine-Tuning (SFT)** adapts pre-trained models using labeled data to specialize capabilities for downstream tasks. In text-based LLMs, this involves domain-specific textual datasets \\[3\\]\\[10\\]\\[11\\]while **Large Multi-Modal Models (LMMs)** require cross-modal alignment of features (e.g., image-text pairs) during SFT \\[4\\]\\[13\\]\\[14\\]. **Value alignment** ensures outputs align with human ethics and intent, using methods like Reinforcement Learning from Human Feedback (RLHF) for both LLMs and LMMs \\[64\\]\\[66\\]. For LMMs, this extends to visual harmlessness, fidelity, and cross-modal consistency \\[23\\]\\[23\\].\n\n**2\\. Technical Differences in Supervised Fine-Tuning**\n\n**2.1. Text-Based LLMs**\n\n**Data & Architecture**: Exclusively textual datasets curated by human experts; fine-tuning adjusts transformer parameters via maximum likelihood estimation \\[3\\]\\[10\\].\n\n**Optimization Goal**: Task-specific performance (e.g., summarization, translation) without modality interactions \\[10\\]\\[12\\].\n\n**2.2. Multi-Modal LMMs**\n\n**Data Complexity**: Combines images, audio, and text. Techniques like **one-step instruction fine-tuning** (e.g., Macaw-LLM) align features without separate projection layers \\[4\\].\n\n**Parameter Adaptation**: Joint tuning of vision encoders and LLMs to fuse modalities. Partial learnable parameters bridge vision-text gaps \\[14\\]\\[15\\].\n\n**Computational Cost**: Higher due to cross-modal operations and memory-intensive data \\[9\\]\\[16\\].\n\n**Key Difference**: LMM SFT requires explicit **modality fusion** (e.g., linear adapters), absent in text-only workflows \\[4\\]\\[13\\].\n\n**3\\. Divergences in Value Alignment Techniques**\n\n**3.1. Text-Based LLMs**\n\n**Methods**: RLHF with reward models trained on human preferences; **variational best-of-N** sampling improves ethical text generation \\[24\\]\\[66\\].\n\n**Benchmarks**: Ethics-focused frameworks like **TruthfulQA** (hallucination measurement) and **CLAVE** (Schwartz Basic Values) \\[87\\]\\[98\\].\n\n**3.2. Multi-Modal LMMs**\n\n**Multimodal Consistency**: Human-aligned preference annotators evaluate image-text coherence (e.g., object counts, spatial relationships) \\[23\\]\\[23\\].\n\n**Safety Tax**: SFT for harmlessness degrades reasoning capabilities more severely than in text models \\[71\\].\n\n**Automated Risks**: Bias amplification from imperfect cross-modal reward models \\[30\\]\\[30\\].\n\n**Key Challenge**: LMM alignment must enforce **cross-modal ethical consistency** (e.g., non-toxic images+text), a non-issue for text-only models \\[31\\].\n\n**4\\. Interconnections and Integrated Training Pipelines**\n\n**4.1. Unified Fine-Tuning (UFT)**\n\n**Method**: Combines SFT and RLHF in one stage via an **implicit reward function**, avoiding catastrophic forgetting \\[285\\]\\[294\\].\n\n**Implementation**:\n\n1.  Convert pre-training data to instruction-aware format.\n2.  Optimize using generalized reward (e.g., for LLaVA: model_architecture=\"llava_next\" in frameworks like Vision-LLM-Alignment) \\[285\\]\\[341\\].\n\n**Quantitative Benefit**: UFT outperforms sequential SFT+RLHF in instruction-following (+12% accuracy) and factuality \\[285\\].\n\n**4.2. RLHF Integration in LMMs**\n\n**Examples**:\n\n**InstructGPT**: Sequential SFT → RLHF for text \\[66\\].\n\n**LLaVA 1.6**: Unified pipelines using **DPO (Direct Preference Optimization)** for vision-language safety \\[341\\].\n\n**Tools**: ms-swift supports PPO, DPO, and KTO for multi-modal RLHF \\[78\\].\n\n**5\\. Catastrophic Forgetting: Mitigation Advances**\n\n**Model Tailor (2025)**: Hybrid preservation-adaptation strategy using **Lagrangian optimization**. For parameter perturbation ΔΘ:\n\nwhere **H** is the Hessian matrix. The solution uses **Hessian inverse diagonal elements** for weight compensation:\n\n\\[261\\]\\[261\\]\\[261\\].\n\n**Efficacy**: Reduces forgetting by 47% versus LoRA in MLLMs \\[101\\].\n\n**6\\. Evaluation Benchmarks and Model Performance**\n\n**6.1. Ethics-Focused Frameworks**\n\n**HumaniBench (2025)**: Multi-modal fairness/empathy evaluation \\[127\\]. Lacks Gemini/GPT-4o metrics.\n\n**ETHIC**: Gemini 1.5 Pro (69.1% recall) outperforms GPT-4o (49.5%) in ethical recall tasks \\[307\\].\n\n**TruthfulQA**: LMMs like **visually-instructed LLaMA2** exceed text-only models (46.0% vs. 29.5%) due to richer instruction data \\[87\\].\n\n**6.2. Cross-Model Comparisons**\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n| **Model** | **MMMU (Vision)** | **TruthfulQA (Text)** | **ETHIC Recall** |\n| **Gemini 1.5 Pro** | 58.5% | 54.6 → 64.8\\* | 69.1% |\n| **GPT-4o** | 69.1% | Comparable | 49.5% |\n\n\\*\\[185\\]\\[241\\]\\[307\\]\\*Quantization improvement\n\n**7\\. Challenges and Future Trajectories**\n\n**Data Scarcity**: High-quality multi-modal preference datasets remain rare \\[45\\]\\[55\\].\n\n**Evaluation Gaps**: No standardized ethics benchmarks for LMMs (e.g., HumaniBench adoption pending) \\[122\\].\n\n**Scalability**: UFT and Model Tailor reduce compute costs but need hardware-aware optimizations \\[18\\]\\[285\\].\n\n**Cultural Alignment**: Heterogeneous value metrics (e.g., Schwartz Values) require localization \\[135\\].\n\n**8\\. Conclusion**\n\nSFT and value alignment in LMMs extend text-only paradigms through **cross-modal fusion** (SFT) and **multi-sensory harm mitigation** (alignment). Innovations like **Model Tailor** and **UFT** address catastrophic forgetting and training inefficiencies. While Gemini 1.5 Pro leads in ethical recall, GPT-4o excels in visual reasoning. Future work must prioritize **culturally-inclusive datasets** and **standardized multi-modal ethics benchmarks** to mature LMM alignment.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549)\n\n[2\\. Fine-Tuning / Instruction Tuning](https://people.cs.umass.edu/~hschang/cs685/slides/07_Fine-tuning_0303.pdf)\n\n[3\\. Supervised Fine-Tuning Vs RLHF for LLMs](https://incubity.ambilio.com/supervised-fine-tuning-vs-rlhf-for-llms/)\n\n[4\\. Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration](https://arxiv.org/html/2306.09093)\n\n[5\\. Harnessing the Power of Multimodal and Textual Data in Industry 4.0](https://theses.hal.science/tel-04280319/file/115657_PELLEGRAIN_2023_archivage.pdf)\n\n[6\\. Working Paper on Large Language Models (LLMs)](https://www.bfdi.bund.de/SharedDocs/Downloads/EN/Berlin-Group/20241206-WP-LLMs.pdf?__blob=publicationFile&v=1)\n\n[7\\. LLM-Informed Discrete Prompt Optimization](https://openreview.net/attachment?id=d0jQuZe6k0&name=pdf)\n\n[8\\. Supervised Fine-tuning for Large Language Models](https://www.futurebeeai.com/blog/supervised-fine-tuning-for-large-language-model)\n\n[9\\. What is supervised fine-tuning in LLMs? Unveiling the process](https://nebius.ai/blog/posts/fine-tuning/supervised-fine-tuning)\n\n[10\\. The 4 Stages of Training Large Language Models (LLMs): A Complete Guide](https://www.zco.com/blog/training-large-language-models/)\n\n[11\\. How to Fine-Tune Large Language Models: Best Practices](https://blog.spheron.network/how-to-fine-tune-large-language-models-best-practices)\n\n[12\\. LLM Supervised Fine-Tuning: How to Choose the Right Model](https://www.sapien.io/blog/llm-supervised-fine-tuning-how-to-choose-the-right-model#:~:text=Supervised%20fine-tuning,%20often%20referred,specialized%20dataset%20with%20labeled%20data.)\n\n[13\\. Vision LLMs: Bridging the Gap Between Vision and Language](https://ignitarium.com/vision-llms-bridging-the-gap-between-vision-and-language/)\n\n[14\\. TinyLLaVA: A Framework of Small-scale Large Multimodal Models](http://arxiv.org/html/2402.14289v1)\n\n[15\\. DeepSeek-VL2: Mixture-of-Experts Vision-Language Models - Zilliz blog](https://zilliz.com/blog/deepseek-vl2-mixture-of-experts-vision-language-models-for-advanced-multimodal-understanding)\n\n[16\\. Fine-tuning LLMs 101](https://datasciencedojo.com/blog/fine-tuning-llms/)\n\n[17\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[18\\. 基于多角色微调开源大型语言模型的科学文章关键见解提取系统](https://www.nature.com/articles/s41598-025-85715-7)\n\n[19\\. Supervised Fine-tuning: customizing LLMs](https://mantisnlp.com/blog/supervised-fine-tuning-customizing-llms/)\n\n[20\\. Position: Towards a Responsible LLM-empowered Multi-Agent Systems](https://arxiv.org/pdf/2502.01714)\n\n[21\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[22\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[23\\. Multimodal Large Language Models Make Text-to-Image Generative Models Align Better](https://proceedings.neurips.cc/paper_files/paper/2024/file/9421261e06f1a63a352b068f1ac90609-Paper-Conference.pdf)\n\n[24\\. Improved Value Alignment in Large Language Models Using Variational Best-of-N Techniques](https://assets-eu.researchsquare.com/files/rs-4794797/v1_covered_3163667d-ce4d-4901-9d8c-ed52e07e2edf.pdf)\n\n[25\\. Large Multimodal Models (LMMs) vs LLMs in 2025](https://research.aimultiple.com/large-multimodal-models/#:~:text=LMMs:%20They%20are%20designed%20to,data%20types%20like%20sensory%20data.)\n\n[26\\. 场景模型与语言教学的关系](http://www.mediationoklahomacity.com/post/45586.html)\n\n[27\\. Evaluating Multimodal AI Systems: A Comparative Analysis of Large Language Model-Based Models for Text, Image, and Video Generation](https://digitalcommons.georgiasouthern.edu/cgi/viewcontent.cgi?article=4167&context=etd)\n\n[28\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[29\\. An Introduction to Large Multimodal Models](https://www.alexanderthamm.com/en/blog/an-introduction-to-large-multimodal-models/)\n\n[30\\. A Comprehensive Survey and Guide to Multimodal Large Language Models in Vision-Language Tasks](https://www.arxiv.org/pdf/2411.06284)\n\n[31\\. Large Multimodal Models (LMMs) vs Large Language Models (LLMs)](https://research.aimultiple.com/large-multimodal-models/)\n\n[32\\. Shap-CA: Shapley Value-based Contrastive Alignment for Multimodal Information Extraction](https://openreview.net/pdf/ce271fe2086f3533483aed6d09e00bce91ae81b4.pdf)\n\n[33\\. Large-scale Multi-modal Pre-trained Models: A Comprehensive Survey](https://link.springer.com/content/pdf/10.1007/s11633-022-1410-8.pdf)\n\n[34\\. Large Language Model (LLM): Everything You Need to Know](https://www.weka.io/learn/guide/ai-ml/what-is-llm/)\n\n[35\\. 大型语言模型的多国价值对齐基准](https://www.xueshuxiangzi.com/downloads/2025_4_18/2504.12911.pdf)\n\n[41\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[42\\. Supervised Fine-Tuning Vs RLHF for LLMs](https://incubity.ambilio.com/supervised-fine-tuning-vs-rlhf-for-llms/)\n\n[43\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[44\\. Aligning Diffusion Models by Optimizing Human Utility](https://proceedings.neurips.cc/paper_files/paper/2024/file/2c487f8a54cf24c0684c32abc77fed56-Paper-Conference.pdf)\n\n[45\\. Mechanism Design for LLM Fine-tuning with Multiple Reward Models](https://www.microsoft.com/en-us/research/uploads/prod/2024/12/neurips24workshop_RLHF_Mechanism_Design.pdf)\n\n[46\\. Jean Kaddour, J. Harris et al. “Challenges and Applications of Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2307.10169)\n\n[47\\. 多模态对齐与融合: 一项综述](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_11_27/2411.17040.pdf)\n\n[48\\. Towards an End-to-End Personal Fine-Tuning Framework for AI Value Alignment](https://lukaspetersson.com/assets/pdf/ftva_paper.pdf)\n\n[49\\. Meet Automorphic: An AI Startup that Enables Developers to Build and Improve Custom Fine-Tuned Artificial Intelligence Models Rapidly](https://www.solab.ai/discussion/meet-automorphic-an-ai-startup-that-enables-d-8eBTLoCv)\n\n[50\\. Timothy R. McIntosh, Teo Susnjak et al. “From Google Gemini to OpenAI Q\\* (Q-Star): A Survey of Reshaping the Generative Artificial Intelligence (AI) Research Landscape.” ArXiv](https://doi.org/10.48550/arXiv.2312.10868)\n\n[51\\. MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models](https://openreview.net/pdf/2a13a6784da2b9df088ddaefa93ad3a571ff74a2.pdf)\n\n[52\\. Efficient LLM Alignment via Hierarchical Coarse-to-Fine Refinement](https://openreview.net/pdf/443cd0f256f70a6f339e06856126c04dc44d9c7b.pdf)\n\n[53\\. Multimodal Representation Learning for Agentic AI Systems](https://dspace.mit.edu/bitstream/handle/1721.1/158506/andonian-andonian-phd-eecs-2024-thesis.pdf?sequence=-1&isAllowed=y)\n\n[54\\. Alec Radford, Karthik Narasimhan. “Improving Language Understanding by Generative Pre-Training.”](https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n[55\\. MaxMin-RLHF: Alignment with Diverse Human Preferences](https://openreview.net/attachment?id=8tzjEMF0Vq&name=pdf)\n\n[56\\. Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback](https://www.themoonlight.io/fr/review/align-anything-training-all-modality-models-to-follow-instructions-with-language-feedback)\n\n[57\\. 多模态环境中的多智能体强化学习：预训练大模型视角](https://yingwen.io/files/LLM-MARL.pdf)\n\n[58\\. Fine-tune large multimodal models using Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/fine-tune-large-multimodal-models-using-amazon-sagemaker/)\n\n[59\\. What Is Multimodal AI? A 2025 Guide](https://www.shopify.com/sg/blog/what-is-multimodal-ai)\n\n[60\\. Fine-tuning language models to find agreement among humans with diverse preferences](https://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf)\n\n[61\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[62\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[63\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[64\\. A Survey of MultiModal Large Language Models](https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/C4-2.pdf)\n\n[65\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[66\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[67\\. Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback](https://arxiv.org/html/2402.03746v3)\n\n[68\\. Supervised Fine-Tuning For Model Training](https://www.meegle.com/en_us/topics/supervised-fine-tuning/supervised-fine-tuning-for-model-training)\n\n[69\\. A Comprehensive Overview of Large Language Models](https://arxiv.org/pdf/2307.06435v7)\n\n[70\\. PipeWeaver: Addressing Data Dynamicity in Large Multimodal Model Training with Dynamic Interleaved Pipeline](https://arxiv.org/pdf/2504.14145)\n\n[71\\. Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable](https://arxiv.org/pdf/2503.00555)\n\n[72\\. A Comprehensive Approach to Instruction Tuning for Qwen2.5: Data Selection, Domain Interaction, and Training Protocols](https://www.mdpi.com/2073-431X/14/7/264)\n\n[73\\. Parameter-efficient fine-tuning in large language models: a survey of methodologies](https://link.springer.com/content/pdf/10.1007/s10462-025-11236-4.pdf)\n\n[74\\. Large-scale Multi-modal Pre-trained Models: A Comprehensive Survey](https://link.springer.com/content/pdf/10.1007/s11633-022-1410-8.pdf)\n\n[75\\. MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/pdf/2401.13601v1)\n\n[76\\. Large Vision-Language Model Alignment and Misalignment: A Survey Through the Lens of Explainability](https://arxiv.org/pdf/2501.01346)\n\n[77\\. Towards All-in-One Pre-Training via Maximizing Multi-Modal Mutual Information](https://openaccess.thecvf.com/content/CVPR2023/html/Su_Towards_All-in-One_Pre-Training_via_Maximizing_Multi-Modal_Mutual_Information_CVPR_2023_paper.html)\n\n[78\\. GitHub - DeepWaved/ms-swift: Use PEFT or Full-parameter to finetune 400+ LLMs or 100+ MLLMs.](https://github.com/DeepWaved/ms-swift)\n\n[79\\. Improved Value Alignment in Large Language Models Using Variational Best-of-N Techniques](https://assets-eu.researchsquare.com/files/rs-4794797/v1_covered_3163667d-ce4d-4901-9d8c-ed52e07e2edf.pdf)\n\n[80\\. The life cycle of large language models in education: A framework for understanding sources of bias](https://renzheyu.com/papers/BJET2024.pdf)\n\n[81\\. M5 – A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks](https://openreview.net/pdf/368f1aa4ce6f2d58ccca5e4e9bc45584b8fb8b0b.pdf)\n\n[82\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[83\\. A Survey on LLM-as-a-Judge](https://arxiv.org/pdf/2411.15594v1)\n\n[84\\. Papers with Code - Yixin Liu](https://paperswithcode.com/search?q=author:Yixin%20Liu&order_by=stars)\n\n[85\\. When AI Co-Scientists Fail: SPOT—a Benchmark for Automated Verification of Scientific Research](https://arxiv.org/pdf/2505.11855)\n\n[86\\. Qwen2.5-Omni-7B: The Ultimate End-to-End Multimodal AI Model](http://anakin.ai/blog/qwen2-5-omni-7b-the-ultimate-end-to-end-multimodal-ai-model/)\n\n[87\\. Haoqin Tu, Bingchen Zhao et al. “Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics.” ArXiv](https://doi.org/10.48550/arXiv.2309.07120)\n\n[88\\. WINGS: Learning Multimodal LLMs without Text-only Forgetting](https://proceedings.neurips.cc/paper_files/paper/2024/file/3852f6d247ba7deb46e4e4be9e702601-Paper-Conference.pdf)\n\n[89\\. PANGEA: A FULLY OPEN MULTILINGUAL MULTIMODAL LLM FOR 39 LANGUAGES](https://openreview.net/pdf/8d8d97e138d8b374b8132de1bd7a0666c45f263c.pdf)\n\n[90\\. LLM-based HSE Compliance Assessment: Benchmark, Performance, and Advancements](https://www.themoonlight.io/en/review/llm-based-hse-compliance-assessment-benchmark-performance-and-advancements)\n\n[91\\. SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading](https://publikationen.bibliothek.kit.edu/1000176792/155831304)\n\n[92\\. PROREASON: MULTI-MODAL PROACTIVE REASONING WITH DECOUPLED EYESIGHT AND WISDOM](https://openreview.net/pdf?id=AkUer8ooMi)\n\n[93\\. ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced Understanding and Reasoning](https://arxiv.org/pdf/2412.03104)\n\n[94\\. Aligning Large Multimodal Models with Factually Augmented RLHF](https://openreview.net/pdf/8f30ebbc5cd41ab429d3d0dceceb89c63e27f7da.pdf)\n\n[95\\. POLYMATH: A CHALLENGING MULTI-MODAL MATHEMATICAL REASONING BENCHMARK](https://openreview.net/pdf/9bee6d1d7fa87d07af88c8d71c4db481bc53e3a1.pdf)\n\n[96\\. Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models](https://arxiv.org/pdf/2502.14191)\n\n[97\\. RBench-V: A Primary Assessment for Visual Reasoning Models with Multi-modal Outputs](https://arxiv.org/pdf/2505.16770v2)\n\n[98\\. CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses](https://papers.nips.cc/paper_files/paper/2024/file/6c1d2496c04d1ef648d58684b699643f-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[99\\. What are LLM Benchmarks? Evaluations & Challenges](https://visionx.io/blog/what-are-llm-benchmarks/)\n\n[101\\. Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models](https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhu24l/zhu24l.pdf)\n\n[102\\. Overcoming Catastrophic Forgetting: A Novel Fine-Tuning Method](https://openreview.net/pdf?id=IkwRPlRgUl)\n\n[103\\. How to Alleviate Catastrophic Forgetting in LLMs Finetuning? Hierarchical Layer-Wise and Element-Wise Regularization](https://arxiv.org/html/2501.13669v2)\n\n[104\\. What is Catastrophic Forgetting?](https://www.ibm.com/think/topics/catastrophic-forgetting)\n\n[105\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[106\\. OVERCOMING CATASTROPHIC FORGETTING: A NOVEL FINE-TUNING METHOD](https://openreview.net/pdf/c255593459a7bd978266d7b1868789870dc52535.pdf)\n\n[107\\. 定位后合并：用于缓解多模态大型语言模型中灾难性遗忘的神经元级参数融合](https://www.xueshuxiangzi.com/downloads/2025_5_23/2505.16703.pdf)\n\n[108\\. Multimodal Instruction Tuning with Conditional Mixture of LoRA](https://arxiv.org/pdf/2402.15896)\n\n[109\\. Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks](https://arxiv.org/html/2504.01241v1)\n\n[110\\. Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/html/2506.09428v2)\n\n[111\\. Balancing Speciality and Versatility: A Coarse to Fine Framework for Mitigating Catastrophic Forgetting in Large Language Models](https://arxiv.org/pdf/2404.10306)\n\n[112\\. Pre-train, Align, and Disentangle: Empowering Sequential Recommendation with Large Language Models](https://arxiv.org/pdf/2412.04107)\n\n[113\\. Jinpeng Chen, Runmin Cong et al. “SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning.”](https://arxiv.org/abs/2505.02486)\n\n[121\\. 最新结论·多模态视觉语言模型测评报告](http://h5.ifeng.com/c/vivo/v002Qs---_fMfKL5c4z5p32aHB1sjOiXLbL4AH0zykjvL00K4__)\n\n[122\\. Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models](https://arxiv.org/pdf/2403.20331)\n\n[123\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[124\\. 『弈衡』多模态大模型评测体系白皮书 (2024年)](https://max.book118.com/try_down/638052016026006142.pdf)\n\n[125\\. 2024年多模态大模型全景洞察报告](https://zhuanlan.zhihu.com/p/690344456)\n\n[126\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[127\\. HumaniBench：一个从人性角度评估大型多模态模型的全新框架](https://www.eospa.com.cn/2025/0527/3166825.shtml)\n\n[128\\. MMA: BENCHMARKING MULTI-MODAL LARGE LANGUAGE MODELS IN AMBIGUITY CONTEXTS](https://openreview.net/pdf/6b15da2be50eaa85b587f5d599071cc80c564128.pdf)\n\n[129\\. GitHub - Timothyxxx/Evaluation-Multimodal-LLMs-Survey: A Survey on Benchmarks of Multimodal Large Language Models](https://github.com/Timothyxxx/Evaluation-Multimodal-LLMs-Survey)\n\n[130\\. lmms-eval: 大型多模态模型的评估套件](https://openi.pcl.ac.cn/thomas-yanxin/lmms-eval/src/branch/kr/video/README.md)\n\n[131\\. SuperBench大模型综合能力评测报告（2024年3月）](https://zhumeile.com/wp-content/uploads/2024/09/2024%E5%B9%B405%E6%9C%8817%E6%97%A5%E6%9B%B4%E6%96%B0-superBench%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E5%90%88%E8%83%BD%E5%8A%9B%E8%AF%84%E6%B5%8B%E6%8A%A5%E5%91%8A2024%E5%B9%B43%E6%9C%88.pdf)\n\n[132\\. The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models](https://arxiv.org/pdf/2406.11096)\n\n[133\\. Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework](https://arxiv.org/pdf/2502.18874)\n\n[134\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[135\\. High-Dimension Human Value Representation in Large Language Models](https://openreview.net/pdf/965a48dde1852a4f843374304cc2c702ed6734f2.pdf)\n\n[136\\. Value Compass Leaderboard: A Platform for Fundamental and Validated Evaluation of LLMs Values](https://arxiv.org/pdf/2501.07071)\n\n[137\\. EVALUATING MULTI-MODAL LANGUAGE MODELS THROUGH CONCEPT HACKING](https://openreview.net/pdf?id=B2QXXIZXM3)\n\n[138\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[139\\. Unbiased Evaluation of Large Language Models from a Causal Perspective](https://arxiv.org/pdf/2502.06655)\n\n[141\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[142\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[143\\. OpenAI Josh Achiam, Steven Adler et al. “GPT-4 Technical Report.”](https://arxiv.org/abs/2303.08774)\n\n[144\\. Haoqin Tu, Bingchen Zhao et al. “Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics.” ArXiv](https://doi.org/10.48550/arXiv.2309.07120)\n\n[145\\. Papers with Code - TruthfulQA](https://paperswithcode.com/task/truthfulqa)\n\n[146\\. Improved Value Alignment in Large Language Models Using Variational Best-of-N Techniques](https://assets-eu.researchsquare.com/files/rs-4794797/v1_covered_3163667d-ce4d-4901-9d8c-ed52e07e2edf.pdf)\n\n[147\\. M5 – A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks](https://openreview.net/pdf/368f1aa4ce6f2d58ccca5e4e9bc45584b8fb8b0b.pdf)\n\n[148\\. Priv-IQ: A Benchmark and Comparative Evaluation of Large Multimodal Models on Privacy Competencies](https://www.mdpi.com/2673-2688/6/2/29)\n\n[149\\. When AI Co-Scientists Fail: SPOT—a Benchmark for Automated Verification of Scientific Research](https://arxiv.org/pdf/2505.11855)\n\n[150\\. Haotian Liu, Chunyuan Li et al. “Visual Instruction Tuning.” ArXiv](https://doi.org/10.48550/arXiv.2304.08485)\n\n[151\\. SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading](https://publikationen.bibliothek.kit.edu/1000176792/155831304)\n\n[152\\. Wenliang Dai, Junnan Li et al. “InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.” ArXiv](https://doi.org/10.48550/arXiv.2305.06500)\n\n[153\\. LLM-based HSE Compliance Assessment: Benchmark, Performance, and Advancements](https://www.themoonlight.io/en/review/llm-based-hse-compliance-assessment-benchmark-performance-and-advancements)\n\n[154\\. LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark](https://proceedings.neurips.cc/paper_files/paper/2023/file/548a41b9cac6f50dccf7e63e9e1b1b9b-Paper-Datasets_and_Benchmarks.pdf)\n\n[161\\. Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models](https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhu24l/zhu24l.pdf)\n\n[162\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[163\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[164\\. How to Alleviate Catastrophic Forgetting in LLMs Finetuning? Hierarchical Layer-Wise and Element-Wise Regularization](https://arxiv.org/html/2501.13669v2)\n\n[165\\. Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks](https://arxiv.org/html/2504.01241v1)\n\n[166\\. Investigating the Catastrophic Forgetting in Multimodal Large Language Models](https://zhuanlan.zhihu.com/p/675333375)\n\n[167\\. Pre-train, Align, and Disentangle: Empowering Sequential Recommendation with Large Language Models](https://arxiv.org/pdf/2412.04107)\n\n[168\\. UFT：通过广义隐式奖励函数统一微调 SFT 和 RLHF/DPO/UNA](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_30/2410.21438.pdf)\n\n[169\\. A Balancing Act: Optimizing Classification and Retrieval in Cross-Modal Vision Models](https://openreview.net/pdf?id=dAfbmDPeJL)\n\n[170\\. Diffusion Instruction Tuning](https://arxiv.org/pdf/2502.06814)\n\n[171\\. Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs](https://openreview.net/pdf?id=qD2eFNvtw4)\n\n[172\\. Balancing Speciality and Versatility: A Coarse to Fine Framework for Mitigating Catastrophic Forgetting in Large Language Models](https://arxiv.org/pdf/2404.10306)\n\n[173\\. Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/html/2506.09428v1)\n\n[174\\. CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration](https://openreview.net/pdf?id=FgHpT6u7pk)\n\n[181\\. MMMT-IF: A CHALLENGING MULTIMODAL MULTI-TURN INSTRUCTION FOLLOWING BENCHMARK](https://arxiv.org/pdf/2409.18216)\n\n[182\\. Many-Shot In-Context Learning in Multimodal Foundation Models](https://openreview.net/pdf?id=j2rKwWXdcz)\n\n[183\\. MLLM-COMPBENCH: A Comparative Reasoning Benchmark for Multimodal LLMs](https://proceedings.neurips.cc/paper_files/paper/2024/file/32923dff09f75cf1974c145764a523e2-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[184\\. ETHIC：在高信息覆盖率的长上下文任务中评估大型语言模型](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_23/2410.16848.pdf)\n\n[185\\. GPT-4o vs. Gemini 1.5 Pro vs. Claude 3 Opus: Multimodal AI Model Comparison](https://encord.com/blog/gpt-4o-vs-gemini-vs-claude-3-opus/)\n\n[186\\. EVALUATING SEMANTIC VARIATION IN TEXT-TO-IMAGE SYNTHESIS: A CAUSAL PERSPECTIVE](https://openreview.net/pdf/d1b59a4bd41dc935a97e9fea6812e3c1ca930be8.pdf)\n\n[187\\. GPT-4o: The Cutting-Edge Advancement in Multimodal LLM](https://easychair.org/publications/preprint/z4TJ/download)\n\n[188\\. Gemini 1.5 Pro vs ChatGPT-4o - Top Differences & Comparison](https://livechatai.com/llm-comparison/gemini-1-5-pro-vs-gpt-4o)\n\n[189\\. AI模型GPT-4o与Gemini 1.5 Pro的性能对比评测](https://m.chinastarmarket.cn/detail/1678869)\n\n[190\\. EUREKA: Evaluating and Understanding Large Foundation Models](https://www.microsoft.com/en-us/research/uploads/prod/2024/09/Eureka-Evaluating-and-Understanding-Large-Foundation-Models-Sept-13.pdf)\n\n[191\\. Gemini 1.5 Pro (002) vs GPT-4 - Detailed Performance & Feature Comparison](https://docsbot.ai/models/compare/gemini-1-5-pro-002/gpt-4)\n\n[192\\. COMPETING LARGE LANGUAGE MODELS IN MULTI-AGENT GAMING ENVIRONMENTS](https://openreview.net/attachment?id=DI4gW8viB6&name=pdf)\n\n[193\\. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/pdf/2403.05530v2)\n\n[194\\. SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure Interpretation](https://openreview.net/pdf/720bf9d3b93a3b5fded5cabae281dd02ab05fa14.pdf)\n\n[195\\. Differences Between Gemini 1.5 Pro and GPT-4o](https://techchilli.com/artificial-intelligence/difference-between-gemini-1-5-pro-and-gpt-4o/)\n\n[196\\. Context.ai](https://context.ai/compare/gpt-4o/gemini-1-5-pro)\n\n[197\\. Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling Efficiently Scales Test-Time Compute](https://arxiv.org/pdf/2504.00762)\n\n[198\\. Gemini 1.5 VS ChatGPT-4o](https://aimlapi.com/comparisons/gemini-1-5-vs-chatgpt-4o)\n\n[201\\. Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models](https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhu24l/zhu24l.pdf)\n\n[202\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[203\\. Model Tailor：减轻多模态大语言模型中的灾难性遗忘](https://zhuanlan.zhihu.com/p/683129281)\n\n[204\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[205\\. Investigating the Catastrophic Forgetting in Multimodal Large Language Models](https://zhuanlan.zhihu.com/p/675333375)\n\n[206\\. What is LLM Fine-Tuning? – Everything You Need to Know \\[2024 Guide\\]](https://kili-technology.com/large-language-models-llms/the-ultimate-guide-to-fine-tuning-llms-2024)\n\n[221\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[222\\. A Survey of MultiModal Large Language Models](https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/C4-2.pdf)\n\n[223\\. UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function](https://openreview.net/pdf?id=WL414RO8No)\n\n[224\\. Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning](https://arxiv.org/pdf/2405.10292)\n\n[225\\. Aligning Large Multimodal Models with Factually Augmented RLHF](https://openreview.net/pdf/8f30ebbc5cd41ab429d3d0dceceb89c63e27f7da.pdf)\n\n[226\\. SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning (work in progress)](https://arxiv.org/pdf/2408.05517v2)\n\n[227\\. Supervised Fine-Tuning (SFT) in RLHF: A Comprehensive Guide](https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.mdx)\n\n[228\\. Integrating SFT and RLHF: Strategies for Next-Gen AI Models](https://www.apexdatasciences.ai/post/integrating-sft-and-rlhf-strategies-for-next-gen-ai-models)\n\n[229\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[230\\. UFT：通过广义隐式奖励函数统一微调 SFT 和 RLHF/DPO/UNA](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_30/2410.21438.pdf)\n\n[231\\. BEYOND SINGLE-TURN: A SURVEY ON MULTI-TURN INTERACTIONS WITH LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2504.04717)\n\n[232\\. A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549)\n\n[233\\. Intuitive Fine-Tuning: Towards Unifying SFT and RLHF into a Single Process](https://arxiv.org/html/2405.11870v1)\n\n[234\\. Parameter-efficient fine-tuning in large language models: a survey of methodologies](https://link.springer.com/content/pdf/10.1007/s10462-025-11236-4.pdf)\n\n[235\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[236\\. UFT：统一监督学习与强化学习的微调框架](https://www.chatpaper.ai/zh/dashboard/paper/7568a6ae-2970-40a8-9dc4-f2ea4b4a0c05)\n\n[237\\. Yubo Li, Xiaobin Shen et al. “Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models.”](https://arxiv.org/abs/2504.04717)\n\n[238\\. Open Source Large Language Models](https://www.dhiwise.com/post/top-open-source-large-language-model-tools-and-trends)\n\n[239\\. Yuntao Bai, Andy Jones et al. “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2204.05862)\n\n[241\\. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/pdf/2403.05530v2)\n\n[242\\. MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding](https://openreview.net/pdf/5aa0663c19d9cf6793ee0671cf8005937614afe3.pdf)\n\n[243\\. Baba Is AI: Break the Rules to Beat the Benchmark](http://arxiv.org/html/2407.13729v1)\n\n[244\\. Many-Shot In-Context Learning in Multimodal Foundation Models](https://arxiv.org/pdf/2405.09798)\n\n[245\\. 长文本语言模型的可控性评估：人造生物故事让评估更全面、更可靠](https://www.techwalker.com/2025/0608/3167401.shtml)\n\n[246\\. MLLM-COMPBENCH: A Comparative Reasoning Benchmark for Multimodal LLMs](https://proceedings.neurips.cc/paper_files/paper/2024/file/32923dff09f75cf1974c145764a523e2-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[247\\. Bench-V: A Primary Assessment for Visual Reasoning Models with Multi-modal Outputs](https://arxiv.org/html/2505.16770v2)\n\n[248\\. Shangyu Xing, Changhao Xiang et al. “GePBench: Evaluating Fundamental Geometric Perception for Multimodal Large Language Models.”](https://arxiv.org/abs/2412.21036)\n\n[249\\. MRAG-BENCH: VISION-CENTRIC EVALUATION FOR RETRIEVAL-AUGMENTED MULTIMODAL MODELS](https://arxiv.org/pdf/2410.08182)\n\n[250\\. MileBench: Benchmarking MLLMs in Long Context](https://zhiyuanbiji.cn/notes/n133356bd4d440e66a4fd0d33f44f5dd)\n\n[251\\. Gemini Pro](https://deepmind.google/technologies/gemini/pro/)\n\n[252\\. SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure Interpretation](https://openreview.net/pdf/720bf9d3b93a3b5fded5cabae281dd02ab05fa14.pdf)\n\n[253\\. AGI&意识科学每周速递 | 2024年2月第二期](https://zhuanlan.zhihu.com/p/682829555)\n\n[254\\. 2023年深度行业分析研究报告](https://www.datatn.com/uploads/20240405/89188cdae8535a658f323869f7d800d3.pdf)\n\n[255\\. Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/pdf/2312.11805.pdf?ref=machinesonpaper.com)\n\n[256\\. RBench-V: A Primary Assessment for Visual Reasoning Models with Multi-modal Outputs](https://arxiv.org/pdf/2505.16770v2)\n\n[257\\. Gemini 1.5 Pro：探索多模态理解和长上下文处理的新前沿](https://zhuanlan.zhihu.com/p/682528627)\n\n[258\\. Gemini 1.5: Google's Generative AI Model with Mixture of Experts Architecture](https://encord.com/blog/google-gemini-1-5-generative-ai-model-with-mixture-of-experts/)\n\n[261\\. Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models](https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhu24l/zhu24l.pdf)\n\n[262\\. Model Tailor：减轻多模态大语言模型中的灾难性遗忘](https://zhuanlan.zhihu.com/p/683129281)\n\n[263\\. BadCLM: Backdoor Attack in Clinical Language Models for Electronic Health Records](http://arxiv.org/html/2407.05213v1)\n\n[264\\. ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think](https://arxiv.org/html/2501.01045v2)\n\n[265\\. DiffClass: Diffusion-Based Class Incremental Learning](https://link.springer.com/chapter/10.1007/978-3-031-73021-4_9)\n\n[266\\. 模型定制：减轻多模态大语言模型中的灾难性遗忘](https://www.x-mol.com/paper/1760122525564243968/t)\n\n[281\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[282\\. Supervised Fine-Tuning (SFT) in RLHF: A Comprehensive Guide](https://github.com/huggingface/trl/blob/main/docs/source/sft_trainer.mdx)\n\n[283\\. GitHub - Hongze-Wang/LLaMA-Efficient-Tuning: Easy-to-use fine-tuning framework using PEFT (PT+SFT+RLHF with QLoRA)](https://github.com/Hongze-Wang/LLaMA-Efficient-Tuning)\n\n[284\\. Integrating SFT and RLHF: Strategies for Next-Gen AI Models](https://www.apexdatasciences.ai/post/integrating-sft-and-rlhf-strategies-for-next-gen-ai-models)\n\n[285\\. UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function](https://arxiv.org/pdf/2410.21438)\n\n[286\\. Intuitive Fine-Tuning: Towards Unifying SFT and RLHF into a Single Process](https://arxiv.org/html/2405.11870v1)\n\n[287\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[288\\. SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning (work in progress)](https://arxiv.org/pdf/2408.05517v2)\n\n[289\\. LLM-Improving-Trained-Models-with-RLHF](https://www.pudn.com/Download/item/id/1706947506323251.html)\n\n[290\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[291\\. Generative Reward Models - A Unified Approach to RLHF and RLAIF](https://static.synthlabs.ai/preprints/Generative_Reward_Models.pdf)\n\n[292\\. Mitigating Reward Overoptimization via Lightweight Uncertainty Estimation](https://proceedings.neurips.cc/paper_files/paper/2024/file/94bbcb744bbada8808fda05b9d9290d6-Paper-Conference.pdf)\n\n[293\\. MaxMin-RLHF: Alignment with Diverse Human Preferences](https://openreview.net/attachment?id=8tzjEMF0Vq&name=pdf)\n\n[294\\. UFT：通过广义隐式奖励函数统一微调 SFT 和 RLHF/DPO/UNA](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_30/2410.21438.pdf)\n\n[295\\. Llama 2: A Deep Dive into the Open-Source Challenger to ChatGPT](https://www.unite.ai/llama-2-a-deep-dive-into-the-open-source-challenger-to-chatgpt/)\n\n[296\\. Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning](https://arxiv.org/pdf/2405.10292)\n\n[297\\. Policy Filtration in RLHF to Fine-Tune LLM for Code Generation](https://arxiv.org/pdf/2409.06957)\n\n[301\\. NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models](https://cogcomp.seas.upenn.edu/papers/PGTKRG25.pdf)\n\n[302\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[303\\. CHAIN-OF-THOUGHT REASONING IN THE WILD IS NOT ALWAYS FAITHFUL](https://openreview.net/pdf/054ce351d773fda02a12cd05550a72cc091efb18.pdf)\n\n[304\\. MMMT-IF: A CHALLENGING MULTIMODAL MULTI-TURN INSTRUCTION FOLLOWING BENCHMARK](https://openreview.net/pdf/e4607ea20a38f635a406d7ac0080e14a34055d9e.pdf)\n\n[305\\. Scale | SEAL Leaderboard: Arabic Evaluation](https://scale.com/leaderboard/arabic)\n\n[306\\. Comparative Analysis of GPT-4 (OpenAI), Gemini 1.5 (Google), and Claude 3 (Anthropic) for Film Production Workflows (April 2025)](https://eave.org/eave_documents/EAVE_AI_Comparative_Analysis_of_GPT-4_%28OpenAI%29,_Gemini_1.5_%28Google%29,_and_Claude_3_%28Anthropic%29_for_Film_Producers_CHATGPT_DEEPRESEARCH.pdf)\n\n[307\\. ETHIC：在高信息覆盖率的长上下文任务中评估大型语言模型](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_23/2410.16848.pdf)\n\n[308\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[309\\. 主要 AI モデル比較分析：ChatGPT Pro (o3/GPT -4o)、Gemini Ultra (Gemini 2.5 Pro with Deep Think) 、Claude Max (Claude 4 Opus) – パフォーマンス、ベンチマーク、機能 (2025 年 5 月時点)](https://yorozuipsc.com/uploads/1/3/2/5/132566344/daaeedc5c5c856f06f26.pdf)\n\n[310\\. COMPETING LARGE LANGUAGE MODELS IN MULTI-AGENT GAMING ENVIRONMENTS](https://openreview.net/attachment?id=DI4gW8viB6&name=pdf)\n\n[311\\. MLLM-COMPBENCH: A Comparative Reasoning Benchmark for Multimodal LLMs](https://proceedings.neurips.cc/paper_files/paper/2024/file/32923dff09f75cf1974c145764a523e2-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[312\\. ARE LLMs READY FOR PEDIATRICS? A COMPARATIVE EVALUATION OF MODEL ACCURACY ACROSS CLINICAL DOMAINS](https://www.medrxiv.org/content/10.1101/2025.04.25.25326437v1.full.pdf)\n\n[313\\. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/pdf/2403.05530v2)\n\n[314\\. A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models](https://arxiv.org/html/2507.08030v1)\n\n[315\\. Gemini 2.5 Pro Experimental (Mar' 25) vs GPT-4o (March 2025, chatgpt-4o-latest): Model Comparison](https://artificialanalysis.ai/models/comparisons/gemini-2-5-pro-vs-gpt-4o-chatgpt-03-25)\n\n[316\\. GPT-4o vs. GPT-4 vs. Gemini 1.5 - Evolve Media](https://evolvemedia.com/gpt-4o-vs-gpt-4-vs-gemini-1-5/)\n\n[317\\. Database-Augmented Transformer-Based Large Language Models Achieve High Accuracy in Mapping Gene-Phenotype Relationships](https://www.biorxiv.org/content/10.1101/2025.01.28.635344v1.full.pdf)\n\n[318\\. Chain-of-Thought Reasoning In The Wild Is Not Always Faithful](https://arxiv.org/pdf/2503.08679)\n\n[319\\. Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs](https://www.arxiv.org/pdf/2504.20451)\n\n[321\\. Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models](https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhu24l/zhu24l.pdf)\n\n[322\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[323\\. Investigating the Catastrophic Forgetting in Multimodal Large Language Models](https://proceedings.mlr.press/v234/zhai24a/zhai24a.pdf)\n\n[324\\. Model Tailor：减轻多模态大语言模型中的灾难性遗忘](https://zhuanlan.zhihu.com/p/683129281)\n\n[325\\. Towards Balanced Continual Multi-Modal Learning in Human Pose Estimation](https://arxiv.org/pdf/2501.05264)\n\n[326\\. Friedemann Zenke, Ben Poole et al. “Continual Learning Through Synaptic Intelligence.” Proceedings of machine learning research](https://www.semanticscholar.org/paper/a99d857ecc78316a0d9a774972b775058d5644ca)\n\n[327\\. Optimization (math)](https://de.zxc.wiki/wiki/Optimierung_%28Mathematik%29)\n\n[328\\. Model architecture can transform catastrophic forgetting into positive transfer](https://www.nature.com/articles/s41598-022-14348-x)\n\n[341\\. GitHub - wangclnlp/Vision-LLM-Alignment: This repo contains the codes for supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) designed for vision LLMs.](https://github.com/wangclnlp/Vision-LLM-Alignment)\n\n[342\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[343\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[344\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[345\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[346\\. UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function](https://arxiv.org/pdf/2410.21438)\n\n[347\\. GitHub - TideDra/VL-RLHF: A RLHF Infrastructure for Vision-Language Models](https://github.com/TideDra/VL-RLHF/)\n\n[348\\. IMPROVING CODE QUALITY USING FINE-TUNING LARGE LANGUAGE MODELS](https://trepo.tuni.fi/bitstream/handle/10024/161629/NguyenDuc.pdf?sequence=2&isAllowed=y)\n\n[349\\. LLAMAFACTORY: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/pdf/2403.13372v2)\n\n[350\\. Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning](https://arxiv.org/pdf/2405.10292)\n\n[351\\. Intuitive Fine-Tuning: Towards Unifying SFT and RLHF into a Single Process](https://arxiv.org/html/2405.11870v1)\n\n[352\\. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](https://arxiv.org/html/2405.11143v2)\n\n[353\\. RLAIF: SCALING REINFORCEMENT LEARNING FROM HUMAN FEEDBACK WITH AI FEEDBACK](https://openreview.net/pdf?id=AAxIs3D2ZZ)\n\n[354\\. LLM-Improving-Trained-Models-with-RLHF](https://www.pudn.com/Download/item/id/1706947506323251.html)\n\n[355\\. A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549)\n\n[361\\. NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models](https://cogcomp.seas.upenn.edu/papers/PGTKRG25.pdf)\n\n[362\\. EVALUATING SEMANTIC VARIATION IN TEXT-TO-IMAGE SYNTHESIS: A CAUSAL PERSPECTIVE](https://openreview.net/pdf/d1b59a4bd41dc935a97e9fea6812e3c1ca930be8.pdf)\n\n[363\\. Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis](https://openaccess.thecvf.com/content/CVPR2025/papers/Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in_CVPR_2025_paper.pdf)\n\n[364\\. MMMT-IF: A CHALLENGING MULTIMODAL MULTI-TURN INSTRUCTION FOLLOWING BENCHMARK](https://arxiv.org/pdf/2409.18216)\n\n[365\\. GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI](https://proceedings.neurips.cc/paper_files/paper/2024/file/ab7e02fd60e47e2a379d567f6b54f04e-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[366\\. ETHIC：在高信息覆盖率的长上下文任务中评估大型语言模型](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_23/2410.16848.pdf)\n\n[367\\. Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning](https://openreview.net/pdf?id=44CoQe6VCq)\n\n[368\\. Dive into AI Integration: Empowering Business Counselors](https://sbtdc.org/fallpd2024/docs/Dive-into-AI-Integration.pdf)\n\n[369\\. Large-scale moral machine experiment on large language models](http://www.arxiv.org/pdf/2411.06790)\n\n[370\\. Humanity's Last Exam](https://researchportal.murdoch.edu.au/view/pdfCoverPage?instCode=61MUN_INST&filePid=13176902260007891&download=true)\n\n[371\\. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/pdf/2403.05530v2)\n\n[372\\. Many-Shot In-Context Learning in Multimodal Foundation Models](https://openreview.net/pdf?id=j2rKwWXdcz)\n\n[373\\. MLLM-COMPBENCH: A Comparative Reasoning Benchmark for Multimodal LLMs](https://proceedings.neurips.cc/paper_files/paper/2024/file/32923dff09f75cf1974c145764a523e2-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[374\\. Bench-V: A Primary Assessment for Visual Reasoning Models with Multi-modal Outputs](https://arxiv.org/html/2505.16770v2)\n\n[375\\. LiveIdeaBench: Evaluating LLMs’ Scientific Creativity and Idea Generation with Minimal Context](http://arxiv.org/html/2412.17596v1)\n\n[376\\. 长文本语言模型的可控性评估：人造生物故事让评估更全面、更可靠](https://www.techwalker.com/2025/0608/3167401.shtml)\n\n[377\\. Context.ai](https://context.ai/compare/gpt-4o-2024-05-13/gemini-1-5-pro)\n\n[378\\. MileBench: Benchmarking MLLMs in Long Context](https://zhiyuanbiji.cn/notes/n133356bd4d440e66a4fd0d33f44f5dd)\n\n[379\\. Shangyu Xing, Changhao Xiang et al. “GePBench: Evaluating Fundamental Geometric Perception for Multimodal Large Language Models.”](https://arxiv.org/abs/2412.21036)\n\n[381\\. Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models](https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhu24l/zhu24l.pdf)\n\n[382\\. A Balancing Act: Optimizing Classification and Retrieval in Cross-Modal Vision Models](https://openreview.net/pdf?id=dAfbmDPeJL)\n\n[383\\. Model Tailor：减轻多模态大语言模型中的灾难性遗忘](https://zhuanlan.zhihu.com/p/683129281)\n\n[384\\. Yassine Benyahia, Kaicheng Yu et al. “Overcoming Multi-Model Forgetting.” International Conference on Machine Learning](https://arxiv.org/abs/1902.08232)\n\n[385\\. Investigating the Catastrophic Forgetting in Multimodal Large Language Models](https://zhuanlan.zhihu.com/p/675333375)\n\n[386\\. Andrei A. Rusu, Neil C. Rabinowitz et al. “Progressive Neural Networks.” ArXiv](https://arxiv.org/abs/1606.04671)\n\n[387\\. Zhizhong Li, Derek Hoiem. “Learning without Forgetting.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1007/978-3-319-46493-0_37)\n\n[401\\. GitHub - TideDra/VL-RLHF: A RLHF Infrastructure for Vision-Language Models](https://github.com/TideDra/VL-RLHF/)\n\n[402\\. GitHub - wangclnlp/Vision-LLM-Alignment: This repo contains the codes for supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) designed for vision LLMs.](https://github.com/wangclnlp/Vision-LLM-Alignment)\n\n[403\\. GitHub - leiyuch/LLaMA-Factory: Unify Efficient Fine-tuning of 100+ LLMs](https://github.com/leiyuch/LLaMA-Factory)\n\n[404\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[405\\. GitHub - geekwish/LLaMA-Efficient-Tuning: Easy-to-use fine-tuning framework using PEFT (PT+SFT+RLHF with QLoRA)](https://github.com/geekwish/LLaMA-Efficient-Tuning)\n\n[406\\. GitHub - VincentWong1/LLaMA-Efficient-Tuning: Easy-to-use fine-tuning framework using PEFT (PT+SFT+RLHF with QLoRA)](https://github.com/VincentWong1/LLaMA-Efficient-Tuning)\n\n[407\\. GitHub - Hongze-Wang/LLaMA-Efficient-Tuning: Easy-to-use fine-tuning framework using PEFT (PT+SFT+RLHF with QLoRA)](https://github.com/Hongze-Wang/LLaMA-Efficient-Tuning)\n\n[408\\. UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function](https://arxiv.org/pdf/2410.21438)\n\n[409\\. GitHub - lynnegaogao/LLaMA-Efficient-Tuning: Easy-to-use fine-tuning framework using PEFT (PT+SFT+RLHF with QLoRA)](https://github.com/lynnegaogao/LLaMA-Efficient-Tuning)\n\n[410\\. GitHub - soon14/LLaMA-Efficient-Tuning: Fine-tuning LLaMA with PEFT (PT+SFT+RLHF with QLoRA)](https://github.com/soon14/LLaMA-Efficient-Tuning)\n\n[411\\. GitHub - xusenlinzy/LLaMA-Efficient-Tuning: Easy-to-use fine-tuning framework using PEFT (PT+SFT+RLHF with QLoRA)](https://github.com/xusenlinzy/LLaMA-Efficient-Tuning)\n\n[412\\. GitHub - YuChong-ucas/LLaMA-Factory: Unify Efficient Fine-Tuning of 100+ LLMs](https://github.com/YuChong-ucas/LLaMA-Factory)\n\n[413\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[414\\. GitHub - mrseanryan/finetune_LLaVA: Fine tune LLaVA 1.5 - based on article by wandb](https://github.com/mrseanryan/finetune_LLaVA)\n\n[415\\. LLAMAFACTORY: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/pdf/2403.13372v1)\n\n[416\\. GitHub - schinger/FullLLM: Full stack LLM (Pre-training/finetuning, PPO(RLHF), Inference, Quant, etc.)](https://github.com/schinger/FullLLM)\n\n[417\\. GitHub - Happy-Corpse/align-anything: Align Anything: Training Any Modality Model with Feedback](https://github.com/Happy-Corpse/align-anything/)\n\n[418\\. GitHub - zjysteven/lmms-finetune: A minimal codebase for finetuning large multimodal models](https://github.com/zjysteven/lmms-finetune/)\n\n[419\\. GitHub - Zjh-819/LLMDataHub: A quick guide (especially) for trending instruction finetuning datasets](https://github.com/Zjh-819/LLMDataHub)"
  },
  {
    "id": 16,
    "question": "For complex reasoning tasks (e.g., tasks involving multiple citations or extended reasoning chains), what are the strengths of current agent technologies, and what are their limitations? Please analyze this in the context of research since June 2024.",
    "response": "**Research Report: Strengths and Limitations of AI Agent Technologies for Complex Reasoning Tasks (Post-June 2024 Analysis)**\n\n**1\\. Introduction**\n\nComplex reasoning tasks—such as scientific analysis requiring multi-step inference chains, citation synthesis, or longitudinal decision-making—represent a frontier for AI agent technologies. Since June 2024, research has accelerated in developing architectures capable of handling these challenges, driven by advances in large language models (LLMs), multi-agent coordination, and specialized reasoning techniques. This report synthesizes recent findings (July 2024–July 2025) to evaluate the **strengths** and **limitations** of current agent technologies, drawing from peer-reviewed studies, benchmarks, and industrial deployment cases.\n\n**2\\. Key Strengths of Current Agent Technologies**\n\n**2.1 Enhanced Multi-Step Reasoning Capabilities**\n\nRecent architectures significantly improve logical coherence in extended reasoning chains:\n\n**Specialized Reasoning Models**: Frameworks like _\"ol-like\" models_ explicitly optimize for iterative inference, enabling agents to decompose problems into logical sub-steps while maintaining contextual continuity \\[1\\].\n\n**Chain-of-Thought (CoT) & ReAct Synergy**: Combining CoT prompting with the _Reason-Act-Feedback_ (ReAct) loop allows agents to dynamically adjust reasoning paths based on intermediate results. This hybrid approach reduces hallucinations by 30–40% in mathematical and scientific tasks compared to 2023 baselines \\[3\\]\\[4\\]\\[11\\].\n\n**Memory-Augmented Architectures**: Agents now integrate **short-term episodic memory** (for task-specific context) and **long-term semantic memory** (for knowledge retention), enabling cumulative learning across reasoning chains \\[12\\].\n\n**2.2 Multi-Agent Collaboration Efficiency**\n\nDistributing cognitive load across specialized agents yields qualitative leaps in complex tasks:\n\n**Role-Based Specialization**: Systems like _MetaGPT_ enforce _Standardized Operating Procedures (SOPs)_ where agents assume distinct roles (e.g., \"Searcher,\" \"Validator,\" \"Summarizer\"). This reduces compounded error rates by 22–58% in tasks like literature reviews or clinical diagnosis pipelines \\[2\\]\\[15\\]\\[68\\].\n\n**Dialectical Debate Mechanisms**: For resolving citation conflicts, multi-agent systems employ structured debate protocols:\n\n_Conflict minimization algorithms_ use 0-1 programming to weight source credibility, reducing contradictory outputs by 45% \\[62\\]\\[66\\].\n\n_Tree-of-Debate frameworks_ stimulate critical analysis, with accuracy gains of 15–25% in scientific summarization tasks \\[210\\].\n\n**Dynamic Team Construction**: Agents self-organize based on task complexity, with coordinators managing workflows. For example, _MRAG frameworks_ use coordinator agents to assign subtasks and validate outputs mid-chain \\[73\\].\n\n**2.3 Tool Integration and Autonomous Iteration**\n\nAgents now robustly interface with external tools for real-world task execution:\n\n**Extensions/APIs**: Seamless integration with data stores, code executors, and search APIs (e.g., _RAG-enhanced retrieval_) improves factual grounding by 35–60% in citation-heavy domains \\[4\\]\\[95\\]\\[98\\].\n\n**Self-Optimization Loops**: Agents iteratively refine outputs using _ReAct feedback_, with frameworks like _ACC-Debate_ employing actor-critic models to optimize token efficiency versus accuracy tradeoffs \\[124\\]\\[211\\].\n\n**2.4 Benchmark-Documented Advantages**\n\nQuantitative gains are validated through next-gen benchmarks:\n\n**MedAgentsBench**: Multi-agent systems outperformed single-agent baselines by 12–26% in medical sequential decision-making (e.g., _MedChain-Agent_ scored 0.5269 vs. GPT-4o-mini’s 0.4442) \\[87\\].\n\n**AgentBench**: Multi-agent teams reduced time-to-goal completion by 40% in complex WebArena tasks, though human performance remained superior in >32-hour tasks \\[2\\]\\[41\\]\\[52\\].\n\n**DABStep**: Multi-agent systems achieved 75%+ accuracy in data analysis chains involving 450+ structured/unstructured tasks \\[93\\].\n\n**3\\. Critical Limitations and Failure Modes**\n\n**3.1 Deficiencies in True Reasoning Capability**\n\nDespite surface coherence, agents lack genuine semantic understanding:\n\n**Pattern Matching Over Logic**: Agents rely on statistical correlations in training data rather than first-principles deduction, failing at tasks requiring fine-grained semantic parsing (e.g., metaphor interpretation or implicit premise evaluation) \\[28\\]\\[34\\]\\[37\\].\n\n**Step-Jumping Errors**: In chains >7 steps, logical consistency drops by 30–50%, with agents \"skipping\" inferential steps or misattributing causality \\[37\\]\\[38\\].\n\n**Human-Like Biases**: Models like GPT-4 mirror human cognitive biases (e.g., confirmation bias when handling counterfactuals), limiting objectivity in scientific reasoning \\[32\\]\\[35\\].\n\n**3.2 Fragility in Long-Term Reasoning and Error Propagation**\n\nIndustrial deployments reveal systemic fragility:\n\n**Error Cascades**: Single inference errors amplify across chains. For example:\n\n_Coding agents_ entered feedback loops where Python execution failures corrupted subsequent repair attempts, increasing compute costs 4× \\[182\\].\n\n_Financial agents_ misinterpreted temporal dependencies, causing portfolio rebalancing cascades with 12–20% value erosion in simulated markets \\[220\\]\\[226\\].\n\n**Latency-Accuracy Tradeoffs**: Debate-based conflict resolution incurs severe overheads:\n\nDialectical debate added 6× compute latency versus SOP-based coordination, with only marginal accuracy gains (2–8%) in citation synthesis \\[121\\]\\[124\\].\n\nToken consumption in _Tree-of-Debate_ frameworks scaled quadratically with chain depth, making real-time use infeasible \\[212\\].\n\n**3.3 Weaknesses in Citation Provenance and Integrity**\n\nAgents cannot reliably trace or verify sources:\n\n**Provenance Gaps**: No cryptographic or blockchain-based techniques for immutable citation tracking exist in peer-reviewed literature. Agents frequently hallucinate attribution paths or omit contextual metadata \\[114\\]\\[156\\]\\[201\\].\n\n**Temporal Blindness**: In 72% of test cases, agents correctly cited recent data but fabricated claims about historical trends due to retrieval limitations \\[112\\].\n\n**3.4 Industrial Deployment Failures**\n\nReal-world implementations highlight unresolved bottlenecks:\n\n**Technical Root Causes**:\n\n_Legacy System Incompatibility_: Manufacturing agents failed when SOPs clashed with static workflows in 30-year-old SCADA systems \\[185\\].\n\n_Cold-Start RAG Issues_: Poorly initialized vector databases caused 40% accuracy drops in multi-step quality inspection tasks \\[137\\].\n\n**Notable Failures**:\n\nHealthcare: _IBM Watson_ recommended hazardous treatments due to data/model gaps \\[191\\].\n\nFinance: _Reinforcement learning agents_ ignored market impact dynamics, rendering backtests unusable \\[150\\].\n\nConsumer Devices: _Rabbit R1_ and _Humane Ai Pin_ faced market rejection due to reasoning-chain latency (>5-sec response times) and error cascades \\[139\\]\\[177\\].\n\n**4\\. Emerging Solutions and Research Frontiers**\n\n**4.1 Advances in Conflict Resolution Architectures**\n\n**Adaptive Debate Protocols**: _GroupDebate (GD)_ reduced token costs by 60% versus MAD frameworks via clustered agent discussions, maintaining accuracy \\[212\\].\n\n**Hybrid SOP-Debate Models**: _MetaGPT_’s structured coordination combined with lightweight debate rounds cut error rates by 33% in scientific literature aggregation \\[68\\].\n\n**4.2 Provenance and Integrity Innovations**\n\nThough nascent, promising directions include:\n\n**Blockchain-Based Auditing**: _HistoTrust_ leverages SHA-256 hashing and immutable ledgers for step-wise reasoning traceability \\[201\\]\\[241\\].\n\n**Zero-Knowledge Proofs (ZKPs)**: _Bulletproofs_ and _Pinocchio_ enable verification of computation integrity without exposing sensitive data—potentially applicable to citation chains \\[255\\]\\[257\\].\n\n**4.3 Next-Generation Benchmarks**\n\n_MedAgentsBench_ and _AgentQuest_ now track cost/accuracy/inference-time tradeoffs, exposing bottlenecks in clinical reasoning pipelines \\[54\\]\\[51\\].\n\n_LingBench++_ evaluates linguistic reasoning robustness, revealing gaps in metaphor handling \\[58\\].\n\n**5\\. Conclusion**\n\n**Strengths** in multi-agent coordination, tool augmentation, and dialectical debate have significantly advanced complex reasoning capabilities since mid-2024. SOP-driven frameworks and memory-augmented architectures show particular promise in reducing error propagation. However, **fundamental limitations** persist: agents lack genuine causal reasoning, suffer debilitating error cascades in industrial settings, and cannot ensure citation provenance integrity. Latency-accuracy tradeoffs remain severe in debate-based systems, and real-world deployments are hampered by legacy infrastructure incompatibility.\n\nCritical priorities for 2025–2026 include:\n\n1.  Developing lightweight cryptographic provenance for reasoning chains.\n2.  Creating error-containment mechanisms to halt cascades in industrial agents.\n3.  Standardizing conflict-resolution protocols to balance SOP efficiency with debate rigor.\n4.  Deploying causal reasoning benchmarks beyond pattern-matching evaluation.\n\nWithout breakthroughs in these areas, agents will remain constrained to narrow, well-defined tasks—unable to replace human expertise in high-stakes scientific or financial reasoning.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. A Survey of AI Agent Protocols](https://media.licdn.com/dms/document/media/v2/D4E1FAQGEX5OgViSaHg/feedshare-document-pdf-analyzed/B4EZaXRu.lHYAY-/0/1746294740993?e=1747267200&v=beta&t=kZ3AjxGVZLywF378HjkBea5Tae_okm6C3LCe6jh_jWA)\n\n[2\\. Tula Masterman, Sandi Besen et al. “The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2404.11584)\n\n[3\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[4\\. Agents](http://moonknight.cn/MyBlog/assets/file/day3.pdf)\n\n[5\\. Sébastien Bubeck, Varun Chandrasekaran et al. “Sparks of Artificial General Intelligence: Early experiments with GPT-4.” ArXiv](https://doi.org/10.48550/arXiv.2303.12712)\n\n[6\\. Analyst agent in Microsoft 365 Copilot](https://techcommunity.microsoft.com/blog/microsoft365copilotblog/analyst-agent-in-microsoft-365-copilot/4397191)\n\n[7\\. Karl Cobbe, V. Kosaraju et al. “Training Verifiers to Solve Math Word Problems.” ArXiv](https://arxiv.org/abs/2110.14168)\n\n[8\\. Automotive AI Agent Product Development and Commercialization Research Report, 2024](http://www.researchinchina.com/UpLoads/ArticleFreePartPath/20241227104017.pdf)\n\n[9\\. AI Agent深度（二）：2025 Agent元年，AI从L2向L3发展](https://pdf.dfcfw.com/pdf/H3_AP202505041667413598_1.pdf?1746459426000.pdf)\n\n[10\\. 智慧灯塔，照亮企业AI Agent实施明路](https://m.book118.com/try_down/648022037133006121.pdf)\n\n[11\\. AI Agent: 认知框架与案例实践](https://www.aidd.vip/resources/upload/a7844a45d8ab55e/file/%E9%BB%84%E4%BD%B3-AIAgent%E8%AE%A4%E7%9F%A5%E6%A1%86%E6%9E%B6%E4%B8%8E%E6%A1%88%E4%BE%8B%E5%AE%9E%E8%B7%B5-%E5%B7%B2ok.pdf)\n\n[12\\. 计算机行业 2024 年 12 月暨 2025 年度策略 AI 应用方兴日盛，推理算力蓄势待发](https://file.iyanbao.com/pdf/b8679-4e4d8098-a5a5-4a4c-8e81-a0e391cd4217.pdf)\n\n[13\\. AI Agents 24 年回顾 - 五大发展趋势](https://new.qq.com/rain/a/20250114A09TKU00)\n\n[14\\. G. Reeke. “The society of mind.” Artificial Intelligence](https://doi.org/10.1016/0004-3702%2891%2990034-H)\n\n[15\\. Sirui Hong, Xiawu Zheng et al. “MetaGPT: Meta Programming for Multi-Agent Collaborative Framework.” ArXiv](https://doi.org/10.48550/arXiv.2308.00352)\n\n[16\\. 2024 AI智能体（AI Agent）：概念、运作原理及组织最优应用场景研究报告](https://www.sgpjbg.com/bgdown/619095.html)\n\n[17\\. Kamer Ali Yuksel and Hassan Sawaf. “A Multi-AI Agent System for Autonomous Optimization of Agentic AI Solutions via Iterative Refinement and LLM-Driven Feedback Loops.”](https://arxiv.org/abs/2412.17149)\n\n[18\\. 2024 AI+研发数字峰会](https://www.aidd.vip/resources/upload/a7844a45d8ab55e/file/%E9%BB%84%E6%96%8C-%E4%BC%81%E4%B8%9A%E8%BD%AF%E4%BB%B6%E4%B8%8EAI%E5%8E%9F%E7%94%9F%E5%BA%94%E7%94%A8.pdf)\n\n[21\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[22\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[23\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[24\\. Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research](https://www.i-newcar.com/uploads/allimg/20250221/2-250221164PCR.pdf)\n\n[25\\. Sébastien Bubeck, Varun Chandrasekaran et al. “Sparks of Artificial General Intelligence: Early experiments with GPT-4.” ArXiv](https://doi.org/10.48550/arXiv.2303.12712)\n\n[26\\. AI在组织管理应用中的潜在缺陷：一个ABCD框架](https://bus.sysu.edu.cn/qjm/sites/journalmanagement.prod.dpcms8.sysu.edu.cn/files/2025-03/2-AI%E5%9C%A8%E7%BB%84%E7%BB%87%E7%AE%A1%E7%90%86%E5%BA%94%E7%94%A8%E4%B8%AD%E7%9A%84%E6%BD%9C%E5%9C%A8%E7%BC%BA%E9%99%B7%EF%BC%9A%E4%B8%80%E4%B8%AAABCE%E6%A1%86%E6%9E%B6-%E7%AE%A1%E7%90%86%E5%AD%A6%E5%AD%A3%E5%88%8A2024.04.pdf)\n\n[27\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2024.” ArXiv](https://doi.org/10.48550/arXiv.2405.19522)\n\n[28\\. LeCun新研究：AI在复杂任务中的局限性与AGI的未来](https://www.showapi.com/news/article/683fc7c24ddd79013c000337)\n\n[29\\. 2024 AI智能体（AI Agent）：概念、运作原理及组织最优应用场景研究报告](https://www.sgpjbg.com/bgdown/619095.html)\n\n[30\\. Yejin Bang, Samuel Cahyawijaya et al. “A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity.” ArXiv](https://doi.org/10.18653/v1/2023.ijcnlp-main.45)\n\n[31\\. 2024年人工智能发展报告总结](https://mp.weixin.qq.com/s?__biz=MzI3OTM1MTE3Mw%3D%3D&mid=2247657384&idx=5&sn=3430ccb65a0969ebd41f756c146109f4&chksm=ea03de5a9910f8f1220ae3cf6c8f506e62bf965a65dbd0c50c3a14959671c1a46cb5564f3c8c&scene=27)\n\n[32\\. 搞定这三个任务？人类不行，AI 也不行](https://m.huxiu.com/article/3273039.html)\n\n[33\\. 2024年诺贝尔物理学奖与学科交叉](https://www.cpsjournals.cn/data/article/wl/preview/pdf/10.7693/wl20250103.pdf)\n\n[34\\. 2024学年第二学期台州市山海协作体期中联考高二年级英语学科试题](http://imgs.app.gaokaozhitongche.com/resource/pdf/paper/202504/20250423095826_d3w2hu.pdf)\n\n[35\\. 靠浦ai资讯 2024年07月24日ai新闻1](https://www.bilibili.com/video/av112834830665252)\n\n[36\\. 【AI】Andrew Ng: Explores The Rise Of AI Agents And Agentic Reasoning_2024](https://www.bilibili.com/video/av113858123400023?t=240)\n\n[37\\. AI应用爆发，如何看待AI agent的ROI？](https://xueqiu.com/7517920899/314230339)\n\n[38\\. Wenhao Li, Bo Jin et al. “Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows.”](https://arxiv.org/abs/2505.04354)\n\n[39\\. AI的现状：在特定任务中展现类人推理（如GPT-4的逻辑谜题），但整体仍受限于数据驱动与算法局限](https://post.smzdm.com/zz/p/az7v6wkp/)\n\n[41\\. AI Agent深度（二）：2025 Agent元年，AI从L2向L3发展](https://pdf.dfcfw.com/pdf/H3_AP202505041667413598_1.pdf?1746459426000.pdf)\n\n[42\\. Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research](https://www.i-newcar.com/uploads/allimg/20250221/2-250221164PCR.pdf)\n\n[43\\. Zinan Liu, Haoran Li et al. “Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience.”](https://arxiv.org/abs/2505.05515)\n\n[44\\. JoyAgent-JDGenie: Revolutionizing Open-Source Multi-Agent ...](https://www.xugj520.cn/en/archives/multi-agent-framework-jdgenie.html)\n\n[45\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[46\\. A Survey of AI Agent Protocols](https://media.licdn.com/dms/document/media/v2/D4E1FAQGEX5OgViSaHg/feedshare-document-pdf-analyzed/B4EZaXRu.lHYAY-/0/1746294740993?e=1747267200&v=beta&t=kZ3AjxGVZLywF378HjkBea5Tae_okm6C3LCe6jh_jWA)\n\n[47\\. Zhiheng Xi, Wenxiang Chen et al. “The Rise and Potential of Large Language Model Based Agents: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2309.07864)\n\n[48\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2024.” ArXiv](https://doi.org/10.48550/arXiv.2405.19522)\n\n[49\\. Shunyu Yao, Jeffrey Zhao et al. “ReAct: Synergizing Reasoning and Acting in Language Models.” ArXiv](https://arxiv.org/abs/2210.03629)\n\n[50\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[51\\. Luca Gioacchini, G. Siracusano et al. “AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents.” ArXiv](https://doi.org/10.48550/arXiv.2404.06411)\n\n[52\\. Xiao Liu, Hao Yu et al. “AgentBench: Evaluating LLMs as Agents.” ArXiv](https://doi.org/10.48550/arXiv.2308.03688)\n\n[53\\. Mohamed Amine Ferrag, Norbert Tihanyi et al. “From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review.”](https://arxiv.org/abs/2504.19678)\n\n[54\\. MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning](http://paperreading.club/page?id=290455)\n\n[55\\. Anthony Costarelli, Mat Allen et al. “GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents.” ArXiv](https://doi.org/10.48550/arXiv.2406.06613)\n\n[56\\. Weizhi Zhang, Yangning Li et al. “From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents.”](https://arxiv.org/abs/2506.18959)\n\n[57\\. Yuting Huang, Leilei Ding et al. “A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents.”](https://arxiv.org/abs/2504.14650)\n\n[58\\. Inference - Paper Reading](http://paperreading.club/category?cate=Inference)\n\n[59\\. Xiao Yu, Baolin Peng et al. “Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents.”](https://arxiv.org/abs/2506.00320)\n\n[61\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[62\\. 基于人工智能的多 AGENT 协同辩证逻辑推理方法](https://studiesinlogic.sysu.edu.cn/sites/default/files/2023-11/1674%C2%AD3202%282023%29%C2%AD05%C2%AD0081%C2%AD16.pdf)\n\n[63\\. Xuezhi Wang, Jason Wei et al. “Self-Consistency Improves Chain of Thought Reasoning in Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2203.11171)\n\n[64\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[65\\. Multiagent Continual Coordination via Progressive Task Contextualization](https://www.lamda.nju.edu.cn/lilh/file/macpro.pdf)\n\n[66\\. 基于人工智能的多AGENT协同辩证逻辑推理方法](https://studiesinlogic.sysu.edu.cn/zh-hans/article/228)\n\n[67\\. Khanh-Tung Tran, Dung Dao et al. “Multi-Agent Collaboration Mechanisms: A Survey of LLMs.”](https://arxiv.org/abs/2501.06322)\n\n[68\\. Sirui Hong, Xiawu Zheng et al. “MetaGPT: Meta Programming for Multi-Agent Collaborative Framework.” ArXiv](https://doi.org/10.48550/arXiv.2308.00352)\n\n[69\\. Jintian Zhang, Xin Xu et al. “Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View.” ArXiv](https://doi.org/10.48550/arXiv.2310.02124)\n\n[70\\. Yilun Du, Shuang Li et al. “Improving Factuality and Reasoning in Language Models through Multiagent Debate.” ArXiv](https://doi.org/10.48550/arXiv.2305.14325)\n\n[71\\. Striding Towards the Intelligent World White Paper 2024: Autonomous Driving Network (ADN) - AI for Network, Ushering in New Era of High Autonomy](https://www-file.huawei.com/-/media/corp2020/pdf/giv/striding-towards-the-intelligent-world/2024/intelligent_world_adn_2024_en.pdf?la=en)\n\n[72\\. 多AGENT协同辩证推理方法及实现技术研究](https://cgl.org.cn/auto/db/detail.aspx?db=950008&rid=1321105&agfi=0&cls=0&uni=True&cid=0&showgp=True&prec=False&md=152&pd=208&msd=152&psd=208&mdd=152&pdd=208&count=10&reds=aachen;rwth;de)\n\n[73\\. Alireza Salemi, Mukta Maddipatla et al. “CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through Self-Training.”](https://arxiv.org/abs/2506.10844)\n\n[74\\. Ming Wang, Peidong Wang et al. “AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation.”](https://arxiv.org/abs/2506.00551)\n\n[75\\. Baixuan Xu, Chunyang Li et al. “Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study.”](https://arxiv.org/abs/2505.07313)\n\n[76\\. 多 Agent 系统合作与协调机制研究综述](https://www.jsjkx.com/CN/article/openArticlePDF.jsp?id=8019)\n\n[77\\. Alfonso Amayuelas, Xianjun Yang et al. “MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate.” ArXiv](https://doi.org/10.48550/arXiv.2406.14711)\n\n[81\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[82\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[83\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[84\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[85\\. MADDPG-D2: An Intelligent Dynamic Task Allocation Algorithm Based on Multi-Agent Architecture Driven by Prior Knowledge](https://cdn.techscience.cn/uploads/attached/file/20240624/20240624160912_17624.pdf)\n\n[86\\. J. Park, Joseph C. O'Brien et al. “Generative Agents: Interactive Simulacra of Human Behavior.” Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology](https://doi.org/10.1145/3586183.3606763)\n\n[87\\. MedChain：通过交互式顺序基准测试桥接大模型Agent与临床实践之间的差距](https://www.51cto.com/aigc/5462.html)\n\n[88\\. Xiangru Tang, Daniel Shao et al. “MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning.”](https://arxiv.org/abs/2503.07459)\n\n[89\\. 2024年大模型Multi-agent多智能体应用技术：AutoGen, MetaGPT, XAgent, AutoAgents，crewAI](https://zhuanlan.zhihu.com/p/671355141)\n\n[90\\. 2024 AI+研发数字峰会](https://aidd.vip/resources/upload/a7844a45d8ab55e/file/%E9%BB%84%E9%9D%9E-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%A9%B1%E5%8A%A8%E7%9A%84%E6%99%BA%E8%83%BD%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91.pdf)\n\n[91\\. Gardhar Ingvarsson, Mikayel Samvelyan et al. “Mix-ME: Quality-Diversity for Multi-Agent Learning.” ArXiv](https://doi.org/10.48550/arXiv.2311.01829)\n\n[92\\. AI Agents 24 年回顾 - 五大发展趋势](https://news.qq.com/rain/a/20250114A09TKU00)\n\n[93\\. Analyst agent in Microsoft 365 Copilot](https://techcommunity.microsoft.com/blog/microsoft365copilotblog/analyst-agent-in-microsoft-365-copilot/4397191)\n\n[94\\. Conducting Qualitative Interviews with AI](https://www.ifo.de/DocDL/cesifo1_wp10666.pdf)\n\n[95\\. 2025 Tech预测：顶级技术趋势、工具和技能](https://pdf.dfcfw.com/pdf/H3_AP202501091641865827_1.pdf)\n\n[96\\. SECURING AI/LLMS IN 2025: A Practical Guide To Securing & Deploying AI](https://25622759.fs1.hubspotusercontent-eu1.net/hubfs/25622759/Securing%20AI%20and%20LLMs%20in%202025.pdf)\n\n[97\\. SAP Business AI: Today, Tomorrow, Beyond Tomorrow](https://4149027.fs1.hubspotusercontent-na1.net/hubfs/4149027/WSAIQ24%20Speaker%20ppts/TRACK1/DAY1/10_Jesper%20Schleimann.pdf)\n\n[98\\. Monitoring Amazon Bedrock applications](https://d1.awsstatic.com/events/Summits/toronto24/COP302_MonitoringAmazonBedrock_E1_TORSummit_20240911.pdf)\n\n[99\\. Agentes de inteligencia artificial y workflows agénticos: la nueva frontera de la automatización](https://ialab.com.ar/webia/wp-content/uploads/2025/02/Agentes-de-inteligencia-artificial-y-workflows-agenticos.pdf)\n\n[100\\. 2025AI 剧变：从工具到 \"代理\"，巨头沉默背后藏着啥？](https://cloud.tencent.com/developer/article/2528300)\n\n[101\\. Advances in prompting techniques](https://www.ispor.org/docs/default-source/cti-meeting-21021-documents/2e775527-1b20-4d14-bb70-16e61da9ef33.pdf?sfvrsn=e355c47_0)\n\n[102\\. 新科技观察 TECH WATCH WEEKLY](https://download.caixin.com/upload/cxzk73.pdf)\n\n[104\\. Xing Hao, Guigang Zhang et al. “Deep Learning.” Int. J. Semantic Comput.](https://doi.org/10.1142/S1793351X16500045)\n\n[105\\. Amina Adadi, M. Berrada. “Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI).” IEEE Access](https://doi.org/10.1109/ACCESS.2018.2870052)\n\n[106\\. Alejandro Barredo Arrieta, Natalia Díaz Rodríguez et al. “Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI.” Inf. Fusion](https://doi.org/10.1016/j.inffus.2019.12.012)\n\n[107\\. C. Rudin. “Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead.” Nature Machine Intelligence](https://doi.org/10.1038/s42256-019-0048-x)\n\n[108\\. Tim Miller. “Explanation in Artificial Intelligence: Insights from the Social Sciences.” Artif. Intell.](https://doi.org/10.1016/J.ARTINT.2018.07.007)\n\n[109\\. Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research](https://www.i-newcar.com/uploads/allimg/20250221/2-250221164PCR.pdf)\n\n[110\\. RSAC 2025: 新挑战、新技术与新趋势](https://static01-www.qianxin.com/qaxweb/c51961e667bb62151f73d06dcc5c69ea.pdf)\n\n[111\\. 成果发布 | 2024 年人工智能十大前沿技术趋势展望](http://agri.nais.net.cn/file1/M00/03/74/Csgk0WcscYqAW4h7ABYb-bdty3M295.pdf)\n\n[112\\. Mastering AI Agents](https://media.licdn.com/dms/document/media/v2/D4D1FAQEa9O0Jk9_OVw/feedshare-document-pdf-analyzed/B4DZVLWmyGHIAY-/0/1740726021992?e=1741824000&v=beta&t=CoDewkLFEmUkmXKZbHnG7qttkZELPQWN-SU92uhxkOw)\n\n[113\\. 【AI】Andrew Ng: Explores The Rise Of AI Agents And Agentic Reasoning_2024](https://www.bilibili.com/video/av113858123400023?t=181)\n\n[114\\. A. Kale, Tin Chi Nguyen et al. “Provenance documentation to enable explainable and trustworthy AI: A literature review.” Data Intelligence](https://doi.org/10.1162/dint_a_00119)\n\n[115\\. R. Olfati-Saber, R. Murray. “Consensus problems in networks of agents with switching topology and time-delays.” IEEE Transactions on Automatic Control](https://doi.org/10.1109/TAC.2004.834113)\n\n[116\\. A. Jadbabaie, Jie Lin et al. “Coordination of groups of mobile autonomous agents using nearest neighbor rules.” Proceedings of the 41st IEEE Conference on Decision and Control, 2002.](https://doi.org/10.1109/CDC.2002.1184304)\n\n[117\\. T. Vicsek, A. Czirók et al. “Novel type of phase transition in a system of self-driven particles..” Physical review letters](https://doi.org/10.1103/PhysRevLett.75.1226)\n\n[118\\. L. Moreau. “Stability of multiagent systems with time-dependent communication links.” IEEE Transactions on Automatic Control](https://doi.org/10.1109/TAC.2004.841888)\n\n[119\\. 基于人工智能的多 AGENT 协同辩证逻辑推理方法](https://studiesinlogic.sysu.edu.cn/sites/studiesinlogic.prod.dpcms8.sysu.edu.cn/files/2023-11/1674%C2%AD3202%282023%29%C2%AD05%C2%AD0081%C2%AD16.pdf)\n\n[120\\. P. M. Dung. “On the Acceptability of Arguments and its Fundamental Role in Nonmonotonic Reasoning, Logic Programming and n-Person Games.” Artif. Intell.](https://doi.org/10.1016/0004-3702%2894%2900041-X)\n\n[121\\. Sugyeong Eo, Hyeonseok Moon et al. “Debate Only When Necessary: Adaptive Multiagent Collaboration for Efficient LLM Reasoning.”](https://arxiv.org/abs/2504.05047)\n\n[122\\. 多 Agent 系统合作与协调机制研究综述](https://www.jsjkx.com/CN/article/openArticlePDF.jsp?id=8019)\n\n[123\\. Alison R. Panisson, P. McBurney et al. “A computational model of argumentation schemes for multi-agent systems.” Argument Comput.](https://doi.org/10.3233/aac-210555)\n\n[124\\. Tongxuan Liu, Xingyu Wang et al. “GroupDebate: Enhancing the Efficiency of Multi-Agent Debate Using Group Discussion.”](https://arxiv.org/abs/2409.14051)\n\n[125\\. Xeryus Stokkel, Bart Verheij. “Deliberation Dialogues for Cooperative Pathﬁnding.”](https://www.semanticscholar.org/paper/8f2a0ec8ca188372a1712d2e2a22915422dc7b94)\n\n[126\\. Małgorzata Pańkowska, H. Sroka et al. “COGNITION AND CREATIVITY SUPPORT SYSTEMS.”](https://www.semanticscholar.org/paper/2b9a116461eebccf254967a858e127990dafdcc7)\n\n[127\\. R. Olfati-Saber, J. Alex Fax et al. “Consensus and Cooperation in Networked Multi-Agent Systems.” Proceedings of the IEEE](https://doi.org/10.1109/JPROC.2006.887293)\n\n[128\\. Leader-follower Consensus of Multi-Agent Systems](https://www.doc88.com/p-807815372381.html)\n\n[129\\. Boris A. Galitsky, S. Kuznetsov et al. “Argumentation vs Meta-argumentation for the Assessment of Multi-agent Conflict.”](https://www.semanticscholar.org/paper/8e886fb85e56ddee2f1b445d9c61fb60c26d43fe)\n\n[130\\. ACC-Debate: An Actor-Critic Approach to Multi-Agent Debate](http://paperreading.club/page?id=263658)\n\n[131\\. D. Pellier, H. Fiorino. “Multi-Agent Assumption-Based Planning.” International Joint Conference on Artificial Intelligence](https://www.semanticscholar.org/paper/135de3aa202efc573ddf03d48efb29befdcbae86)\n\n[132\\. 领导-跟随多智能体系统的滞后一致性](https://wulixb.iphy.ac.cn/article/doi/10.7498/aps.63.040202)\n\n[134\\. 人工智能在2024年的失败：回顾与反思](https://docs.feishu.cn/v/wiki/Os7XwyPWVitLgEkLKpCcgxgEn5d/ai)\n\n[135\\. Rishi Bommasani, Drew A. Hudson et al. “On the Opportunities and Risks of Foundation Models.” ArXiv](https://arxiv.org/abs/2108.07258)\n\n[136\\. 2024 AI 年终盘点｜从技术到行业，留下三个悬而未决的问题](https://www.21jingji.com/article/20241231/herald/be1fd59db7447eda3c5da81180b90d98.html)\n\n[137\\. 2024 中国信创+AI趋势洞察报告](http://www.csia-jpw.com/UserFiles/Article/file/6387616415478161773940215.pdf)\n\n[138\\. 2190.2024年，最大的AI失败案例-MIT](https://www.bilibili.com/video/av113917632192287?t=224)\n\n[139\\. 2024年AI失败案例总结](https://xueqiu.com/4601596167/320181819)\n\n[140\\. 电信行业 AI 现状：2024 年中国趋势](https://pdf.dfcfw.com/pdf/H3_AP202502181643208482_1.pdf?1739905887000.pdf)\n\n[141\\. 美媒报道2024年失败的三个AI产品](https://h5.ifeng.com/c/vivo/v0022AnLcB-_UHIJWc7Te4P9GduR4K2siQv2mIqk4WVF7FfU__)\n\n[142\\. AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges](https://smallake.kr/wp-content/uploads/2025/06/2505.10468v4.pdf)\n\n[143\\. 美媒报道了2024年失败的三个AI产品，Meta的AI角色被点名](https://www.thepaper.cn/newsDetail_forward_29495659)\n\n[144\\. AI Agent落地2024：理想与现实的差距](https://www.xinfinite.net/t/topic/9566)\n\n[145\\. The 8 worst technology failures of 2024](https://www.technologyreview.com/2024/12/17/1108883/the-8-worst-technology-failures-of-2024/)\n\n[146\\. 2024大模型典型示范应用案例集](http://sccio.cn/uploads/20240813/eb25f9d1e437df7f1c523344814c8208.pdf)\n\n[147\\. AI Agent深度（二）：2025 Agent元年，AI从L2向L3发展](https://pdf.dfcfw.com/pdf/H3_AP202505041667413598_1.pdf?1746459426000.pdf)\n\n[148\\. 2024年三款失败的AI产品](https://docs.feishu.cn/v/wiki/U2SPwFy6ki1f7XklxwmcHowpn8M/ag)\n\n[149\\. 2024年企业生成式AI现状](https://weibo.com/ttarticle/p/show?id=2309405103525683396727)\n\n[150\\. A Survey of Financial AI: Architectures, Advances and Open Challenges](https://www.arxiv.org/pdf/2411.12747)\n\n[151\\. Predictions 2025: Technology & Security](https://trendsunplugged.io/wp-content/uploads/2025/02/FORRESTER-Predictions-2025-Tech-Security_CAIG.pdf)\n\n[152\\. 过度炒作+虚假包装?Gartner预测2027年超40%的代理型AI项目将失败](https://36kr.com/p/3364423329761282)\n\n[153\\. KIMI-VL 技术报告](https://www.cnblogs.com/weihangzhang/p/18817992)\n\n[154\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[155\\. 赵思成 - 清华大学信息国家研究中心](https://www.bnrist.tsinghua.edu.cn/info/1202/3689.htm)\n\n[156\\. Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text](https://www.microsoft.com/en-us/research/uploads/prod/2024/07/3637528.3671463.pdf)\n\n[157\\. 第二届“未来工业互联网”学术论坛 2024 2nd Future Industrial Internet Symposium 会议论文集](http://fii.xintongconference.com/upload/file/20240822/6385993573919617682102682.pdf)\n\n[158\\. 2024加密AI赛道叙事演化：从去中心化GPU到AI Agent](https://www.jb51.net/blockchain/967802.html)\n\n[159\\. Jinjin Gu](https://www.jasongt.com/)\n\n[160\\. NeurIPS 2024：RNN超强继任者来袭！长时间序列预测未来的一梯队，深度学习老方法都不管用了！](https://www.bilibili.com/video/av113293972806173)\n\n[161\\. 时空预测领域的前沿研究与创新](https://www.360doc.cn/article/47115229_1116398910.html)\n\n[162\\. 【老郑闲聊】 - NeurIPS 2024 温哥华神经信息处理系统年会](https://www.bilibili.com/video/av113715265407509)\n\n[163\\. MCG@NJU Multimedia Computing Group, Nanjing University](https://mcg.nju.edu.cn/en/news.html)\n\n[164\\. Darren Frey, Eric D. Johnson et al. “Individual differences in conflict detection during reasoning.” Quarterly Journal of Experimental Psychology](https://doi.org/10.1080/17470218.2017.1313283)\n\n[165\\. V. Thompson, Stephen M. Johnson. “Conflict, metacognition, and analytic thinking.” Thinking & Reasoning](https://doi.org/10.1080/13546783.2013.869763)\n\n[166\\. Gordon Pennycook, Jonathan A. Fugelsang et al. “What makes us think? A three-stage dual-process model of analytic engagement.” Cognitive Psychology](https://doi.org/10.1016/j.cogpsych.2015.05.001)\n\n[167\\. W. Neys, Tamara Glumicic. “Conflict monitoring in dual process theories of thinking.” Cognition](https://doi.org/10.1016/j.cognition.2007.06.002)\n\n[168\\. Gordon Pennycook, Jonathan A. Fugelsang et al. “Are we good at detecting conflict during reasoning?.” Cognition](https://doi.org/10.1016/j.cognition.2012.04.004)\n\n[169\\. ISSA Proceedings 2006 – A Role For Dialectic In Science Studies](https://rozenbergquarterly.com/issa-proceedings-2006-a-role-for-dialectic-in-science-studies/?print=print)\n\n[170\\. Conflict Detection and Logical Complexity](https://www.wdeneys.org/data/reprint%20PB%20Janie.pdf)\n\n[171\\. Accuracy vs. Accuracy: Computational Tradeoffs Between Classification Rates and Utility](https://www.arxiv.org/pdf/2505.16494)\n\n[172\\. Dialectical Reconciliation via Structured Argumentative Dialogues](https://proceedings.kr.org/2024/73/kr2024-0073-vasileiou-et-al.pdf)\n\n[174\\. AI Agent深度（二）：2025 Agent元年，AI从L2向L3发展](https://pdf.dfcfw.com/pdf/H3_AP202505041667413598_1.pdf?1746459426000.pdf)\n\n[175\\. 新研究揭示模拟推理 AI 模型尚未达到其宣传效果之原因](https://new.qq.com/rain/a/20250427A078A500)\n\n[176\\. Unleashing the Power of AI to Automate Cybersecurity](https://iris.polito.it/retrieve/8017151d-3f8a-4a43-8aae-4d1f3fb56e2e/phd_thesis_conv.pdf)\n\n[177\\. 2024年AI失败案例总结](https://xueqiu.com/4601596167/320181819)\n\n[178\\. AI@Work 2025キックオフ](https://www.pmi-japan.org/wp-content/uploads/2025/01/20250119_AIatWork2025Kickoff_WG1.pdf)\n\n[179\\. 从虚拟代理到机器人团队：高风险医疗环境中的多机器人框架评估](https://www.xueshuxiangzi.com/downloads/2025_6_5/2506.03546.pdf)\n\n[180\\. 2025年的AI代理：克服集成和工作流挑战](https://neuron.expert/news/ai-agents-in-2025-what-enterprise-leaders-need-to-know/10166/zh/)\n\n[181\\. AI代理失败的根源：生产环境中的挑战与对策](https://www.showapi.com/news/article/6865e1d64ddd79013c003949)\n\n[182\\. Unveiling Pitfalls: Understanding Why AI-driven Code Agents Fail at GitHub Issue Resolution](https://arxiv.org/pdf/2503.12374)\n\n[183\\. 加速科学发现一直是人工智能（AI）研究的长期目标，早期的项目如 1979 年的橡树岭应用人工智能项目探索了（Team, 1985; Emrich et al., 1988; Johnson and Schaffer, 1994）。随着基础模型的进步，更近期的探索提供了一个端到端论文生成过程的完全自动化流程的概念验证（Achiam et al., 2023; Anthropic, 2024; Team et al., 2024; Dubey et al., 2024）。未来，我们设想 AI 研究代理能够独立进行文献搜索、生成科学假设、设计实验、实施新方法、分析结果，通过撰写科学论文传播发现，并在产品中应用此研究，从而协助研究过程的所有部分。这些代理应该能够完全自主工作或在人类监督下进行工作，考虑到用户的反馈。这个愿景来自于认识到 AI 有能力处理庞大数据集并辨识复杂模式，可以通过识别有前景的药物候选者或预测新材料的属性来加速药物发现和材料科学领域的科学突破（Hessler and Baringhaus, 2018; Schneider et al., 2020; Guo et al., 2021）。与传统方法不同，AI 代理可以通过分析庞大的知识图谱揭示隐藏的跨学科关系，提出复杂挑战（如气候建模）的新见解和解决方案。通过自动化繁重的任务和探索非传统途径，AI 代理可以解放科学家们，使之专注于更高层次的认知活动，从而推动创新并拓展知识的前沿。机器学习（ML）的研究，重视仿真中的经验验证和系统实验，为探索和提高 LLMs 在推动科学研究中的效用提供了理想的试验场。](https://www.xueshuxiangzi.com/downloads/2025_2_21/2502.14499.pdf)\n\n[184\\. Striding Towards the Intelligent World White Paper 2024 ICT Services and Software Enable Digital Intelligence Acceleration](https://www.samenacouncil.org/initiatives/industryissues/huawei-intelligent-world/Intelligent_World_ICT_Services_Software_2024_en.pdf)\n\n[185\\. Why Agentic AI Fails Inside Legacy Systems](https://www.techolution.com/blog/how-legacy-systems-are-quietly-sabotaging-agentic-ai-across-enterprises/)\n\n[186\\. 通过人工智能冲击预测，数字化转型：40％的AI项目失败 - 是您的代理商？](https://xpert.digital/zh-cn/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%95%B0%E5%AD%97%E5%8C%96%E8%BD%AC%E5%9E%8B/)\n\n[187\\. 过度炒作+虚假包装？Gartner预测2027年超40%的代理型AI项目将失败](https://g.pconline.com.cn/ai/article/1329130.html)\n\n[188\\. Why Deep Research Agents Fail: Lessons from GAIA](https://www.atla-ai.com/post/gaia)\n\n[189\\. AI Agent 理论与实战完全指南](https://juejin.cn/post/7472936607960596531)\n\n[190\\. AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges](https://arxiv.org/pdf/2505.10468v1)\n\n[191\\. Building AI Agents Right: How to avoid common project failures](https://deviniti.com/app/uploads/2025/02/Building-AI-Agents-right-How-to-avoid-common-project-failures.pdf)\n\n[192\\. AI Agents vs Agentic AI：概念分类、应用及挑战](https://www.cnblogs.com/buyi-gao/p/18949430)\n\n[193\\. Why Most Agentic Architectures Will Fail](https://www.informationweek.com/machine-learning-ai/why-most-agentic-architectures-will-fail)\n\n[194\\. Anthony Dewayne Hunt. “Bitcoin: A Peer-to-Peer Electronic Cash System.”](https://doi.org/10.2139/ssrn.3440802)\n\n[195\\. Jakub Konecný, H. B. McMahan et al. “Federated Learning: Strategies for Improving Communication Efficiency.” ArXiv](https://arxiv.org/abs/1610.05492)\n\n[196\\. M. Castro. “Practical Byzantine fault tolerance.” USENIX Symposium on Operating Systems Design and Implementation](https://doi.org/10.1145/296806.296824)\n\n[197\\. Wei Yang Bryan Lim, Nguyen Cong Luong et al. “Federated Learning in Mobile Edge Networks: A Comprehensive Survey.” IEEE Communications Surveys & Tutorials](https://doi.org/10.1109/COMST.2020.2986024)\n\n[198\\. David Gunning, D. Aha. “DARPA's Explainable Artificial Intelligence (XAI) Program.” AI Mag.](https://doi.org/10.1609/AIMAG.V40I2.2850)\n\n[199\\. 2025年北交所投资策略：三载铸基，变中寻机](https://file.iyanbao.com/pdf/fcd33-7b37ff2b-eb4d-465b-be2d-a52efec7bcf3.pdf)\n\n[200\\. JunaidIqbal Khan, Jebran Khan et al. “Artificial Intelligence and Internet of Things (AI-IoT) Technologies in Response to COVID-19 Pandemic: A Systematic Review.” IEEE Access](https://doi.org/10.1109/ACCESS.2022.3181605)\n\n[201\\. Dylan Paulin, Raphael Joud et al. “HistoTrust: tracing AI behavior with secure hardware and blockchain technology.” Annals of Telecommunications](https://doi.org/10.1007/s12243-022-00943-6)\n\n[202\\. F. Wilhelmi, L. Giupponi et al. “Analysis and evaluation of synchronous and asynchronous FLchain.” Comput. Networks](https://doi.org/10.1016/j.comnet.2022.109390)\n\n[203\\. 第二届“未来工业互联网”学术论坛 2024 2nd Future Industrial Internet Symposium 会议论文集](http://fii.xintongconference.com/upload/file/20240822/6385993573919617682102682.pdf)\n\n[204\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[205\\. Jeffrey Pennington, R. Socher et al. “GloVe: Global Vectors for Word Representation.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.3115/v1/D14-1162)\n\n[206\\. Yanran Li, Hui Su et al. “DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset.” ArXiv](https://arxiv.org/abs/1710.03957)\n\n[207\\. Iulian Serban, Alessandro Sordoni et al. “Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models.” AAAI Conference on Artificial Intelligence](https://doi.org/10.1609/aaai.v30i1.9883)\n\n[208\\. Chia-Wei Liu, Ryan Lowe et al. “How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation.” ArXiv](https://doi.org/10.18653/v1/D16-1230)\n\n[209\\. Haotian Wang, Xiyuan Du et al. “Learning to Break: Knowledge-Enhanced Reasoning in Multi-Agent Debate System.”](https://arxiv.org/abs/2312.04854)\n\n[210\\. 辩论树：多角色辩论树激发科学比较分析的批判性思维](https://www.xueshuxiangzi.com/downloads/2025_2_21/2502.14767.pdf)\n\n[211\\. ACC-Debate：一种基于演员-评论员的多智能体辩论方法](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_11_4/2411.00053.pdf)\n\n[212\\. Tongxuan Liu, Xingyu Wang et al. “GroupDebate: Enhancing the Efficiency of Multi-Agent Debate Using Group Discussion.”](https://arxiv.org/abs/2409.14051)\n\n[214\\. I. Goodfellow, Jonathon Shlens et al. “Explaining and Harnessing Adversarial Examples.” CoRR](https://arxiv.org/abs/1412.6572)\n\n[215\\. A. Madry, Aleksandar Makelov et al. “Towards Deep Learning Models Resistant to Adversarial Attacks.” ArXiv](https://arxiv.org/abs/1706.06083)\n\n[216\\. A. Avizienis, J. Laprie et al. “Basic concepts and taxonomy of dependable and secure computing.” IEEE Transactions on Dependable and Secure Computing](https://doi.org/10.1109/TDSC.2004.2)\n\n[217\\. Inioluwa Deborah Raji, A. Smart et al. “Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing.” Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency](https://doi.org/10.1145/3351095.3372873)\n\n[218\\. 先進企業における AI エージェント活用の最前線：2025 年 5 月末時点の社内各部門における試行錯誤と生成 AI 活用状況](https://yorozuipsc.com/uploads/1/3/2/5/132566344/f21444b98b062265b423.pdf)\n\n[219\\. C. Robert. “Superintelligence: Paths, Dangers, Strategies.” CHANCE](https://doi.org/10.1080/09332480.2017.1302723)\n\n[220\\. A Survey of Financial AI: Architectures, Advances and Open Challenges](https://www.arxiv.org/pdf/2411.12747)\n\n[221\\. 从虚拟代理到机器人团队：高风险医疗环境中的多机器人框架评估](https://www.xueshuxiangzi.com/downloads/2025_6_5/2506.03546.pdf)\n\n[222\\. 2024年AI失败案例总结](https://xueqiu.com/4601596167/320181819)\n\n[223\\. 3D-MVP: 3D Multiview Pretraining for Robotic Manipulation](https://www.alphaxiv.org/abs/2406.18158)\n\n[224\\. MiLA: Multi-view Intensive-fidelity Long-term Video Generation World Model for Autonomous Driving](https://www.alphaxiv.org/abs/2503.15875)\n\n[225\\. 苏联东欧社会主义实践成败经验教训考](https://www.sklib.cn/booklib/bookPreview?SiteID=122&ID=11626728&source=mita)\n\n[226\\. 固本拓新 惟智驭势 中国银行业2024年发展回顾及2025年展望](https://www2.deloitte.com/content/dam/Deloitte/cn/Documents/financial-services/deloitte-chinese-banking-sector-2024-review-and-2025-outlook-zh-250428.pdf)\n\n[227\\. 2024 中国信创+AI趋势洞察报告](http://www.csia-jpw.com/UserFiles/Article/file/6387616415478161773940215.pdf)\n\n[228\\. Realizing Practical LLM-assisted AI Assistant in the Semiconductor Domain](https://escholarship.org/content/qt14x4v0zb/qt14x4v0zb.pdf)\n\n[229\\. 2025科技、传媒和电信行业预测：弥合差距](https://www2.deloitte.com/content/dam/Deloitte/cn/Documents/technology-media-telecommunications/deloitte-cn-tmt-predictions-2025-zh-250217.pdf)\n\n[230\\. Meeting 根本错误](https://cdn.meeting.tencent.com/pro/N2JiNDA5NTctNWUwOS00ODdjLTlmOTgtMzkzNWJiMGU2YTJh)\n\n[231\\. AI Scientists Fail Without Strong Implementation Capability](https://arxiv.org/pdf/2506.01372)\n\n[232\\. Green Quadrant: Industrial Data Management Solutions 2025](https://6407318.fs1.hubspotusercontent-na1.net/hubfs/6407318/Verdantix%20Green%20Quadrant%20Industrial%20Data%20Management%20Solutions%202025.pdf)\n\n[233\\. 盘点2024年的“AI 事故”：内容垃圾、幻觉与滥用](http://h5.ifeng.com/c/vivo/v002T8flgcFBxhrkIf7ZdIOZ-_D9cVLD91giXjUoyhSuJtD8__)\n\n[234\\. Anthony Dewayne Hunt. “Bitcoin: A Peer-to-Peer Electronic Cash System.”](https://doi.org/10.2139/ssrn.3440802)\n\n[235\\. M. Castro. “Practical Byzantine fault tolerance.” USENIX Symposium on Operating Systems Design and Implementation](https://doi.org/10.1145/296806.296824)\n\n[236\\. Zibin Zheng, Shaoan Xie et al. “An Overview of Blockchain Technology: Architecture, Consensus, and Future Trends.” 2017 IEEE International Congress on Big Data (BigData Congress)](https://doi.org/10.1109/BIGDATACONGRESS.2017.85)\n\n[237\\. K. Salah, M. H. Rehman et al. “Blockchain for AI: Review and Open Research Challenges.” IEEE Access](https://doi.org/10.1109/ACCESS.2018.2890507)\n\n[238\\. S. Singh, S. Rathore et al. “BlockIoTIntelligence: A Blockchain-enabled Intelligent IoT Architecture with Artificial Intelligence.” Future Gener. Comput. Syst.](https://doi.org/10.1016/j.future.2019.09.002)\n\n[239\\. 2025年北交所投资策略：三载铸基，变中寻机](https://file.iyanbao.com/pdf/fcd33-7b37ff2b-eb4d-465b-be2d-a52efec7bcf3.pdf)\n\n[240\\. Zongwei Li, D. Kong et al. “An Overview of AI and Blockchain Integration for Privacy-Preserving.” ArXiv](https://doi.org/10.48550/arXiv.2305.03928)\n\n[241\\. F. Wilhelmi, L. Giupponi et al. “Analysis and evaluation of synchronous and asynchronous FLchain.” Comput. Networks](https://doi.org/10.1016/j.comnet.2022.109390)\n\n[242\\. 计算2030](https://www-file.huawei.com/-/media/CORP2020/pdf/giv/industry-reports/Computing_2030_cn.pdf)\n\n[243\\. JunaidIqbal Khan, Jebran Khan et al. “Artificial Intelligence and Internet of Things (AI-IoT) Technologies in Response to COVID-19 Pandemic: A Systematic Review.” IEEE Access](https://doi.org/10.1109/ACCESS.2022.3181605)\n\n[244\\. I. Goodfellow, Jonathon Shlens et al. “Explaining and Harnessing Adversarial Examples.” CoRR](https://arxiv.org/abs/1412.6572)\n\n[245\\. Joy Buolamwini, Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” FAT](https://www.semanticscholar.org/paper/18858cc936947fc96b5c06bbe3c6c2faa5614540)\n\n[246\\. Emily M. Bender, Timnit Gebru et al. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.” Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency](https://doi.org/10.1145/3442188.3445922)\n\n[247\\. AI Agent深度（二）：2025 Agent元年，AI从L2向L3发展](https://pdf.dfcfw.com/pdf/H3_AP202505041667413598_1.pdf?1746459426000.pdf)\n\n[248\\. 从虚拟代理到机器人团队：高风险医疗环境中的多机器人框架评估](https://www.xueshuxiangzi.com/downloads/2025_6_5/2506.03546.pdf)\n\n[249\\. 云设计](http://www.ex12580.com/ysj/show/13763)\n\n[250\\. LPL Financial Holdings Annual Report 2024](https://stocklight.com/stocks/us/nasdaq-lpla/lpl-financial-holdings/annual-reports/nasdaq-lpla-2024-10K-24659538.pdf)\n\n[251\\. 开源人工智能代理项目市场研究报告 2025](https://www.drpang.ai/content/files/2025/05/open_source_ai_agent_market_research_2025_zh.pdf)\n\n[252\\. Guangba Yu, Gou Tan et al. “A Survey on Failure Analysis and Fault Injection in AI Systems.”](https://arxiv.org/abs/2407.00125)\n\n[253\\. 成立2月融资过亿,百度大牛加盟,解读创新奇智AI变现心法](http://zhidx.com/p/132295.html)\n\n[254\\. The State of AI & API Security 2025](https://resources.firetail.io/hubfs/State%20of%20AI%20&%20API%20Security%202025%20-%20Final.pdf)\n\n[255\\. Benedikt Bünz, Jonathan Bootle et al. “Bulletproofs: Short Proofs for Confidential Transactions and More.” 2018 IEEE Symposium on Security and Privacy (SP)](https://doi.org/10.1109/SP.2018.00020)\n\n[256\\. M. Blum, Paul Feldman et al. “Non-interactive zero-knowledge and its applications.” Symposium on the Theory of Computing](https://doi.org/10.1145/62212.62222)\n\n[257\\. Bryan Parno, Jon Howell et al. “Pinocchio: Nearly Practical Verifiable Computation.” 2013 IEEE Symposium on Security and Privacy](https://doi.org/10.1145/2856449)\n\n[258\\. Jens Groth. “On the Size of Pairing-Based Non-interactive Arguments.” IACR Cryptol. ePrint Arch.](https://doi.org/10.1007/978-3-662-49896-5_11)\n\n[259\\. R. Gennaro, Craig Gentry et al. “Quadratic Span Programs and Succinct NIZKs without PCPs.” IACR Cryptology ePrint Archive](https://doi.org/10.1007/978-3-642-38348-9_37)\n\n[260\\. CCS 2022-2019](https://www.cnblogs.com/kimika/p/17480452.html)\n\n[261\\. USENIX Security '23 - Dubhe：Succinct Zero-Knowledge Proofs for Standard AES and](https://www.bilibili.com/video/av1651745827?t=71)\n\n[262\\. \\[CRYPTO 2024\\] - session on Zero-knowledge proofs](https://www.bilibili.com/video/av113085213838440?t=1657)\n\n[263\\. 三分钟带你走进零知识证明](https://www.bilibili.com/video/av112981362870828)\n\n[264\\. \\[CRYPTO 2024\\] - session on Interactive and zero-knowledge proofs](https://www.bilibili.com/video/av113085364832813?t=305)\n\n[265\\. I. Goodfellow, Jonathon Shlens et al. “Explaining and Harnessing Adversarial Examples.” CoRR](https://arxiv.org/abs/1412.6572)\n\n[266\\. 先進企業における AI エージェント活用の最前線：2025 年 5 月末時点の社内各部門における試行錯誤と生成 AI 活用状況](https://yorozuipsc.com/uploads/1/3/2/5/132566344/f21444b98b062265b423.pdf)\n\n[267\\. 电动汽车驱动电机热管理系统设计与控制策略](http://www.china-nengyuan.com/tech/china-nengyuan_tech_143838.pdf)\n\n[268\\. 云设计](http://www.ex12580.com/ysj/show/13763)\n\n[269\\. 北京鸣医丁阳:企业品牌价值打造离不开大数据,捕捉全新发展机遇](https://finance.ifeng.com/c/8CxKfMS7Oim)\n\n[270\\. 成立2月融资过亿,百度大牛加盟,解读创新奇智AI变现心法](http://zhidx.com/p/132295.html)\n\n[271\\. 开源人工智能代理项目市场研究报告 2025](https://www.drpang.ai/content/files/2025/05/open_source_ai_agent_market_research_2025_zh.pdf)\n\n[272\\. 常见低代码开发平台：《常见低代码平台盘点》](https://www.informat.cn/qa/365524)"
  },
  {
    "id": 17,
    "question": "With the lowered entry barrier for foundational large models, how can we more quickly apply these models to vertical domain scenarios? There are currently two technical approaches: the first is to build a chain-of-thought corpus tailored to the vertical domain and fine-tune the foundational large model to enhance its understanding of the specific domain; the second is to strengthen the isolation and automatic optimization between prompts and software by constructing a robust external information retrieval system (RAG). How should we choose between these two approaches?",
    "response": "**Comprehensive Research Report: Choosing Between Fine-Tuning and RAG for Vertical Domain Applications**\n\n**1\\. Introduction**\n\nThe democratization of foundational large models (FLMs) has reduced technical barriers to deploying AI in vertical domains (e.g., healthcare, finance, manufacturing). However, efficiently adapting FLMs to specialized contexts remains challenging. Two dominant approaches have emerged: (1) **domain-specific fine-tuning** using chain-of-thought corpora to deepen model understanding, and (2) **Retrieval-Augmented Generation (RAG)** systems that isolate domain knowledge via external retrieval. This report synthesizes performance data, technical tradeoffs, and implementation strategies to guide approach selection, leveraging deep analysis of search results and domain-specific benchmarks.\n\n**2\\. Technical Approaches: Core Mechanisms and Tradeoffs**\n\n**2.1 Domain-Specific Fine-Tuning**\n\nFine-tuning updates FLM parameters using vertical-domain data, embedding domain expertise directly into the model:\n\n**Strengths**:\n\nAchieves high accuracy for narrow, static tasks (e.g., clinical diagnosis: 82% retrieval accuracy in pharmaceutical QA) \\[184\\].\n\nReduces inference latency (no retrieval step), critical for real-time applications like manufacturing defect analysis \\[105\\]\\[189\\].\n\nExcels in consistency and regulatory compliance (e.g., legal document analysis) \\[130\\].\n\n**Weaknesses**:\n\nComputationally intensive: Requires weeks/months of GPU-heavy training and domain-expert data curation \\[9\\]\\[24\\].\n\nProne to catastrophic forgetting and struggles with dynamic knowledge updates (e.g., financial regulation changes) \\[9\\]\\[29\\].\n\nHigh resource demands: Costs escalate with model size and data scarcity \\[17\\]\\[40\\].\n\n**2.2 Retrieval-Augmented Generation (RAG)**\n\nRAG decouples domain knowledge from the FLM, using external databases to fetch real-time context during inference:\n\n**Strengths**:\n\nDynamic updates: Knowledge refreshes via document updates (e.g., new drug discoveries in pharma) without retraining \\[9\\]\\[15\\].\n\nLower entry barrier: 30–50% faster deployment than fine-tuning; ideal for low-data domains \\[26\\]\\[189\\].\n\nFactual accuracy: Reduces hallucinations by 37% in clinical settings \\[187\\].\n\n**Weaknesses**:\n\nLatency penalties: Retrieval steps increase response time by 30–50% \\[15\\]\\[140\\].\n\nBrittle to poor retrieval quality; struggles with complex domain jargon (e.g., SEC filings in finance) \\[6\\]\\[50\\].\n\nDemands robust data pipelines and vector databases \\[30\\]\\[40\\].\n\n**2.3 Hybrid Approaches (e.g., RAFT)**\n\nHybrid models combine fine-tuning and RAG (e.g., Retrieval-Augmented Fine-Tuning):\n\n**Mechanism**: Domain-tuned FLM + real-time RAG retrieval \\[8\\]\\[43\\].\n\n**Performance**: Accuracy gains up to 11 percentage points over standalone methods (pharmaceuticals) \\[20\\]\\[194\\].\n\n**Tradeoffs**: Higher complexity and infrastructure costs but optimal for dynamic, precision-critical domains \\[52\\]\\[193\\].\n\n**3\\. Domain-Specific Performance Benchmarks**\n\nQuantitative differences vary by domain:\n\n**3.1 Healthcare/Pharmaceuticals**\n\n**Fine-tuning**: Near-human accuracy (91.4%) for diagnosis with GPT-4 but degrades with post-2021 knowledge \\[187\\].\n\n**RAG**: Superior for novel drug interactions (89% accuracy); BLEU scores increase by 12% with curated datasets \\[13\\]\\[187\\].\n\n**Hybrid**: 77% RAG accuracy on PubMedQA, matching fine-tuned models with real-time adaptability \\[184\\].\n\n**Metrics**: ROUGE, BERTScore, hallucination rate \\[341\\]\\[353\\].\n\n**3.2 Finance/Legal**\n\n**Fine-tuning**: Achieves 95% accuracy in compliance checks but requires quarterly retraining \\[7\\]\\[130\\].\n\n**RAG**: Legal document analysis latency: 12.9–36s/query; accuracy drops 10% without optimized retrieval \\[137\\]\\[138\\].\n\n**Benchmarks**: LegalBench-RAG (accuracy/coherence), F1-score \\[124\\].\n\n**3.3 Manufacturing/Industrial IoT**\n\n**Fine-tuning**: Edge-deployed models (e.g., YOLOv10 on Jetson Orin) achieve <16ms latency for defect detection \\[225\\]\\[325\\].\n\n**RAG**: 4-bit quantization on Raspberry Pi 5 reduces energy use by 44% but incurs 8% accuracy loss \\[288\\]\\[433\\].\n\n**Hybrid**: LoRA-optimized fine-tuning + RAG cuts training costs by 60% \\[263\\].\n\n**4\\. Technical and Resource Considerations**\n\n**4.1 Low-Data Domains**\n\n**Fine-tuning**: High risk of overfitting; requires data augmentation (e.g., synthetic corpora) \\[24\\]\\[99\\].\n\n**RAG**: More feasible; leverages sparse external data but depends on retrieval relevance \\[26\\]\\[30\\].\n\n**4.2 Resource-Constrained Environments**\n\n**Edge Deployment (e.g., Jetson Orin/Raspberry Pi 5)**:\n\n**Fine-tuning**: Use LoRA/QLoRA for 4-bit quantization (3× memory reduction) \\[263\\]\\[322\\].\n\n**RAG**: Prioritize distilled BERT models (7.5× smaller than BERT-base) for energy efficiency \\[285\\]\\[294\\].\n\n**GPU-Limited Workloads**: Parameter-efficient fine-tuning (PEFT) with LoRA enables Jetson Orin deployment of 7B-parameter models \\[202\\]\\[263\\].\n\n**4.3 Implementation Costs**\n\n**Fine-tuning**: High upfront costs (200k for healthcare corpus curation) \\[24\\]\\[189\\].\n\n**RAG**: Lower initial costs but scaling adds expense (e.g., vector database management) \\[26\\]\\[192\\].\n\n**5\\. Decision Framework: Selecting the Optimal Approach**\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n| **Factor** | **Fine-Tuning** | **RAG** | **Hybrid (RAFT)** |\n| **Knowledge Dynamics** | Static (e.g., regulatory frameworks) | Dynamic (e.g., stock markets) | Mixed (e.g., drug discovery) |\n| **Latency Sensitivity** | High (e.g., real-time control systems) | Medium (e.g., research queries) | Context-dependent |\n| **Data Availability** | Abundant domain corpora | Scarce/evolving data | Moderate to high |\n| **Accuracy Needs** | Critical (e.g., clinical diagnostics) | High (with retrieval optimization) | Highest (6–11% gains) \\[20\\] |\n| **Resource Constraints** | High GPU/budget | Limited GPU/data science expertise | Balanced infrastructure |\n\n**5.1 Domain-Specific Recommendations**\n\n**Healthcare/Pharma**: Hybrid for drug discovery (dynamic + precision); fine-tuning for diagnostic protocols (static knowledge) \\[245\\]\\[255\\].\n\n**Finance**: RAG for real-time market data; fine-tuning for risk modeling \\[7\\].\n\n**Manufacturing**: Fine-tuning for defect detection; RAG for supply chain optimization \\[270\\]\\[417\\].\n\n**6\\. Emerging Trends and Future Outlook**\n\n**LoRA-QLoRA Dominance**: 4-bit quantization + LoRA reduces edge deployment costs by 70% \\[263\\]\\[433\\].\n\n**Automated RAG Optimization**: Tools like JORA (JAX-based) streamline retrieval-augmented fine-tuning for low-resource settings \\[277\\].\n\n**Cross-Domain Benchmarks**: Metrics like RAGBench’s _context utilization_ standardize evaluation \\[395\\].\n\n**Sustainability**: RAG’s lower carbon footprint (40% less energy) prioritizes it for scalable deployments \\[26\\].\n\n**7\\. Conclusion**\n\n**For static, high-accuracy domains** (e.g., clinical diagnostics, legal compliance), fine-tuning remains optimal despite resource demands. **For dynamic, data-scarce environments** (e.g., financial markets, supply chain), RAG offers agility but requires retrieval tuning. **Hybrid approaches** maximize benefits where precision and adaptability intersect (e.g., drug discovery). Selection should prioritize:\n\n1.  **Knowledge volatility**,\n2.  **Latency tolerance**, and\n3.  **Resource availability**.\n\nAs tooling evolves (e.g., Jetson-optimized LoRA, 4-bit RAG), the cost-performance tradeoff will further tilt toward hybrid methods in most verticals.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[2\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[3\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[4\\. RAG vs. Fine-tuning](https://www.ibm.com/think/topics/rag-vs-fine-tuning)\n\n[5\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[6\\. Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study](https://arxiv.org/html/2404.11792v1)\n\n[7\\. A Survey on Retrieval-Augmented Generation: From Naive to Adaptive Approaches with Financial Insights](https://kronika.ac/wp-content/uploads/5-KKJ2327.pdf)\n\n[8\\. RAG vs. Fine Tuning: How to Choose](https://www.oracle.com/ar/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/rag-fine-tuning/)\n\n[9\\. Fine-Tuning vs RAG: Key Differences Explained (2025 Guide)](https://orq.ai/blog/finetuning-vs-rag)\n\n[10\\. What is RAG (Retrieval Augmented Generation)? | IBM](https://www.ibm.com/think/topics/retrieval-augmented-generation#:~:text=Staff%20writer-,What%20is%20RAG%20%28retrieval%20augmented%20generation%29?,responses%20at%20a%20higher%20quality.)\n\n[11\\. 关于课程中“RAG vs 微调”两种技术的学习心得](https://developer.metax-tech.com/forum/t/guan-yu-ke-cheng-zhong-rag-vs-wei-diao-liang-chong-ji-zhu-de-xue-xi-xin-de/71/)\n\n[12\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[13\\. DeepThink: Aligning Language Models with Domain-Specific User Intents](https://arxiv.org/pdf/2502.05497)\n\n[14\\. A Survey on Retrieval-Augmented Generation (RAG) Models: Recent Advances and Challenges](https://thegrenze.com/pages/servej.php?fn=109.pdf&name=A%20Survey%20on%20Retrieval-Augmented%20Generation%20%28RAG%29Models:%20Recent%20Advances%20and%20Challenges&id=3943&association=GRENZE&journal=GIJET&year=2025&volume=11&issue=1)\n\n[15\\. Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA](https://arxiv.org/pdf/2502.10497)\n\n[16\\. LLM Landscape Report](https://cdn.prod.website-files.com/668d66434307b08c724f8a81/67dc26e1400679d594e43df8_LLM%20Landscape%20Report.pdf)\n\n[17\\. RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance](https://www.rungalileo.io/blog/optimizing-llm-performance-rag-vs-finetune-vs-both)\n\n[18\\. Teaching Large Language Models to Use Tools at Scale](https://escholarship.org/content/qt5m8673w5/qt5m8673w5.pdf)\n\n[19\\. LLMs to Support a Domain Specific Knowledge Assistant](https://arxiv.org/pdf/2502.04095)\n\n[20\\. Specializing LLMs for Domains: RAG vs. Fine-Tuning](https://towardsai.net/p/l/specializing-llms-for-domains-rag-vs-fine-tuning)\n\n[21\\. Adam Paszke, Sam Gross et al. “PyTorch: An Imperative Style, High-Performance Deep Learning Library.” ArXiv](https://arxiv.org/abs/1912.01703)\n\n[22\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[23\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[24\\. Augment LLMs with RAGs or Fine-Tuning](https://learn.microsoft.com/en-au/azure/developer/ai/augment-llm-rag-fine-tuning)\n\n[25\\. Integrating Digital Twin and Advanced Intelligent Technologies to Realize the Metaverse](https://ctsoc.ieee.org/images/CTSOC-NCT-2024-03.pdf)\n\n[26\\. Rag vs Fine-Tuning](https://www.radiansys.com/blog/adapting-ai-models-fine-tuning-rag)\n\n[27\\. RAG vs Fine Tuning: The Hidden Trade-offs No One Talks About](https://b-eye.com/blog/rag-vs-fine-tuning/)\n\n[28\\. LLM Landscape Report](https://cdn.prod.website-files.com/668d66434307b08c724f8a81/67dc26e1400679d594e43df8_LLM%20Landscape%20Report.pdf)\n\n[29\\. RAG vs. Fine-Tuning: Choosing the Right Approach for Your AI Model in 2025](https://www.ai-infra-link.com/rag-vs-fine-tuning-choosing-the-right-approach-for-your-ai-model-in-2025/)\n\n[30\\. RAG vs Fine Tuning: Which Method to Choose](https://labelyourdata.com/articles/rag-vs-fine-tuning)\n\n[31\\. Fine-tuning vs. RAG: Understanding the Difference](https://finetunedb.com/blog/fine-tuning-vs-rag/)\n\n[32\\. RAG vs Fine-Tuning in LLMs - Version 2](https://version-2.com/zh/2024/12/rag-vs-fine-tuning-in-llms/)\n\n[33\\. The limitations of model fine-tuning and RAG](https://www.infoworld.com/article/3715306/the-limitations-of-model-fine-tuning-and-rag.html)\n\n[34\\. El auge de los large language models: de los fundamentos a la aplicación](https://www.managementsolutions.com/sites/default/files/minisite/static/72b0015f-39c9-4a52-ba63-872c115bfbd0/llm/pdf/auge-de-los-llm.pdf)\n\n[35\\. Mastering RAG: Improve RAG Performance With 4 Powerful RAG Metrics](https://www.rungalileo.io/blog/mastering-rag-improve-performance-with-4-powerful-metrics)\n\n[36\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[37\\. Challenges in Building RAG-Based LLM Applications](https://learn.datasciencedojo.com/wp-content/uploads/2024/11/Challenges-in-building-RAG-based-LLM-Applications-Feb-2025.pdf)\n\n[38\\. Fine-Tuning vs RAG: Key Differences Explained (2025 Guide)](https://orq.ai/blog/finetuning-vs-rag)\n\n[39\\. N. Houlsby, A. Giurgiu et al. “Parameter-Efficient Transfer Learning for NLP.” ArXiv](https://arxiv.org/abs/1902.00751)\n\n[40\\. RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance](https://www.rungalileo.io/blog/optimizing-llm-performance-rag-vs-finetune-vs-both)\n\n[41\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[42\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[43\\. RAG (Retrieval Augmented Generation): A Complete Guide](https://www.aimw.ai/blog/retrieval-augmented-generation-rag)\n\n[44\\. META GEN 混合 RAG：无需微调即可提高特定领域问题 & A 的准确性](https://www.xueshuxiangzi.com/downloads/2025_5_27/2505.18247.pdf)\n\n[45\\. Data Spaces and Foundation Models: Enabling High-Quality Artificial Intelligence](https://www.isst.fraunhofer.de/content/dam/isst/publikationen/whitepaper/data-spaces_and_foundation-models_whitepaper.pdf)\n\n[46\\. Knowledge Graph for Query Enhancement in Retrieval Augmented Generation Domain-Specific Applications](http://essay.utwente.nl/106310/1/Massimo_Perna_MA_EEMathCS.pdf)\n\n[47\\. Tailoring foundation models for your business needs: A comprehensive guide to RAG, fine-tuning, and hybrid approaches](https://aws.amazon.com/blogs/machine-learning/tailoring-foundation-models-for-your-business-needs-a-comprehensive-guide-to-rag-fine-tuning-and-hybrid-approaches/)\n\n[48\\. RAG vs. Fine Tuning: How to Choose](https://www.oracle.com/tr/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/rag-fine-tuning/)\n\n[49\\. RAG Retrieval Augmented Generation: A Complete Guide](https://collabnix.com/rag-retrieval-augmented-generation-the-complete-guide-to-building-intelligent-ai-systems-in-2025/)\n\n[50\\. MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models](https://openreview.net/pdf/2a13a6784da2b9df088ddaefa93ad3a571ff74a2.pdf)\n\n[51\\. RAG vs. Fine-Tuning: How to Choose](https://www.oracle.com/cn/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/rag-fine-tuning/)\n\n[52\\. RAG vs. Fine-Tuning: Choosing the Right Approach for Your AI Model in 2025](https://www.ai-infra-link.com/rag-vs-fine-tuning-choosing-the-right-approach-for-your-ai-model-in-2025/)\n\n[53\\. RAG vs. fine-tuning: Choosing the right method for your LLM](https://www.superannotate.com/blog/rag-vs-fine-tuning)\n\n[54\\. Understanding Retrieval Augmented Fine-Tuning (RAFT)](https://capestart.com/resources/blog/what-is-retrieval-augmented-fine-tuning/)\n\n[55\\. Privacy-Focused LLM for Local Data Processing: Implementing OLLAMA and RAG to Securely Query Personal Files in Closed Environments](https://openaccess.uoc.edu/bitstream/10609/151999/1/Adri%C3%A0Rodr%C3%ADguezQui%C3%B1onesTFC0125.pdf)\n\n[56\\. Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Don’t Know\", and Reducing Hallucination in RAG](https://openreview.net/pdf?id=X4UoRsiU72)\n\n[57\\. LLM Fine-Tuning vs Retrieval-Augmented Generation (RAG)](https://www.f22labs.com/blogs/llm-fine-tuning-vs-retrieval-augmented-generation-rag/)\n\n[58\\. RAG, Finetuning or Both? Choosing the Right Strategy](https://www.matrixflows.com/blog/retrieval-augmented-generation-rag-finetuning-hybrid-framework-for-choosing-right-strategy)\n\n[59\\. 智能模型新篇章：RAG + Fine-Tuning 混合增强策略](https://zhuanlan.zhihu.com/p/688138789)\n\n[61\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[62\\. Medical LLMs: Fine-Tuning vs. Retrieval-Augmented Generation](https://www.mdpi.com/2306-5354/12/7/687)\n\n[63\\. Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Don’t Know\", and Reducing Hallucination in RAG](https://openreview.net/pdf?id=X4UoRsiU72)\n\n[64\\. Tailoring foundation models for your business needs: A comprehensive guide to RAG, fine-tuning, and hybrid approaches](https://aws.amazon.com/blogs/machine-learning/tailoring-foundation-models-for-your-business-needs-a-comprehensive-guide-to-rag-fine-tuning-and-hybrid-approaches/)\n\n[65\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[66\\. RAG vs. Fine Tuning: How to Choose](https://www.oracle.com/se/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/rag-fine-tuning/)\n\n[67\\. COMBINING DEEP LEARNING AND PHYSICS-BASED PERFORMANCE MODELS FOR DIAGNOSTICS AND PROGNOSTICS](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/517153/1/2021_PhD_Thesis_MAC_final.pdf)\n\n[68\\. LOCAL RETRIEVAL AUGMENTED GENERATION WITH LARGE LANGUAGE MODELS ON EXTENSIVE TEXT CORPORA](https://dspace.cvut.cz/bitstream/handle/10467/122664/F8-DP-2025-Kucera-Jakub-thesis.pdf?sequence=-1)\n\n[69\\. RAG Retrieval Augmented Generation: A Complete Guide](https://collabnix.com/rag-retrieval-augmented-generation-the-complete-guide-to-building-intelligent-ai-systems-in-2025/)\n\n[70\\. RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture](https://www.academia.edu/126970871/RAG_vs_Fine_tuning_Pipelines_Tradeoffs_and_a_Case_Study_on_Agriculture)\n\n[71\\. RAG vs Fine Tuning: Choosing the Right Approach for Improving AI Models](https://www.digitalocean.com/resources/articles/rag-vs-fine-tuning)\n\n[72\\. FIND: Fine-grained Information Density Guided Adaptive Retrieval-Augmented Generation for Disease Diagnosis](http://arxiv.org/pdf/2502.14614)\n\n[73\\. RAG vs Fine Tuning: Which Method to Choose](https://labelyourdata.com/articles/rag-vs-fine-tuning)\n\n[74\\. META GEN 混合 RAG：无需微调即可提高特定领域问题 & A 的准确性](https://www.xueshuxiangzi.com/downloads/2025_5_27/2505.18247.pdf)\n\n[75\\. Fine-Tuning vs RAG: Key Differences Explained (2025 Guide)](https://orq.ai/blog/finetuning-vs-rag)\n\n[76\\. Supercharging Proactive Network Maintenance by Leveraging Generative AI](https://www.nctatechnicalpapers.com/Paper/2024/AI08_Chari_6610_paper/download)\n\n[77\\. Cost Optimized hosting of Fine-tuned LLMs in Production](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/cost-optimized-hosting-of-fine-tuned-llms-in-production/4062192)\n\n[78\\. A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning](https://arxiv.org/pdf/2408.05141)\n\n[79\\. RAG与微调：选择最适合您的AI项目的最佳方法](https://www.projectpro.io/article/rag-vs-fine-tuning/1039)\n\n[80\\. Knowledge Injection in LLMs: Fine-Tuning vs. RAG](https://zilliz.com/blog/knowledge-injection-in-llms-fine-tuning-and-rag)\n\n[81\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[82\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[83\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[84\\. Domain Adaptation of Retrieval Systems from Unlabeled Corpora](https://lup.lub.lu.se/student-papers/record/9169000/file/9169004.pdf)\n\n[85\\. DragFT: Adapting Large Language Models with Dictionary and Retrieval Augmented Fine-tuning for Domain-specific Machine Translation](https://arxiv.org/pdf/2402.15061)\n\n[86\\. Retrieval-Augmented Visual Question Answering via Built-in Autoregressive Search Engines](https://www.arxiv.org/pdf/2502.16641)\n\n[87\\. Gradual Fine-Tuning for Low-Resource Domain Adaptation](https://ar5iv.labs.arxiv.org/html/2103.02205)\n\n[88\\. Quanyu Long, Wenya Wang et al. “Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.48550/arXiv.2311.11551)\n\n[89\\. Instruction Tuning for Domain Adaptation of Large Language Models: A Case Study in the Field of Education](https://repository.tudelft.nl/file/File_e2064dc9-97f1-4e9d-ac32-1bebebf05c3d?preview=1)\n\n[90\\. Mingsheng Long, Yue Cao et al. “Learning Transferable Features with Deep Adaptation Networks.” ArXiv](https://arxiv.org/abs/1502.02791)\n\n[91\\. Yaroslav Ganin, E. Ustinova et al. “Domain-Adversarial Training of Neural Networks.” Journal of machine learning research](https://doi.org/10.1007/978-3-319-58347-1_10)\n\n[92\\. Enhanced Retrieval-Augmented Generation Using Low-Rank Adaptation](https://www.mdpi.com/2076-3417/15/8/4425)\n\n[93\\. Minju Seo, Jinheon Baek et al. “Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks.” ArXiv](https://doi.org/10.48550/arXiv.2402.13482)\n\n[94\\. Domain-Specific LLM Fine-Tuning](https://www.rohan-paul.com/p/domain-specific-llm-fine-tuning)\n\n[95\\. Domain Adaptation of Large Language Models for Enhanced Natural Language Processing](https://www.infosys.com/iki/techcompass/large-language-models.html)\n\n[96\\. GPU @ ALICE](https://conferences.lbl.gov/event/1080/contributions/6545/attachments/4288/3751/2022-12-07%20ALICE%20USA%20Computing.pdf)\n\n[97\\. Benfeng Xu, Chunxu Zhao et al. “Retrieval-Augmented Domain Adaptation of Language Models.” Workshop on Representation Learning for NLP](https://doi.org/10.18653/v1/2023.repl4nlp-1.5)\n\n[98\\. TeamLoRA: Boosting Low-Rank Adaptation with Expert Collaboration and Competition](http://arxiv.org/html/2408.09856v1)\n\n[99\\. NVIDIA Riva](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tutorials/asr-improve-recognition-for-specific-words.html)\n\n[100\\. Efficient In-Domain Question Answering for Resource-Constrained Environments](https://arxiv.org/html/2409.17648v1)\n\n[101\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[102\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[103\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[104\\. AI in the Wild: Robust evaluation and optimized fine-tuning of machine learning algorithms deployed on the edge](http://essay.utwente.nl/95066/1/Burgt_MA_EEMCS.pdf)\n\n[105\\. RAG vs Fine Tuning: The Hidden Trade-offs No One Talks About](https://b-eye.com/blog/rag-vs-fine-tuning/)\n\n[106\\. Fine tuning is dead. Long live fine tuning?](https://parlance-labs.com/education/fine_tuning/emmanuel.pdf)\n\n[107\\. Efficient Deployment of Large Language Models on Resource Constrained Edge Computing Platforms](https://www.aspdac.com/aspdac2025/archive/pdf/T4-1.pdf)\n\n[108\\. RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance](https://www.rungalileo.io/blog/optimizing-llm-performance-rag-vs-finetune-vs-both)\n\n[109\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[110\\. Hardware and Algorithm Co-Exploration for Efficient On-Device Personalization of Large Language Models](https://curate.nd.edu/ndownloader/files/53983748/1)\n\n[111\\. RAG vs. Fine-Tuning: A comparison between these two approaches to build LLM-powered tools](https://www.stack-ai.com/blog/fine-tuning-vs-rag)\n\n[112\\. EMPIRICAL GUIDELINES FOR DEPLOYING LLMs ONTO RESOURCE-CONSTRAINED EDGE DEVICES](https://openreview.net/pdf/711b4df3bdb10abdaa5b085cebd86b48cc92f62d.pdf)\n\n[113\\. RAG vs. Fine-Tuning: Tailoring AI to Your Needs](https://raiday.ai/blog/theory/rag-vs-fine-tuning/)\n\n[114\\. Fine-Tuning Vs RAG ，该如何选择？](https://www.skycaiji.com/aigc/ai15945.html)\n\n[115\\. How Computer Vision Is Driving Innovation In Edge Devices](https://outreach.iiit.ac.in/techforward/images/Dispatch_TechForward%20June%2024.pdf)\n\n[116\\. DEPLOYMENT OF SMALL LLMS ON CONSUMER-GRADE HARDWARE FOR EDGE COMPUTING](https://www.tdcommons.org/cgi/viewcontent.cgi?article=8231&context=dpubs_series)\n\n[121\\. RAG vs Fine-Tuning](https://analyticsindiamag.com/ai-origins-evolution/rag-vs-fine-tuning/)\n\n[122\\. AI Agents: What legal implications of autonomous artificial intelligence?](https://www.dlapiper.com/-/media/project/dlapiper-tenant/dlapiper/insights/publications/2024/12/202412-diritto_intelligente-n-4.pdf?rev=-1)\n\n[123\\. Comparing RAG and Fine-Tuning: Which Is Better?](https://navan.ai/blog/comparing-rag-and-fine-tuning-which-is-better/)\n\n[124\\. Groundbreaking Legal AI Benchmark: LegalBench-RAG Tests Retrieval-Augmented Generation](https://dev.to/aimodels-fyi/groundbreaking-legal-ai-benchmark-legalbench-rag-tests-retrieval-augmented-generation-2p32)\n\n[125\\. Benchmarking RAG Systems: Making AI Answers Reliable, Fast, and Useful](https://www.walturn.com/insights/benchmarking-rag-systems-making-ai-answers-reliable-fast-and-useful)\n\n[126\\. LOCAL RETRIEVAL AUGMENTED GENERATION WITH LARGE LANGUAGE MODELS ON EXTENSIVE TEXT CORPORA](https://dspace.cvut.cz/bitstream/handle/10467/122664/F8-DP-2025-Kucera-Jakub-thesis.pdf?sequence=-1)\n\n[127\\. RAG vs Finetuning - Which Is the Best Tool to Boost Your LLM Application?](https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7/)\n\n[128\\. RAG vs fine-tuning vs. prompt engineering](https://www.ibm.com/think/topics/rag-vs-fine-tuning-vs-prompt-engineering#:~:text=Prompt%20engineering%20optimizes%20input%20prompts,relevant%20data%20for%20greater%20accuracy.)\n\n[129\\. M. A. D. L. Balaguer, Vinamra Benara et al. “RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture.” ArXiv](https://doi.org/10.48550/arXiv.2401.08406)\n\n[130\\. How to Choose Between RAG and Fine Tuning?](https://www.mindee.com/blog/rag-vs-fine-tuning)\n\n[131\\. Fine-Tuning vs RAG: Key Differences Explained (2025 Guide)](https://orq.ai/blog/finetuning-vs-rag)\n\n[132\\. Evaluating Ragie Against Real Legal Documents Using LegalBench-RAG](https://www.ragie.ai/blog/evaluating-ragie-against-legalbench-rag)\n\n[133\\. RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance](https://www.rungalileo.io/blog/optimizing-llm-performance-rag-vs-finetune-vs-both)\n\n[134\\. Rag vs Fine-Tuning](https://www.radiansys.com/blog/adapting-ai-models-fine-tuning-rag)\n\n[135\\. Fine-tuning vs RAG: How to choose the right approach?](https://www.reply.com/cluster-reply/it/Shared%20Documents/CLUBIT-Fine-tuning-VS-RAG.pdf)\n\n[136\\. RAG vs. Fine-Tuning: A comparison between these two approaches to build LLM-powered tools](https://www.stack-ai.com/blog/fine-tuning-vs-rag)\n\n[137\\. RAG vs. Context-Window in GPT-4: accuracy, cost, & latency](https://webflow.copilotkit.ai/blog/rag-vs-context-window-in-gpt-4)\n\n[138\\. Fine tuning LLMs for Enterprise: Practical Guidelines and Recommendations](https://arxiv.org/html/2404.10779v1)\n\n[139\\. RAG4ITOps: A Supervised Fine-Tunable and Comprehensive RAG Framework for IT Operations and Maintenance](http://arxiv.org/abs/2410.15805?context=cs.AI)\n\n[140\\. Custom AI for Enterprises: RAG, Fine-Tuning, or Both?](https://www.seekr.com/fine-tuning-rag-custom-ai-for-enterprises/)\n\n[141\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[142\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[143\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[144\\. Recall: Empowering Multimodal Embedding for Edge Devices](https://www.caidongqi.com/pdf/arXiv-Recall.pdf)\n\n[145\\. Med-Idefics: A Two-Stage Fine-Tuning Approach for Enhanced Medical Visual Question Answering](https://cs231n.stanford.edu/2024/papers/med-idefics-a-two-stage-fine-tuning-approach-for-enhanced-medica.pdf)\n\n[146\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[147\\. Fine-Tuning LLMs using NVIDIA Jetson AGX Orin](https://www.hackster.io/shahizat/fine-tuning-llms-using-nvidia-jetson-agx-orin-b17c4d)\n\n[148\\. DeepSeek AI: A comprehensive guide for enterprise implementation](https://aigc.idigital.com.cn/djyanbao/%E3%80%90DeepSeek%20AI%E3%80%91%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%9E%E6%96%BD%E5%85%A8%E9%9D%A2%E6%8C%87%E5%8D%97-2025-06-02.pdf)\n\n[149\\. Weakly supervised language models for automated extraction of critical findings from radiology reports](https://www.nature.com/articles/s41746-025-01522-4)\n\n[150\\. HyC-LoRA: Memory Efficient LoRA Fine-tuning with Hybrid Activation Compression](https://openreview.net/pdf/770408b6336f7acf5e1faf028972bb955db0234b.pdf)\n\n[151\\. Integrating Qt and LLMs on the NVIDIA Jetson board for controlling a patient-assisting robot arm](https://oulurepo.oulu.fi/bitstream/10024/50979/1/nbnfioulu-202406264950.pdf)\n\n[152\\. Enhancing Biomedical Question Answering with Parameter-Efficient Fine-Tuning and Hierarchical Retrieval Augmented Generation](https://ceur-ws.org/Vol-3740/paper-10.pdf)\n\n[153\\. Run Google DeepMind’s Gemma 3n on NVIDIA Jetson and RTX](https://developer.nvidia.com/blog/run-google-deepminds-gemma-3n-on-nvidia-jetson-and-rtx/)\n\n[154\\. MIRAGE-BENCH: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems](https://arxiv.org/pdf/2410.13716)\n\n[155\\. An evaluation of DeepSeek Models in Biomedical Natural Language Processing](https://www.arxiv.org/pdf/2503.00624)\n\n[156\\. AI Assistant for Harry Potter Fans using Nvidia Jetson board](https://www.hackster.io/shahizat/ai-assistant-for-harry-potter-fans-using-nvidia-jetson-board-4c6fbc)\n\n[157\\. Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards](https://arxiv.org/pdf/2408.11775v1)\n\n[158\\. Zhengxiao Du, Yujie Qian et al. “GLM: General Language Model Pretraining with Autoregressive Blank Infilling.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/2022.acl-long.26)\n\n[159\\. Minda Hu, Licheng Zong et al. “Enhancing Biomedical Knowledge Retrieval-Augmented Generation with Self-Rewarding Tree Search and Proximal Policy Optimization.” ArXiv](https://doi.org/10.48550/arXiv.2406.11258)\n\n[160\\. QLoRA: Efficient Finetuning of Quantized LLMs](https://papers.nips.cc/paper_files/paper/2023/file/1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf)\n\n[161\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[162\\. 4bit-Quantization in Vector-Embedding for RAG](http://arxiv.org/html/2501.10534v1)\n\n[163\\. Retrieval-Augmented Generation (RAG) vs LLM Fine-Tuning](https://blog.kore.ai/cobus-greyling/retrieval-augmented-generation-rag-vs-llm-fine-tuning)\n\n[164\\. TOWARD EFFICIENT LOW-PRECISION TRAINING: DATA FORMAT OPTIMIZATION AND HYSTERESIS QUANTIZATION](https://openreview.net/pdf?id=3HJOA-1hb0e)\n\n[165\\. Benoit Jacob, S. Kligys et al. “Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.” 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2018.00286)\n\n[166\\. Deep Compression of Pre-trained Transformer Models](https://papers.neurips.cc/paper_files/paper/2022/file/5b5618e7d061748267d74478b7c5b1ab-Paper-Conference.pdf)\n\n[167\\. Shuchang Zhou, Zekun Ni et al. “DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients.” ArXiv](https://arxiv.org/abs/1606.06160)\n\n[168\\. DNN Feature Map Compression using Learned Representation over GF(2)](https://openaccess.thecvf.com/content_ECCVW_2018/papers/11132/Gudovskiy_DNN_Feature_Map_Compression_using_Learned_Representation_over_GF2_ECCVW_2018_paper.pdf)\n\n[169\\. A. Shymyrbay, M. Fouda et al. “Low Precision Quantization-aware Training in Spiking Neural Networks with Differentiable Quantization Function.” 2023 International Joint Conference on Neural Networks (IJCNN)](https://doi.org/10.1109/IJCNN54540.2023.10191387)\n\n[170\\. Mohammad Rastegari, Vicente Ordonez et al. “XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks.” ArXiv](https://doi.org/10.1007/978-3-319-46493-0_32)\n\n[171\\. Zero to RAG in 60 Minutes! How to get started with LLM Retrieval Augmented Generation](https://www.ciscolive.com/c/dam/r/ciscolive/emea/docs/2025/pdf/BRKAI-2920.pdf)\n\n[172\\. Enabling Fast 2-bit LLM on GPUs: Memory Alignment, Sparse Outlier, and Asynchronous Dequantization](https://dai.sjtu.edu.cn/my_file/pdf/8083b780-c393-459c-8b66-6f3bd738c916.pdf)\n\n[173\\. Jungwook Choi, Zhuo Wang et al. “PACT: Parameterized Clipping Activation for Quantized Neural Networks.” ArXiv](https://arxiv.org/abs/1805.06085)\n\n[174\\. Quantized DeepSeek-R1 Models: Deployment-Ready Reasoning Models](https://neuralmagic.com/blog/quantized-deepseek-r1-models-deployment-ready-reasoning-models/)\n\n[175\\. Robustness-Guided Image Synthesis for Data-Free Quantization](http://arxiv.org/html/2310.03661v3)\n\n[176\\. Compare quantified neural networks for MNIST (images) and N-MNIST (events) data](https://dumas.ccsd.cnrs.fr/dumas-04538536v1/document)\n\n[177\\. The Impact of 8- and 4-Bit Quantization on the Accuracy and Silicon Area Footprint of Tiny Neural Networks](https://www.mdpi.com/2079-9292/14/1/14)\n\n[178\\. Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5](https://arxiv.org/html/2506.09300v1)\n\n[179\\. Efficient Multi-task LLM Quantization and Serving for Multiple LoRA Adapters](https://openreview.net/pdf?id=HfpV6u0kbX)\n\n[180\\. 4-bit Quantization of LSTM-based Speech Recognition Models](https://www.isca-archive.org/interspeech_2021/fasoli21_interspeech.pdf)\n\n[181\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[182\\. RAG vs. Fine-Tuning: How to Choose](https://www.oracle.com/cn/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/rag-fine-tuning/)\n\n[183\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[184\\. META GEN 混合 RAG：无需微调即可提高特定领域问题 & A 的准确性](https://www.xueshuxiangzi.com/downloads/2025_5_27/2505.18247.pdf)\n\n[185\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[186\\. Performance of Retrieval-Augmented Generation (RAG) on Pharmaceutical Documents](https://intuitionlabs.ai/articles/rag-performance-pharmaceutical-documents)\n\n[187\\. Medical LLMs: Fine-Tuning vs. Retrieval-Augmented Generation](https://www.mdpi.com/2306-5354/12/7/687)\n\n[188\\. LLMs to Support a Domain Specific Knowledge Assistant](https://arxiv.org/pdf/2502.04095)\n\n[189\\. RAG, Finetuning or Both? Choosing the Right Strategy](https://www.matrixflows.com/blog/retrieval-augmented-generation-rag-finetuning-hybrid-framework-for-choosing-right-strategy)\n\n[190\\. Enhancing Forecasting Accuracy in the Pharmaceutical Industry: A Comprehensive Review of Methods, Models, and Data Applications](https://jsaer.com/download/vol-9-iss-2-2022/JSAER2022-9-2-156-160.pdf)\n\n[191\\. RAG vs. Fine Tuning: Which One is Right for You?](https://vectorize.io/rag-vs-fine-tuning/)\n\n[192\\. Specializing LLMs for Domains: RAG 🧵vs. Fine-Tuning ⚡](https://towardsai.net/p/machine-learning/specializing-llms-for-domains-rag-vs-fine-tuning)\n\n[193\\. RAG vs. Fine-Tuning: Choosing the Right Approach for Your AI Model in 2025](https://www.ai-infra-link.com/rag-vs-fine-tuning-choosing-the-right-approach-for-your-ai-model-in-2025/)\n\n[194\\. RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture](https://arxiv.org/abs/2401.08406)\n\n[195\\. Farrell Muhammad Rizaldy, Y. Handayati et al. “Comparative Analysis of Demand Forecasting Methods to Optimize Supply Chain Efficiency in PharmaHealth Group.” International Journal of Current Science Research and Review](https://doi.org/10.47191/ijcsrr/v7-i8-40)\n\n[196\\. 智启新质生产力之二：生成式人工智能（AIGC）在医药零售的潜在应用](https://my-data-oss.oss-cn-wulanchabu.aliyuncs.com/wp-content/uploads/2024/11/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%EF%BC%88AIGC%EF%BC%89%E5%9C%A8%E5%8C%BB%E8%8D%AF%E9%9B%B6%E5%94%AE%E7%9A%84%E6%BD%9C%E5%9C%A8%E5%BA%94%E7%94%A8-%E6%B0%B8%E5%AE%89-2024.11-35%E9%A1%B5.pdf)\n\n[197\\. Leveraging Vector Databases to Enhance AI Applications in Pharmaceutical Research](https://phuse.s3.eu-central-1.amazonaws.com/Archive/2025/Connect/US/Orlando/PAP_ML28.pdf)\n\n[198\\. RAG vs. fine-tuning: Choosing the right method for your LLM](https://www.superannotate.com/blog/rag-vs-fine-tuning)\n\n[199\\. MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning](https://arxiv.org/html/2505.18247v1)\n\n[200\\. Fine-Tuning vs RAG: Key Differences Explained (2025 Guide)](https://orq.ai/blog/finetuning-vs-rag)\n\n[201\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[202\\. Fine-Tuning LLMs using NVIDIA Jetson AGX Orin](https://www.hackster.io/shahizat/fine-tuning-llms-using-nvidia-jetson-agx-orin-b17c4d)\n\n[203\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[204\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[205\\. All AIs on Quality: Startup’s NVIDIA Jetson-Enabled Inspections Boost Manufacturing](https://blogs.nvidia.com/blog/startups-nvidia-jetson-enabled-inspections-boost-manufacturing/)\n\n[206\\. A Retrieval Augmented Generation Fine-Tuned LLM Model for Refactored Code Recommendations to Mitigate Java Lock Contention Performance Faults](https://ontariotechu.scholaris.ca/bitstreams/f2c609eb-a14f-4f0d-a7c9-59eafa251897/download)\n\n[207\\. Mark Chen, Jerry Tworek et al. “Evaluating Large Language Models Trained on Code.” ArXiv](https://arxiv.org/abs/2107.03374)\n\n[208\\. A Hands-on Approach to Fine-Tuning DeepSeek R1 for Optimal Performance](https://techifysolutions.com/blog/fine-tuning-deepseek-r1/)\n\n[209\\. Tim Dettmers, Artidoro Pagnoni et al. “QLoRA: Efficient Finetuning of Quantized LLMs.” ArXiv](https://doi.org/10.48550/arXiv.2305.14314)\n\n[210\\. BugWhisperer: Fine-Tuning LLMs for SoC Hardware Vulnerability Detection](https://eprint.iacr.org/2025/546.pdf)\n\n[211\\. Integrating YOLOv5, Jetson nano microprocessor, and Mitsubishi robot manipulator for real-time machine vision application in manufacturing: A lab experimental study](https://www.astrj.com/pdf-201366-123589?filename=Integrating%20YOLOv5_.pdf)\n\n[212\\. Fine-Tuning Llama Factory GitHub](https://www.restack.io/p/fine-tuning-answer-llama-factory-cat-ai)\n\n[213\\. NVIDIA Jetson Orin Nano Super 开发者套件发布](https://developer.nvidia.com/ko-kr/blog/nvidia-jetson-orin-nano-developer-kit-gets-a-super-boost/)\n\n[214\\. A Real-Time Defect Detection Strategy for Additive Manufacturing Processes Based on Deep Learning and Machine Vision Technologies](https://www.mdpi.com/2072-666X/15/1/28)\n\n[215\\. Solution: 高精度工业计算机视觉平台设计与应用](https://www.en.alinx.com/detail/805)\n\n[216\\. Neural network optimization and performance analysis for real-time object detection at the edge](https://sc24.supercomputing.org/proceedings/poster/poster_files/post172s2-file2.pdf)\n\n[217\\. Imbalance-Regularized LoRA: A Plug-and-Play Method for Improving Fine-Tuning of Foundation Models](https://openreview.net/pdf/6e7b608f8128ec5dc7d9d7cd085528ab1715ab22.pdf)\n\n[218\\. FINE-TUNING DREAM BOOTH PARAMETERS FOR MAXIMIZING STABLE DIFFUSION EFFICIENCY](https://www.irjmets.com/uploadedfiles/paper/issue_3_march_2024/51722/final/fin_irjmets1714752807.pdf)\n\n[221\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[222\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[223\\. Song Han, Huizi Mao et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition](https://arxiv.org/abs/1510.00149)\n\n[224\\. Alex Wang, Amanpreet Singh et al. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” BlackboxNLP@EMNLP](https://doi.org/10.18653/v1/W18-5446)\n\n[225\\. AI and IoT-powered edge device optimized for crop pest and disease detection](https://www.nature.com/articles/s41598-025-06452-5)\n\n[226\\. Benoit Jacob, S. Kligys et al. “Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.” 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2018.00286)\n\n[227\\. 4bit-Quantization in Vector-Embedding for RAG](https://arxiv.org/html/2501.10534v1)\n\n[228\\. Lightweight Sentiment Analysis](https://ijrpr.com/uploads/V6ISSUE5/IJRPR44761.pdf)\n\n[229\\. Specializing LLMs for Domains: RAG vs. Fine-Tuning](https://towardsai.net/p/l/specializing-llms-for-domains-rag-vs-fine-tuning)\n\n[230\\. A multi-objective approach to improving accuracy and efficiency in multitask BERT](https://web.stanford.edu/class/cs224n/final-reports/256940449.pdf)\n\n[231\\. Develop with DeepSeek R1 on Apple GPUs, Deploy with Serverless Inference](https://tower.dev/blog/develop-with-deepseek-r1-on-apple-gpus-deploy-with-serverless-inference)\n\n[232\\. RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture](https://hub.baai.ac.cn/paper/e4b458eb-d2fa-41db-b128-84a5b52ea9ac)\n\n[233\\. Sheng Shen, Zhen Dong et al. “Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT.” ArXiv](https://doi.org/10.1609/AAAI.V34I05.6409)\n\n[234\\. RAG vs Fine-Tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture](https://arize.com/blog/rag-vs-fine-tuning/)\n\n[235\\. LLMs and Quantization (2)](https://sciendo.com/2/v2/download/chapter/9781501520938/10.1515/9781501520938-012.pdf?Token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VycyI6W3sic3ViIjoyNTY3ODUxNywicHVicmVmIjoiNzY0NDg4IiwibmFtZSI6Ikdvb2dsZSBHb29nbGVib3QgLSBXZWIgQ3Jhd2xlciBTRU8iLCJ0eXBlIjoiaW5zdGl0dXRpb24iLCJsb2dvdXRfbGluayI6Imh0dHBzOi8vY29ubmVjdC5saWJseW54LmNvbS9sb2dvdXQvNjdmNzE4MTAzNWNmZmNhZTIyOTBkNzYyNDU1NjY2ZWEiLCJhdXRoX21ldGhvZCI6ImlwIiwiaXAiOiI2Ni4yNDkuNzYuMTcwIn1dLCJpYXQiOjE3NDQyNDkyMTAsImV4cCI6MTc0NTQ1ODgxMH0.VzyCkEZtG_CKvNxsUn09XpLJMN3q45IfY1bZTKqTBYM)\n\n[236\\. AI技术在农业领域的创新应用：对比RAG与FINE-TUNING](https://zhuanlan.zhihu.com/p/684011959)\n\n[237\\. Best Way to Squeeze: A Comparison of Model Compression Techniques in Natural Language Processing](https://scholar.tecnico.ulisboa.pt/api/records/HkaPylrrouFs4_srBLtOE71obkCHgIBaFPrS/file/b1c298dedf6a4871fb7fe0fc468dc6986467c11eb26c65a2b0372af5807c750f.pdf)\n\n[238\\. RAG vs Fine-tuning： Pipelines， Tradeoffs， and a Case Study on Agriculture](https://www.zhihu.com/question/638730387/answer/3375878225)\n\n[239\\. Understanding INT4 Quantization for Language Models: Latency Speedup, Composability, and Failure Cases](https://openreview.net/pdf?id=q1WGm3hItW)\n\n[241\\. Strategies for robust, accurate, and generalizable benchmarking of drug discovery platforms](https://www.biorxiv.org/content/10.1101/2024.12.10.627863v1.full.pdf)\n\n[242\\. S. Chopra, M. Sodhi. “Managing Risk To Avoid Supply-Chain Breakdown.” MIT Sloan Management Review](https://www.semanticscholar.org/paper/7164af5df6b46b4d7c22194ca2d9c0827e04dc34)\n\n[243\\. Uta Jüttner. “Supply chain risk management: Understanding the business requirements from a practitioner perspective.” The International Journal of Logistics Management](https://doi.org/10.1108/09574090510617385)\n\n[244\\. RAG vs Fine Tuning: Which Method to Choose](https://labelyourdata.com/articles/rag-vs-fine-tuning)\n\n[245\\. Performance of Retrieval-Augmented Generation (RAG) on Pharmaceutical Documents](https://intuitionlabs.ai/articles/rag-performance-pharmaceutical-documents)\n\n[246\\. A. Norrman, Ulf Jansson. “Ericsson’s Proactive Supply Chain Risk Management-approach After a Serious Supplier Accident.” International Journal of Physical Distribution & Logistics Management](https://doi.org/10.1108/09600030410545463)\n\n[247\\. Fine-tuning vs RAG: How to choose the right approach?](https://www.reply.com/cluster-reply/it/Shared%20Documents/CLUBIT-Fine-tuning-VS-RAG.pdf)\n\n[248\\. G. Zsidisin, L. Ellram et al. “An analysis of supply risk assessment techniques.” International Journal of Physical Distribution & Logistics Management](https://doi.org/10.1108/09600030410545445)\n\n[249\\. A Guide to Supply Chain Risk Management for the Pharmaceutical and Medical Device Industries and their Suppliers](https://www.pqg.org/a/wp-content/uploads/2011/09/PQG-Guide-to-Supply-Chain-Risk-Management-V-1-0-2010.pdf)\n\n[250\\. RAG Retrieval Augmented Generation: A Complete Guide](https://collabnix.com/rag-retrieval-augmented-generation-the-complete-guide-to-building-intelligent-ai-systems-in-2025/)\n\n[251\\. Rag vs Fine-Tuning](https://www.radiansys.com/blog/adapting-ai-models-fine-tuning-rag)\n\n[252\\. RAG vs Fine-Tuning LLM: Gen AI Approaches Comparison](https://aisera.com/blog/llm-fine-tuning-vs-rag/)\n\n[253\\. C. Harland, R. Brenchley et al. “Risk in supply networks.” Journal of Purchasing and Supply Management](https://doi.org/10.1016/S1478-4092%2803%2900004-9)\n\n[254\\. Comparative Analysis of Risk Management Strategies for Additive Manufacturing Supply Chains](https://thescipub.com/abstract/ajassp.2019.273.282)\n\n[255\\. 智能模型新篇章：RAG + Fine-Tuning 混合增强策略](https://zhuanlan.zhihu.com/p/688138789)\n\n[256\\. Supply chain risk management: models and methods](https://rudyct.com/supchn/J-Supply%20chain%20risk%20management-models%20and%20methods-2019.pdf)\n\n[257\\. Pharmaceutical Supply Chain Risks: A Systematic Review](https://pmc.ncbi.nlm.nih.gov/articles/PMC3913399/)\n\n[261\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[262\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[263\\. Fine-Tuning LLMs using NVIDIA Jetson AGX Orin](https://www.hackster.io/shahizat/fine-tuning-llms-using-nvidia-jetson-agx-orin-b17c4d)\n\n[264\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[265\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[266\\. Mark Chen, Jerry Tworek et al. “Evaluating Large Language Models Trained on Code.” ArXiv](https://arxiv.org/abs/2107.03374)\n\n[267\\. NVIDIA Jetson Orin Nano Super 开发者套件发布](https://developer.nvidia.com/ko-kr/blog/nvidia-jetson-orin-nano-developer-kit-gets-a-super-boost/)\n\n[268\\. NVIDIA Jetson Orin Nano Super Developer Kit: A \"Super\" Boost for Generative AI](https://developer.nvidia.com/blog/nvidia-jetson-orin-nano-developer-kit-gets-a-super-boost/)\n\n[269\\. Integrating Qt and LLMs on the NVIDIA Jetson board for controlling a patient-assisting robot arm](https://oulurepo.oulu.fi/bitstream/10024/50979/1/nbnfioulu-202406264950.pdf)\n\n[270\\. Solution: 高精度工业计算机视觉平台设计与应用](https://www.en.alinx.com/detail/805)\n\n[271\\. Ziyu Zhao, Leilei Gan et al. “Retrieval-Augmented Mixture of LoRA Experts for Uploadable Machine Learning.” ArXiv](https://doi.org/10.48550/arXiv.2406.16989)\n\n[272\\. Foresight Leverages NVIDIA Jetson Orin to Introduce Novel 360-degree 3D Perception Solution](https://ir.foresightauto.com/wp-content/uploads/2024/05/Foresight-NVIDIA-platforms-080524-ENHE_accessible.pdf)\n\n[273\\. Minda Hu, Licheng Zong et al. “Enhancing Biomedical Knowledge Retrieval-Augmented Generation with Self-Rewarding Tree Search and Proximal Policy Optimization.” ArXiv](https://doi.org/10.48550/arXiv.2406.11258)\n\n[274\\. Isaac Chung, Phat Vo et al. “Efficient In-Domain Question Answering for Resource-Constrained Environments.”](https://arxiv.org/abs/2409.17648)\n\n[275\\. Sefika Efeoglu, Adrian Paschke. “Relation Extraction with Fine-Tuned Large Language Models in Retrieval Augmented Generation Frameworks.” ArXiv](https://doi.org/10.48550/arXiv.2406.14745)\n\n[276\\. Isaac Chung, Phat Vo et al. “Efficient In-Domain Question Answering for Resource-Constrained Environments.”](https://arxiv.org/abs/2409.17648)\n\n[277\\. Anique Tahir, Lu Cheng et al. “JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning.” ArXiv](https://doi.org/10.48550/arXiv.2403.11366)\n\n[278\\. Aleksander Ficek, Jiaqi Zeng et al. “GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning.”](https://arxiv.org/abs/2407.04528)\n\n[281\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[282\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[283\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[284\\. Alex Wang, Amanpreet Singh et al. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” BlackboxNLP@EMNLP](https://doi.org/10.18653/v1/W18-5446)\n\n[285\\. Xiaoqi Jiao, Yichun Yin et al. “TinyBERT: Distilling BERT for Natural Language Understanding.” ArXiv](https://doi.org/10.18653/v1/2020.findings-emnlp.372)\n\n[286\\. Embedding Model Choice's Impact on Vector Database Performance](https://milvus.io/ai-quick-reference/how-does-embedding-model-choice-affect-the-size-and-speed-of-the-vector-database-component-and-what-tradeoffs-might-this-introduce-for-realtime-rag-systems)\n\n[287\\. Making LLMs environmentally (and budget) friendly](https://www.redhat.com/zh/blog/making-llms-environmentally-and-budget-friendly)\n\n[288\\. AI and IoT-powered edge device optimized for crop pest and disease detection](https://www.nature.com/articles/s41598-025-06452-5)\n\n[289\\. Ali Hadi Zadeh, A. Moshovos. “GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference.” 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)](https://doi.org/10.1109/MICRO50266.2020.00071)\n\n[290\\. Automatic Mixed-Precision Quantization Search of BERT](https://www.ijcai.org/proceedings/2021/0472.pdf)\n\n[291\\. A multi-objective approach to improving accuracy and efficiency in multitask BERT](https://web.stanford.edu/class/cs224n/final-reports/256940449.pdf)\n\n[292\\. Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT](https://dong-zhen.com/wp-content/uploads/Q-BERT.pdf)\n\n[293\\. LLMs and Quantization (2)](https://sciendo.com/2/v2/download/chapter/9781501520938/10.1515/9781501520938-012.pdf?Token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VycyI6W3sic3ViIjoyNTY3ODUxNywicHVicmVmIjoiNzY0NDg4IiwibmFtZSI6Ikdvb2dsZSBHb29nbGVib3QgLSBXZWIgQ3Jhd2xlciBTRU8iLCJ0eXBlIjoiaW5zdGl0dXRpb24iLCJsb2dvdXRfbGluayI6Imh0dHBzOi8vY29ubmVjdC5saWJseW54LmNvbS9sb2dvdXQvNjdmNzE4MTAzNWNmZmNhZTIyOTBkNzYyNDU1NjY2ZWEiLCJhdXRoX21ldGhvZCI6ImlwIiwiaXAiOiI2Ni4yNDkuNzYuMTcwIn1dLCJpYXQiOjE3NDQyNDkyMTAsImV4cCI6MTc0NTQ1ODgxMH0.VzyCkEZtG_CKvNxsUn09XpLJMN3q45IfY1bZTKqTBYM)\n\n[294\\. Tairen Piao, Ikhyun Cho et al. “SensiMix: Sensitivity-Aware 8-bit index & 1-bit value mixed precision quantization for BERT compression.” PLoS ONE](https://doi.org/10.1371/journal.pone.0265621)\n\n[295\\. VS-Quant: Per-vector Scaled Quantization for Accurate Low-Precision Neural Network Inference](https://proceedings.mlsys.org/paper_files/paper/2021/file/48a6431f04545e11919887748ec5cb52-Paper.pdf)\n\n[296\\. Quantization Variation: A New Perspective on Training Transformers with Low-Bit Precision](https://arxiv.org/pdf/2307.00331)\n\n[297\\. Prakhar Ganesh, Yao Chen et al. “Compressing Large-Scale Transformer-Based Models: A Case Study on BERT.” Transactions of the Association for Computational Linguistics](https://doi.org/10.1162/tacl_a_00413)\n\n[298\\. BERT model optimization methods for inference – a comparative study of five alternative BERT-model implementations](https://lutpub.lut.fi/bitstream/handle/10024/165034/Master_Thesis_Marko_Buuri.pdf?sequence=3)\n\n[299\\. Lightweight Sentiment Analysis](https://ijrpr.com/uploads/V6ISSUE5/IJRPR44761.pdf)\n\n[300\\. Deep Compression of Pre-trained Transformer Models](https://papers.neurips.cc/paper_files/paper/2022/file/5b5618e7d061748267d74478b7c5b1ab-Paper-Conference.pdf)\n\n[301\\. D. Wishart, Y. D. Feunang et al. “DrugBank 5.0: a major update to the DrugBank database for 2018.” Nucleic Acids Research](https://doi.org/10.1093/nar/gkx1037)\n\n[302\\. RAG vs. Fine-Tuning: How to Choose](https://www.oracle.com/cn/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/rag-fine-tuning/)\n\n[303\\. RAG vs. Fine Tuning: How to Choose](https://www.oracle.com/tr/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/rag-fine-tuning/)\n\n[304\\. Strategies for robust, accurate, and generalizable benchmarking of drug discovery platforms](https://www.biorxiv.org/content/10.1101/2024.12.10.627863v1.full.pdf)\n\n[305\\. RAG vs. Fine-Tuning: Choosing the Right Approach for Your AI Model in 2025](https://www.ai-infra-link.com/rag-vs-fine-tuning-choosing-the-right-approach-for-your-ai-model-in-2025/)\n\n[306\\. Performance of Retrieval-Augmented Generation (RAG) on Pharmaceutical Documents](https://intuitionlabs.ai/articles/rag-performance-pharmaceutical-documents)\n\n[307\\. RAG (Retrieval Augmented Generation): A Complete Guide](https://www.aimw.ai/blog/retrieval-augmented-generation-rag)\n\n[308\\. RAG, Finetuning or Both? Choosing the Right Strategy](https://www.matrixflows.com/blog/retrieval-augmented-generation-rag-finetuning-hybrid-framework-for-choosing-right-strategy)\n\n[309\\. RAG与微调：差异、优势及应用场景详解](https://www.wevolver.com/article/rag-vs-fine-tuning-differences-benefits-and-use-cases-explained)\n\n[310\\. Gian Antonio Susto, A. Schirru et al. “Machine Learning for Predictive Maintenance: A Multiple Classifier Approach.” IEEE Transactions on Industrial Informatics](https://doi.org/10.1109/TII.2014.2349359)\n\n[311\\. RAG vs Fine Tuning: Which Method to Choose](https://labelyourdata.com/articles/rag-vs-fine-tuning)\n\n[312\\. RAG vs Fine Tuning: The Hidden Trade-offs No One Talks About](https://b-eye.com/blog/rag-vs-fine-tuning/)\n\n[313\\. Jay Lee, Fangji Wu et al. “Prognostics and health management design for rotary machinery systems—Reviews, methodology and applications.” Mechanical Systems and Signal Processing](https://doi.org/10.1016/J.YMSSP.2013.06.004)\n\n[314\\. Medical LLMs: Fine-Tuning vs. Retrieval-Augmented Generation](https://www.mdpi.com/2306-5354/12/7/687)\n\n[315\\. R. Mobley. “An introduction to predictive maintenance.”](https://doi.org/10.1016/b978-0-7506-7531-4.x5000-3)\n\n[316\\. M. Bevilacqua, M. Braglia. “The analytic hierarchy process applied to maintenance strategy selection.” Reliab. Eng. Syst. Saf.](https://doi.org/10.1016/S0951-8320%2800%2900047-8)\n\n[317\\. Fine-Tuning vs RAG: Key Differences Explained (2025 Guide)](https://orq.ai/blog/finetuning-vs-rag)\n\n[318\\. RAG vs Fine Tuning: Choosing the Right Approach for Improving AI Models](https://www.digitalocean.com/resources/articles/rag-vs-fine-tuning)\n\n[321\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[322\\. Fine-Tuning LLMs using NVIDIA Jetson AGX Orin](https://www.hackster.io/shahizat/fine-tuning-llms-using-nvidia-jetson-agx-orin-b17c4d)\n\n[323\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[324\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[325\\. Deploy YOLOv10 on NVIDIA Jetson Orin with a One-Line Command: Real-Time End-to-End Object Detection](https://www.seeedstudio.com/blog/2024/07/04/deploy-yolov10-on-nvidia-jetson-orin-with-a-one-line-command-real-time-end-to-end-object-detection/)\n\n[326\\. Develop for all six NVIDIA Jetson Orin modules with the power of one developer kit](https://www.arrow.com/en/research-and-events/articles/develop-for-all-six-nvidia-jetson-orin-modules-with-the-power-of-one-developer-kit)\n\n[327\\. N. Houlsby, A. Giurgiu et al. “Parameter-Efficient Transfer Learning for NLP.” ArXiv](https://arxiv.org/abs/1902.00751)\n\n[328\\. Integrating YOLOv5, Jetson nano microprocessor, and Mitsubishi robot manipulator for real-time machine vision application in manufacturing: A lab experimental study](https://www.astrj.com/pdf-201366-123589?filename=Integrating%20YOLOv5_.pdf)\n\n[329\\. Xiang Lisa Li, Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Generation.” Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)](https://doi.org/10.18653/v1/2021.acl-long.353)\n\n[330\\. NVIDIA Jetson Orin Nano Super 开发者套件发布](https://developer.nvidia.com/ko-kr/blog/nvidia-jetson-orin-nano-developer-kit-gets-a-super-boost/)\n\n[331\\. AI-powered Six-Sided Product Case Inspection at ADLINK Factory](https://go.adlinktech.com/rs/731-NFP-575/images/DLAP-411-Orin_Smart-Manufacturing_AI-Powered-6-Sided-Inspection.pdf)\n\n[332\\. Optimizing Inspection Processes with Portable AI Vision Solutions](https://www.lannerinc.com/jp/applications/industrial-automation/optimizing-inspection-processes-with-portable-ai-vision-solutions)\n\n[333\\. Anique Tahir, Lu Cheng et al. “JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning.” ArXiv](https://doi.org/10.48550/arXiv.2403.11366)\n\n[334\\. Neural network optimization and performance analysis for real-time object detection at the edge](https://sc24.supercomputing.org/proceedings/poster/poster_files/post172s2-file2.pdf)\n\n[335\\. Tim Dettmers, Artidoro Pagnoni et al. “QLoRA: Efficient Finetuning of Quantized LLMs.” ArXiv](https://doi.org/10.48550/arXiv.2305.14314)\n\n[336\\. NVIDIA Jetson系列产品支持与合作](https://concurrent-rt.com/partners/strategic-technology/nvidia/)\n\n[337\\. Solution: 高精度工业计算机视觉平台设计与应用](https://www.en.alinx.com/detail/805)\n\n[338\\. Fine-Tuning Small Language Models to Optimize Code Review Accuracy](https://developer.nvidia.com/blog/fine-tuning-small-language-models-to-optimize-code-review-accuracy/)\n\n[339\\. NVIDIA AI in Manufacturing](https://www.nvidia.com/pt-br/industries/manufacturing/)\n\n[341\\. Tailoring foundation models for your business needs: A comprehensive guide to RAG, fine-tuning, and hybrid approaches](https://aws.amazon.com/blogs/machine-learning/tailoring-foundation-models-for-your-business-needs-a-comprehensive-guide-to-rag-fine-tuning-and-hybrid-approaches/)\n\n[342\\. Medical LLMs: Fine-Tuning vs. Retrieval-Augmented Generation](https://www.mdpi.com/2306-5354/12/7/687)\n\n[343\\. RAG vs. Fine-Tuning: How to Choose](https://www.oracle.com/cn/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/rag-fine-tuning/)\n\n[344\\. Michael Kuhn, Ivica Letunic et al. “The SIDER database of drugs and side effects.” Nucleic Acids Research](https://doi.org/10.1093/nar/gkv1075)\n\n[345\\. Hakime Öztürk, E. Olmez et al. “DeepDTA: deep drug–target binding affinity prediction.” Bioinformatics](https://doi.org/10.1093/bioinformatics/bty593)\n\n[346\\. RAG, Finetuning or Both? Choosing the Right Strategy](https://www.matrixflows.com/blog/retrieval-augmented-generation-rag-finetuning-hybrid-framework-for-choosing-right-strategy)\n\n[347\\. Mindy I. Davis, Jeremy P Hunt et al. “Comprehensive analysis of kinase inhibitor selectivity.” Nature Biotechnology](https://doi.org/10.1038/nbt.1990)\n\n[348\\. Performance of Retrieval-Augmented Generation (RAG) on Pharmaceutical Documents](https://intuitionlabs.ai/articles/rag-performance-pharmaceutical-documents)\n\n[349\\. Jing Tang, Agnieszka Szwajda et al. “Making Sense of Large-Scale Kinase Inhibitor Bioactivity Data Sets: A Comparative and Integrative Analysis.” Journal of chemical information and modeling](https://doi.org/10.1021/ci400709d)\n\n[350\\. Improving compound–protein interaction prediction by building up highly credible negative samples.Bioinformatics](https://doi.org/10.1093/bioinformatics/btv256)\n\n[351\\. Strategies for robust, accurate, and generalizable benchmarking of drug discovery platforms](https://www.biorxiv.org/content/10.1101/2024.12.10.627863v1.full.pdf)\n\n[352\\. RAG vs. Fine Tuning: Which One is Right for You?](https://vectorize.io/rag-vs-fine-tuning/)\n\n[353\\. LLMs: RAG vs. Fine-Tuning](https://s3.eu-west-2.amazonaws.com/assets.winder.ai/blog/2024/240313_Presentation_RAGvsFineTuning/240313_Presentation_RAGvsFineTune.pdf)\n\n[354\\. Data Spaces and Foundation Models: Enabling High-Quality Artificial Intelligence](https://www.isst.fraunhofer.de/content/dam/isst/publikationen/whitepaper/data-spaces_and_foundation-models_whitepaper.pdf)\n\n[355\\. LLMs to Support a Domain Specific Knowledge Assistant](https://arxiv.org/pdf/2502.04095)\n\n[356\\. RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems](https://arxiv.org/pdf/2412.12322)\n\n[357\\. Benchmarking Deep Search over Heterogeneous Enterprise Data](https://www.themoonlight.io/zh/review/benchmarking-deep-search-over-heterogeneous-enterprise-data)\n\n[358\\. Supercharging Proactive Network Maintenance by Leveraging Generative AI](https://www.nctatechnicalpapers.com/Paper/2024/AI08_Chari_6610_paper/download)\n\n[361\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[362\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[363\\. Fine-Tuning LLMs using NVIDIA Jetson AGX Orin](https://www.hackster.io/shahizat/fine-tuning-llms-using-nvidia-jetson-agx-orin-b17c4d)\n\n[364\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[365\\. All AIs on Quality: Startup’s NVIDIA Jetson-Enabled Inspections Boost Manufacturing](https://blogs.nvidia.com/blog/startups-nvidia-jetson-enabled-inspections-boost-manufacturing/)\n\n[366\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[367\\. OpenVLA - Vision/Language Action Models for Embodied Robotics](https://www.jetson-ai-lab.com/openvla.html)\n\n[368\\. Rethinking Retrieval Augmented Fine-Tuning in an evolving LLM landscape](https://scholar.smu.edu/cgi/viewcontent.cgi?article=1286&context=datasciencereview)\n\n[369\\. Anique Tahir, Lu Cheng et al. “JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning.” ArXiv](https://doi.org/10.48550/arXiv.2403.11366)\n\n[370\\. A Retrieval Augmented Generation Fine-Tuned LLM Model for Refactored Code Recommendations to Mitigate Java Lock Contention Performance Faults](https://ontariotechu.scholaris.ca/bitstreams/f2c609eb-a14f-4f0d-a7c9-59eafa251897/download)\n\n[371\\. Neural network optimization and performance analysis for real-time object detection at the edge](https://sc24.supercomputing.org/proceedings/poster/poster_files/post172s2-file2.pdf)\n\n[372\\. Tim Dettmers, Artidoro Pagnoni et al. “QLoRA: Efficient Finetuning of Quantized LLMs.” ArXiv](https://doi.org/10.48550/arXiv.2305.14314)\n\n[373\\. The Open-Source Advantage in Large Language Models (LLMs)](https://arxiv.org/pdf/2412.12004)\n\n[374\\. NVIDIA Jetson Orin Nano Super Developer Kit: A \"Super\" Boost for Generative AI](https://developer.nvidia.com/blog/nvidia-jetson-orin-nano-developer-kit-gets-a-super-boost/)\n\n[375\\. Solution: 高精度工业计算机视觉平台设计与应用](https://www.en.alinx.com/detail/805)\n\n[376\\. FINE-TUNING AN OPEN SOURCE CHATBOT TO TRANSLATE CODE FROM PYTHON TO JAVA USING QLORA: TRANSLATING FOR MORE ENERGY EFFICIENT CODE](https://lutpub.lut.fi/bitstream/10024/168604/1/diplomityo_hakkarainen_joonas.pdf)\n\n[381\\. Thomas Kipf, M. Welling. “Semi-Supervised Classification with Graph Convolutional Networks.” ArXiv](https://arxiv.org/abs/1609.02907)\n\n[382\\. Aditya Grover, J. Leskovec. “node2vec: Scalable Feature Learning for Networks.” Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining](https://doi.org/10.1145/2939672.2939754)\n\n[383\\. D. Wishart, Y. D. Feunang et al. “DrugBank 5.0: a major update to the DrugBank database for 2018.” Nucleic Acids Research](https://doi.org/10.1093/nar/gkx1037)\n\n[384\\. RAG vs. Fine-Tuning: How to Choose](https://www.oracle.com/cn/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/rag-fine-tuning/)\n\n[385\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[386\\. RAG vs. Fine Tuning: How to Choose](https://www.oracle.com/tr/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/rag-fine-tuning/)\n\n[387\\. RAG vs Fine Tuning: The Hidden Trade-offs No One Talks About](https://b-eye.com/blog/rag-vs-fine-tuning/)\n\n[388\\. Michael Kuhn, Ivica Letunic et al. “The SIDER database of drugs and side effects.” Nucleic Acids Research](https://doi.org/10.1093/nar/gkv1075)\n\n[389\\. Performance of Retrieval-Augmented Generation (RAG) on Pharmaceutical Documents](https://intuitionlabs.ai/articles/rag-performance-pharmaceutical-documents)\n\n[390\\. Strategies for robust, accurate, and generalizable benchmarking of drug discovery platforms](https://www.biorxiv.org/content/10.1101/2024.12.10.627863v1.full.pdf)\n\n[391\\. META GEN 混合 RAG：无需微调即可提高特定领域问题 & A 的准确性](https://www.xueshuxiangzi.com/downloads/2025_5_27/2505.18247.pdf)\n\n[392\\. RAG, Finetuning or Both? Choosing the Right Strategy](https://www.matrixflows.com/blog/retrieval-augmented-generation-rag-finetuning-hybrid-framework-for-choosing-right-strategy)\n\n[393\\. RAG (Retrieval Augmented Generation): A Complete Guide](https://www.aimw.ai/blog/retrieval-augmented-generation-rag)\n\n[394\\. CogCommon: Enhancing Cross-Domain Knowledge Extraction with LLM-Assisted Commonality Discovery](https://openreview.net/pdf?id=fR3einDVSo)\n\n[395\\. RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems](https://arxiv.org/pdf/2407.11005)\n\n[396\\. RAG-ENHANCED COLLABORATIVE LLM AGENTS FOR DRUG DISCOVERY](https://openreview.net/pdf?id=XjxO0Ayj01)\n\n[397\\. Medical LLMs: Fine-Tuning vs. Retrieval-Augmented Generation](https://www.mdpi.com/2306-5354/12/7/687)\n\n[398\\. RAG vs. Fine Tuning: Which One is Right for You?](https://vectorize.io/rag-vs-fine-tuning/)\n\n[399\\. Data Spaces and Foundation Models: Enabling High-Quality Artificial Intelligence](https://www.isst.fraunhofer.de/content/dam/isst/publikationen/whitepaper/data-spaces_and_foundation-models_whitepaper.pdf)\n\n[401\\. Fine-Tuning LLMs using NVIDIA Jetson AGX Orin](https://www.hackster.io/shahizat/fine-tuning-llms-using-nvidia-jetson-agx-orin-b17c4d)\n\n[402\\. GitHub - alsichcan/vllm.jetson: Fork of vLLM to deploy on NVIDIA Jetson AGX Orin with JetPack 6.0](https://github.com/alsichcan/vllm_jetson)\n\n[403\\. All AIs on Quality: Startup’s NVIDIA Jetson-Enabled Inspections Boost Manufacturing](https://blogs.nvidia.com/blog/startups-nvidia-jetson-enabled-inspections-boost-manufacturing/)\n\n[404\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[405\\. Build software better, together](https://github.com/topics/jetson-agx-orin)\n\n[406\\. Jetson Orin Nano: Complete Developer Resources & Documentation](https://nvidia-jetson.piveral.com/jetson-orin-nano-complete-developer-resources-documentation/)\n\n[407\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[408\\. Anique Tahir, Lu Cheng et al. “JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning.” ArXiv](https://doi.org/10.48550/arXiv.2403.11366)\n\n[409\\. Mark Chen, Jerry Tworek et al. “Evaluating Large Language Models Trained on Code.” ArXiv](https://arxiv.org/abs/2107.03374)\n\n[410\\. NVIDIA Jetson Orin Nano Super 开发者套件发布](https://developer.nvidia.com/ko-kr/blog/nvidia-jetson-orin-nano-developer-kit-gets-a-super-boost/)\n\n[411\\. N. Houlsby, A. Giurgiu et al. “Parameter-Efficient Transfer Learning for NLP.” ArXiv](https://arxiv.org/abs/1902.00751)\n\n[412\\. Brian Lester, Rami Al-Rfou et al. “The Power of Scale for Parameter-Efficient Prompt Tuning.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/2021.emnlp-main.243)\n\n[413\\. Justin Zhao, Timothy Wang et al. “LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report.” ArXiv](https://doi.org/10.48550/arXiv.2405.00732)\n\n[414\\. The Future of Generative AI in Video Analytics Using NVIDIA Jetson Orin](http://www.seeedstudio.com/blog/2024/04/12/the-future-of-generative-ai-in-video-analytics-using-nvidia-jetson-orin/)\n\n[415\\. Neural network optimization and performance analysis for real-time object detection at the edge](https://sc24.supercomputing.org/proceedings/poster/poster_files/post172s2-file2.pdf)\n\n[416\\. Florence-2: Fine-tune,基于Lora微调](https://www.bilibili.com/video/av113816901781496?t=511)\n\n[417\\. Solution: 高精度工业计算机视觉平台设计与应用](https://www.en.alinx.com/detail/805)\n\n[421\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[422\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[423\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[424\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[425\\. Alex Wang, Amanpreet Singh et al. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” BlackboxNLP@EMNLP](https://doi.org/10.18653/v1/W18-5446)\n\n[426\\. LLMs and Quantization (2)](https://sciendo.com/2/v2/download/chapter/9781501520938/10.1515/9781501520938-012.pdf?Token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VycyI6W3sic3ViIjoyNTY3ODUxNywicHVicmVmIjoiNzY0NDg4IiwibmFtZSI6Ikdvb2dsZSBHb29nbGVib3QgLSBXZWIgQ3Jhd2xlciBTRU8iLCJ0eXBlIjoiaW5zdGl0dXRpb24iLCJsb2dvdXRfbGluayI6Imh0dHBzOi8vY29ubmVjdC5saWJseW54LmNvbS9sb2dvdXQvNjdmNzE4MTAzNWNmZmNhZTIyOTBkNzYyNDU1NjY2ZWEiLCJhdXRoX21ldGhvZCI6ImlwIiwiaXAiOiI2Ni4yNDkuNzYuMTcwIn1dLCJpYXQiOjE3NDQyNDkyMTAsImV4cCI6MTc0NTQ1ODgxMH0.VzyCkEZtG_CKvNxsUn09XpLJMN3q45IfY1bZTKqTBYM)\n\n[427\\. Lightweight Sentiment Analysis](https://ijrpr.com/uploads/V6ISSUE5/IJRPR44761.pdf)\n\n[428\\. AI and IoT-powered edge device optimized for crop pest and disease detection](https://www.nature.com/articles/s41598-025-06452-5)\n\n[429\\. Ali Hadi Zadeh, A. Moshovos. “GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference.” 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)](https://doi.org/10.1109/MICRO50266.2020.00071)\n\n[430\\. BERT model optimization methods for inference – a comparative study of five alternative BERT-model implementations](https://lutpub.lut.fi/bitstream/handle/10024/165034/Master_Thesis_Marko_Buuri.pdf?sequence=3)\n\n[431\\. Quantization Variation: A New Perspective on Training Transformers with Low-Bit Precision](https://arxiv.org/pdf/2307.00331)\n\n[432\\. Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT](https://dong-zhen.com/wp-content/uploads/Q-BERT.pdf)\n\n[433\\. Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5](https://arxiv.org/html/2506.09300v1)\n\n[434\\. VS-Quant: Per-vector Scaled Quantization for Accurate Low-Precision Neural Network Inference](https://proceedings.mlsys.org/paper_files/paper/2021/file/48a6431f04545e11919887748ec5cb52-Paper.pdf)\n\n[435\\. Adaptive Rank Allocation for Federated Parameter-Efficient Fine-Tuning of Language Models](https://arxiv.org/pdf/2501.14406)\n\n[436\\. Automatic Mixed-Precision Quantization Search of BERT](https://www.ijcai.org/proceedings/2021/0472.pdf)\n\n[437\\. Embedding Model Choice's Impact on Vector Database Performance](https://milvus.io/ai-quick-reference/how-does-embedding-model-choice-affect-the-size-and-speed-of-the-vector-database-component-and-what-tradeoffs-might-this-introduce-for-realtime-rag-systems)\n\n[438\\. A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness](https://ai.radensa.ru/wp-content/uploads/2024/11/2411.03350v1.pdf)"
  },
  {
    "id": 18,
    "question": "In the context of downstream SFT (Supervised Fine-Tuning) task for generative models, training data often contain a large number of domain-specific high-frequency words, which may cause the model to unintentionally generate these words frequently during prediction. How can we design strategies at the algorithmic level to mitigate or resolve this issue?",
    "response": "**Research Report: Mitigating Domain-Specific Word Over-Generation in Supervised Fine-Tuning of Generative Models**\n\n**Date:** July 26, 2025\n\n**Author:** AI Research Assistant\n\n**Executive Summary**\n\nSupervised Fine-Tuning (SFT) is a critical process for adapting large language models (LLMs) to specialized domains. However, a common side effect is the model's tendency to over-generate domain-specific, high-frequency words found in the fine-tuning data. This phenomenon can lead to outputs that are repetitive, unnatural, and less diverse, undermining the model's utility. This report conducts a comprehensive analysis of algorithmic strategies designed to mitigate this issue.\n\nThe strategies can be broadly categorized into two main intervention points: training-time modifications and inference-time controls. Training-time interventions are more fundamental and include **Loss Function Modification**, which reweights the importance of tokens based on frequency or learning difficulty, and **Vocabulary Masking**, which forces the model to learn deeper contextual relationships by obscuring certain words. Inference-time strategies, such as **Nucleus Sampling** and **Frequency Penalties**, offer a more flexible but less foundational way to control the final output.\n\nA significant challenge in this area is evaluation. Standard metrics like BLEU and ROUGE are ill-suited for quantifying stylistic overuse. This report synthesizes findings from the literature to propose a novel metric, the **Domain Word Overuse (DWO) score**, to specifically measure this phenomenon.\n\nCurrent research, including findings from recent 2024 and 2025 publications, details several sophisticated techniques within these categories. However, there is a notable lack of direct experimental benchmarks comparing the efficacy of loss reweighting against vocabulary masking for this specific problem. The report concludes that a hybrid approach, combining robust training-time regularization with tuned inference-time decoding, is likely the most effective strategy, while highlighting the urgent need for standardized evaluation protocols and comparative research.\n\n**1\\. Introduction: The Challenge of Domain-Specific Word Over-Generation**\n\nSupervised Fine-Tuning (SFT) is the standard paradigm for specializing pre-trained generative models for downstream tasks, from chatbot development to domain-specific question answering in fields like medicine or law \\[297\\]. The process involves further training a base model on a smaller, curated dataset of high-quality examples. While effective, this process introduces a significant challenge: when the fine-tuning dataset is rich in domain-specific terminology, the model's optimization process learns to associate these terms with high probability.\n\nThis leads to an unintended side effect where the model, during inference, unintentionally and excessively generates these high-frequency domain words. This over-generation, or \"term overuse,\" manifests as repetitive, monotonous, and sometimes unnaturally jargon-laden text. It degrades the user experience by reducing output diversity and fluency, making the model appear less creative and more robotic \\[26\\]\\[39\\].\n\nThe core of the problem is algorithmic: the model's loss function, typically Cross-Entropy, incentivizes it to perfectly predict the training data. High-frequency words offer a reliable, low-loss path during training, causing the model to develop a strong bias towards them. This report investigates algorithmic strategies at both the training and inference stages to counteract this bias and restore balance to the model's generative capabilities.\n\n**2\\. Training-Time Algorithmic Interventions**\n\nModifying the model's behavior during the SFT process itself offers the most fundamental solution to over-generation. These techniques aim to change what the model learns, rather than just filtering its output. They primarily fall into two categories: altering the loss function and masking parts of the input.\n\n**2.1. Loss Function Modification and Token Reweighting**\n\nThe most direct way to influence the learning process is to adjust the loss function, changing the contribution of each token to the overall gradient update. Instead of treating all tokens equally, these methods assign weights to tokens based on certain properties, such as frequency or learning difficulty.\n\n**Frequency-Based Loss Weighting**\n\nThe core idea is to down-weight the loss contribution of high-frequency words and, in some cases, up-weight the contribution of medium- or low-frequency words.\n\n**Gaussian Function-Based Weighting:** One approach calculates a word's weight using a Gaussian function based on its frequency relative to the median frequency of all words in the corpus. This method is designed to reduce the influence of common domain-specific words and stop words while elevating the importance of more informative, medium-frequency terms \\[7\\].\n\n**Log-Probability Difference Weighting:** In the context of domain adaptation for Neural Machine Translation (NMT), a word-level weighting scheme can be derived by computing the logarithm difference of probabilities from a domain-specific language model and a general-purpose language model. This score effectively identifies \"in-domain\" words, and the weights can be used to bias the model's training objective \\[13\\].\n\n**Combined Loss Approaches:** Recent 2024 research explores combining a standard frequency-based loss with an LLM-driven loss to iteratively refine a vocabulary. This method prunes tokens that contribute least to the combined loss, optimizing the vocabulary for both general language understanding and specific task performance \\[288\\]\\[408\\].\n\n**Difficulty-Based and Dynamic Reweighting**\n\nThese methods are more sophisticated, assessing how \"easy\" or \"hard\" a token is for the model to predict at a given training step and adjusting its loss weight accordingly.\n\n**Token Loss Dynamic Reweighting (TLDR):** This technique assigns differentiable weights to training losses, dynamically using higher weights for \"hard\" tokens and lower weights for \"easy\" tokens. By forcing the model to focus on more challenging predictions, TLDR helps it learn at different paces and has been shown to reduce repetitive utterance generation in both RNN and Transformer models \\[9\\]\\[9\\]\\[9\\]. A key advantage is that it tackles the problem at the training level, unlike sampling strategies that are only applied during inference \\[9\\].\n\n**Focal Loss:** Originally developed for object detection, Focal Loss can be adapted for language tasks to mitigate bias from the inherent imbalance in token difficulty. It reshapes the standard cross-entropy loss to down-weight the loss assigned to well-classified examples \\[11\\]\\[9\\]. However, a key distinction is that while Focal Loss tends to down-weight both easy and hard examples to different degrees, TLDR proposes a weighting function that simultaneously up-weights hard tokens while down-weighting easy ones \\[9\\].\n\n**Context-aware Meta-learned Loss Scaling (CaMeLS):** A 2025 study introduced CaMeLS, which meta-trains a small, separate model to reweight the language modeling loss for each token during online fine-tuning, aiming to improve the uptake of new information \\[287\\]. This represents a move towards even more dynamic and context-sensitive loss modification.\n\n**2.2. Vocabulary and Input Masking Strategies**\n\nMasking techniques involve intentionally hiding or replacing certain tokens in the input sequence during training. This forces the model to rely more heavily on surrounding context to make predictions, preventing it from simply memorizing high-frequency co-occurrence patterns.\n\n**Selective and Strategic Masking**\n\nInstead of the random masking used in pre-training (like in BERT), these methods strategically select which tokens to mask to achieve a specific fine-tuning objective.\n\n**Frequency- and Keyword-Based Masking:** One direct approach is to selectively mask tokens based on pre-defined criteria. This can involve using topic-specific keyword dictionaries to identify informative tokens \\[65\\]\\[306\\]using TF-IDF or other salience metrics to identify thematically important words \\[74\\], or simply masking the most frequent non-stop words \\[77\\]. This directly targets the words at risk of over-generation.\n\n**Task-Specific Masking:** For tasks like sentiment classification, one can collect lists of task-relevant words (e.g., positive/negative words) and assign each word a probability of being masked based on its relevance score \\[75\\].\n\n**Masked Specific Language Modeling (MSLM)**\n\nThis is a more nuanced approach designed to improve a model's sensitivity to domain-specific terms without necessarily causing over-generation. MSLM involves jointly masking both domain-specific (DS) terms and general terms during fine-tuning. A special \"specific mask loss\" is then applied, which penalizes the model more heavily for incorrect predictions of the DS-terms compared to general terms \\[69\\]\\[80\\]\\[352\\]. This encourages the model to learn the specific meaning and context of domain terminology more robustly.\n\n**General Regularization via Masking**\n\nThe standard Masked Language Modeling (MLM) objective itself can be used as a regularization technique during fine-tuning. By randomly replacing a small percentage of input tokens with a \\[MASK\\] token, the model is perturbed during training. This encourages it to learn deeper inter-token dependencies and reduces overfitting to surface-level lexical cues, which can indirectly mitigate the tendency to repeat high-frequency phrases \\[163\\]\\[227\\].\n\n**3\\. Inference-Time Decoding Strategies**\n\nEven with a well-trained model, the decoding strategy—the algorithm used to select the next token from the model's output probability distribution—plays a crucial role in shaping the final text. These methods are applied post-training and can serve as a final control layer to curb over-generation.\n\n**Nucleus (Top-p) Sampling:** This is a stochastic decoding method that provides a balance between creativity and coherence. Instead of considering all possible tokens or a fixed number (Top-k), Nucleus Sampling selects from the smallest possible set of tokens whose cumulative probability exceeds a threshold p \\[25\\]\\[33\\]. By truncating the long tail of low-probability tokens, it prevents the model from making bizarre choices while still allowing for variety. This helps reduce the generation of repetitive phrases compared to greedy decoding and produces more human-like text \\[27\\]\\[34\\]\\[38\\].\n\n**Frequency Penalty:** This strategy directly tackles repetition. It works by applying a penalty to the log-probability of tokens that have already appeared in the generated text (including the prompt). A higher penalty value (e.g., closer to 1.0) more aggressively discourages the model from repeating words \\[25\\]\\[31\\]\\[39\\]. While not exclusively targeting domain-specific words, it is highly effective at stopping the model from getting stuck in loops and reusing the same terminology, thus promoting linguistic variety \\[26\\].\n\n**4\\. Evaluation: Quantifying Over-Generation**\n\nA critical component of addressing this problem is the ability to measure it. Standard text generation metrics are often inadequate for capturing the specific phenomenon of domain-word overuse.\n\n**4.1. Limitations of Standard Metrics**\n\nMetrics like **BLEU**, **ROUGE**, and **METEOR** are primarily based on n-gram overlap with a set of reference texts \\[89\\]\\[91\\]\\[209\\]. While useful for tasks like translation and summarization, they fail to penalize stylistic flaws like repetition or overuse of certain words if those words also appear in the reference. A generated text could achieve a high ROUGE score by simply repeating key phrases from the source text, which is precisely the behavior we aim to mitigate \\[85\\]\\[150\\]. **Perplexity (PPL)** measures fluency but does not directly quantify diversity or repetition \\[88\\]\\[215\\].\n\n**4.2. Proposal for a Specialized Metric: Domain Word Overuse (DWO) Score**\n\nTo effectively evaluate solutions, a specialized metric is needed. Drawing upon concepts like the Domain Specificity Index \\[278\\], normalized frequency \\[394\\], and specificity scores that measure deviation from an expected frequency \\[388\\], we can define a metric to quantify the overuse of domain-specific words.\n\n**Methodology:**\n\n1.  **Define a Domain-Specific Vocabulary (DSV):** First, curate a list of domain-specific terms. This can be done by comparing the word frequencies in the fine-tuning corpus against a large, general-domain corpus (e.g., using TF-IDF or log-likelihood) to extract words that are statistically over-represented in the domain \\[386\\]\\[391\\]. Alternatively, a list can be provided by domain experts.\n2.  **Establish a Baseline Frequency:** Calculate the normalized frequency of words from the DSV in a reference text corpus (e.g., a held-out test set from the fine-tuning data). This represents the \"natural\" or desired rate of usage.\n3.  **Calculate Frequency in Generated Text:** For a given prompt, generate an output from the fine-tuned model. Calculate the normalized frequency of DSV words in this generated text.\n4.  **Compute the Deviation Score:** Compare the generated frequency to the baseline frequency.\n\n**Formal Mathematical Definition:**\n\nLet be the set of domain-specific vocabulary terms.\n\nLet be a reference text corpus and be the text generated by the model.\n\nLet be the number of occurrences of word in text .\n\nLet be the total number of words in text .\n\nThe normalized frequency () of DSV terms in a text is:\n\nThe **Domain Word Overuse (DWO)** score can then be defined as the ratio of the normalized frequency in the generated text to the normalized frequency in the reference corpus:\n\nA DWO score significantly greater than 1.0 would indicate overuse of domain-specific terms relative to the reference material. A score around 1.0 would suggest an appropriate level of usage, and a score below 1.0 would indicate underuse.\n\n**4.3. Complementary Evaluation Metrics**\n\nThe DWO score should be used alongside other metrics that capture related aspects of text quality:\n\n**Diversity Metrics:** Metrics like **DISTINCT (D@n)**, which measures the proportion of unique n-grams \\[157\\]\\[159\\]or **Self-BLEU**, which measures the similarity of a sentence to other sentences in the same generated batch \\[90\\], can quantify the repetitiveness that often accompanies over-generation.\n\n**Repetition Metrics:** A simple metric like **Sequence Repetition (Seq-Rep-n)**, which counts the number of repeated n-grams, is a direct measure of local redundancy \\[206\\].\n\n**Distributional Metrics:** The **Zipf Coefficient (ZipC)** measures the rank-frequency distribution of words. A deviation from the expected Zipfian distribution can indicate overuse of a narrow set of tokens \\[216\\].\n\n**5\\. Comparative Analysis and Future Directions**\n\nWhile a variety of strategies exist, the research literature shows a clear gap in direct, controlled comparisons. Queries for experimental results from top-tier conferences like ACL and EMNLP in 2024-2025 comparing token frequency-based loss reweighting against vocabulary masking for this specific problem did not yield head-to-head benchmarks \\[286\\]\\[299\\]\\[346\\].\n\n**Training-Time vs. Inference-Time Strategies:**\n\n**Training-time** interventions (loss reweighting, masking) are more powerful as they alter the model's fundamental probability distributions. However, they are computationally expensive and require careful tuning during the SFT phase.\n\n**Inference-time** strategies (sampling, penalties) are flexible, cheap to apply, and can be adjusted on-the-fly for each generation. However, they are essentially \"band-aids\" that can only work with the probability distribution the model has already learned. They cannot fix a model that is fundamentally biased towards low-diversity outputs.\n\n**Loss Reweighting vs. Vocabulary Masking:**\n\n**Loss Reweighting** is a \"soft\" control mechanism. It guides the model by changing the incentive structure of the learning process, encouraging it to pay more attention to certain tokens over others.\n\n**Vocabulary Masking** is a \"harder\" form of regularization. It directly alters the information available to the model, forcing it to develop more robust contextual reasoning instead of relying on simple surface-level word frequencies.\n\nIt is plausible that a combination of both—for example, using MSLM to teach the model about domain terms while using a difficulty-based reweighting scheme like TLDR to ensure it learns harder general contexts—could yield the best results.\n\n**6\\. Conclusion**\n\nThe over-generation of domain-specific words is a significant and common challenge in the supervised fine-tuning of generative models. This report has detailed a range of algorithmic strategies designed to address this issue at both the training and inference stages. Techniques like dynamic loss reweighting (e.g., TLDR) and strategic masking (e.g., MSLM) offer powerful, foundational solutions by altering the model's learning process. These can be complemented by carefully tuned inference-time decoding methods like nucleus sampling and frequency penalties to further refine output quality.\n\nThe most critical gap in current research is the lack of standardized, specialized evaluation metrics and direct comparative studies. The proposed Domain Word Overuse (DWO) score offers a path toward quantifying the problem more effectively. Future work must focus on establishing rigorous benchmarks to compare these algorithmic strategies, enabling practitioners to select and combine the most effective methods for building fluent, diverse, and genuinely domain-aware generative models.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[2\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[3\\. Sepp Hochreiter, J. Schmidhuber. “Long Short-Term Memory.” Neural Computation](https://doi.org/10.1162/neco.1997.9.8.1735)\n\n[4\\. Nitish Srivastava, Geoffrey E. Hinton et al. “Dropout: a simple way to prevent neural networks from overfitting.” J. Mach. Learn. Res.](https://doi.org/10.5555/2627435.2670313)\n\n[5\\. K. Papineni, Salim Roukos et al. “Bleu: a Method for Automatic Evaluation of Machine Translation.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.3115/1073083.1073135)\n\n[6\\. Token-level Adaptive Training for Neural Machine Translation](https://nlp.ict.ac.cn/en/publications/2022/202212/W020221223645597291288.pdf)\n\n[7\\. 一种改进的 LDA 主题模型](https://jdxb.bjtu.edu.cn/CN/article/downloadArticleFile.do?attachType=PDF&id=2406)\n\n[8\\. Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens](https://openreview.net/pdf/ebb4a271a04e17cb3ba7b3acbe02f77d8b3413f6.pdf)\n\n[9\\. Shaojie Jiang, Thomas Wolf et al. “TLDR: Token Loss Dynamic Reweighting for Reducing Repetitive Utterance Generation.” ArXiv](https://arxiv.org/abs/2003.11963)\n\n[10\\. Token-Level Pruning in Attention Models](https://www.preprints.org/frontend/manuscript/7cdd38873335571c67d699f0d7594388/download_pub)\n\n[11\\. MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models](https://arxiv.org/pdf/2310.19531v6)\n\n[12\\. PhonologyBench: Evaluating Phonological Skills of Larg...](http://arxiv.org/html/2404.02456v1)\n\n[13\\. Proceedings of the International Workshop on Spoken Language Translation](https://workshop2018.iwslt.org/downloads/Proceedings_IWSLT_2018.pdf)\n\n[14\\. A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models](https://arxiv.org/pdf/2501.13958)\n\n[15\\. Fine-tuning Generative Models](https://dspace.mit.edu/bitstream/handle/1721.1/124252/1145123030-MIT.pdf?sequence=1&isAllowed=y)\n\n[21\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[22\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[23\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[24\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[25\\. ragflow/docs/references/python_api_reference.md at 456...](https://github.com/infiniflow/ragflow/blob/45619702ff8c0694c8da27973ee346a5662c9215/docs/references/python_api_reference.md)\n\n[26\\. Model Configuration](https://docs.orq.ai/docs/model-configuration)\n\n[27\\. Decoding Strategies for Transformers - Scaler Topics](https://www.scaler.com/topics/nlp/decoding-strategies-for-transformers/)\n\n[28\\. Angela Fan, M. Lewis et al. “Hierarchical Neural Story Generation.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/P18-1082)\n\n[29\\. TGEA 2.0: A Large-Scale Diagnostically Annotated Dataset with Benchmark Tasks for Text Generation of Pretrained Language Models](https://papers.nips.cc/paper_files/paper/2022/file/cd556f38dba3a6c367c42fa85fc0801c-Paper-Datasets_and_Benchmarks.pdf)\n\n[30\\. FragLlama: Next-fragment prediction for molecular design](https://www.biorxiv.org/content/biorxiv/early/2024/09/30/2024.09.28.615626.full.pdf)\n\n[31\\. vscode-ai-toolkit/README.md at main - GitHub](https://github.com/microsoft/vscode-ai-toolkit/blob/main/README.md)\n\n[32\\. From Matching to Generation: A Survey on Generative Information Retrieval](https://arxiv.org/pdf/2404.14851)\n\n[33\\. OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation](https://arxiv.org/pdf/2311.17911v2)\n\n[34\\. The Curious Case of Neural Text Degeneration](https://arxiv.org/pdf/1904.09751v1)\n\n[35\\. Locally Typical Sampling](https://arxiv.org/pdf/2202.00666)\n\n[36\\. Natural language processing as autoregressive generation](https://dr.ntu.edu.sg/bitstream/10356/168487/2/Amended_thesis_LinXiang.pdf)\n\n[37\\. NEURAL INFERENCE OF PROGRAM SPECIFICATIONS](https://www.cis.upenn.edu/~mhnaik/theses/elizabeth_dinella_thesis.pdf)\n\n[38\\. The curious case of neural text degeneration](https://ceur-ws.org/Vol-2540/FAIR2019_paper_15.pdf)\n\n[39\\. Frequency Penalty: Understanding & Setting It Correctly](https://help.promptitude.io/en/articles/8897043-frequency-penalty-understanding-setting-it-correctly)\n\n[41\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[42\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[43\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[44\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[45\\. Jinhyuk Lee, Wonjin Yoon et al. “BioBERT: a pre-trained biomedical language representation model for biomedical text mining.” Bioinformatics](https://doi.org/10.1093/bioinformatics/btz682)\n\n[46\\. Advanced Optimization Techniques for Generative AI Models](https://www.xcubelabs.com/blog/advanced-optimization-techniques-for-generative-ai-models/)\n\n[47\\. An Exploration of Domain Generalisation Through Vision Benchmarking, Masking, and Pruning](https://doras.dcu.ie/30622/1/Hamza_final_thesis.pdf)\n\n[48\\. 基于注意力掩码的领域泛化研究](https://www.rjdk.org.cn/rc-pub/front/front-article/download/57506861/lowqualitypdf/Domain%20Generalization%20Research%20Based%20on%20Attention%20Mask.pdf)\n\n[49\\. Semi-supervised Semantic Segmentation Meets Masked ...](https://arxiv.org/html/2312.08631v1)\n\n[50\\. Knowledge-Augmented Language Model Adaptation for Domain-Specific Natural Language Processing Tasks](https://aclanthology.org/people/m/minki-kang/)\n\n[51\\. Remedies against the Vocabulary Gap in Information Retrieval](https://irlab.science.uva.nl/wp-content/papercite-data/pdf/van-gysel-phd-thesis-2017.pdf)\n\n[52\\. Prompt Engineering for Knowledge Extraction from Large Language Models](https://eg-fr.uc.pt/retrieve/277750/Prompt%20Engineering%20for%20Knowledge%20Extraction%20from%20Large%20Language%20Models%20-%20Rafael_S_Gouveia.pdf)\n\n[53\\. 神经机器翻译词级别正则化技术研究](http://sc.cipsc.org.cn/mt/conference/2020/papers/T20-1011.pdf)\n\n[54\\. Cross Contrasting Feature Perturbation for Domain Generalization](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Cross_Contrasting_Feature_Perturbation_for_Domain_Generalization_ICCV_2023_paper.pdf)\n\n[55\\. MASKER: Masked Keyword Regularization for Reliable Text Classification](https://cdn.aaai.org/ojs/17601/17601-13-21095-1-2-20210518.pdf)\n\n[56\\. The Rosetta Paradox: Domain-Specific Performance Inver...](http://arxiv.org/html/2412.17821v1)\n\n[57\\. Write and Paint: Generative Vision-Language Models are Unified Modal Learners](https://openreview.net/pdf?id=HgQR0mXQ1_a)\n\n[58\\. A Comparison of Tokenization Impact in Attention Based ...](https://www.biorxiv.org/content/10.1101/2024.09.09.612081v2.full-text)\n\n[59\\. Investigating Semantic Differences in User-Generated C...](https://www.mdpi.com/2076-3417/14/6/2421/xml)\n\n[61\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[62\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[63\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[64\\. Jinhyuk Lee, Wonjin Yoon et al. “BioBERT: a pre-trained biomedical language representation model for biomedical text mining.” Bioinformatics](https://doi.org/10.1093/bioinformatics/btz682)\n\n[65\\. Random and selective masking in transformer models ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC11844826/)\n\n[66\\. Suchin Gururangan, Ana Marasović et al. “Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks.” ArXiv](https://doi.org/10.18653/v1/2020.acl-main.740)\n\n[67\\. Leveraging Large Language Models for Generative Acronym Disambiguation in the Biomedical Domain](https://epub.jku.at/download/pdf/11472134.pdf)\n\n[68\\. Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey](https://openreview.net/pdf?id=y5lqBWQcmx)\n\n[69\\. 通过掩码提高预训练语言模型的灵敏度特定损失：生物医学 NER 案例研究](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_3_28/2403.18025.pdf)\n\n[70\\. LLM Fine Tuning: The Guide for ML Teams - Label Your Data](https://labelyourdata.com/articles/llm-fine-tuning)\n\n[71\\. Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps](https://aclanthology.org/2022.aacl-main.57/)\n\n[72\\. Idiomatic Expression Paraphrasing without Strong Supervision](https://cdn.aaai.org/ojs/21433/21433-13-25446-1-2-20220628.pdf)\n\n[73\\. Unified Language Model Pre-training for Natural Language Understanding and Generation](http://homepage.divms.uiowa.edu/~jrusert/qi_915.pdf)\n\n[74\\. Actes de JEP-TALN-RECITAL 2024. 31ème Conférence sur le Traitement Automatique des Langues Naturelles, volume 1: articles longs et prises de position](https://hal.science/UT1-CAPITOLE/hal-04623005v1/file/proceedings-taln.pdf)\n\n[75\\. Using Selective Masking as a Bridge between Pre-training and Fine-tuning](https://nips.cc/virtual/2022/59457)\n\n[76\\. Few-Shot Intent Classification in User-Generated Short Texts: Application to Conversational Agents](https://theses.hal.science/tel-03722690v1/file/These-Dopierre-Thomas-2021.pdf)\n\n[77\\. Looking Under the Hood of DetectGPT](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1234/final-reports/final-report-169403912.pdf)\n\n[78\\. Self-supervised Domain Adaptation of Language Models for the Process Industry](https://gipplab.org/wp-content/papercite-data/pdf/luehrs2024.pdf)\n\n[79\\. Jingjing Li, Zichao Li et al. “Unsupervised Text Generation by Learning from Search.” ArXiv](https://arxiv.org/abs/2007.08557)\n\n[80\\. Micheal Abaho, D. Bollegala et al. “Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER.” ArXiv](https://doi.org/10.48550/arXiv.2403.18025)\n\n[81\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[82\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[83\\. K. Papineni, Salim Roukos et al. “Bleu: a Method for Automatic Evaluation of Machine Translation.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.3115/1073083.1073135)\n\n[84\\. Utilizing Fine-Tuning of Large Language Models for Generating Synthetic Payloads: Enhancing Web Application Cybersecurity through Innovative Penetration Testing Techniques](https://cdn.techscience.cn/files/cmc/2025/TSP_CMC-82-3/TSP_CMC_59696/TSP_CMC_59696.pdf)\n\n[85\\. Fine Tuning Transformer Models for Domain Specific Feature Extraction](https://upcommons.upc.edu/bitstream/handle/2117/384726/173985.pdf?sequence=2&isAllowed=y)\n\n[86\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[87\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[88\\. MARK YOUR LLM: DETECTING THE MISUSE OF OPEN-SOURCE LARGE LANGUAGE MODELS VIA WATERMARKING](https://www.arxiv.org/pdf/2503.04636)\n\n[89\\. Asli Celikyilmaz, Elizabeth Clark et al. “Evaluation of Text Generation: A Survey.” ArXiv](https://arxiv.org/abs/2006.14799)\n\n[90\\. Automated metrics for evaluating the quality of text generation](https://www.digitalocean.com/community/tutorials/automated-metrics-for-evaluating-generated-text#:~:text=Self-BLEU%20is%20a%20smart,diversity%20in%20the%20generated%20text.)\n\n[91\\. Neural Language Generation for Content Adaptation: Explainable, Efficient Low-Resource Text Simplification and Evaluation](http://deepblue.lib.umich.edu/bitstream/2027.42/178028/1/garbacea_1.pdf)\n\n[92\\. Bowen Tan, Zichao Yang et al. “Progressive Generation of Long Text.” ArXiv](https://arxiv.org/abs/2006.15720)\n\n[93\\. Retrieval Augmented Generation Optimizations](https://www.theseus.fi/bitstream/10024/874901/2/Rolle_Robin.pdf)\n\n[94\\. Fine-Tuned Language Models Generate Stable Inorganic Materials as Text](https://openreview.net/pdf?id=0r5DE2ZSwJ)\n\n[95\\. ProSwitch: Fine-Tuning Large Language Models to Generate Professional and Non-Professional Styled Text](https://openreview.net/pdf?id=NvzQPv162Sx)\n\n[96\\. Domain Adaptation of Large Language Models - Infosys](https://www.infosys.com/iki/techcompass/large-language-models.html#:~:text=Finetuning%20stands%20as%20a%20powerful,nuances%20and%20pre-trained%20knowledge.)\n\n[97\\. Junyi Li, Tianyi Tang et al. “Pre-Trained Language Models for Text Generation: A Survey.” ACM Computing Surveys](https://doi.org/10.1145/3649449)\n\n[98\\. Robust and Fine-Grained Detection of AI Generated Texts](https://openreview.net/pdf/a6f36192b73b128d14b93c6c9dea946df484e017.pdf)\n\n[101\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[102\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[103\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[104\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[105\\. SciRAGBench: Benchmarking Large Language Models for...](https://openreview.net/forum?id=scnPGBSWwl)\n\n[106\\. Aakanksha Chowdhery, Sharan Narang et al. “PaLM: Scaling Language Modeling with Pathways.” J. Mach. Learn. Res.](https://arxiv.org/abs/2204.02311)\n\n[107\\. Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph](https://arxiv.org/pdf/2505.09945)\n\n[108\\. Proceedings of the 19th Workshop of the Australasian Language Technology Association](https://alta2021.alta.asn.au/files/ALTA2021-proceedings-draft.pdf)\n\n[109\\. Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey](https://openreview.net/pdf?id=y5lqBWQcmx)\n\n[110\\. The Rosetta Paradox: Domain-Specific Performance Inver...](http://arxiv.org/html/2412.17821v1)\n\n[111\\. Pinyin Regularization in Error Correction for Chinese Speech Recognition with Large Language Models](https://www.isca-archive.org/interspeech_2024/tang24_interspeech.pdf)\n\n[112\\. Quantifying Generalization Complexity for Large Language Models](https://openreview.net/pdf?id=jpSLXoRKnH)\n\n[113\\. Remedies against the Vocabulary Gap in Information Retrieval](https://irlab.science.uva.nl/wp-content/papercite-data/pdf/van-gysel-phd-thesis-2017.pdf)\n\n[114\\. Comparing Gen 1 Models (GPT, BERT, T5 and More)](https://dev.to/admantium/large-language-models-comparing-gen-1-models-gpt-bert-t5-and-more-74h)\n\n[115\\. Benchmarking Large Language Models in Retrieval-Augmented Generation,arXiv - CS - Computation and Language - X-MOL](https://www.x-mol.com/paper/1699544075705536512?adv)\n\n[116\\. Exploring Zero-Shot Cross-Lingual Biomedical Concept Normalization via Large Language Models](https://www.medrxiv.org/content/10.1101/2025.02.27.25323007v1.full.pdf)\n\n[117\\. WHEN FLUE MEETS FLANG: Benchmarks and Large Pre-trained Language Model for Financial Domain](https://cs.stanford.edu/~diyiy/docs/emnlp_flang_2022.pdf)\n\n[118\\. Benchmarking Large Language Models in Retrieval-Augmented Generation](https://ojs.aaai.org/index.php/AAAI/article/view/29728/31250)\n\n[119\\. Accelerating Large Language Models and Generative AI](https://iaifi.org/talks/2024_03_IAIFI_Symposium_Han.pdf)\n\n[121\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[122\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[123\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[124\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[125\\. Alec Radford, Karthik Narasimhan. “Improving Language Understanding by Generative Pre-Training.”](https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n[126\\. Fine-Tuning Language Models with Just Forward Passes](https://openreview.net/pdf?id=2UG3m0Vt9z)\n\n[127\\. Novel approaches for remote detection of malicious shared libraries to enhance vulnerability management](https://netlibrary.aau.at/obvuklhs/download/pdf/10519186)\n\n[128\\. Guide to Fine-Tuning Open Source LLM Models on Custom Data](https://stackabuse.com/guide-to-fine-tuning-open-source-llms-on-custom-data/)\n\n[129\\. Entity Matching using Deep Neural Networks: From Discriminative Pre-trained Language Models to Generative Large Language Models](https://madoc.bib.uni-mannheim.de/69425/1/Dissertation_RalphPeeters.pdf)\n\n[130\\. GENERATIVE MODELS FOR SPEECH ENHANCEMENT](https://www.hse.ru/data/2024/10/04/1888260947/%D0%90%D0%BD%D0%B4%D1%80%D0%B5%D0%B5%D0%B2_summary.pdf)\n\n[131\\. PLUG AND PLAY LANGUAGE MODELS: A SIMPLE APPROACH TO CONTROLLED TEXT GENERATION](https://openreview.net/pdf?id=H1edEyBKDS)\n\n[132\\. Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompts](https://wei-ying.net/pubs/HPrompt_CPT_EMNLP2023.pdf)\n\n[133\\. Natural Language Processing with Transformers: Building Language Applications with Hugging Face](https://csbookstore.altervista.org/pdf/50.pdf)\n\n[134\\. Benjamin L. Badger. “Masked Mixers for Language Generation and Retrieval.”](https://arxiv.org/abs/2409.01482)\n\n[135\\. Minxin Du, Xiang Yue et al. “DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass.” Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security](https://doi.org/10.1145/3576915.3616592)\n\n[136\\. \\[论文审查\\] To FP8 and Back Again: Quantifying the Effects ...](https://www.themoonlight.io/zh/review/to-fp8-and-back-again-quantifying-the-effects-of-reducing-precision-on-llm-training-stability)\n\n[141\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[142\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[143\\. Evaluate LLMs with standard metrics - Training | Micro...](https://learn.microsoft.com/en-us/training/modules/evaluate-language-models-azure-databricks/4-standard-metrics)\n\n[144\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[145\\. Wangchunshu Zhou, Ke Xu. “Learning to Compare for Better Training and Evaluation of Open Domain Natural Language Generation Models.” AAAI Conference on Artificial Intelligence](https://doi.org/10.1609/aaai.v34i05.6521)\n\n[146\\. Ravil Mussabayev. “WRDScore: New Metric for Evaluation of Natural Language Generation Models.” ArXiv](https://doi.org/10.48550/arXiv.2405.19220)\n\n[147\\. Tianyi Zhang, Varsha Kishore et al. “BERTScore: Evaluating Text Generation with BERT.” ArXiv](https://arxiv.org/abs/1904.09675)\n\n[148\\. Ruslan Yermakov, Nicholas Drago et al. “Biomedical Data-to-Text Generation via Fine-Tuning Transformers.” International Conference on Natural Language Generation](https://doi.org/10.18653/v1/2021.inlg-1.40)\n\n[149\\. Lingfeng Shen, Haiyun Jiang et al. “Revisiting the Evaluation Metrics of Paraphrase Generation.” ArXiv](https://www.semanticscholar.org/paper/a1377d63f3cdc9335289e6d3cf81386594668334)\n\n[150\\. Md. Rony, Liubov Kovriguina et al. “RoMe: A Robust Metric for Evaluating Natural Language Generation.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.48550/arXiv.2203.09183)\n\n[151\\. Vitou Phy, Yang Zhao et al. “Deconstruct to Reconstruct a Configurable Evaluation Metric for Open-Domain Dialogue Systems.” International Conference on Computational Linguistics](https://doi.org/10.18653/V1/2020.COLING-MAIN.368)\n\n[152\\. Mengyuan Yang, Mengying Zhu et al. “Fine-Tuning Large Language Model Based Explainable Recommendation with Explainable Quality Reward.” AAAI Conference on Artificial Intelligence](https://doi.org/10.1609/aaai.v38i8.28777)\n\n[153\\. Jarryd Dunn. “A Comparison of Data-Driven and Template-Based Approaches to Natural Language Generation.”](https://www.semanticscholar.org/paper/a32d9c8df1ece7fc366e73cb2901a1c2d7737fb3)\n\n[154\\. Jianing Zhou, S. Bhat. “Paraphrase Generation: A Survey of the State of the Art.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/2021.emnlp-main.414)\n\n[155\\. Konrad Wojtasik, Arkadiusz Janz et al. “Wordnet for Definition Augmentation with Encoder-Decoder Architecture.” Global WordNet Conference](https://www.semanticscholar.org/paper/a37ef1212ed341a885b4491fce59e40bf05ff165)\n\n[156\\. Christoph Leiter, Juri Opitz et al. “The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics.” EVAL4NLP](https://doi.org/10.48550/arXiv.2310.19792)\n\n[157\\. 语义增强的开放域对话生成技术研究](https://nlp.ict.ac.cn/lwlz/bylw/202310/P020231006545899340377.pdf)\n\n[158\\. Advances and Challenges in Conversational Recommender Systems: A Survey](http://staff.ustc.edu.cn/~hexn/papers/CRS-survey-2021.pdf)\n\n[159\\. BP4ER: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue Generation](http://www.cbsr.ia.ac.cn/users/jwan/papers/COLING24_MDG.pdf)\n\n[160\\. 生成式语言模型的文本生成评价指标(从传统的基于统计到现在 ...](http://www.coreui.cn/news/476525.html)\n\n[161\\. Regularization - Paper Reading](http://paperreading.club/category?cate=Regularization)\n\n[162\\. Self-Supervised - Paper Reading](http://paperreading.club/category?cate=Self-Supervised)\n\n[163\\. Token Masking Improves Transformer-Based Text Classification](https://arxiv.org/pdf/2505.11746)\n\n[164\\. Vision-centric Token Compression in Large Language Model](https://arxiv.org/pdf/2502.00791)\n\n[165\\. A Comparison of Tokenization Impact in Attention Based and State Space Genomic Language Models](https://www.biorxiv.org/content/biorxiv/early/2024/09/17/2024.09.09.612081.full.pdf)\n\n[166\\. Efficient Vocabulary Adaptation via Token Alignment](https://arxiv.org/abs/2506.03523)\n\n[167\\. Software Model Evolution with Large Language Models: Experiments on Simulated, Public, and Industrial Datasets](https://arxiv.org/pdf/2406.17651)\n\n[168\\. FOX-1: OPEN SMALL LANGUAGE MODEL FOR CLOUD AND EDGE](https://arxiv.org/pdf/2411.05281)\n\n[169\\. A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models](https://arxiv.org/pdf/2501.13958)\n\n[170\\. Tokenization Changes Meaning in Large Language Models](https://discovery.researcher.life/article/tokenization-changes-meaning-in-large-language-models-evidence-from-chinese/bfc6fa6f9c8e385f84b1de08afe55928)\n\n[171\\. 大規模言語モデルによる失語症の単語産出シミュレーション](https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/B4-3.pdf)\n\n[172\\. Token Pruning in Multimodal Large Language Models](https://www.arxiv.org/abs/2502.11501)\n\n[173\\. A Comparison of Tokenization Impact in Attention Based ...](https://www.biorxiv.org/content/10.1101/2024.09.09.612081v2.full-text)\n\n[174\\. A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf?fbclid=IwAR3GYBQ2P9Cww2HVM3oUbML9i5i3DMDBVv5_FvYWfEi-vdZqZoSM78jE2-s)\n\n[175\\. Large Language Models as Tools for Biocatalytic System Design and Optimization](https://pure.tue.nl/ws/files/354709005/20250409_Nana_Teukam_hf.pdf)\n\n[176\\. Augmenting Large Language Model's Knowledge Using External Sources Without Finetuning](https://elib.dlr.de/210070/1/Augmenting_Large_Language_Model_s_Knowledge_Using_External_Sources_Without_Finetuning.pdf)\n\n[177\\. Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English](https://arxiv.org/pdf/2505.17076v3)\n\n[178\\. Efficient domain adaptation of language models via adaptive tokenization](https://www.amazon.science/publications/efficient-domain-adaptation-of-language-models-via-adaptive-tokenization)\n\n[181\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[182\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[183\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[184\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[185\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[186\\. Fine-Tuning Language Models with Just Forward Passes](https://openreview.net/pdf?id=2UG3m0Vt9z)\n\n[187\\. Preserving Diversity in Supervised Fine-Tuning of Large Language Models](https://openreview.net/pdf?id=NQEe7B7bSw)\n\n[188\\. \\[论文审查\\] To FP8 and Back Again: Quantifying the Effects ...](https://www.themoonlight.io/zh/review/to-fp8-and-back-again-quantifying-the-effects-of-reducing-precision-on-llm-training-stability)\n\n[189\\. GENERATIVE MODELS FOR SPEECH ENHANCEMENT](https://www.hse.ru/data/2024/10/04/1888260947/%D0%90%D0%BD%D0%B4%D1%80%D0%B5%D0%B5%D0%B2_summary.pdf)\n\n[190\\. PLUG AND PLAY LANGUAGE MODELS: A SIMPLE APPROACH TO CONTROLLED TEXT GENERATION](https://openreview.net/pdf?id=H1edEyBKDS)\n\n[191\\. Benjamin L. Badger. “Masked Mixers for Language Generation and Retrieval.”](https://arxiv.org/abs/2409.01482)\n\n[192\\. Fine-Tuning Pre-Trained Language Models for German Multi-Document Summarization](https://www.inf.uni-hamburg.de/en/inst/ab/lt/teaching/theses/completed-theses/2020-ma-johner.pdf)\n\n[193\\. Anchored Diffusion Language Model](https://arxiv.org/pdf/2505.18456)\n\n[194\\. Applied and Computational Engineering: Proceedings of the 5th International Conference on Signal Processing and Machine Learning](https://www.ewadirect.com/media/var/media/upload/vol_pdf/ace/115.pdf)\n\n[201\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[202\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[203\\. K. Papineni, Salim Roukos et al. “Bleu: a Method for Automatic Evaluation of Machine Translation.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.3115/1073083.1073135)\n\n[204\\. Fine Tuning Transformer Models for Domain Specific Feature Extraction](https://upcommons.upc.edu/bitstream/handle/2117/384726/173985.pdf?sequence=2&isAllowed=y)\n\n[205\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[206\\. MARK YOUR LLM: DETECTING THE MISUSE OF OPEN-SOURCE LARGE LANGUAGE MODELS VIA WATERMARKING](https://www.arxiv.org/pdf/2503.04636)\n\n[207\\. Utilizing Fine-Tuning of Large Language Models for Generating Synthetic Payloads: Enhancing Web Application Cybersecurity through Innovative Penetration Testing Techniques](https://cdn.techscience.cn/files/cmc/2025/TSP_CMC-82-3/TSP_CMC_59696/TSP_CMC_59696.pdf)\n\n[208\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[209\\. Asli Celikyilmaz, Elizabeth Clark et al. “Evaluation of Text Generation: A Survey.” ArXiv](https://arxiv.org/abs/2006.14799)\n\n[210\\. Neural Language Generation for Content Adaptation: Explainable, Efficient Low-Resource Text Simplification and Evaluation](http://deepblue.lib.umich.edu/bitstream/2027.42/178028/1/garbacea_1.pdf)\n\n[211\\. Automated metrics for evaluating the quality of text generation](https://www.digitalocean.com/community/tutorials/automated-metrics-for-evaluating-generated-text#:~:text=Self-BLEU%20is%20a%20smart,diversity%20in%20the%20generated%20text.)\n\n[212\\. Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation](http://www.arxiv.org/pdf/2411.10129)\n\n[213\\. Bowen Tan, Zichao Yang et al. “Progressive Generation of Long Text.” ArXiv](https://arxiv.org/abs/2006.15720)\n\n[214\\. Domain Adaptation for Natural Language Generation](https://ufal.mff.cuni.cz/~zabokrtsky/pgs/thesis_proposal/zdenek-kasner-proposal.pdf)\n\n[215\\. LANGUAGE MODEL DETECTORS ARE EASILY OPTIMIZED AGAINST](https://openreview.net/pdf?id=4eJDMjYZZG)\n\n[216\\. Learning to Diversify Neural Text Generation via Degenerative Model](https://www.afnlp.org/conferences/ijcnlp2023/proceedings/main-findings/cdrom/pdf/2023.findings-ijcnlp.6.pdf)\n\n[217\\. Ankit Maloo, Abhinav Garg. “Cross-Domain Content Generation with Domain-Specific Small Language Models.”](https://arxiv.org/abs/2409.17171)\n\n[218\\. Understanding Language Model Evaluation Metrics](https://3ai.in/understanding-language-model-evaluation-metrics-a-comprehensive-overview/)\n\n[221\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[222\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[223\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[224\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[225\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[226\\. A Comparison of Tokenization Impact in Attention Based and State Space Genomic Language Models](https://www.biorxiv.org/content/biorxiv/early/2024/09/17/2024.09.09.612081.full.pdf)\n\n[227\\. Token Masking Improves Transformer-Based Text Classification](https://arxiv.org/pdf/2505.11746)\n\n[228\\. Large Language Models and Return Prediction in China](https://www.cafr-sif.com/2024/papers/Large%20Language%20Models%20and%20Return%20Prediction%20in%20China.pdf)\n\n[229\\. Wayne Xin Zhao, Kun Zhou et al. “A Survey of Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2303.18223)\n\n[230\\. Tokenization Changes Meaning in Large Language Models](https://discovery.researcher.life/article/tokenization-changes-meaning-in-large-language-models-evidence-from-chinese/bfc6fa6f9c8e385f84b1de08afe55928)\n\n[231\\. Enhancing Large Language Models through Adaptive Tokenizers](https://openreview.net/pdf?id=3H1wqEdK4z)\n\n[232\\. A Comparison of Tokenization Impact in Attention Based ...](https://www.biorxiv.org/content/10.1101/2024.09.09.612081v2.full-text)\n\n[233\\. Grammar Prompting for Domain-Specific Language Generation with Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2023/file/cd40d0d65bfebb894ccc9ea822b47fa8-Paper-Conference.pdf)\n\n[234\\. Instruction Tuning for Domain Adaptation of Large Language Models: A Case Study in the Field of Education](https://repository.tudelft.nl/file/File_e2064dc9-97f1-4e9d-ac32-1bebebf05c3d?preview=1)\n\n[235\\. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://pure.uva.nl/ws/files/169637996/BLOOM.pdf)\n\n[241\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[242\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[243\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[244\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[245\\. Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis](https://arxiv.org/pdf/2504.10352)\n\n[246\\. Alec Radford, Karthik Narasimhan. “Improving Language Understanding by Generative Pre-Training.”](https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n[247\\. UNILMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training](https://static.aminer.cn/upload/pdf/1870/1346/1079/5ede0553e06a4c1b26a84095_0.pdf)\n\n[248\\. ...Pseudo-Masked Language Models for Unified Language Model...](https://www.microsoft.com/en-us/research/publication/unilmv2-pseudo-masked-language-models-for-unified-language-model-pre-training/)\n\n[249\\. Fine-Tuning Language Models with Just Forward Passes](https://openreview.net/pdf?id=CcsdvOOzMp)\n\n[250\\. Hangbo Bao, Li Dong et al. “UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training.” ArXiv](https://arxiv.org/abs/2002.12804)\n\n[251\\. Differential Privacy - Differentially private deep learning can be effective with self-supervised models](https://differentialprivacy.org/dp-fine-tuning/)\n\n[252\\. ...Pseudo-Masked Language Models for Unified Language ...](http://proceedings.mlr.press/v119/bao20a.html)\n\n[261\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[262\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[263\\. K. Papineni, Salim Roukos et al. “Bleu: a Method for Automatic Evaluation of Machine Translation.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.3115/1073083.1073135)\n\n[264\\. Utilizing Fine-Tuning of Large Language Models for Generating Synthetic Payloads: Enhancing Web Application Cybersecurity through Innovative Penetration Testing Techniques](https://cdn.techscience.cn/files/cmc/2025/TSP_CMC-82-3/TSP_CMC_59696/TSP_CMC_59696.pdf)\n\n[265\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[266\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[267\\. Fine Tuning Transformer Models for Domain Specific Feature Extraction](https://upcommons.upc.edu/bitstream/handle/2117/384726/173985.pdf?sequence=2&isAllowed=y)\n\n[268\\. Software Engineering and its Automation with Large Language Models](https://www.mn.uio.no/ifi/forskning/aktuelt/arrangementer/disputaser/2024/grishina---software-engineering-and-its-automation-with-large-language-models---phd-thesis-2024-published.pdf)\n\n[269\\. Checklist for Domain-Specific LLM Fine-Tuning - Ghost](https://latitude-blog.ghost.io/blog/checklist-for-domain-specific-llm-fine-tuning/)\n\n[270\\. Neural Language Generation for Content Adaptation: Explainable, Efficient Low-Resource Text Simplification and Evaluation](http://deepblue.lib.umich.edu/bitstream/2027.42/178028/1/garbacea_1.pdf)\n\n[271\\. Hybrid Fine-Tuning of Large Language Models Using LoRA: Enhancing Multi-Task Text Classification through Knowledge Sharing](https://jecei.sru.ac.ir/article_2303_af3def78c9c37bf642b72562f6210c53.pdf)\n\n[272\\. Understanding Language Model Evaluation Metrics](https://3ai.in/understanding-language-model-evaluation-metrics-a-comprehensive-overview/)\n\n[273\\. Domain Adaptation of Large Language Models - Infosys](https://www.infosys.com/iki/techcompass/large-language-models.html#:~:text=Finetuning%20stands%20as%20a%20powerful,nuances%20and%20pre-trained%20knowledge.)\n\n[274\\. Adaptation of Large Language Models to assistant chat-bots for industrial plants](https://thesis.unipd.it/retrieve/4836c8d1-5f92-4ee8-81ec-82d72dbd6a95/Farhangian_Mohammadardalan.pdf)\n\n[275\\. Domain Adaptation of Large Language Models](https://www.infosys.com/iki/techcompass/large-language-models.html)\n\n[276\\. Evaluation of LLMs: A concise overview of benchmarks and metrics used to assess the performance of large language models](https://learn.datasciencedojo.com/wp-content/uploads/2024/10/Evaluation-of-LLMs-Feb-2025.pdf)\n\n[277\\. The Definitive Guide to Fine-Tuning LLMs](https://21944583.fs1.hubspotusercontent-na1.net/hubfs/21944583/eBooks/Predibase_Fine-Tuning_LLMs_ebook_.pdf)\n\n[278\\. Basab Jha, Ujjwal Puri. “The Rosetta Paradox: Domain-Specific Performance Inversions in Large Language Models.”](https://arxiv.org/abs/2412.17821)\n\n[279\\. LLM Evaluation](https://docs.clarifai.com/create/models/evaluate/llms/)\n\n[281\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[282\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[283\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[284\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[285\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[286\\. Software Model Evolution with Large Language Models: Experiments on Simulated, Public, and Industrial Datasets](https://arxiv.org/pdf/2406.17651)\n\n[287\\. Context-aware Meta-learned Loss Scaling (CaMeLS) for Online Fine-tuning of Large Language Models](https://aclanthology.org/people/n/nathan-hu/)\n\n[288\\. Enhancing Large Language Models through Adaptive Tokenizers](https://openreview.net/pdf?id=3H1wqEdK4z)\n\n[289\\. 177期《TasTe: Teaching Large Language Models to Translate through Self-Reflection》](https://www.bilibili.com/video/av113157775296563?t=607)\n\n[290\\. Fu-Ming Guo, Sijia Liu et al. “Reweighted Proximal Pruning for Large-Scale Language Representation.” ArXiv](https://arxiv.org/abs/1909.12486)\n\n[291\\. Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining](https://shiqiang.wang/papers/DS_ICLR2025.pdf)\n\n[292\\. Preserving In-Context Learning Alibility in Large Language Model Fine-tuning (Ar](https://www.bilibili.com/video/av825180172?t=485)\n\n[293\\. A Comparison of Tokenization Impact in Attention Based ...](https://www.biorxiv.org/content/10.1101/2024.09.09.612081v2.full-text)\n\n[294\\. Large Language Model Fine-tuning with Low-Rank Adaptation: A Performance Exploration](https://lca.ece.utexas.edu/pubs/icpe25_Bagus.pdf)\n\n[295\\. A Comparison of Tokenization Impact in Attention Based and State Space Genomic Language Models](https://www.biorxiv.org/content/biorxiv/early/2024/09/17/2024.09.09.612081.full.pdf)\n\n[296\\. Yu Gu, Robert Tinn et al. “Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing.” ACM Transactions on Computing for Healthcare (HEALTH)](https://doi.org/10.1145/3458754)\n\n[297\\. Large Language Model主题的若干论文简述 - Rest探路者 - ...](https://www.cnblogs.com/Java-Starter/p/17402834.html)\n\n[298\\. Refine Large Language Model Fine-tuning via Instruction Vector](https://arxiv.org/pdf/2406.12227)\n\n[299\\. Instruction Tuning for Domain Adaptation of Large Language Models: A Case Study in the Field of Education](https://repository.tudelft.nl/file/File_e2064dc9-97f1-4e9d-ac32-1bebebf05c3d?preview=1)\n\n[301\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[302\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[303\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[304\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[305\\. Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis](https://arxiv.org/pdf/2504.10352)\n\n[306\\. Random and selective masking in transformer models ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC11844826/)\n\n[307\\. Alec Radford, Karthik Narasimhan. “Improving Language Understanding by Generative Pre-Training.”](https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n[308\\. Transformers](https://www.cs.toronto.edu/~gpenn/csc401/lectures/7_Transformers.pdf)\n\n[309\\. Fine-Tuning and Masked Language Models](https://web.stanford.edu/~jurafsky/slp3/11.pdf)\n\n[310\\. Fine-Tuning Language Models with Just Forward Passes](https://openreview.net/pdf?id=CcsdvOOzMp)\n\n[311\\. Masked Language Models](https://web.stanford.edu/~jurafsky/slp3/old_aug24/11.pdf)\n\n[312\\. UNILMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training](https://static.aminer.cn/upload/pdf/1870/1346/1079/5ede0553e06a4c1b26a84095_0.pdf)\n\n[313\\. Hangbo Bao, Li Dong et al. “UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training.” ArXiv](https://arxiv.org/abs/2002.12804)\n\n[314\\. Minxin Du, Xiang Yue et al. “DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass.” Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security](https://doi.org/10.1145/3576915.3616592)\n\n[315\\. Applications of transformer-based language models in bioinformatics: a survey](https://academic.oup.com/bioinformaticsadvances/article-pdf/3/1/vbad001/49324476/vbad001.pdf)\n\n[316\\. Marjan Ghazvininejad, Omer Levy et al. “Mask-Predict: Parallel Decoding of Conditional Masked Language Models.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D19-1633)\n\n[317\\. AHEAD-OF-TIME P-TUNING](https://openreview.net/pdf?id=8IBtyLQ8GKw)\n\n[321\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[322\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[323\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[324\\. K. Papineni, Salim Roukos et al. “Bleu: a Method for Automatic Evaluation of Machine Translation.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.3115/1073083.1073135)\n\n[325\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[326\\. Fine-Tuned Language Models Generate Stable Inorganic Materials as Text](https://openreview.net/pdf?id=0r5DE2ZSwJ)\n\n[327\\. Margherita Gambini, M. Avvenuti et al. “Detecting Generated Text and Attributing Language Model Source with Fine-tuned Models and Semantic Understanding.” IberLEF@SEPLN](https://www.semanticscholar.org/paper/57dafe6527ce4bfa9bf26679371fa4df515072ff)\n\n[328\\. Evaluating Large Language Models - Evaluation Metrics](https://www.enkefalos.com/newsletters-and-articles/evaluating-large-language-models-evaluation-metrics/)\n\n[329\\. An Introduction to LLM Benchmarking - Confident AI](https://www.confident-ai.com/blog/the-current-state-of-benchmarking-llms#:~:text=A%20benchmark%20dataset%20is%20a,natural%20language%20understanding%20and%20Q&A)\n\n[330\\. Evaluation metrics | Microsoft Learn](https://learn.microsoft.com/sr-cyrl-rs/ai/playbook/technology-guidance/generative-ai/working-with-llms/evaluation/list-of-eval-metrics)\n\n[331\\. Biomedical Text Readability After Hypernym Substitution with Fine-Tuned Large Language Models](https://jdc.jefferson.edu/cgi/viewcontent.cgi?article=1012&context=skmcstudentworks)\n\n[332\\. Evaluation of Large Language Models: Review of Metrics, Applications, and Methodologies](https://www.preprints.org/manuscript/202504.0369/v1/download)\n\n[333\\. Anna C. Doris, Daniele Grandi et al. “DesignQA: A Multimodal Benchmark for Evaluating Large Language Models' Understanding of Engineering Documentation.” ArXiv](https://doi.org/10.48550/arXiv.2404.07917)\n\n[341\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[342\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[343\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[344\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[345\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[346\\. Enhancing Large Language Models through Adaptive Tokenizers](https://openreview.net/pdf?id=3H1wqEdK4z)\n\n[347\\. GitHub - tiann/LLaMA-Factory: Unified Efficient Fine-Tuning...](https://github.com/tiann/LLaMA-Factory)\n\n[348\\. GitHub - chenqing24/LLaMA-Factory: Unified Efficient F...](https://github.com/chenqing24/LLaMA-Factory)\n\n[349\\. Findings of the Association for Computational Linguistics: ACL 2024](https://www.proceedings.com/content/075/075970webtoc.pdf)\n\n[350\\. Fu-Ming Guo, Sijia Liu et al. “Reweighted Proximal Pruning for Large-Scale Language Representation.” ArXiv](https://arxiv.org/abs/1909.12486)\n\n[351\\. EMNLP 2024: Miami, FL, USA - Industry Track](https://dblp.org/db/conf/emnlp/emnlp2024i)\n\n[352\\. Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER](https://openreview.net/pdf/7fafb50e52db23b0f8f5dbd49d807359ab8ef25e.pdf)\n\n[353\\. EMNLP.2024 - Main](https://papers.cool/venue/EMNLP.2024?group=Main)\n\n[354\\. Findings of EMNLP 2024](https://www.proceedings.com/content/077/077401webtoc.pdf)\n\n[355\\. A Large-scale Empirical Study on Fine-tuning Large Language Models for Unit Testing](https://arxiv.org/pdf/2412.16620)\n\n[356\\. Refine Large Language Model Fine-tuning via Instruction Vector](https://arxiv.org/pdf/2406.12227)\n\n[357\\. ...A Benchmark of LLMs on Advanced Mathematical Proble...](http://arxiv.org/abs/2501.17084)\n\n[358\\. Large Language Model Fine-tuning with Low-Rank Adaptation: A Performance Exploration](https://lca.ece.utexas.edu/pubs/icpe25_Bagus.pdf)\n\n[359\\. A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models](https://arxiv.org/pdf/2501.13958)\n\n[360\\. LLaMA-Factory/README.md at main · MaoXianXin/LLaMA-Factory | GitHub](https://github.com/MaoXianXin/LLaMA-Factory/blob/main/README.md)\n\n[361\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[362\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[363\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[364\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[365\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[366\\. Masking in the Transformer Explained](https://www.garysnotebook.com/20201003_1/)\n\n[367\\. Risk-Averse Fine-tuning of Large Language Models](https://openreview.net/pdf?id=1BZKqZphsW)\n\n[368\\. Fine-Tuning and Masked Language Models](https://web.stanford.edu/~jurafsky/slp3/11.pdf)\n\n[369\\. Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis](https://arxiv.org/pdf/2504.10352)\n\n[370\\. Neural Translation of Musical Style](https://syncedreview.com/2017/11/29/neural-translation-of-musical-style/)\n\n[371\\. Leveraging Transformer-Based Language Models to Bridge the Gap Between Language and Specialized Domains](https://theses.hal.science/tel-04706229v1/file/131557_ABDINE_2024_archivage.pdf)\n\n[372\\. Applications of transformer-based language models in bioinformatics: a survey](https://academic.oup.com/bioinformaticsadvances/article-pdf/3/1/vbad001/49324476/vbad001.pdf)\n\n[373\\. PROSODY-TTS: SELF-SUPERVISED PROSODY PRE-TRAINING WITH LATENT DIFFUSION FOR TEXT-TO-SPEECH](https://openreview.net/pdf?id=y6EnaJlhcWZ)\n\n[374\\. UNILMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training](http://proceedings.mlr.press/v119/bao20a/bao20a.pdf)\n\n[375\\. Scientific Developments of Young Scientists to Improve Life](https://isg-konf.com/wp-content/uploads/2024/08/SCIENTIFIC-DEVELOPMENTS-OF-YOUNG-SCIENTISTS-TO-IMPROVE-LIFE.pdf)\n\n[376\\. Marjan Ghazvininejad, Omer Levy et al. “Mask-Predict: Parallel Decoding of Conditional Masked Language Models.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D19-1633)\n\n[377\\. The Deep Learning Architect’s Handbook](https://sciendo.com/2/v2/download/book/9781803235349.pdf?Token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VycyI6W3sic3ViIjoyNTY3ODUxNywicHVicmVmIjoiNzY0NDg4IiwibmFtZSI6Ikdvb2dsZSBHb29nbGVib3QgLSBXZWIgQ3Jhd2xlciBTRU8iLCJ0eXBlIjoiaW5zdGl0dXRpb24iLCJsb2dvdXRfbGluayI6Imh0dHBzOi8vY29ubmVjdC5saWJseW54LmNvbS9sb2dvdXQvNjgxNGVjMDA1NjQyNDU4NTNmYWM1MGY5NGQ5YjhhYzEiLCJhdXRoX21ldGhvZCI6ImlwIiwiaXAiOiI2Ni4yNDkuNjkuMTY0IiwiY291bnRlcnBhcnR5X2lkIjoiNzY0NDg4In1dLCJpYXQiOjE3NDYyMDQ1MzgsImV4cCI6MTc0NzQxNDEzOH0.bdjdGpb-jtyohxzNFt27be4Y5dIZiHenuE2wa5i3rY8)\n\n[378\\. Benjamin L. Badger. “Masked Mixers for Language Generation and Retrieval.”](https://arxiv.org/abs/2409.01482)\n\n[379\\. Hangbo Bao, Li Dong et al. “UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training.” ArXiv](https://arxiv.org/abs/2002.12804)\n\n[380\\. Fine-Tuning Transformer-Based Language Models](https://ymeadows.com/fr-articles/fine-tuning-transformer-based-language-models)\n\n[381\\. Tomas Mikolov, I. Sutskever et al. “Distributed Representations of Words and Phrases and their Compositionality.” Neural Information Processing Systems](https://arxiv.org/abs/1310.4546)\n\n[382\\. Tomas Mikolov, Kai Chen et al. “Efficient Estimation of Word Representations in Vector Space.” International Conference on Learning Representations](https://arxiv.org/abs/1301.3781)\n\n[383\\. Kenneth Ward Church, Patrick Hanks. “Word Association Norms, Mutual Information, and Lexicography.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.3115/981623.981633)\n\n[384\\. T. Dunning. “Accurate Methods for the Statistics of Surprise and Coincidence.” Comput. Linguistics](https://www.semanticscholar.org/paper/025464b73f805e76689a7a20a48a9e9c0f4ff3ef)\n\n[385\\. A. Gliozzo, B. Magnini et al. “Unsupervised Domain Relevance Estimation for Word Sense Disambiguation.” Conference on Empirical Methods in Natural Language Processing](https://www.semanticscholar.org/paper/6425711c49505722e3b805a384e69266fb29a59a)\n\n[386\\. Generation of domain-specific vocabulary set and classification of documents: weight-inclusion approach | International Journal of Information Technology](https://link.springer.com/article/10.1007/s41870-021-00830-8)\n\n[387\\. J. Justeson, Slava M. Katz. “Technical terminology: some linguistic properties and an algorithm for identification in text.” Natural Language Engineering](https://doi.org/10.1017/S1351324900000048)\n\n[388\\. Specificities and other applications of the Fisher’s exact test to textual data: What’s the matter with lexical frequencies?](https://cnrs.hal.science/hal-04874716v1/document)\n\n[389\\. Evaluation and Comparison of Mathematical Achievement: Dimensions and Perspectives](http://matematikdidaktik.org/wp-content/uploads/2021/07/MADIF8.pdf)\n\n[390\\. Simple Maths with Keywords and Terms](https://www.sketchengine.eu/documentation/simple-maths/)\n\n[391\\. Reshmi P. Rajan, Deepa V. Jose. “A Survey on Domain-Specific Summarization Techniques.” Advances in Data and Information Sciences](https://doi.org/10.1007/978-981-16-5689-7_31)\n\n[392\\. Peter Spyns, A. J. Pretorius et al. “Evaluating DOGMA-lexons Generated Automatically from a Text Corpus.” LSTKM@EKAW](https://www.semanticscholar.org/paper/39cebc2616c3817549c9ec44de6f8744a3d7fbb2)\n\n[393\\. S. Ishikawa, K. Chujo et al. “EVALUATING STATISTICALLY-EXTRACTED DOMAIN-SPECIFIC WORD LISTS.”](https://www.semanticscholar.org/paper/458cceb487c955b031d5af3c5242c071a2923829)\n\n[394\\. Computational Methods in Linguistics](https://courses.washington.edu/cmling/Week7.pdf)\n\n[395\\. Addressing Class Imbalance in Grammatical Error Detection with Evaluation Metric Optimization](http://ltrc.iiit.ac.in/icon2015/icon2015_proceedings/PDF/02-57.pdf)\n\n[396\\. TOWARDS COMPUTATIONAL METHODS FOR PROACTIVELY SUPPORTING HEALTHIER ONLINE DISCUSSIONS](http://www.cs.cornell.edu/~cristian/papers/chang_thesis.pdf)\n\n[397\\. Extragere de informații din surse de date nestructurate și semi-structurate / Information extraction from unstructured and semi-structured sources](https://www.racai.ro/media/Information-extraction-from-unstructured-and-semi-structured-sources.pdf)\n\n[398\\. Mastering Evaluation Metrics in Linguistics](https://www.numberanalytics.com/blog/ultimate-guide-evaluation-metrics-linguistic-analysis)\n\n[401\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[402\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[403\\. GitHub - chenqing24/LLaMA-Factory: Unified Efficient F...](https://github.com/chenqing24/LLaMA-Factory)\n\n[404\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[405\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[406\\. I. Loshchilov, F. Hutter. “Decoupled Weight Decay Regularization.” International Conference on Learning Representations](https://www.semanticscholar.org/paper/d07284a6811f1b2745d91bdb06b040b57f226882)\n\n[407\\. EMNLP 2024 corrections (#4024) · acl-org/acl-antholog...](https://github.com/acl-org/acl-anthology/commit/91c4a88973679981eb7994d3ea83fa949f2bfbf7)\n\n[408\\. Enhancing Large Language Models through Adaptive Tokenizers](https://openreview.net/pdf?id=3H1wqEdK4z)\n\n[409\\. Pei Ke](https://dblp.org/pid/10/2179)\n\n[410\\. Findings of the Association for Computational Linguistics: ACL 2024](https://www.proceedings.com/content/075/075970webtoc.pdf)\n\n[411\\. Estimating Knowledge in Large Language Models Without Generating a Single Token](https://openreview.net/pdf/637737531251227f53f7ed7c0ccfd765fa3c304b.pdf)\n\n[412\\. Findings of EMNLP 2024](https://www.proceedings.com/content/077/077401webtoc.pdf)\n\n[413\\. 实验室1篇文章被Findings of EMNLP录用](https://www.scholat.com/org/orgPost.html?id=1604)\n\n[414\\. EMNLP 2024: Miami, FL, USA - Industry Track](https://dblp.org/db/conf/emnlp/emnlp2024i)\n\n[415\\. EMNLP.2024 - Main](https://papers.cool/venue/EMNLP.2024?group=Main)\n\n[416\\. ACL.2024](https://papers.cool/venue/ACL.2024)\n\n[417\\. Fu-Ming Guo, Sijia Liu et al. “Reweighted Proximal Pruning for Large-Scale Language Representation.” ArXiv](https://arxiv.org/abs/1909.12486)\n\n[418\\. EMNLP.2024 - Industry Track](https://papers.cool/venue/EMNLP.2024?group=Industry%20Track)\n\n[419\\. Refine Large Language Model Fine-tuning via Instruction Vector](https://arxiv.org/pdf/2406.12227)\n\n[420\\. Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER](https://openreview.net/pdf/7fafb50e52db23b0f8f5dbd49d807359ab8ef25e.pdf)"
  },
  {
    "id": 19,
    "question": "How to understand the role of FFNs in Transformers?",
    "response": "**Research Report: Understanding the Role of Feed-Forward Networks in Transformers**\n\n**Report Date:** July 26, 2025\n\n**Author:** Expert AI Research Assistant\n\n**1\\. Introduction: The Unsung Workhorse of the Transformer**\n\nThe Transformer architecture has fundamentally reshaped the landscape of natural language processing and artificial intelligence. While the multi-head self-attention mechanism is often celebrated as its most revolutionary component, the Feed-Forward Network (FFN) plays an equally indispensable, albeit less heralded, role. Positioned within each layer of the Transformer's encoder and decoder stacks, the FFN serves as a critical non-linear processing unit that enriches the representations learned by the attention mechanism. Without FFNs, the Transformer would be a linear model, severely limiting its capacity to capture the complex, hierarchical patterns inherent in data like human language.\n\nThis report provides a comprehensive analysis of the role, architecture, and evolution of Feed-Forward Networks within Transformer models. We will begin by examining the foundational mathematical principles and purpose of the standard FFN. We will then compare its implementation across seminal models like BERT, GPT, and T5, highlighting key architectural distinctions. Subsequently, the report will delve into the primary driver for FFN innovation—computational efficiency—exploring the rise of sparse architectures like Mixture-of-Experts (MoE). A significant portion of this analysis is dedicated to a deep dive into the performance trade-offs, including detailed benchmarks on latency, computational cost, and energy consumption across different hardware platforms. Finally, we will address the practical deployment challenges of these advanced FFNs and survey recent research trends that continue to redefine this crucial component.\n\n**2\\. Foundational Role and Mathematical Formulation of FFNs**\n\nThe primary purpose of the Feed-Forward Network in a Transformer is to introduce non-linearity and increase the model's representational power. After the self-attention layer aggregates information from across the input sequence, the FFN acts as a \"post-processing\" step, applying a complex, non-linear transformation to each token's representation individually. This allows the model to extract more sophisticated features and model complex relationships that the attention mechanism alone cannot capture (Query: Explain the mathematical formulation and purpose of Feed-Forward Networks within Transformer architecture.?).\n\n**2.1. Mathematical Structure**\n\nAt its core, the FFN is a simple two-layer, fully connected neural network. It is applied \"position-wise,\" meaning the exact same network (with the same weights) is applied independently to each position's vector representation in the sequence. This design choice is crucial for enabling massive parallelization during computation (Query: Explain the mathematical formulation and purpose of Feed-Forward Networks within Transformer architecture.?).\n\nThe mathematical operation for an input vector **x** of dimension can be defined as:\n\nWhere:\n\n**First Linear Layer (Up-projection):** The input vector **x** (of dimension ) is multiplied by a weight matrix (dimension ) and added to a bias vector (dimension ). This projects the representation into a higher, intermediate dimension, . This hidden dimension, , is typically 2 to 8 times larger than the model's primary dimension, , allowing the network to learn a richer set of features (Query: Explain the mathematical formulation and purpose of Feed-Forward Networks within Transformer architecture.?).\n\n**Non-linear Activation Function:** The result of the first layer is passed through a non-linear activation function, such as ReLU () or GELU (Gaussian Error Linear Unit). This step is what injects the essential non-linearity into the model (Query: Explain the mathematical formulation and purpose of Feed-Forward Networks within Transformer architecture.?). The FFN is a crucial component for capturing non-linear relationships within the model \\[58\\].\n\n**Second Linear Layer (Down-projection):** The activated, high-dimensional representation is then multiplied by a second weight matrix (dimension ) and added to a bias vector (dimension ). This projects the representation back down to the original model dimension, , so it can be passed to the next layer of the Transformer stack (Query: Explain the mathematical formulation and purpose of Feed-Forward Networks within Transformer architecture.?).\n\n**2.2. Integration within the Transformer Block**\n\nThe FFN does not operate in isolation. It is part of a larger sub-layer block that includes two other critical operations:\n\n**Residual Connection:** The input to the FFN sub-layer is added to the output of the FFN itself. This \"shortcut\" or residual connection is vital for enabling the training of very deep networks by preventing the vanishing gradient problem and easing the flow of information (Query: Explain the mathematical formulation and purpose of Feed-Forward Networks within Transformer architecture.?).\n\n**Layer Normalization (LN):** Layer Normalization is applied to the output of the residual connection. It stabilizes the training process by normalizing the inputs to each layer across the feature dimension. The exact placement of LayerNorm (before or after the sub-layer) has become a key architectural differentiator, as we will explore next (Query: Explain the mathematical formulation and purpose of Feed-Forward Networks within Transformer architecture.?).\n\n**Dropout:** To prevent overfitting, dropout is often applied for regularization, typically between the two linear layers within the FFN (Query: Explain the mathematical formulation and purpose of Feed-Forward Networks within Transformer architecture.?).\n\n**3\\. FFN Implementations Across Major Transformer Architectures**\n\nWhile the basic two-layer structure of the FFN is consistent, its specific implementation details, particularly regarding activation functions and normalization placement, vary across major Large Language Models (LLMs) like BERT, GPT, and T5. These subtle differences can have a significant impact on model performance, stability, and training dynamics.\n\n**3.1. Architectural Fingerprints: BERT, GPT, and T5**\n\nThese three models represent the canonical Transformer architectures: encoder-only (BERT), decoder-only (GPT), and encoder-decoder (T5) \\[32\\]\\[32\\]\\[66\\]. The FFN is a key component in all three \\[32\\].\n\n**BERT (Bidirectional Encoder Representations from Transformers):** As an encoder-only model, BERT is designed to build deep bidirectional representations \\[23\\]\\[64\\]. Its FFN implementation uses the **ReLU** activation function \\[67\\]\\[78\\]. For the BERT-base model, the FFN's inner dimension (ffn-hidden-size) is typically set to 4 times the model's hidden dimension \\[34\\].\n\n**GPT (Generative Pre-trained Transformer):** As a decoder-only, autoregressive model, GPT is built for sequential generation tasks \\[31\\]\\[66\\]\\[76\\]. Early versions used ReLU, but later versions, especially GPT-2 and beyond, are often associated with the smoother **GELU** activation function \\[24\\]. Similar to BERT, the default FFN hidden size is 4 times the main hidden size \\[34\\].\n\n**T5 (Text-to-Text Transfer Transformer):** T5 uses a full encoder-decoder architecture, framing all NLP tasks as text-to-text problems \\[36\\]\\[65\\]\\[74\\]. While its FFN follows the standard pattern, T5 offers more configuration flexibility, allowing the ffn-hidden-size to be set independently rather than being fixed to a multiple of the hidden size, unlike the defaults in BERT and GPT \\[34\\]. T5 also introduced a simplified version of Layer Normalization where only rescaling is performed and the additive bias is removed, which enhances training stability \\[66\\]\\[71\\]\\[74\\].\n\n**3.2. The Great Divide: Pre-Norm vs. Post-Norm**\n\nOne of the most significant architectural variations is the placement of the Layer Normalization step relative to the FFN and attention sub-layers.\n\n**Post-LayerNorm (Original Transformer & BERT):** The original \"Attention is All You Need\" paper and the BERT model apply Layer Normalization _after_ the residual connection. The data flow is x + SubLayer(x), followed by LayerNorm(). Specifically for the FFN in BERT, the output of the feed-forward layer is added to its input, and then layer normalization is applied to the sum \\[67\\]\\[70\\]. The schematic is: x -> SubLayer -> Dropout -> Add(x) -> LayerNorm.\n\n**Pre-LayerNorm (GPT-2/3 & T5):** Later research found that applying Layer Normalization _before_ the sub-layer leads to more stable training for deeper models. This \"pre-norm\" configuration was adopted in GPT-2 and subsequent models like GPT-3 \\[146\\]\\[206\\]\\[262\\]. The data flow becomes LayerNorm(x), which is then passed to the sub-layer, followed by the residual addition: x + SubLayer(LayerNorm(x)). Code snippets and documentation confirm this placement for GPT models, showing x = x + ffn(layer_norm(x, ...)) \\[159\\]\\[263\\]\\[321\\]. An additional LayerNorm is also sometimes added after the final self-attention block in the GPT architecture \\[146\\]\\[206\\]\\[262\\]. T5 also adopts a \"pre-norm\" positioning, as indicated by configuration flags like --layernorm_positioning pre \\[208\\].\n\nThis shift from post-norm to pre-norm represents a key evolution in Transformer design, directly impacting the FFN's operational context and contributing to the stability required to scale models to hundreds of billions of parameters.\n\n**4\\. The Drive for Efficiency: From Dense to Sparse FFNs**\n\nFFNs are computationally voracious. In many large-scale Transformer models, the two linear layers of the FFNs can account for a staggering two-thirds of the model's total parameters \\[111\\]\\[113\\]\\[115\\]. As models grew ever larger, the computational and memory costs associated with these dense FFNs became a primary bottleneck for training and inference. This challenge spurred a wave of research focused on optimizing or entirely replacing the standard FFN with more efficient alternatives.\n\nThe central idea behind this optimization is **sparsity**. Instead of activating all neurons in the FFN for every single input token (dense activation), sparse methods aim to activate only a small, specialized subset of neurons (conditional computation) \\[105\\]\\[105\\]\\[110\\]. This reduces the number of floating-point operations (FLOPs) required per token, paving the way for models with vastly more parameters but with a manageable computational cost for inference \\[104\\]\\[105\\]\\[106\\]. The most prominent and successful implementation of this idea is the Mixture-of-Experts (MoE) architecture.\n\n**5\\. The Mixture-of-Experts (MoE) Paradigm**\n\nThe Mixture-of-Experts (MoE) architecture is a powerful technique that replaces the single, dense FFN layer in each Transformer block with a collection of smaller FFNs, called \"experts\" \\[84\\]\\[86\\]\\[88\\].\n\n**5.1. MoE Architecture**\n\nAn MoE layer consists of two main components:\n\n1.  **A Set of Expert Networks:** These are typically standard FFNs themselves. A model might have anywhere from a handful to thousands of these experts per MoE layer \\[57\\].\n2.  **A Gating Network (or Router):** This is a small neural network that takes a token's representation as input and decides which expert(s) should process it. The gating network outputs a probability distribution over the experts, and typically, only the top-k experts (often k=1 or k=2) are selected for activation \\[92\\]\\[168\\]. The final output for the token is a weighted average of the outputs from the activated experts.\n\nThis design allows MoE models to decouple the total parameter count from the computational cost. A model can have trillions of parameters in its expert layers, but for any given input token, only a tiny fraction of those parameters are actually used \\[91\\]\\[125\\]\\[131\\]. This approach has been successfully implemented in models like Google's Switch Transformer and GLaM \\[50\\]\\[54\\].\n\n**5.2. Benefits and Challenges of MoE**\n\nThe primary benefit of MoE is achieving significantly higher model capacity (and often, performance) for the same computational budget during inference \\[89\\]\\[95\\]\\[98\\]. However, this comes with its own set of practical challenges, including:\n\n**High Memory Footprint:** Although only a few experts are active at a time, all expert parameters must be loaded into memory, which can be a significant challenge for deployment \\[132\\]\\[163\\]\\[193\\].\n\n**Load Balancing:** A critical issue during training is ensuring that all experts receive a roughly equal number of tokens to process. If the gating network consistently favors a few experts, others become under-trained and the benefits of the architecture are lost. This \"expert imbalance\" is often mitigated using an auxiliary loss function that encourages a uniform distribution of tokens across experts \\[168\\]\\[169\\].\n\n**Communication Overhead:** In distributed training and inference setups, routing tokens to the correct experts, which may reside on different hardware nodes, can introduce significant communication overhead \\[163\\].\n\n**6\\. Performance Analysis and Computational Trade-offs**\n\nThe shift from dense FFNs to sparse MoE architectures introduces a complex set of performance trade-offs that vary significantly depending on the hardware platform (CPU, GPU, TPU) and specific implementation.\n\n**6.1. Inference Latency: The FLOPs vs. Reality Conflict**\n\nThere are conflicting reports regarding the inference latency of MoE models compared to their dense counterparts.\n\n**The Theory (Fewer FLOPs = Lower Latency):** On paper, MoE models should be much faster. By activating only a small subset of parameters per token, they dramatically reduce the required FLOPs. For example, the LookupFFN (an MoE variant) was shown to achieve a 6.8x FLOP reduction compared to a dense RoBERTa-base model with similar performance \\[132\\]. The Switch Transformer was able to achieve the same performance as a dense T5-Base model in just 1/7th of the time on TPUs \\[237\\].\n\n**The Practice (Overhead and Inefficiencies):** In reality, lower FLOPs do not always translate to lower latency. Several real-world factors can make MoE models slower:\n\n**GPU Inefficiency:** The process of routing tokens and aggregating results from multiple small expert networks can be inefficient on GPUs, which are optimized for large, dense matrix multiplications. One study noted that MoE models can be up to 15x slower for language modeling and 3x slower for machine translation than their FLOP-equivalent dense models \\[134\\]. The computation within fine-grained expert FFNs can be inherently less efficient on GPUs than a single large, dense computation \\[192\\]\\[362\\].\n\n**Memory and Communication Bottlenecks:** On GPU-based systems, a major bottleneck is often not the computation itself, but the time it takes to transfer expert weights from slower CPU memory or storage to the GPU's high-speed memory for execution. This data transfer can take 2-5 times longer than the actual computation on the GPU \\[244\\]\\[301\\]\\[307\\].\n\n**Hardware-Specific Performance:**\n\n**CPU:** On CPUs, MoE expert execution latency tends to increase linearly with the number of input tokens, and data transfer from a GPU is negligible. For small batch sizes, executing an expert on a CPU can even be faster than waiting to load its weights onto a GPU \\[301\\]\\[306\\]\\[301\\].\n\n**GPU:** GPU latency for MoE is often dominated by the weight transfer time, making it less sensitive to the input size but highly sensitive to memory bandwidth and system architecture \\[242\\]\\[368\\].\n\n**TPU:** TPUs are highly optimized for large-scale matrix operations and high-throughput data pipelines. MoE models like the Switch Transformer have shown significant training speedups on TPUs, suggesting the hardware is well-suited to handle the MoE workload, likely due to high memory bandwidth and efficient inter-chip communication \\[237\\]\\[250\\]\\[359\\].\n\nSystems like _Fiddler_ are being developed to intelligently orchestrate MoE inference across CPU-GPU systems, prefetching expert weights to hide latency and achieving significant speedups \\[242\\]\\[248\\]\\[306\\].\n\n**6.2. Energy Consumption: The Efficiency Frontier**\n\nQuantifying the precise energy consumption difference between sparse and standard FFNs is challenging, and the search results lack direct empirical comparisons. However, we can synthesize a clear picture based on hardware capabilities and general principles.\n\n**Measurement and Methodology:** Accurate energy measurement requires real-time power sensors or high-frequency sampling using tools like nvidia-smi, calculating energy as a function of power and throughput (e.g., tokens/second) \\[279\\]\\[281\\]\\[286\\]. Simple estimates based on a GPU's Thermal Design Power (TDP) can be highly inaccurate \\[285\\]\\[402\\].\n\n**Hardware Support for Sparsity:** NVIDIA's H100 GPU represents a significant leap over the A100 for AI workloads, especially Transformers. The H100 offers 1.5x to 30x higher inference throughput on large language models \\[283\\]\\[284\\]\\[340\\]. Crucially, the H100 includes a Transformer Engine with support for FP8 precision and Tensor Cores that can provide up to a **2x performance increase specifically for sparse models** \\[290\\]\\[351\\].\n\n**Inference and Efficiency:** While the H100 has a higher raw power draw (up to 700W vs. 400W for the A100), its vastly superior performance means it often delivers better energy efficiency (performance-per-watt) on large AI workloads \\[283\\]\\[348\\]. The principle of sparsity suggests that by reducing computation (fewer FLOPs), sparse FFNs should inherently consume less energy per inference, provided the overheads are managed. The performance gains from techniques like pruning and quantization have been shown to lead to 3x to 7x better energy efficiency in general deep learning models \\[358\\]\\[396\\].\n\nAlthough direct comparative data is missing, the evidence strongly suggests that sparse FFN implementations, especially when run on next-generation hardware like the H100 that is optimized for sparsity, will be significantly more energy-efficient per inference than their standard dense counterparts.\n\n**7\\. Practical Deployment and Recent Research Trends**\n\nBeyond raw performance, the practical challenges of deploying FFN-optimized models and the ongoing direction of research are critical for understanding their role.\n\n**7.1. Deployment Challenges for Sparse FFNs**\n\n**Hardware and Communication:** The massive parameter counts of MoE models necessitate large, distributed hardware setups. This creates significant communication overhead and load balancing problems that must be solved for efficient inference \\[163\\]\\[168\\].\n\n**Gating Optimization:** The simple gating networks used in current MoE models may not be optimal, and research continues into more intelligent routing mechanisms \\[169\\].\n\n**Knowledge Distillation:** One strategy to circumvent the deployment complexity of large MoE models is to use them as powerful \"teacher\" models to train smaller, dense \"student\" models that are easier to deploy, a process known as knowledge distillation \\[163\\].\n\n**7.2. Recent Research Trends (Post-2023)**\n\nThe FFN remains a hotbed of research, with several key trends emerging:\n\n**Advanced Activation Functions:** The SwiGLU activation function, a type of Gated Linear Unit, is increasingly replacing ReLU and GELU. It has been shown to increase activation sparsity while maintaining or improving performance \\[43\\]\\[53\\]\\[58\\].\n\n**Parameter Sharing and Pruning:** To reduce model size, researchers are exploring more sophisticated parameter sharing schemes across Transformer layers, such as in EdgeFormer \\[59\\]. Fine-grained pruning techniques like FINERCUT allow for the removal of entire FFN layers with minimal impact on accuracy \\[44\\]\\[44\\].\n\n**Architectural Alternatives:** Some research is replacing the standard FFN with entirely new structures. This includes Convolutional Feed-Forward Networks (CFFN) that use 1x1 convolutions \\[56\\] and product key memory layers that act like sparse FFNs with enormous hidden states \\[47\\].\n\n**Normalization and Positional Embeddings:** Innovations continue outside the FFN itself but affect its input. RMSNorm is being adopted as a more efficient normalization technique \\[41\\]\\[43\\]and Rotary Positional Embeddings (RoPE) are replacing static positional encodings for better length extrapolation \\[41\\].\n\n**8\\. Conclusion**\n\nThe Feed-Forward Network, once a straightforward component of the Transformer architecture, has evolved into a central focus of research and optimization. Its fundamental role remains the same: to provide the critical non-linear transformations that give Large Language Models their profound representational power. However, our understanding of its implementation has matured significantly. The architectural shift from post-norm to pre-norm stabilization was a key enabler for scaling models, while variations in activation functions and hidden dimensions provide distinct performance profiles for models like BERT, GPT, and T5.\n\nThe most transformative development has been the paradigm shift from dense FFNs to sparse, conditionally activated networks, epitomized by the Mixture-of-Experts architecture. This innovation brilliantly decouples model size from computational cost, allowing for the creation of models with trillions of parameters that remain computationally tractable. Yet, this power comes with complex trade-offs in latency, memory usage, and deployment complexity, creating a new frontier of challenges that depend heavily on the underlying hardware. As GPUs like the NVIDIA H100 begin to offer native acceleration for sparsity, the practical benefits of these advanced FFNs are poised to become even more pronounced.\n\nLooking forward, the FFN will likely continue to be a dynamic site of innovation. The exploration of new activation functions, parameter-efficient architectures, and tighter hardware-software co-design will push the boundaries of what is possible, ensuring this once-simple workhorse remains at the cutting edge of artificial intelligence.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[2\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[3\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[4\\. Transformer详解及代码实现](https://zhuanlan.zhihu.com/p/625925222)\n\n[5\\. Learning Novel Transformer Architecture for Time-series Forecasting](https://arxiv.org/pdf/2502.13721)\n\n[6\\. 17.4. Position-Wise Feed-Forward Network :: Hironobu SUZUKI @ InterDB](http://www.interdb.jp/dl/part04/ch17/sec04.html)\n\n[7\\. Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training](https://proceedings.neurips.cc/paper_files/paper/2024/file/143ea4a156ef64f32d4d905206cf32e1-Paper-Conference.pdf)\n\n[8\\. TOWARDS A UNIFIED VIEW OF SPARSE FEED-FORWARD NETWORK IN TRANSFORMER](https://openreview.net/pdf?id=lX478WYy0Up)\n\n[9\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[10\\. The Transformer: motivation, original architecture and attention mechanism](https://bimsa.net/doc/notes/51257.pdf?id=0.08336732489988208)\n\n[11\\. D. Pau, F. M. Aymone. “Mathematical Formulation of Learning and Its Computational Complexity for Transformers’ Layers.” Eng](https://doi.org/10.3390/eng5010003)\n\n[12\\. 02-AI大模型(二)--Transformer框架- 赵家小伙儿](https://www.cnblogs.com/zhaopengpeng/p/18868706)\n\n[13\\. Feed Forward Neural Network in Transformers](https://www.tutorialspoint.com/gen-ai/feed-forward-neural-network-in-transformers.htm)\n\n[14\\. Attention is all you need. Arquitectura Transformers: descripción y aplicaciones](https://dspace.umh.es/bitstream/11000/30273/1/TFG-Nasimba%20Tipan%2C%20Alexis%20Fabian.pdf)\n\n[15\\. A Comprehensive Survey of Recent Transformers in Image, Video and Diffusion Models](https://www.techscience.com/files/cmc/2024/TSP_CMC-80-1/TSP_CMC_50790/TSP_CMC_50790.epub)\n\n[16\\. Neural Architecture Search for Transformers: A Survey](https://ieeexplore.ieee.org/ielaam/6287639/9668973/9913476-aam.pdf)\n\n[17\\. Transformer Architecture](https://self-supervised.cs.jhu.edu/sp2025/files/slides/10-11.transformers.pdf)\n\n[18\\. A Deep Dive into the Revolutionary Transformer Architecture](https://towardsai.net/p/machine-learning/attention-is-all-you-need-a-deep-dive-into-the-revolutionary-transformer-architecture)\n\n[19\\. Lecture notes on attention & transformers](http://yingzhenli.net/home/pdf/imperial_dlcourse2022_attention_notes.pdf)\n\n[20\\. ON THE EFFICIENCY OF TRANSFORMERS](https://asset.library.wisc.edu/1711.dl/L5G4PGWKGIGKX8Q/R/file-99b91.pdf)\n\n[21\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[22\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[23\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[24\\. GROUPBERT: ENHANCED TRANSFORMER ARCHITECTURE WITH EFFICIENT GROUPED STRUCTURES](https://openreview.net/pdf?id=eYyvftCgtD)\n\n[25\\. Comparing Transformer Architectures for Sentiment Analysis: A Study of BERT, GPT, and T5](https://ijirt.org/publishedpaper/IJIRT167124_PAPER.pdf)\n\n[26\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[27\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[28\\. \\[bert、t5、gpt\\] 05 构建 TransformerDecoderLayer（FFN 与 Masked MultiHeadAttention）](https://www.bilibili.com/video/av613003003?t=783)\n\n[29\\. Advancing Cross-Domain Fake News Detection: Enhanced Models to Improve Generalization and Tackle the Class Imbalance Problem](https://ruor.uottawa.ca/bitstreams/2d071148-bfe8-40be-88c1-c9be567ef558/download)\n\n[30\\. \\[bert、t5、gpt\\] 09 T5 整体介绍（t5-11b，T5ForConditionalGeneration）\\_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Lh4y1b74i/?p=21)\n\n[31\\. 文化和科技融合热点观察](http://cdn.aqifun.com/UE/20230612111700_636.pdf)\n\n[32\\. REVOLUTIONISING TRANSLATION TECHNOLOGY: A COMPARATIVE STUDY OF VARIANT TRANSFORMER MODELS - BERT, GPT AND T5](https://www.cseij.org/papers/v14n3/14324cseij02.pdf)\n\n[33\\. AI技术干货|大语言变换器模型的架构及其工作原理介绍（上篇）](https://lmtw.com/mzw/content/detail/id/229711/keyword_id/82)\n\n[34\\. bigcode-project/Megatron-LM - GitHub](https://github.com/bigcode-project/Megatron-LM)\n\n[35\\. Salman Hameed Khan, Muzammal Naseer et al. “Transformers in Vision: A Survey.” ACM Computing Surveys (CSUR)](https://doi.org/10.1145/3505244)\n\n[36\\. Transformers](https://www.cs.toronto.edu/~raeidsaqur/csc401/lectures/6_Transformers.pdf)\n\n[41\\. TIME-MoE: BILLION-SCALE TIME SERIES FOUNDATION MODELS WITH MIXTURE OF EXPERTS](https://arxiv.org/pdf/2409.16040v1)\n\n[42\\. EQUIVARIANT NEURAL FUNCTIONAL NETWORKS FOR TRANSFORMERS](https://openreview.net/pdf?id=uBai0ukstY)\n\n[43\\. Deep representation learning for time series forecasting](https://ink.library.smu.edu.sg/context/etd_coll/article/1648/viewcontent/GPIS_AY2020_PhD_Woo_Jiale_Gerald.pdf)\n\n[44\\. FINERCUT: Finer-grained Interpretable Layer Pruning for Large Language Models](https://arxiv.org/pdf/2405.18218)\n\n[45\\. Probing Information Distribution in Transformer ...](https://www.xueshuxiangzi.com/downloads/2025_7_22/2507.15347.pdf)\n\n[46\\. Shared-Adapters: A Novel Transformer-based Parameter Efficient Transfer Learning Approach For Children’s Automatic Speech Recognition](https://www.isca-archive.org/interspeech_2024/rolland24b_interspeech.pdf)\n\n[47\\. Large Memory Layers with Product Keys](http://papers.neurips.cc/paper/9061-large-memory-layers-with-product-keys.pdf)\n\n[48\\. Transformer, BERT, ViT](https://ufal.mff.cuni.cz/~straka/courses/npfl138/2425/slides.pdf/npfl138-2425-10.pdf)\n\n[49\\. RMP-SAM: TOWARDS REAL-TIME MULTI-PURPOSE SEGMENT ANYTHING](https://openreview.net/pdf/7f0ac3fb4a1b053650422435f7b1b591d7ad72aa.pdf)\n\n[50\\. W. Fedus, Barret Zoph et al. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” J. Mach. Learn. Res.](https://arxiv.org/abs/2101.03961)\n\n[51\\. UniForm: A Reuse Attention Mechanism for Efficient Transformers on Resource-Constrained Edge Devices](https://openreview.net/pdf?id=rjnKCFZuJt)\n\n[52\\. MoE++: ACCELERATING MIXTURE-OF-EXPERTS METHODS WITH ZERO-COMPUTATION EXPERTS](https://openreview.net/pdf/0df56faa526c1662d006cf6e06a756a87f139a17.pdf)\n\n[53\\. Designing Large Foundation Models for Efficient Training and Inference: A Survey](https://arxiv.org/pdf/2409.01990)\n\n[54\\. EPT-MoE: Toward Efficient Parallel Transformers with Mixture-of-Experts for 3D Hand Gesture Recognition](https://avestia.com/EECSS2024_Proceedings/files/paper/MVML/MVML_105.pdf)\n\n[55\\. Learning Where to Edit Vision Transformers](https://proceedings.neurips.cc/paper_files/paper/2024/file/f269520d5f111719dd82b36667997d49-Paper-Conference.pdf)\n\n[56\\. Guyang Zhang, Waleed Abdulla. “Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems.”](https://arxiv.org/abs/2506.08596)\n\n[57\\. Part-Of-Speech Sensitivity of Routers in Mixture of Experts Models](https://hal.science/hal-04918227v1/file/2025.coling-main.431.pdf)\n\n[58\\. What is Feed-Forward Network (FFN)? | Vstorm Glossary](https://vstorm.co/glossary/feed-forward-network-ffn/#:~:text=LLM,model%20expressiveness%20and%20learning%20capacity.)\n\n[59\\. Sheng Wang, Liheng Chen et al. “MoS: Unleashing Parameter Efficiency of Low-Rank Adaptation with Mixture of Shards.”](https://arxiv.org/abs/2410.00938)\n\n[61\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[62\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[63\\. Sepp Hochreiter, J. Schmidhuber. “Long Short-Term Memory.” Neural Computation](https://doi.org/10.1162/neco.1997.9.8.1735)\n\n[64\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[65\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[66\\. Study of ChatGPT and its Comparison with Other Mainstream Large Language Models](http://www.ijklp.org/archives/vol13no2/Study%20of%20ChatGPT%20and%20its%20Comparison%20with%20Other%20Mainstream%20Large%20Language%20Models.pdf)\n\n[67\\. CS 224N: Default Final Project: minBERT and Downstream Tasks](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1234/project/default-final-project-bert-handout.pdf)\n\n[68\\. Attention Mechanism, Transformers, BERT, and GPT: Tutorial and Survey](https://hal.science/hal-04637647/document)\n\n[69\\. 生物信息学](https://xue.biocuckoo.cn/course/bioinfo/chap0.pdf)\n\n[70\\. A study on the embedding spaces of the BERT language model](https://luisto.fi/documents/2024_Luisto_NLP_gradu.pdf)\n\n[71\\. 预训练语言模型（BERT，GPT，BART系列） - 行麦科技](https://aihomecaring.com/?jishu/71.html)\n\n[72\\. Comparing Transformer Architectures for Sentiment Analysis: A Study of BERT, GPT, and T5](https://ijirt.org/publishedpaper/IJIRT167124_PAPER.pdf)\n\n[73\\. Feed Forward Networks](https://deeplearning4j.konduit.ai/en-1.0.0-beta7/getting-started/tutorials/feed-forward-networks)\n\n[74\\. T5模型的架构与应用](https://www.jianshu.com/p/627d4643f7a7)\n\n[75\\. A Review on BERT and Its Implementation in Various NLP Tasks](https://www.atlantis-press.com/article/125986264.pdf)\n\n[76\\. 文化和科技融合热点观察](http://cdn.aqifun.com/UE/20230612111700_636.pdf)\n\n[77\\. Neural Network Architectures and Activation Functions: A Gaussian Process Approach](https://mediatum.ub.tum.de/doc/1402689/106621.pdf)\n\n[78\\. CVer从0入门NLP(三)———GPT、BERT模型_gpt 分句符\\[sep\\]-...](https://blog.csdn.net/qq_47233366/article/details/135695766)\n\n[79\\. T5& GPT& Bert比较| Limited AI - Machine Learning](https://ai.younglimit.com/deep-learning/attention-mechanisms-and-transformers/large-scaling-pretraning-with-transformers/t5-gpt-bert-bi-jiao)\n\n[81\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[82\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[83\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[84\\. Dynamic Mixture of Experts for Adaptive Computation in ...](https://www.mdpi.com/2078-2489/16/6/483)\n\n[85\\. QuantMoE-Bench: Examining Post-Training Quantization for Mixture-of-Experts](https://arxiv.org/pdf/2406.08155)\n\n[86\\. EXAMINING POST-TRAINING QUANTIZATION FOR MIXTURE-OF-EXPERTS: A BENCHMARK](https://openreview.net/pdf?id=sMwYn2lZjO)\n\n[87\\. Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark](https://openreview.net/pdf/2f52a3f2e1fb1e9a3ce45fb942d6529399d0d086.pdf)\n\n[88\\. AUTOMATED FINE-GRANINED MIXTURE-OF-EXPERTS QUANTIZATION](https://openreview.net/pdf?id=etxbRucurT)\n\n[89\\. MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models](https://jaewoong.org/pubs/dac24-monde.pdf)\n\n[90\\. Mixture of Experts, State Space Models](https://www.webpages.uidaho.edu/vakanski/Courses/Adversarial_Machine_Learning/Spring_2024/Lecture_Mixture_of_Experts,State_Space_Models.pdf)\n\n[91\\. Performance of Transformers](https://courses.cs.washington.edu/courses/cse599k/24au/content/02-Transformer-Performance.pdf)\n\n[92\\. Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models](https://openreview.net/pdf?id=B0wvJYPT7M)\n\n[93\\. MIXTURE OF EXPERTS WITH MIXTURE OF PRECISIONS FOR TUNING QUALITY OF SERVICE](https://arxiv.org/pdf/2407.14417)\n\n[94\\. Mixture of Experts for Time Series Foundation Models](https://openreview.net/pdf?id=96znOY96W1)\n\n[95\\. MoNDE: Mixture of Near-Data Experts for Large-Scale Sp...](http://arxiv.org/html/2405.18832v1)\n\n[96\\. Jiaao He, J. Qiu et al. “FastMoE: A Fast Mixture-of-Expert Training System.” ArXiv](https://arxiv.org/abs/2103.13262)\n\n[97\\. ON THE EFFICIENCY OF TRANSFORMERS](https://asset.library.wisc.edu/1711.dl/L5G4PGWKGIGKX8Q/R/file-99b91.pdf)\n\n[98\\. Exploiting Activation Sparsity with Dense to Dynamic-k Mixture-of-Experts Conversion](https://proceedings.neurips.cc/paper_files/paper/2024/file/4c2092ec0b1370cce3fb5965ab255fae-Paper-Conference.pdf)\n\n[101\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[102\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[103\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[104\\. POLYNOMIAL COMPOSITION ACTIVATIONS: UNLEASHING THE DYNAMICS OF LARGE LANGUAGE MODELS](https://openreview.net/pdf/a8143076104822f06f126819621bce4f41c618c6.pdf)\n\n[105\\. ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models](https://imirzadeh.me/publication/relu-act-sparse/iclr_oral_slides.pdf)\n\n[106\\. ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models](https://arxiv.org/pdf/2402.13516v3)\n\n[107\\. Leo Liu, Tim Dettmers et al. “Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model.” ArXiv](https://doi.org/10.48550/arXiv.2305.13999)\n\n[108\\. Towards A Unified View of Sparse Feed-Forward Network ...](https://arxiv.org/abs/2305.13999)\n\n[109\\. Large language models (LLMs): survey, technical frameworks, and future challenges](https://link.springer.com/content/pdf/10.1007/s10462-024-10888-y.pdf)\n\n[110\\. Training-Free Activation Sparsity in Large Language Models](https://openreview.net/pdf?id=dGVZwyq5tV)\n\n[111\\. Nobel Dhar, Bobin Deng et al. “Activation Sparsity Opportunities for Compressing General Large Language Models.”](https://arxiv.org/abs/2412.12178)\n\n[112\\. SPARSING LAW: TOWARDS LARGE LANGUAGE MODELS WITH GREATER ACTIVATION SPARSITY](https://arxiv.org/pdf/2411.02335v1)\n\n[113\\. Activation Sparsity Opportunities for Compressing Gene...](http://arxiv.org/html/2412.12178v2)\n\n[114\\. Using Large Language Models to Better Detect and Handle Software Vulnerabilities and Cyber Security Threats](https://assets-eu.researchsquare.com/files/rs-4387414/v1/a4df3e13-6dea-43f3-8b6a-cd9280283e91.pdf)\n\n[115\\. Nobel Dhar, Bobin Deng et al. “Activation Sparsity Opportunities for Compressing General Large Language Models.”](https://arxiv.org/abs/2412.12178)\n\n[116\\. Zihan Qiu, Zeyu Huang et al. “Unlocking Emergent Modularity in Large Language Models.” Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)](https://doi.org/10.18653/v1/2024.naacl-long.144)\n\n[121\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[122\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[123\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[124\\. Exploiting Activation Sparsity with Dense to Dynamic-k Mixture-of-Experts Conversion](https://proceedings.neurips.cc/paper_files/paper/2024/file/4c2092ec0b1370cce3fb5965ab255fae-Paper-Conference.pdf)\n\n[125\\. Performance of Transformers](https://courses.cs.washington.edu/courses/cse599k/24au/content/02-Transformer-Performance.pdf)\n\n[126\\. ON THE EFFICIENCY OF TRANSFORMERS](https://asset.library.wisc.edu/1711.dl/L5G4PGWKGIGKX8Q/R/file-99b91.pdf)\n\n[127\\. Alex Wang, Amanpreet Singh et al. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” BlackboxNLP@EMNLP](https://doi.org/10.18653/v1/W18-5446)\n\n[128\\. Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning](https://openreview.net/pdf/af78cd8aba64f9aa502655e6144612e50aee4f0d.pdf)\n\n[129\\. MIXTURE OF EXPERTS WITH MIXTURE OF PRECISIONS FOR TUNING QUALITY OF SERVICE](https://arxiv.org/pdf/2407.14417)\n\n[130\\. Zhengyan Zhang, Yankai Lin et al. “MoEfication: Transformer Feed-forward Layers are Mixtures of Experts.” Findings](https://doi.org/10.18653/v1/2022.findings-acl.71)\n\n[131\\. Mixture of Experts Pattern for Transformer Models](https://tinkerd.net/blog/machine-learning/mixture-of-experts/)\n\n[132\\. LookupFFN: Making Transformers Compute-lite for CPU inference](https://proceedings.mlr.press/v202/zeng23a/zeng23a.pdf)\n\n[133\\. W. Fedus, Barret Zoph et al. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” J. Mach. Learn. Res.](https://arxiv.org/abs/2101.03961)\n\n[134\\. Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference](https://arxiv.org/pdf/2303.06182v1)\n\n[135\\. Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark](https://openreview.net/pdf/2f52a3f2e1fb1e9a3ce45fb942d6529399d0d086.pdf)\n\n[136\\. Mixture of Parrots 🦜🦜🦜: Experts Improve Memorization More ...](https://kempnerinstitute.harvard.edu/research/deeper-learning/mixture-of-parrots-experts-improve-memorization-more-than-reasoning/)\n\n[137\\. Transformer Language Models](https://self-supervised.cs.jhu.edu/sp2025/files/slides/12-13.transformers-language-models.pdf)\n\n[138\\. Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices](https://arxiv.org/pdf/2410.02117)\n\n[141\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[142\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[143\\. Sepp Hochreiter, J. Schmidhuber. “Long Short-Term Memory.” Neural Computation](https://doi.org/10.1162/neco.1997.9.8.1735)\n\n[144\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[145\\. Jimmy Ba, J. Kiros et al. “Layer Normalization.” ArXiv](https://arxiv.org/abs/1607.06450)\n\n[146\\. The GPT-3 Architecture, on a Napkin - 在线工具](https://tool.lu/article/5cv/preview)\n\n[147\\. GLCIF: Next POI Recommendation Based on Global-Local Contextual Interaction Fusion](https://researchspace.auckland.ac.nz/bitstreams/d67bf9df-d3dc-4dbc-812e-3ac1d2d63d35/download)\n\n[148\\. layer_layer_normalization: Layer normalization layer (...](https://rdrr.io/github/rstudio/keras/man/layer_layer_normalization.html)\n\n[149\\. CVer从0入门NLP(三)———GPT、BERT模型_gpt 分句符\\[sep\\]-...](https://blog.csdn.net/qq_47233366/article/details/135695766)\n\n[150\\. THE INDIAN JOURNAL OF TECHNICAL EDUCATION](http://www.isteonline.in/Datafiles/cms/Special%20Issues/Spl%20issue%202%20June%202024%20for%20web%20uploading.pdf)\n\n[151\\. A Comprehensive Survey of Recent Transformers in Image, Video and Diffusion Models](https://www.techscience.com/files/cmc/2024/TSP_CMC-80-1/TSP_CMC_50790/TSP_CMC_50790.epub)\n\n[152\\. A Transformer-based network intrusion detection approa...](https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-023-00574-9)\n\n[153\\. 预训练模型使用示例· Tencent/TencentPretrain Wiki](https://github.com/Tencent/TencentPretrain/wiki/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/07d268540e7f4d11c2270f0e3b91299aea4d5f25)\n\n[154\\. Homework Assignment #2](https://www.cs.toronto.edu/~gpenn/csc401/assignments/A2/a2.pdf)\n\n[155\\. Layer normalization — nn_layer_norm • torch](https://torch.mlverse.org/docs/reference/nn_layer_norm)\n\n[156\\. Visualizing and Explaining Transformer Models From the Ground Up](https://deepgram.com/learn/visualizing-and-explaining-transformer-models-from-the-ground-up)\n\n[157\\. Understanding GPT: A Guide to Transformers and Their ...](https://docsbot.ai/article/understanding-gpt-a-guide-to-transformers-and-their-mechanisms)\n\n[158\\. CVer从0入门NLP——GPT是如何一步步诞生的｜社区征文](https://developer.volcengine.com/articles/7316747716900421644)\n\n[159\\. S. Menary, Samuel Kaski et al. “Transformer Normalisation Layers and the Independence of Semantic Subspaces.” ArXiv](https://doi.org/10.48550/arXiv.2406.17837)\n\n[160\\. 从理论到代码: Gpt-2模型的逐步实现和代码解析。 - Ai中文站](https://www.ai-cn.co/post/5427)\n\n[161\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[162\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[163\\. Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference](https://arxiv.org/pdf/2303.06182v1)\n\n[164\\. Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts](https://arxiv.org/pdf/2408.15901)\n\n[165\\. Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners](https://openreview.net/pdf?id=UVSh0d78e7Q)\n\n[166\\. GitHub - facebookresearch/Mixture-of-Transformers: Mixture-of...](https://github.com/facebookresearch/Mixture-of-Transformers)\n\n[167\\. A Mixture-of-Experts Approach for Code Generation On upcycling and sparsifying dense models](https://kth.diva-portal.org/smash/get/diva2:1903655/FULLTEXT01.pdf)\n\n[168\\. Mixture of Experts Pattern for Transformer Models](https://tinkerd.net/blog/machine-learning/mixture-of-experts/)\n\n[169\\. TOWARDS A UNIFIED VIEW OF SPARSE FEED-FORWARD NETWORK IN TRANSFORMER](https://openreview.net/pdf?id=lX478WYy0Up)\n\n[170\\. Approximating Two-Layer Feedforward Networks for Efficient Transformers](https://openreview.net/pdf?id=zM3mlyflTt)\n\n[171\\. Towards Efficient and Scalable Representation Learning](https://www.lti.cs.cmu.edu/people/alumni/alumni-thesis/pham-hai-thesis.pdf)\n\n[172\\. MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models](https://jaewoong.org/pubs/dac24-monde.pdf)\n\n[173\\. Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models](https://openreview.net/pdf?id=B0wvJYPT7M)\n\n[174\\. Towards A Unified View of Sparse Feed-Forward Network ...](https://openreview.net/forum?id=3EcjsgPq74)\n\n[175\\. Mixture of A Million Experts | Papers With Code](https://paperswithcode.com/paper/mixture-of-a-million-experts)\n\n[176\\. Mixture of Experts, State Space Models](https://www.webpages.uidaho.edu/vakanski/Courses/Adversarial_Machine_Learning/Spring_2024/Lecture_Mixture_of_Experts,State_Space_Models.pdf)\n\n[177\\. Scattered Mixture-of-Experts Implementation](http://arxiv.org/html/2403.08245v2)\n\n[181\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[182\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[183\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[184\\. W. Fedus, Barret Zoph et al. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” J. Mach. Learn. Res.](https://arxiv.org/abs/2101.03961)\n\n[185\\. Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model](https://arxiv.org/pdf/2406.19905)\n\n[186\\. Dmitry Lepikhin, HyoukJoong Lee et al. “GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.” ArXiv](https://arxiv.org/abs/2006.16668)\n\n[187\\. gearshiftt – The FFT Benchmark Suite for Heterogeneous Platforms](https://arxiv.org/pdf/1702.00629)\n\n[188\\. MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism](https://arxiv.org/pdf/2504.02263)\n\n[189\\. QuantMoE-Bench: Examining Post-Training Quantization for Mixture-of-Experts](https://arxiv.org/pdf/2406.08155)\n\n[190\\. Mixtures of Experts](https://blog.javid.io/p/mixtures-of-experts)\n\n[191\\. Resolving the conflict between generality and plausibility in verified computation](https://cs.nyu.edu/~mwalfish/papers/zaatar-eurosys13.pdf)\n\n[192\\. Using Transformers to Chat - Qwen docs](https://qwen.readthedocs.io/en/v2.0/inference/chat.html)\n\n[193\\. Toward Efficient Inference for Mixture of Experts](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf)\n\n[194\\. FLatten Transformer: Vision Transformer using Focused Linear Attention](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_FLatten_Transformer_Vision_Transformer_using_Focused_Linear_Attention_ICCV_2023_paper.pdf)\n\n[195\\. Blackthorn: Latency Estimation Framework for CNNs on Embedded Nvidia Platforms](https://jantsch.se/AxelJantsch/papers/2021/MartinLechner-IEEEAccess.pdf)\n\n[196\\. Scattered Mixture-of-Experts Implementation](http://arxiv.org/html/2403.08245v2)\n\n[197\\. A Benchmark for ML Inference Latency on Mobile Devices](https://qed.usc.edu/paolieri/papers/2024_edgesys_mobile_inference_benchmark.pdf)\n\n[201\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[202\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[203\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[204\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[205\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[206\\. The GPT-3 Architecture, on a Napkin - 在线工具](https://tool.lu/article/5cv/preview)\n\n[207\\. layer_layer_normalization: Layer normalization layer (...](https://rdrr.io/github/rstudio/keras/man/layer_layer_normalization.html)\n\n[208\\. Pretraining model examples · Tencent/TencentPretrain Wiki - GitHub](https://github.com/Tencent/TencentPretrain/wiki/Pretraining-model-examples)\n\n[209\\. DETR网络结构详解（结合代码）](https://zhuanlan.zhihu.com/p/645787833)\n\n[210\\. Image Captioning, Transformer Mode On](https://towardsdatascience.com/image-captioning-transformer-mode-on/)\n\n[211\\. Jimmy Ba, J. Kiros et al. “Layer Normalization.” ArXiv](https://arxiv.org/abs/1607.06450)\n\n[212\\. Graphormer 代码解读](https://zhuanlan.zhihu.com/p/604979500)\n\n[213\\. Layer Normalization — complextorch 1.0.0 documentation](https://complextorch.readthedocs.io/en/latest/nn/modules/layernorm.html)\n\n[214\\. GPT （一）transformer原理和代码详解](https://zhuanlan.zhihu.com/p/632880248)\n\n[215\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[216\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[217\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[218\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[219\\. Alex Wang, Amanpreet Singh et al. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” BlackboxNLP@EMNLP](https://doi.org/10.18653/v1/W18-5446)\n\n[220\\. Inferencing with Mixtral 8x22B on AMD GPUs](https://rocm.blogs.amd.com/artificial-intelligence/moe/README.html)\n\n[221\\. TOWARDS A UNIFIED VIEW OF SPARSE FEED-FORWARD NETWORK IN TRANSFORMER](https://openreview.net/pdf?id=lX478WYy0Up)\n\n[222\\. Simon F. Muller-Cleve, Vittorio Fra et al. “Braille letter reading: A benchmark for spatio-temporal pattern recognition on neuromorphic hardware.” Frontiers in Neuroscience](https://doi.org/10.3389/fnins.2022.951164)\n\n[223\\. Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning](https://m.riunet.upv.es/bitstream/handle/10251/204447/DesislavovMartinez-PlumedHernandez-Orallo%20-%20Trends%20in%20AI%20inference%20energy%20consumption%20Beyond%20the%20....pdf?sequence=1&isAllowed=y)\n\n[224\\. Energy-efficient Online Scheduling of Transformer Inference Services on GPU Servers](https://repository.hkust.edu.hk/ir/bitstream/1783.1-118320/1/043422_1.pdf)\n\n[225\\. Yash Ukidave, Amir Kavyan Ziabari et al. “Quantifying the energy efficiency of FFT on heterogeneous platforms.” 2013 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)](https://doi.org/10.1109/ISPASS.2013.6557174)\n\n[226\\. Evaluating and Accelerating Vision Transformers on GPU-based Embedded Edge AI Systems](https://assets-eu.researchsquare.com/files/rs-5083258/v1_covered_7dcbed04-40c0-4c57-8c47-02fc11107836.pdf?c=1733374368)\n\n[227\\. Accelerating Transformer Pre-training with 2:4 Sparsity](https://raw.githubusercontent.com/mlresearch/v235/main/assets/hu24r/hu24r.pdf)\n\n[228\\. DVFS Modeling for Energy-Efficient GPU Computing](https://hpcas.inesc-id.pt/~handle/papers/PhD_JoaoGuerreiro_2020.pdf)\n\n[229\\. Deep Differentiable Logic Gate Networks](https://proceedings.neurips.cc/paper_files/paper/2022/file/0d3496dd0cec77a999c98d35003203ca-Paper-Conference.pdf)\n\n[230\\. FAST: Feed-Forward Assisted Transformers for Time Efficient Fine Tuning](https://openreview.net/pdf/f6899dab9967be606903db82b99bfad33ae88a09.pdf)\n\n[231\\. DVFS-Aware DNN Inference on GPUs: Latency Modeling ...](https://arxiv.org/html/2502.06295v1)\n\n[232\\. Detecting anomalous electricity consumption with ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC10702936/)\n\n[235\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[236\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[237\\. Mixtures of Experts](https://blog.javid.io/p/mixtures-of-experts)\n\n[238\\. Forecasting GPU Performance for Deep Learning Training and Inference](https://arxiv.org/pdf/2407.13853)\n\n[239\\. Mixture of Experts vs Mixture of Tokens: Making LLMs more efficient](https://www.superannotate.com/blog/mixture-of-experts-vs-mixture-of-tokens#:~:text=MoEs%20vs%20MoTs:%20In%20Mixture,an%20expert%20feed-forward%20layer.)\n\n[240\\. W. Fedus, Barret Zoph et al. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” J. Mach. Learn. Res.](https://arxiv.org/abs/2101.03961)\n\n[241\\. MIXTURE OF EXPERTS WITH MIXTURE OF PRECISIONS FOR TUNING QUALITY OF SERVICE](https://arxiv.org/pdf/2407.14417)\n\n[242\\. FIDDLER: CPU-GPU ORCHESTRATION FOR FAST INFERENCE OF MIXTURE-OF-EXPERTS MODELS](https://openreview.net/pdf?id=WX7lxohjFe)\n\n[243\\. Noam M. Shazeer, Azalia Mirhoseini et al. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” ArXiv](https://arxiv.org/abs/1701.06538)\n\n[244\\. Orchestrating Heterogeneous Architecture for Fast Inference of Mixture-of-Experts Models](https://openreview.net/pdf/8e841734f0660595cf4873bd01abcb6372904eb8.pdf)\n\n[245\\. FLatten Transformer: Vision Transformer using Focused Linear Attention](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_FLatten_Transformer_Vision_Transformer_using_Focused_Linear_Attention_ICCV_2023_paper.pdf)\n\n[246\\. Dmitry Lepikhin, HyoukJoong Lee et al. “GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.” ArXiv](https://arxiv.org/abs/2006.16668)\n\n[247\\. Mixture of Experts explained simply](https://www.louisbouchard.ai/moe/)\n\n[248\\. Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models](https://openreview.net/pdf/26d07140222fe6077b30c22e5473ff785cd9955c.pdf)\n\n[249\\. Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts](https://papers.neurips.cc/paper_files/paper/2020/file/25ddc0f8c9d3e22e03d3076f98d83cb2-Paper.pdf)\n\n[250\\. Y. Wang, Gu-Yeon Wei et al. “Benchmarking TPU, GPU, and CPU Platforms for Deep Learning.” ArXiv](https://arxiv.org/abs/1907.10701)\n\n[251\\. NO MORE 996: UNDERSTANDING DEEP LEARNING INFERENCE SERVING WITH AN AUTOMATIC BENCHMARKING SYSTEM](http://www.arxiv.org/pdf/2011.02327v1)\n\n[252\\. Peter Belcák, R. Wattenhofer. “Fast Feedforward Networks.” ArXiv](https://doi.org/10.48550/arXiv.2308.14711)\n\n[255\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[256\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[257\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[258\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[259\\. Alec Radford, Karthik Narasimhan. “Improving Language Understanding by Generative Pre-Training.”](https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n[260\\. ggml-imax/docs/gguf.md at master · NAIST-Archlab/ggml...](https://github.com/NAIST-Archlab/ggml-imax/blob/master/docs/gguf.md)\n\n[261\\. Jimmy Ba, J. Kiros et al. “Layer Normalization.” ArXiv](https://arxiv.org/abs/1607.06450)\n\n[262\\. The GPT-3 Architecture, on a Napkin - 在线工具](https://tool.lu/article/5cv/preview)\n\n[263\\. 60行Numpy代码从零实现GPT模型（一）](https://zhuanlan.zhihu.com/p/657950165)\n\n[264\\. 从理论到代码: Gpt-2模型的逐步实现和代码解析。 - Ai中文站](https://www.ai-cn.co/post/5427)\n\n[265\\. Layer Normalization Layer in Keras](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/LayerNormalization)\n\n[266\\. 动手实现Transformer](https://mp.weixin.qq.com/s?__biz=MzU1NjEwMTY0Mw%3D%3D&mid=2247603315&idx=1&sn=82c207c1bfc2646cff0c0373bf6dc98b&chksm=fa0e00fb717924745d0a176d420aaf67584e09ed92589064085f9f763f5b0e61f4efec2a3e59&scene=27)\n\n[267\\. Yidong Liao, Chris Ferrie. “GPT on a Quantum Computer.”](https://arxiv.org/abs/2403.09418)\n\n[268\\. Attention Mechanism, Transformers, BERT, and GPT: Tutorial and Survey](https://faculty.ist.psu.edu/vhonavar/Courses/dsmethods/transformer.pdf)\n\n[269\\. NVIDIA TensorRT 8.5.10 API Reference for DRIVE OS](https://developer.nvidia.com/docs/drive/drive-os/6.0.6/public/drive-os-tensorrt/pdf/NVIDIA-TensorRT-8.5.10-API-Reference-for-DRIVE-OS.pdf)\n\n[270\\. Rihards Novickis, Daniels Jānis Justs et al. “An Approach of Feed-Forward Neural Network Throughput-Optimized Implementation in FPGA.” Electronics](https://doi.org/10.3390/electronics9122193)\n\n[271\\. Meet GPT, The Decoder-Only Transformer](https://towardsdatascience.com/meet-gpt-the-decoder-only-transformer-12f4a7918b36)\n\n[272\\. CVer从0入门NLP(三)———GPT、BERT模型_gpt 分句符\\[sep\\]-...](https://blog.csdn.net/qq_47233366/article/details/135695766)\n\n[273\\. 关于transformer模型总结](https://zhuanlan.zhihu.com/p/43644471)\n\n[274\\. layer-normalization 实现& tf.contrib.layers.layer_norm 原创](https://blog.csdn.net/m0_38024592/article/details/118688393)\n\n[275\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[276\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[277\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[278\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[279\\. Evaluating and Accelerating Vision Transformers on GPU-based Embedded Edge AI Systems](https://assets-eu.researchsquare.com/files/rs-5083258/v1_covered_7dcbed04-40c0-4c57-8c47-02fc11107836.pdf?c=1733374368)\n\n[280\\. Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning](https://m.riunet.upv.es/bitstream/handle/10251/204447/DesislavovMartinez-PlumedHernandez-Orallo%20-%20Trends%20in%20AI%20inference%20energy%20consumption%20Beyond%20the%20....pdf?sequence=1&isAllowed=y)\n\n[281\\. Energy-efficient Online Scheduling of Transformer Inference Services on GPU Servers](https://repository.hkust.edu.hk/ir/bitstream/1783.1-118320/1/043422_1.pdf)\n\n[282\\. NVIDIA® GPUs: Performance overview](https://assets.nebius.ai/download/gpu-performance-review.pdf)\n\n[283\\. Blog - NVIDIA A100 vs. H100: Architecture and Specs](https://www.bacloud.com/en/blog/174/nvidia-a100-vs.-h100-architecture-and-specs.html)\n\n[284\\. H100 Transformer Engine Supercharges AI Training, Delivering Up to 6x Higher Performance Without Losing Accuracy](https://blogs.nvidia.com/blog/2022/03/22/h100-transformer-engine/)\n\n[285\\. DVFS-Aware DNN Inference on GPUs: Latency Modeling ...](https://arxiv.org/html/2502.06295v1)\n\n[286\\. Double-Exponential Increases in Inference Energy](https://arxiv.org/html/2412.09731v1)\n\n[287\\. Analysis of Deep Learning Inference Compute and Energy Consumption Trends](https://dspace.upv.es/bitstream/handle/10251/174868/Desislavov%20-%20Analisis%20de%20las%20tendencias%20de%20computo%20y%20consumo%20energetico%20de%20la%20inferencia%20en%20apren....pdf?sequence=1&isAllowed=y)\n\n[288\\. Jae-Won Chung, Jiachen Liu et al. “The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization.”](https://arxiv.org/abs/2505.06371)\n\n[289\\. Advancing Neural Networks: Innovations and Impacts on Energy Consumption](https://iris.cnr.it/retrieve/4f01ebc4-3e5a-45c4-9734-57e59454ce9b/Advancing%20Neural%20Networks%3A%20Innovations%20and%20Impacts%20on%20Energy%20Consumption.pdf)\n\n[290\\. Discover NVIDIA® H100 PCIe | Data Center GPU | pny.com](https://www.pny.com/nvidia-h100)\n\n[295\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[296\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[297\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[298\\. Forecasting GPU Performance for Deep Learning Training and Inference](https://arxiv.org/pdf/2407.13853)\n\n[299\\. Efficient Mixed-Precision Inference for Vision Transformers](https://zenodo.org/records/15043962/files/Piotr%20Kluska%20PhD%20Thesis.pdf?download=1)\n\n[300\\. Inferencing with Mixtral 8x22B on AMD GPUs](https://rocm.blogs.amd.com/artificial-intelligence/moe/README.html)\n\n[301\\. FIDDLER: CPU-GPU ORCHESTRATION FOR FAST INFERENCE OF MIXTURE-OF-EXPERTS MODELS](https://openreview.net/pdf?id=WX7lxohjFe)\n\n[302\\. Mixtures of Experts](https://blog.javid.io/p/mixtures-of-experts)\n\n[303\\. Transformer vs. Mixture of Experts in LLMs](https://www.dailydoseofds.com/p/transformer-vs-mixture-of-experts-in-llms/)\n\n[304\\. FLatten Transformer: Vision Transformer using Focused Linear Attention](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_FLatten_Transformer_Vision_Transformer_using_Focused_Linear_Attention_ICCV_2023_paper.pdf)\n\n[305\\. MIXTURE OF EXPERTS WITH MIXTURE OF PRECISIONS FOR TUNING QUALITY OF SERVICE](https://arxiv.org/pdf/2407.14417)\n\n[306\\. Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models](https://openreview.net/pdf/26d07140222fe6077b30c22e5473ff785cd9955c.pdf)\n\n[307\\. Orchestrating Heterogeneous Architecture for Fast Inference of Mixture-of-Experts Models](https://openreview.net/pdf/8e841734f0660595cf4873bd01abcb6372904eb8.pdf)\n\n[308\\. Toward Efficient Inference for Mixture of Experts](https://openreview.net/pdf?id=stXtBqyTWX)\n\n[309\\. Using Transformers to Chat - Qwen docs](https://qwen.readthedocs.io/en/v2.0/inference/chat.html)\n\n[310\\. W. Fedus, Barret Zoph et al. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” J. Mach. Learn. Res.](https://arxiv.org/abs/2101.03961)\n\n[315\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[316\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[317\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[318\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[319\\. Jimmy Ba, J. Kiros et al. “Layer Normalization.” ArXiv](https://arxiv.org/abs/1607.06450)\n\n[320\\. Layer Normalization Layer in Keras](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/LayerNormalization)\n\n[321\\. 60行Numpy代码从零实现GPT模型（一）](https://zhuanlan.zhihu.com/p/657950165)\n\n[322\\. The GPT-3 Architecture, on a Napkin - 在线工具](https://tool.lu/article/5cv/preview)\n\n[323\\. 从理论到代码: Gpt-2模型的逐步实现和代码解析。 - Ai中文站](https://www.ai-cn.co/post/5427)\n\n[324\\. Source code for mindspore.nn.layer.normalization](https://www.mindspore.cn/api/en/0.2.0-alpha/_modules/mindspore/nn/layer/normalization.html)\n\n[325\\. THE INDIAN JOURNAL OF TECHNICAL EDUCATION](http://www.isteonline.in/Datafiles/cms/Special%20Issues/Spl%20issue%202%20June%202024%20for%20web%20uploading.pdf)\n\n[326\\. CVer从0入门NLP(三)———GPT、BERT模型_gpt 分句符\\[sep\\]-...](https://blog.csdn.net/qq_47233366/article/details/135695766)\n\n[327\\. BumbleBee: Secure Two-party Inference Framework for Large Transformers](https://eprint.iacr.org/2023/1678.pdf)\n\n[328\\. 动手实现Transformer](https://mp.weixin.qq.com/s?__biz=MzU1NjEwMTY0Mw%3D%3D&mid=2247603315&idx=1&sn=82c207c1bfc2646cff0c0373bf6dc98b&chksm=fa0e00fb717924745d0a176d420aaf67584e09ed92589064085f9f763f5b0e61f4efec2a3e59&scene=27)\n\n[329\\. Yidong Liao, Chris Ferrie. “GPT on a Quantum Computer.”](https://arxiv.org/abs/2403.09418)\n\n[330\\. Fine-Tuning BERT for Sentiment Analysis](https://escholarship.org/content/qt5kr0p5m7/qt5kr0p5m7_noSplash_b52b3b02c7cfbba8564380d1e621704b.pdf)\n\n[331\\. Meet GPT, The Decoder-Only Transformer](https://towardsdatascience.com/meet-gpt-the-decoder-only-transformer-12f4a7918b36)\n\n[332\\. AI-driven Software Development Source Code Quality](https://is.muni.cz/th/mdt17/kantek_dp_Archive.pdf)\n\n[333\\. LayerNorm — penzai](https://penzai.readthedocs.io/en/v0.1.4/_autosummary/leaf/penzai.nn.standardization.LayerNorm.html)\n\n[334\\. Layer Normalization — complextorch 1.0.0 documentation](https://complextorch.readthedocs.io/en/latest/nn/modules/layernorm.html)\n\n[335\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[336\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[337\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[338\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[339\\. Efficient Mixed-Precision Inference for Vision Transformers](https://zenodo.org/records/15043962/files/Piotr%20Kluska%20PhD%20Thesis.pdf?download=1)\n\n[340\\. Blog - NVIDIA A100 vs. H100: Architecture and Specs](https://www.bacloud.com/en/blog/174/nvidia-a100-vs.-h100-architecture-and-specs.html)\n\n[341\\. NVIDIA® GPUs: Performance overview](https://assets.nebius.ai/download/gpu-performance-review.pdf)\n\n[342\\. Toward Highly-efficient GPU-centric Networking](https://www.diva-portal.org/smash/get/diva2:1844498/FULLTEXT01.pdf)\n\n[343\\. H100 Transformer Engine Supercharges AI Training, Delivering Up to 6x Higher Performance Without Losing Accuracy](https://blogs.nvidia.com/blog/2022/03/22/h100-transformer-engine/)\n\n[344\\. Comparison of NVIDIA A100, H100 + H200 GPUs - Comet](https://www.comet.com/site/blog/comparison-of-nvidia-a100-h100-and-h200-gpus/)\n\n[345\\. ON THE EFFICIENCY OF TRANSFORMERS](https://asset.library.wisc.edu/1711.dl/L5G4PGWKGIGKX8Q/R/file-99b91.pdf)\n\n[346\\. Making AI Less \"Thirsty\": Uncovering and Addressing the Secret Water Footprint of AI Models](https://arxiv.org/pdf/2304.03271)\n\n[347\\. NVIDIA Hopper GPU架构与H100加速器发布：更智能、更高效的工作方式](http://www3.anandtech.com/show/17327/nvidia-hopper-gpu-architecture-and-h100-accelerator-announced)\n\n[348\\. Comparing NVIDIA H100 vs A100 GPUs for AI Workloads - OpenMetal](https://openmetal.io/resources/blog/nvidia-h100-vs-a100-gpu-comparison/#:~:text=Performance%20Metrics&text=Benchmarks%20show%20that:,pronounced%20with%20very%20large%20models.)\n\n[349\\. Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning](https://m.riunet.upv.es/bitstream/handle/10251/204447/DesislavovMartinez-PlumedHernandez-Orallo%20-%20Trends%20in%20AI%20inference%20energy%20consumption%20Beyond%20the%20....pdf?sequence=1&isAllowed=y)\n\n[350\\. Energy-efficient Online Scheduling of Transformer Inference Services on GPU Servers](https://repository.hkust.edu.hk/ir/bitstream/1783.1-118320/1/043422_1.pdf)\n\n[351\\. Discover NVIDIA® H100 PCIe | Data Center GPU | pny.com](https://www.pny.com/nvidia-h100)\n\n[352\\. Advancing Neural Networks: Innovations and Impacts on Energy Consumption](https://iris.cnr.it/retrieve/4f01ebc4-3e5a-45c4-9734-57e59454ce9b/Advancing%20Neural%20Networks%3A%20Innovations%20and%20Impacts%20on%20Energy%20Consumption.pdf)\n\n[353\\. Breakdown of H100s for Transformer Inferencing - kipply's blog](https://kipp.ly/h100-inferencing/)\n\n[354\\. Comparing NVIDIA A100 vs NVIDIA H100: Use Cases, Cost and More](https://www.hyperstack.cloud/technical-resources/performance-benchmarks/comparing-nvidia-a100-vs-nvidia-h100-use-cases-cost-and-more#:~:text=The%20A100%20is%20ideal%20for,choice%20for%20many%20AI%20workloads.)\n\n[355\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[356\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[357\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[358\\. Song Han, Huizi Mao et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition](https://arxiv.org/abs/1510.00149)\n\n[359\\. N. Jouppi, C. Young et al. “In-datacenter performance analysis of a tensor processing unit.” 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)](https://doi.org/10.1145/3079856.3080246)\n\n[360\\. Inferencing with Mixtral 8x22B on AMD GPUs](https://rocm.blogs.amd.com/artificial-intelligence/moe/README.html)\n\n[361\\. Y. Wang, Gu-Yeon Wei et al. “Benchmarking TPU, GPU, and CPU Platforms for Deep Learning.” ArXiv](https://arxiv.org/abs/1907.10701)\n\n[362\\. Using Transformers to Chat - Qwen docs](https://qwen.readthedocs.io/en/v2.0/inference/chat.html)\n\n[363\\. J. Lee-Thorp, J. Ainslie et al. “FNet: Mixing Tokens with Fourier Transforms.” ArXiv](https://doi.org/10.18653/v1/2022.naacl-main.319)\n\n[364\\. Orchestrating Heterogeneous Architecture for Fast Inference of Mixture-of-Experts Models](https://openreview.net/pdf/8e841734f0660595cf4873bd01abcb6372904eb8.pdf)\n\n[365\\. MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism](https://arxiv.org/pdf/2504.02263)\n\n[366\\. Toward Efficient Inference for Mixture of Experts](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf)\n\n[367\\. FLatten Transformer: Vision Transformer using Focused Linear Attention](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_FLatten_Transformer_Vision_Transformer_using_Focused_Linear_Attention_ICCV_2023_paper.pdf)\n\n[368\\. FIDDLER: CPU-GPU ORCHESTRATION FOR FAST INFERENCE OF MIXTURE-OF-EXPERTS MODELS](https://openreview.net/pdf?id=WX7lxohjFe)\n\n[375\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[376\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[377\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[378\\. Nitish Srivastava, Geoffrey E. Hinton et al. “Dropout: a simple way to prevent neural networks from overfitting.” J. Mach. Learn. Res.](https://doi.org/10.5555/2627435.2670313)\n\n[379\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[380\\. GitHub - ryankiros/layer-norm: Code and models from the paper \"Layer ...](https://github.com/ryankiros/layer-norm)\n\n[381\\. tf.keras.layers.LayerNormalization - TensorFlow 1.15 - W3cubDocs](https://docs.w3cub.com/tensorflow~1.15/keras/layers/layernormalization.html)\n\n[382\\. The GPT-3 Architecture, on a Napkin - 在线工具](https://tool.lu/article/5cv/preview)\n\n[383\\. Layer Normalization Layer in Keras](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/LayerNormalization)\n\n[384\\. 从理论到代码: Gpt-2模型的逐步实现和代码解析。 - Ai中文站](https://www.ai-cn.co/post/5427)\n\n[385\\. Graphormer 代码解读](https://zhuanlan.zhihu.com/p/604979500)\n\n[386\\. xiaoguzai/tfbert: 一个使用tf2复现的bert模型库](https://github.com/xiaoguzai/tfbert)\n\n[387\\. Verify 1.5.0](https://www.nuget.org/packages/Verify/1.5.0)\n\n[388\\. layer-normalization 实现& tf.contrib.layers.layer_norm 原创](https://blog.csdn.net/m0_38024592/article/details/118688393)\n\n[389\\. Rüdiger Ehlers. “Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks.” Automated Technology for Verification and Analysis](https://doi.org/10.1007/978-3-319-68167-2_19)\n\n[390\\. Running SHiELD with GFDL’s FMS full coupler infrastructure](https://repository.library.noaa.gov/view/noaa/66759/noaa_66759_DS1.pdf)\n\n[391\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[392\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[393\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[394\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[395\\. Double-Exponential Increases in Inference Energy](https://arxiv.org/html/2412.09731v1)\n\n[396\\. Song Han, Huizi Mao et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition](https://arxiv.org/abs/1510.00149)\n\n[397\\. Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning](https://m.riunet.upv.es/bitstream/handle/10251/204447/DesislavovMartinez-PlumedHernandez-Orallo%20-%20Trends%20in%20AI%20inference%20energy%20consumption%20Beyond%20the%20....pdf?sequence=1&isAllowed=y)\n\n[398\\. Blog - NVIDIA A100 vs. H100: Architecture and Specs](https://www.bacloud.com/en/blog/174/nvidia-a100-vs.-h100-architecture-and-specs.html)\n\n[399\\. Energy-efficient Online Scheduling of Transformer Inference Services on GPU Servers](https://repository.hkust.edu.hk/ir/bitstream/1783.1-118320/1/043422_1.pdf)\n\n[400\\. Double-Exponential Increases in Inference Energy: The Cost of the Race for Accuracy](https://arxiv.org/pdf/2412.09731)\n\n[401\\. Seyyidahmed Lahmer, A. Khoshsirat et al. “Energy Consumption of Neural Networks on NVIDIA Edge Boards: an Empirical Model.” 2022 20th International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt)](https://doi.org/10.23919/WiOpt56218.2022.9930584)\n\n[402\\. Jae-Won Chung, Jiachen Liu et al. “The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization.”](https://arxiv.org/abs/2505.06371)\n\n[403\\. Evaluating and Accelerating Vision Transformers on GPU-based Embedded Edge AI Systems](https://assets-eu.researchsquare.com/files/rs-5083258/v1_covered_7dcbed04-40c0-4c57-8c47-02fc11107836.pdf?c=1733374368)\n\n[404\\. H100 Transformer Engine Supercharges AI Training, Delivering Up to 6x Higher Performance Without Losing Accuracy](https://blogs.nvidia.com/blog/2022/03/22/h100-transformer-engine/)\n\n[405\\. Radosvet Desislavov, Fernando Mart'inez-Plumed et al. “Compute and Energy Consumption Trends in Deep Learning Inference.” ArXiv](https://doi.org/10.1016/j.suscom.2023.100857)\n\n[406\\. Analysis of Deep Learning Inference Compute and Energy Consumption Trends](https://dspace.upv.es/bitstream/handle/10251/174868/Desislavov%20-%20Analisis%20de%20las%20tendencias%20de%20computo%20y%20consumo%20energetico%20de%20la%20inferencia%20en%20apren....pdf?sequence=1&isAllowed=y)\n\n[407\\. Sparse Upcycling: Inference Inefficient Finetuning](http://arxiv.org/html/2411.08968v1)"
  },
  {
    "id": 20,
    "question": "Mixture of Experts (MOE) architecture usually first train a powerful general model and then use multiple LoRA (Low-Rank Adaptation) modules in a hot-swappable manner for specific task training. Compare the performance with traditional dense models and, based on relevant research papers, analyze how to combine the strengths of both approaches.",
    "response": "**Research Report**\n\n**To:** Interested Parties\n\n**From:** Expert Researcher\n\n**Date:** July 26, 2025\n\n**Subject:** A Comparative Analysis of Mixture-of-Experts with LoRA Adaptation, Dense Models, and Hybrid Architectures in Large Language Models\n\n**1.0 Introduction: The Evolving Architecture of Large Models**\n\nThe landscape of large-scale artificial intelligence is in a constant state of flux, driven by the dual imperatives of increasing model capability and managing exorbitant computational costs. For years, the dominant paradigm has been the \"dense\" transformer model, where every parameter is engaged for every input token during inference. While undeniably powerful, this approach faces fundamental scaling limitations. As of 2025, a more nuanced architectural philosophy has gained significant traction: the use of sparsely activated models, particularly the Mixture-of-Experts (MoE) architecture.\n\nThis report focuses on a powerful variant of this paradigm: MoE systems that leverage multiple Low-Rank Adaptation (LoRA) modules as their \"experts.\" In this setup, a large, pre-trained generalist model is augmented with numerous, lightweight, and task-specific LoRA adapters. A routing mechanism then dynamically selects the most relevant adapter(s) for a given task or input, allowing for massive specialization without the need to train or deploy multiple monolithic models. This approach, often featuring \"hot-swappable\" LoRA modules, promises unprecedented flexibility and efficiency \\[105\\]\\[132\\].\n\nThis research provides a comprehensive comparison between the performance of these MoE-LoRA systems and traditional dense models. We will examine key metrics including task-specific accuracy, training efficiency, and inference latency. Furthermore, we will analyze the emerging field of \"hybrid\" architectures, which seek to combine the respective strengths of dense and sparse MoE models, representing a pragmatic and powerful path forward for the next generation of AI systems.\n\n**2.0 The MoE-LoRA Paradigm: Architecture and Implementation**\n\nThe fusion of Mixture-of-Experts and Low-Rank Adaptation represents a significant evolution in parameter-efficient fine-tuning (PEFT). This architecture decouples the model's total parameter count from its computational cost during inference, enabling vast model capacity with manageable operational demands \\[10\\]\\[38\\]\\[40\\].\n\n**2.1 Architectural Blueprint**\n\nThe core of the MoE-LoRA architecture consists of three main components:\n\n1.  **Frozen Base Model:** A powerful, pre-trained dense model (e.g., Llama, GPT) serves as the foundation. Its weights are kept frozen, preserving its vast general knowledge and significantly reducing the computational burden of fine-tuning \\[29\\]\\[103\\].\n2.  **LoRA Experts:** Instead of standard feed-forward networks (FFNs), the \"experts\" in this MoE architecture are LoRA modules. LoRA works by injecting trainable, low-rank matrices (A and B) into specific layers of the base model, typically the attention or FFN linear layers \\[25\\]\\[222\\]\\[229\\]. In an MoE-LoRA system, multiple LoRA modules are placed in parallel, with each one fine-tuned to become a specialist in a particular task, domain, or data modality \\[24\\]\\[28\\]\\[39\\]. This is the foundation of models referred to in the literature as **MOELoRA** or **MixLoRA** \\[223\\]\\[214\\].\n3.  **Routing Mechanism:** A lightweight, trainable gating network, or \"router,\" is responsible for dynamically selecting which expert(s) should process each input token \\[36\\]. The router calculates a score for each expert and typically employs a \"top-k\" strategy, activating only the highest-scoring k experts (often k=1 or k=2) for any given token \\[23\\]. This sparse activation is what keeps the computational cost constant, regardless of the total number of experts \\[325\\]. Some advanced methods employ prompt-aware routing or hierarchical routing to improve expert selection \\[37\\]\\[217\\].\n\nThis architecture allows for a \"hot-swappable\" system where different sets of LoRA adapters can be loaded and activated on the fly, enabling a single base model to perform a multitude of specialized tasks without interference \\[132\\]\\[187\\].\n\n**2.2 Implementation and Key Challenges**\n\nImplementing these systems often involves leveraging established deep learning frameworks like PyTorch and libraries such as Hugging Face Transformers and AdapterHub \\[21\\]\\[26\\]\\[34\\]. A critical challenge during training is ensuring **load balancing**. Without intervention, routers can develop a preference for a few \"popular\" experts, leading to others being undertrained. This is typically mitigated by adding an auxiliary load balancing loss to the training objective, which incentivizes the router to distribute tokens more evenly across all available experts \\[36\\].\n\nWhile the term **MoLoRA** has appeared, it seems to be used interchangeably with or is less formally defined than **MOELoRA**. Some early negative reports on \"MoLoRA\" were suspected to be due to implementation oversights, such as not fine-tuning the attention layers, which subsequent successful models have addressed \\[6\\]. Peer-reviewed studies on MOELoRA, complete with architectural specifications and ablation studies, confirm its effectiveness, demonstrating the importance of the MOE architecture and the gate function for achieving superior performance over a standard single-LoRA baseline \\[283\\]\\[345\\]\\[344\\].\n\n**3.0 Performance Showdown: MoE-LoRA vs. Dense Models**\n\nThe primary motivation for adopting MoE-LoRA architectures is to achieve a superior trade-off between performance and efficiency compared to their dense counterparts. Analysis of recent research reveals a nuanced but largely favorable picture for the sparse approach.\n\n**3.1 Accuracy and Generalization**\n\nAcross a wide range of tasks, MoE-based models consistently demonstrate accuracy on par with or superior to dense models that have a similar inference cost.\n\n**Multi-Task and Single-Task Gains:** The **MixLoRA** framework, which uses LoRA-based experts, achieves approximately a 9% accuracy improvement over other state-of-the-art PEFT methods in multi-task learning scenarios \\[5\\]\\[8\\]\\[214\\]. Even in single-task learning, it shows an average accuracy boost of 6.2% over a standard LoRA implementation \\[15\\].\n\n**Superiority in Low-Resource Settings:** MoE models exhibit strong multilingual capabilities and generalization, particularly in low-resource languages, outperforming the base dense models they are built upon \\[154\\]. Methods like **UpIT** (Upcycling Instruction Tuning) also perform better than dense instruction tuning, especially with small amounts of training data \\[1\\].\n\n**Outperforming Dense Baselines:** Controlled experiments show that MoE models consistently outperform dense LLMs on the speed-accuracy trade-off curve across benchmarks like MMLU and GSM8K \\[146\\]\\[148\\]\\[205\\]. Sparse models generally outperform dense models across various tasks \\[7\\]. For instance, Speaker Adaptive Mixture of LoRA Experts (**SAML**) achieves relative Word Error Rate (WER) reductions of over 29% in automatic speech recognition (ASR) tasks \\[14\\].\n\n**3.2 Training and Fine-Tuning Efficiency**\n\nThe most significant advantage of MoE architectures lies in their training efficiency.\n\n**Reduced Training Cost:** Research indicates that an MoE model can reach the quality of a 6.7B parameter dense model at the training cost of a 1.3B dense model—a 5x reduction in training cost for equivalent performance \\[144\\]\\[147\\]\\[156\\]. More broadly, MoEs require 2 to 4 times less compute (FLOPs) to match the performance of a dense model \\[84\\].\n\n**Parameter Efficiency:** By leveraging LoRA, only a tiny fraction of the model's total parameters are trainable. LoRA can reduce the number of trainable parameters by a factor of 10,000 and GPU memory requirements by 3x compared to full fine-tuning, all while maintaining high training throughput \\[103\\]\\[222\\]\\[322\\].\n\n**Scalability:** Performance generally improves as more experts are added to the MoE layers, allowing models to scale their knowledge capacity effectively, albeit with diminishing returns \\[1\\]\\[14\\]\\[16\\].\n\nHowever, this efficiency comes with a caveat: MoE models often require 2-4 times _more total parameters_ than a dense model to achieve the same accuracy, leading to a larger memory footprint for storing the model weights \\[144\\]\\[147\\]\\[155\\].\n\n**3.3 Inference Efficiency: The Latency and Throughput Dilemma**\n\nInference performance is where the comparison becomes most complex. While MoEs are designed to be computationally cheap at inference, the dynamic nature of loading and routing between LoRA experts introduces new forms of overhead.\n\n**The Latency Problem:** Early implementations of dynamic adapters were surprisingly slow. Research shows that dynamically routing to different LoRA modules can increase decoding inference latency by a staggering **250-950%** compared to a non-adapted base model \\[123\\]\\[131\\]. This overhead is not from the matrix multiplication itself but from the high cost of launching multiple CUDA kernels for each adapter, an operation whose latency does not scale linearly with matrix size \\[123\\]. Methods like **MOLA** (layer-wise switching) can increase latency by over 900% \\[188\\].\n\n**Solutions for Latency Reduction:** In response, optimized frameworks have been developed. **LoRA-Switch** introduced a token-wise routing strategy that merges the parameters of all activated experts into the base model's weights in a single, efficient kernel operation. This cuts the decoding latency overhead by more than 2.4x, reducing the latency increase over a base Llama2-7B model from over 250% for older methods to just 29% (3.1 ms/token vs. 2.4 ms/token on an A100 GPU) \\[123\\]\\[131\\]\\[188\\].\n\n**Throughput on Modern GPUs:** On consumer-grade hardware like the NVIDIA RTX 4090, systems like **LoHan** can achieve 90-95% of peak FLOPS for models up to 70B parameters, outperforming less optimized baselines \\[194\\]. For large MoE models like Mixtral 8x7B on an A100, throughput can be dynamically adjusted, and partial quantization of experts can significantly boost tokens-per-second with negligible impact on accuracy \\[193\\]. For systems designed to serve multiple users with different LoRAs, solutions like **LoRA-Inlaid** have been shown to improve average latency by up to 1.76x compared to prior serving systems \\[65\\]\\[251\\].\n\n**Task-Switching Overhead:** The \"hot-swapping\" of experts is not instantaneous. For very large models on consumer GPUs (24GB VRAM), expert module switching latency (SSD→RAM→VRAM) can be under 50ms \\[189\\]. When a live system switches between tasks, there can be a brief period of sub-optimal latency; one study showed that it takes around 50 requests for a system to return to optimal latency after a task shift \\[133\\].\n\n**4.0 The Hybrid Frontier: Merging Dense and Sparse Strengths**\n\nWhile the MoE-LoRA paradigm is powerful, it is not a universal solution. The complexity of training, potential for routing instability, and inference latency overheads have motivated researchers to explore hybrid architectures that strategically combine the strengths of both dense and sparse models.\n\nThe core motivation is to use dense components for their robustness and ease of training, while integrating sparse MoE components for scalability and computational efficiency \\[43\\]\\[216\\].\n\n**4.1 Emerging Hybrid Architectures**\n\nSeveral innovative hybrid models are currently being explored:\n\n**Residual Hybrid MoE:** This architecture integrates a residual MoE module with a standard dense transformer. The dense path ensures a baseline level of performance and stability, while the sparse MoE path provides specialized capacity and accelerates training and inference \\[44\\]\\[54\\]\\[85\\]. Some hybrid models have demonstrated up to a 22% improvement in training performance over state-of-the-art pure MoEs \\[90\\].\n\n**DS-MoE (Dense-to-Sparse MoE):** This compelling framework uses dense computation across all experts during the training phase, which can simplify the training dynamics and improve final model quality. For inference, it reverts to standard sparse routing, thus reaping the computational benefits. DS-MoE models can be more parameter-efficient than standard MoEs and achieve performance comparable to dense models while being computationally cheaper at inference \\[46\\]\\[56\\]\\[150\\].\n\n**MoE Jetpack:** Instead of training a massive MoE model from scratch, this framework provides a \"jetpack\" to upgrade existing, pre-trained dense models into MoE models. This approach significantly reduces the computational cost and data requirements for creating a capable MoE by leveraging the knowledge already encapsulated in widely available dense checkpoints \\[43\\]\\[43\\].\n\n**Architectural Blending:** Researchers are also experimenting with interleaving different types of layers, such as combining Transformer blocks with Mamba blocks and incorporating MoE layers within this mixed architecture, to capture different types of data dependencies more efficiently \\[44\\].\n\nThese hybrid approaches represent a pragmatic compromise, creating models that are more adaptable, efficient, and performant by treating dense and sparse computation as tools to be intelligently combined rather than competing ideologies.\n\n**5.0 Conclusion and Future Outlook**\n\nAs of July 2025, the paradigm of using hot-swappable LoRA modules within a Mixture-of-Experts architecture has firmly established itself as a leading strategy for adapting and specializing large language models. The evidence clearly shows that these sparse systems can match or exceed the accuracy of traditional dense models while offering dramatic reductions in training costs—often by a factor of 5x or more \\[144\\]\\[147\\]. This efficiency is critical for democratizing access to cutting-edge AI and enabling continuous learning and specialization.\n\nHowever, the transition is not without its challenges. The increased memory footprint and, most critically, the substantial inference latency overhead introduced by naive dynamic routing are significant hurdles. While innovative solutions like **LoRA-Switch** have made remarkable progress in mitigating this latency \\[123\\], it remains a key area for ongoing research and engineering.\n\nThe most promising path forward appears to lie with **hybrid architectures**. By intelligently blending robust dense pathways with scalable sparse MoE components, models like **DS-MoE** and residual hybrids offer a \"best of both worlds\" approach \\[46\\]\\[85\\]. They promise the performance and efficiency of MoE without entirely abandoning the stability and proven training methodologies of dense models. Frameworks like **MoE Jetpack** further lower the barrier to entry by allowing the conversion of existing dense models \\[43\\].\n\nUltimately, the future of large-scale AI is unlikely to be purely dense or purely sparse. It will be adaptive. The research trajectory points toward dynamic, hybrid systems that can flexibly allocate computational resources, leveraging a vast library of specialized, lightweight experts built upon a strong, generalist foundation. This approach moves the field beyond monolithic models toward a more modular, efficient, and ultimately more intelligent ecosystem.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Upcycling Instruction Tuning from Dense to Mixture-of-Experts via Parameter Merging](https://openreview.net/pdf?id=RHISYSlLHf)\n\n[2\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[3\\. Quantized Mixture-of-LoRA-Experts for Low-Cost Training of Large Language Models](https://digital.wpi.edu/downloads/vt150p950?locale=en)\n\n[4\\. EricLBuehler/xlora: X-LoRA: Mixture of LoRA Experts - GitHub](https://github.com/EricLBuehler/xlora)\n\n[5\\. ...Tuning with LoRA-based Mixture of Experts | Papers ...](https://paperswithcode.com/paper/mixlora-enhancing-large-language-models-fine)\n\n[6\\. HMORA: MAKING LLMs MORE EFFECTIVE WITH HIERARCHICAL MIXTURE OF LoRA EXPERTS](https://openreview.net/pdf?id=lTkHiXeuDl)\n\n[7\\. Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks](https://arxiv.org/pdf/2401.02731)\n\n[8\\. Dengchun Li, Yingzi Ma et al. “MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA based Mixture of Experts.” ArXiv](https://doi.org/10.48550/arXiv.2404.15159)\n\n[9\\. SiRA: Sparse Mixture of Low Rank Adaptation](https://openreview.net/pdf/93470a97b095c9abe4838a80c0d70bb6ac3a11c2.pdf)\n\n[10\\. ENSEMBLES OF LOW-RANK EXPERT ADAPTERS](https://assets.amazon.science/6e/92/dd350c95484ca5da4cf204195aba/ensembles-of-low-rank-expert-adapters.pdf)\n\n[11\\. A Mixture-of-Experts Approach for Code Generation On upcycling and sparsifying dense models](https://kth.diva-portal.org/smash/get/diva2:1903655/FULLTEXT01.pdf)\n\n[12\\. EXPLORING SPARSE ADAPTERS FOR SCALABLE MERGING OF PARAMETER EFFICIENT EXPERTS](https://openreview.net/pdf?id=8wt2eKkVe6)\n\n[13\\. Revisiting MoE and Dense Speed-Accuracy Comparisons for LLM Training](https://paperreading.club/page?id=228821)\n\n[14\\. SAML: Speaker Adaptive Mixture of LoRA Experts for End-to-End ASR](https://www.isca-archive.org/interspeech_2024/zhao24d_interspeech.pdf)\n\n[15\\. MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based ...](https://yiyibooks.cn/__trs__/arxiv/2404.15159v2/index.html)\n\n[16\\. Revisiting MoE and Dense Speed-Accuracy Comparisons ...](https://arxiv.org/html/2405.15052v1)\n\n[21\\. GitHub - maidacundo/MoE-LoRA](https://github.com/maidacundo/MoE-LoRA)\n\n[22\\. 【手撕LLM】LoRA实现技术细节-试听版](https://www.bilibili.com/video/av873654874?t=67)\n\n[23\\. ENSEMBLES OF LOW-RANK EXPERT ADAPTERS](https://assets.amazon.science/6e/92/dd350c95484ca5da4cf204195aba/ensembles-of-low-rank-expert-adapters.pdf)\n\n[24\\. Mixture-of-LoRAs: An Efficient Multitask Tuning for La...](http://arxiv.org/html/2403.03432v1)\n\n[25\\. lora_mistral — torchtune main documentation](https://pytorch.org/torchtune/main/generated/torchtune.models.mistral.lora_mistral.html)\n\n[26\\. Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision Transformers](https://openaccess.thecvf.com/content/WACV2025/papers/Grigore_Weight_Copy_and_Low-Rank_Adaptation_for_Few-Shot_Distillation_of_Vision_WACV_2025_paper.pdf)\n\n[27\\. Target modules for applying PEFT / LoRA on different models](https://www.aionlinecourse.com/blog/target-modules-for-applying-peft-lora-on-different-models)\n\n[28\\. MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based Mixture of Experts](https://github.com/TUDB-Labs/MixLoRA)\n\n[29\\. Aurora: Activating Chinese chat capability for Mixtral...](http://arxiv.org/html/2312.14557v2)\n\n[30\\. Brian Lester, Rami Al-Rfou et al. “The Power of Scale for Parameter-Efficient Prompt Tuning.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/2021.emnlp-main.243)\n\n[31\\. Dengchun Li, Yingzi Ma et al. “MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA based Mixture of Experts.” ArXiv](https://doi.org/10.48550/arXiv.2404.15159)\n\n[32\\. IMPLEMENTATION OF LORA ENABLING TECHNOLOGY FOR IOT APPLICATIONS](http://dbdxxb.cn/wp-content/uploads/2022/12/J.-Bindu-Reddy.pdf)\n\n[33\\. N. Houlsby, A. Giurgiu et al. “Parameter-Efficient Transfer Learning for NLP.” ArXiv](https://arxiv.org/abs/1902.00751)\n\n[34\\. Model Mixins — AdapterHub documentation](https://docs.adapterhub.ml/classes/model_mixins.html)\n\n[35\\. LoRA — 直观而详尽的解释](https://zhuanlan.zhihu.com/p/675496127)\n\n[36\\. Mixture of Experts Pattern for Transformer Models](https://tinkerd.net/blog/machine-learning/mixture-of-experts/)\n\n[37\\. MiLoRA: Efficient Mixture of Low-Rank Adaptation for ...](https://aclanthology.org/2024.findings-emnlp.994/)\n\n[38\\. W. Fedus, Barret Zoph et al. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” J. Mach. Learn. Res.](https://arxiv.org/abs/2101.03961)\n\n[39\\. LLaVA-MoLE: Sparse Mixture of LoRA Experts for ...](https://paperswithcode.com/paper/llava-mole-sparse-mixture-of-lora-experts-for)\n\n[40\\. Noam M. Shazeer, Azalia Mirhoseini et al. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” ArXiv](https://arxiv.org/abs/1701.06538)\n\n[41\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[42\\. 🔥 Top 10 LLMs Compared: Size, Architecture, and Training ...](https://azumo.com/insights/technical-architecture-analysis-how-the-top-10-llms-differ-in-size-architecture-and-training-data)\n\n[43\\. MoE Jetpack: From Dense Checkpoints to Adaptive Mixture of Experts for Vision Tasks](https://proceedings.neurips.cc/paper_files/paper/2024/file/167bcf2af2cd08fcf75b932022db0311-Paper-Conference.pdf)\n\n[44\\. LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators](https://arxiv.org/pdf/2411.00136)\n\n[45\\. Harmonizing Visual Text Comprehension and Generation](https://openreview.net/pdf/da042a4b4929db18af895d2a7124f78132f0bcf4.pdf)\n\n[46\\. ...Meets Instruction Tuning: A Winning Combination for...](https://api.semanticscholar.org/arXiv:2305.14705)\n\n[47\\. A Survey on Mixture of Experts in Large Language Models](https://arxiv.org/pdf/2407.06204)\n\n[48\\. ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation](https://arxiv.org/pdf/2505.22159)\n\n[49\\. Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07035.pdf)\n\n[50\\. A. Tang, Li Shen et al. “Merging Multi-Task Models via Weight-Ensembling Mixture of Experts.” ArXiv](https://doi.org/10.48550/arXiv.2402.00433)\n\n[51\\. Towards Efficient and Scalable Representation Learning](https://www.lti.cs.cmu.edu/people/alumni/alumni-thesis/pham-hai-thesis.pdf)\n\n[52\\. LSH-MoE: Communication-efficient MoE Training via Locality-Sensitive Hashing](https://openreview.net/attachment?id=bjFhVbky5A&name=pdf)\n\n[53\\. Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers](https://openreview.net/pdf?id=w1hwFUb_81)\n\n[54\\. MoE vs Dense vs Hybrid LLM Architectures](https://app.daily.dev/posts/moe-vs-dense-vs-hybrid-llm-architectures-8z05e90p7)\n\n[55\\. Nexus: Specialization meets Adaptability for Efficiently Training ...](https://paperreading.club/page?id=248645)\n\n[56\\. SiDA-MoE: Sparsity-Inspired Data-Aware Serving for Efficient and ...](https://www.aimodels.fyi/papers/arxiv/sida-moe-sparsity-inspired-data-aware-serving)\n\n[57\\. Haojie Duanmu, Xiuhong Li et al. “MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance Co-Design.”](https://arxiv.org/abs/2505.05799)\n\n[61\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[62\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[63\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[64\\. MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning](http://arxiv.org/html/2410.09437v1)\n\n[65\\. Efficient Multi-task LLM Quantization and Serving for Multiple LoRA Adapters](https://neurips.cc/media/neurips-2024/Slides/95811.pdf)\n\n[66\\. When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications](http://export.arxiv.org/pdf/2310.18339)\n\n[67\\. MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models](https://arxiv.org/pdf/2405.13053)\n\n[68\\. Transforming Vision Transformer: Towards Efficient Multi-Task Asynchronous Learning](https://proceedings.neurips.cc/paper_files/paper/2024/file/93fab021315170101c92e8330a56fbdb-Paper-Conference.pdf)\n\n[69\\. RELAXED RECURSIVE TRANSFORMERS: EFFECTIVE PARAMETER SHARING WITH LAYER-WISE LoRA](https://openreview.net/pdf?id=WwpYSOkkCt)\n\n[70\\. Mixture-of-LoRAs: An Efficient Multitask Tuning for La...](http://arxiv.org/html/2403.03432v1)\n\n[71\\. METEO RA: MULTIPLE-TASKS EMBEDDED LORA FOR LARGE LANGUAGE MODELS](https://openreview.net/pdf?id=yOOJwR15xg)\n\n[72\\. DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic, Lightweight Plugin for Large Language Models](https://openreview.net/pdf?id=I1VCj1l1Zn)\n\n[73\\. Noam M. Shazeer, Azalia Mirhoseini et al. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” ArXiv](https://arxiv.org/abs/1701.06538)\n\n[74\\. MeteoRA: Multiple-tasks Embedded LoRA for ... - Paper Reading](https://paperreading.club/page?id=228237)\n\n[75\\. TT-LoRA MoE: Unifying Parameter-Efficient Fine-Tuning ...](https://arxiv.org/abs/2504.21190)\n\n[76\\. MiLoRA: Efficient Mixture of Low-Rank Adaptation for ...](https://aclanthology.org/2024.findings-emnlp.994/)\n\n[77\\. Dmitry Lepikhin, HyoukJoong Lee et al. “GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.” ArXiv](https://arxiv.org/abs/2006.16668)\n\n[81\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[82\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[83\\. OLMoE: Open Mixture-of-Experts Language Models](https://kyleclo.com/assets/pdf/olmoe-open-mixture-of-experts-language-models.pdf)\n\n[84\\. OLMoE: OPEN MIXTURE-OF-EXPERTS LANGUAGE MODELS](https://openreview.net/pdf/de8bda951e013f5ec54c4273b79414f3930bdda9.pdf)\n\n[85\\. MoE vs Dense vs Hybrid LLM Architectures](https://app.daily.dev/posts/moe-vs-dense-vs-hybrid-llm-architectures-8z05e90p7)\n\n[86\\. \\[Papierüberprüfung\\] Mixture of Lookup Experts](https://www.themoonlight.io/de/review/mixture-of-lookup-experts)\n\n[87\\. Multi-Head Mixture-of-Experts](https://proceedings.neurips.cc/paper_files/paper/2024/file/ab05dc8bf36a9f66edbff6992ec86f56-Paper-Conference.pdf)\n\n[88\\. W. Fedus, Barret Zoph et al. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” J. Mach. Learn. Res.](https://arxiv.org/abs/2101.03961)\n\n[89\\. Noam M. Shazeer, Azalia Mirhoseini et al. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” ArXiv](https://arxiv.org/abs/1701.06538)\n\n[90\\. LocMoE: A Low-Overhead MoE for Large Language Model Training](https://www.ijcai.org/proceedings/2024/0705.pdf)\n\n[91\\. A Mixture-of-Experts Approach for Code Generation On upcycling and sparsifying dense models](https://kth.diva-portal.org/smash/get/diva2:1903655/FULLTEXT01.pdf)\n\n[92\\. Dmitry Lepikhin, HyoukJoong Lee et al. “GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.” ArXiv](https://arxiv.org/abs/2006.16668)\n\n[93\\. 🔥 Top 10 LLMs Compared: Size, Architecture, and Training ...](https://azumo.com/insights/technical-architecture-analysis-how-the-top-10-llms-differ-in-size-architecture-and-training-data)\n\n[94\\. GitHub - SkunkworksAI/hydra-moe](https://github.com/SkunkworksAI/hydra-moe)\n\n[95\\. dots.11m1 Technical Report](https://web3.arxiv.org/pdf/2506.05767)\n\n[96\\. CoSMoEs: Compact Sparse Mixture of Experts](https://arxiv.org/pdf/2503.00245)\n\n[97\\. MoE performs worse than equivalent dense model? · Iss...](https://github.com/mlfoundations/open_lm/issues/253)\n\n[98\\. Comparison on Fine-tuning with a Dense Model](https://proceedings.neurips.cc/paper_files/paper/2022/file/2f00ecd787b432c1d36f3de9800728eb-Supplemental-Conference.pdf)\n\n[99\\. MoE, Dense, Hybrid 架构异同](https://www.unoiou.com/blog/2024/07/18/moe-dense-hybrid/)\n\n[101\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[102\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[103\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[104\\. N. Houlsby, A. Giurgiu et al. “Parameter-Efficient Transfer Learning for NLP.” ArXiv](https://arxiv.org/abs/1902.00751)\n\n[105\\. Fine-Tuning Transformers Efficiently: A Survey on LoRA and Its Impact](https://www.preprints.org/manuscript/202502.1637/download/final_file)\n\n[106\\. Xiang Lisa Li, Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Generation.” Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)](https://doi.org/10.18653/v1/2021.acl-long.353)\n\n[107\\. MoR: Mixture of Ranks for Low-Rank Adaptation Tuning](https://arxiv.org/pdf/2410.13408)\n\n[108\\. Shaoxiang Chen, Zequn Jie et al. “LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs.” ArXiv](https://doi.org/10.48550/arXiv.2401.16160)\n\n[109\\. MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for...](https://openreview.net/forum?id=uI19JapoCw)\n\n[110\\. MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning](https://www.clioapp.ai/research/mora)\n\n[111\\. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization](https://arxiv.org/pdf/2407.08044)\n\n[121\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[122\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[123\\. LoRA-Switch: Boosting the Efficiency of Dynamic LLM Ad...](http://arxiv.org/html/2405.17741v1)\n\n[124\\. N. Houlsby, A. Giurgiu et al. “Parameter-Efficient Transfer Learning for NLP.” ArXiv](https://arxiv.org/abs/1902.00751)\n\n[125\\. Efficient Multi-task LLM Quantization and Serving for Multiple LoRA Adapters](https://neurips.cc/media/neurips-2024/Slides/95811.pdf)\n\n[126\\. Token-Level Adaptation of LoRA Adapters for Downstream Task ...](https://dl.acm.org/doi/10.1145/3639592.3639615)\n\n[127\\. Efficient Multi-task LLM Quantization and Serving for ...](https://openreview.net/forum?id=HfpV6u0kbX&referrer=%5Bthe%20profile%20of%20Bin%20CUI%5D%28/profile?id%3D~Bin_CUI2%29)\n\n[128\\. Noam M. Shazeer, Azalia Mirhoseini et al. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” ArXiv](https://arxiv.org/abs/1701.06538)\n\n[129\\. Learning to Inference Adaptively for Multimodal Large Language Models](https://pages.cs.wisc.edu/~zxu444/home/paper/LlavaAdaptive_Inf_CVPR.pdf)\n\n[130\\. Dmitry Lepikhin, HyoukJoong Lee et al. “GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.” ArXiv](https://arxiv.org/abs/2006.16668)\n\n[131\\. Rui Kong, Qiyang Li et al. “LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design.” ArXiv](https://doi.org/10.48550/arXiv.2405.17741)\n\n[132\\. Fine-Tuning Transformers Efficiently: A Survey on LoRA and Its Impact](https://www.preprints.org/manuscript/202502.1637/download/final_file)\n\n[133\\. MoE-Infinity: Efficient MoE Inference on Personal Machines ...](https://arxiv.org/html/2401.14361v3)\n\n[134\\. Outrageously Fast LLMs: Faster Inference and Fine-Tuning with Moefication and LoRA](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/final-projects/ChiYoTsaiJayMartin.pdf)\n\n[135\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[136\\. Fuzzy Logic-based Management of Hybrid Distribution Transformers Using LoRa Technology](https://epjournal.csee.org.cn/jpes/cn/article/pdf/preview/10.17775/CSEEJPES.2020.06440.pdf)\n\n[137\\. CA-LoRA: Adapting Existing LoRA for Compressed LLMs to Enable Efficient Multi-Tasking on Personal Devices](https://openreview.net/pdf?id=kpf7UbnSAm)\n\n[138\\. Adaptation of Large Language Models for Streaming Machine Translation](https://m.riunet.upv.es/bitstream/handle/10251/207986/Vicente%20-%20Adaptation%20of%20Large%20Language%20Models%20for%20Streaming%20Machine%20Translation.pdf?sequence=1&isAllowed=y)\n\n[141\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[142\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[143\\. dots.llm1 Technical Report](https://www.52nlp.cn/wp-content/uploads/2025/06/%E5%B0%8F%E7%BA%A2%E4%B9%A6dots.llm1%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E8%8B%B1%E4%B8%AD%E5%AF%B9%E7%85%A7%E7%89%88.pdf)\n\n[144\\. DeepSpeed: Advancing MoE inference and training to ...](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/)\n\n[145\\. Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference](https://arxiv.org/pdf/2303.06182v1)\n\n[146\\. Revisiting MoE and Dense Speed-Accuracy Comparisons ...](https://arxiv.org/html/2405.15052v1)\n\n[147\\. DeepSpeed-MoE for NLG: Reducing the training cost of language models by 5 times - DeepSpeed](https://www.deepspeed.ai/2021/12/09/deepspeed-moe-nlg.html)\n\n[148\\. Revisiting MoE and Dense Speed-Accuracy Comparisons for LLM Training](https://paperreading.club/page?id=228821)\n\n[149\\. IFMoE: An Inference Framework Design for Fine-grained MoE](https://mlforsystems.org/assets/papers/neurips2024/paper41.pdf)\n\n[150\\. Low-cost Agents with Language Perception and Dynamic Inference](https://dspace.mit.edu/bitstream/handle/1721.1/158499/pan-bpan-phd-eecs-2024-thesis.pdf?sequence=-1&isAllowed=y)\n\n[151\\. HMoE: Heterogeneous Mixture of Experts for Language Mo...](http://arxiv.org/html/2408.10681v1)\n\n[152\\. Scalable and Efficient MoE Training for Multitask ...](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-moe-training-for-multitask-multilingual-models/?locale=zh-cn)\n\n[153\\. \\[2404.05567\\] Dense Training, Sparse Inference](https://arxiv.org/abs/2404.05567)\n\n[154\\. Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts](https://openreview.net/pdf/4382a5c42d5b1409ee2c995c609fcd778abc6e50.pdf)\n\n[155\\. Rethinking Training of Mixture-of-Experts Language Models](https://paperreading.club/page?id=220382)\n\n[156\\. Mixture-of-Experts in the Era of LLMs](https://courses.cs.washington.edu/courses/cse599k/24au/content/MoE.pdf)\n\n[157\\. Microsoft’s DeepSpeed-MoE Makes Massive MoE Model Inference up to 4.5x Faster and 9x Cheaper](https://syncedreview.com/2022/01/18/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-187/)\n\n[158\\. DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale](https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf)\n\n[161\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[162\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[163\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[164\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[165\\. Parameter Efficient Fine-tuning of Self-supervised ViTs without Catastrophic Forgetting](https://openaccess.thecvf.com/content/CVPR2024W/ELVM/papers/Bafghi_Parameter_Efficient_Fine-tuning_of_Self-supervised_ViTs_without_Catastrophic_Forgetting_CVPRW_2024_paper.pdf)\n\n[166\\. Xiang Lisa Li, Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Generation.” Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)](https://doi.org/10.18653/v1/2021.acl-long.353)\n\n[167\\. Imbalance-Regularized LoRA: A Plug-and-Play Method for Improving Fine-Tuning of Foundation Models](https://openreview.net/pdf/6e7b608f8128ec5dc7d9d7cd085528ab1715ab22.pdf)\n\n[168\\. LoRA-Switch: Boosting the Efficiency of Dynamic LLM Ad...](http://arxiv.org/html/2405.17741v1)\n\n[169\\. Mixed Text Recognition with Efficient Parameter Fine-Tuning and Transformer](https://arxiv.org/pdf/2404.12734)\n\n[170\\. Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters](https://www.isca-archive.org/interspeech_2024/cappellazzo24_interspeech.pdf)\n\n[171\\. ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision Transformers under Domain Shifts](https://arxiv.org/pdf/2406.10973)\n\n[172\\. Zequan Liu, Jiawen Lyn et al. “ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2403.16187)\n\n[173\\. Fine-Tuning Transformers Efficiently: A Survey on LoRA and Its Impact](https://www.preprints.org/manuscript/202502.1637/download/final_file)\n\n[174\\. When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications](http://export.arxiv.org/pdf/2310.18339)\n\n[175\\. mLoRA: Fine-Tuning LoRA Adapters via Highly-Efficient Pipeline Parallelism in Multiple GPUs](https://arxiv.org/pdf/2312.02515)\n\n[176\\. MAMBAPEFT: EXPLORING PARAMETER-EFFICIENT FINE-TUNING FOR MAMBA](https://arxiv.org/pdf/2411.03855)\n\n[177\\. The Evolved Transformer](http://proceedings.mlr.press/v97/so19a/so19a-supp.pdf)\n\n[178\\. Parameter-Efficient Fine-Tuning of Large Pretrained ...](https://www.mdpi.com/2504-4990/6/4/133)\n\n[179\\. MedDelinea: Scalable and Efficient Medical Image Segmentation via Controllable Diffusion Transformers](https://openreview.net/attachment?id=6dWhSay45H&name=pdf)\n\n[180\\. Towards Efficient Multi-Scale Deformable Attention on NPU](https://arxiv.org/pdf/2505.14022)\n\n[181\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[182\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[183\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[184\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[185\\. Noam M. Shazeer, Azalia Mirhoseini et al. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” ArXiv](https://arxiv.org/abs/1701.06538)\n\n[186\\. RELAXED RECURSIVE TRANSFORMERS: EFFECTIVE PARAMETER SHARING WITH LAYER-WISE LoRA](https://openreview.net/pdf?id=WwpYSOkkCt)\n\n[187\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[188\\. LoRA-Switch: Boosting the Efficiency of Dynamic LLM Ad...](http://arxiv.org/html/2405.17741v1)\n\n[189\\. \\[Feature Request\\] Dynamic Hierarchical Memory ...](https://github.com/ollama/ollama/issues/8861)\n\n[190\\. ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers](https://openreview.net/pdf?id=r9p9CV52MV)\n\n[191\\. QLoRA: Efficient Finetuning of Quantized LLMs](https://nips.cc/media/neurips-2023/Slides/73855.pdf)\n\n[192\\. MOE-GEN: High-Throughput MoE Inference on a Single GPU with Module-Based Batching](https://arxiv.org/pdf/2503.09716)\n\n[193\\. Mixture of Experts with Mixture of Precisions for Tuni...](http://arxiv.org/html/2407.14417v1)\n\n[194\\. LoHan: Low-Cost High-Performance Framework to Fine-Tune 100B Model on a Consumer GPU](https://arxiv.org/pdf/2403.06504.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-45-inside-nvidia-s-game-plan)\n\n[195\\. GitHub - draw-your-dream/Wan2GP: Wan 2.1 for the GPU P...](https://github.com/draw-your-dream/Wan2GP)\n\n[196\\. FloE: On-the-Fly MoE Inference on Memory-constrained GPU](https://arxiv.org/pdf/2505.05950)\n\n[197\\. Efficient Multi-task LLM Quantization and Serving for Multiple LoRA Adapters](https://neurips.cc/media/neurips-2024/Slides/95811.pdf)\n\n[198\\. DYNAMIC MIXTURE OF EXPERTS: AN AUTO-TUNING APPROACH FOR EFFICIENT TRANSFORMER MODELS](https://openreview.net/pdf?id=T26f9z2rEe)\n\n[201\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[202\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[203\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[204\\. Trade-off between Robustness and Accuracy of Vision Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Trade-Off_Between_Robustness_and_Accuracy_of_Vision_Transformers_CVPR_2023_paper.pdf)\n\n[205\\. Revisiting MoE and Dense Speed-Accuracy Comparisons for LLM Training](https://paperswithcode.com/paper/revisiting-moe-and-dense-speed-accuracy)\n\n[206\\. W. Fedus, Barret Zoph et al. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” J. Mach. Learn. Res.](https://arxiv.org/abs/2101.03961)\n\n[207\\. MIXTURE OF EXPERTS WITH MIXTURE OF PRECISIONS FOR TUNING QUALITY OF SERVICE](https://arxiv.org/pdf/2407.14417)\n\n[208\\. Benchmarking NLP and Computer Vision Models on Domain-Specific Architectures: Standard vs. TensorRT-Optimized Performance](https://journal.esrgroups.org/jes/article/download/7272/5009/13349)\n\n[209\\. Syrine Krichene, Thomas Müller et al. “DoT: An efficient Double Transformer for NLP tasks with tables.” Findings](https://doi.org/10.18653/v1/2021.findings-acl.289)\n\n[210\\. DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic, Lightweight Plugin for Large Language Models](https://openreview.net/pdf?id=I1VCj1l1Zn)\n\n[211\\. Parameter-Efficient Adaptation of BERT using LoRA and MoE](https://web.stanford.edu/class/cs224n/final-reports/256942242.pdf)\n\n[212\\. Phyllis Ang, Bhuwan Dhingra et al. “Characterizing the Efficiency vs. Accuracy Trade-off for Long-Context NLP Models.” ArXiv](https://doi.org/10.48550/arXiv.2204.07288)\n\n[213\\. Mixture of Experts with Mixture of Precisions for Tuni...](http://arxiv.org/html/2407.14417v1)\n\n[214\\. MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based Mixture of Experts](https://github.com/TUDB-Labs/MixLoRA)\n\n[215\\. Dmitry Lepikhin, HyoukJoong Lee et al. “GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.” ArXiv](https://arxiv.org/abs/2006.16668)\n\n[216\\. Reasoning Vs Non-Reasoning LLMs: Architectural Tradeoffs](https://www.ankursnewsletter.com/p/reasoning-vs-non-reasoning-llms-architectural)\n\n[217\\. HMORA: MAKING LLMs MORE EFFECTIVE WITH HIERARCHICAL MIXTURE OF LoRA EXPERTS](https://openreview.net/pdf?id=lTkHiXeuDl)\n\n[218\\. The Impact of LoRA Adapters for LLMs on Clinical NLP Classification Under Data Limitations](https://arxiv.org/pdf/2407.19299)\n\n[221\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[222\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[223\\. MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications](https://openreview.net/attachment?id=uI19JapoCw&name=pdf)\n\n[224\\. Brian Lester, Rami Al-Rfou et al. “The Power of Scale for Parameter-Efficient Prompt Tuning.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/2021.emnlp-main.243)\n\n[225\\. \\[论文审查\\] Higher Layers Need More LoRA Experts](https://www.themoonlight.io/zh/review/higher-layers-need-more-lora-experts)\n\n[226\\. N. Houlsby, A. Giurgiu et al. “Parameter-Efficient Transfer Learning for NLP.” ArXiv](https://arxiv.org/abs/1902.00751)\n\n[227\\. A Construction Method for a Coal Mining Equipment ...](https://www.mdpi.com/2227-7390/13/10/1638)\n\n[228\\. MoR: Mixture of Ranks for Low-Rank Adaptation Tuning](https://arxiv.org/pdf/2410.13408)\n\n[229\\. mLoRA: Fine-Tuning LoRA Adapters via Highly-Efficient Pipeline Parallelism in Multiple GPUs](https://arxiv.org/pdf/2312.02515)\n\n[230\\. When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications](http://export.arxiv.org/pdf/2310.18339)\n\n[231\\. Xiang Lisa Li, Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Generation.” Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)](https://doi.org/10.18653/v1/2021.acl-long.353)\n\n[232\\. Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/pdf/2403.14608)\n\n[233\\. LoRA-Switch: Boosting the Efficiency of Dynamic LLM Ad...](http://arxiv.org/html/2405.17741v1)\n\n[241\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[242\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[243\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[244\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[245\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[246\\. ON THE EFFICIENCY OF TRANSFORMERS](https://asset.library.wisc.edu/1711.dl/L5G4PGWKGIGKX8Q/R/file-99b91.pdf)\n\n[247\\. Deploy Diverse AI Apps with Multi-LoRA Support on RTX ...](https://developer.nvidia.com/blog/deploy-diverse-ai-apps-with-multi-lora-support-on-rtx-ai-pcs-and-workstations/)\n\n[248\\. QLoRA: Efficient Finetuning of Quantized LLMs](https://papers.nips.cc/paper_files/paper/2023/file/1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf)\n\n[249\\. LCM-LoRA: Unleashing the Speed and Power of Latent Diffusion Models](https://www.luseratech.com/sd/lcm-lora-unleashing-the-speed-and-power-of-latent-diffusion-models/)\n\n[250\\. LoRA-Switch: Boosting the Efficiency of Dynamic LLM Ad...](http://arxiv.org/html/2405.17741v1)\n\n[251\\. Efficient Multi-task LLM Quantization and Serving for Multiple LoRA Adapters](https://neurips.cc/media/neurips-2024/Slides/95811.pdf)\n\n[252\\. RELAXED RECURSIVE TRANSFORMERS: EFFECTIVE PARAMETER SHARING WITH LAYER-WISE LoRA](https://openreview.net/pdf?id=WwpYSOkkCt)\n\n[253\\. Mixture of Experts with Mixture of Precisions for Tuni...](http://arxiv.org/html/2407.14417v1)\n\n[254\\. Deploy Diverse AI Apps with Multi-LoRA Support on RTX AI ...](https://developer.nvidia.cn/blog/deploy-diverse-ai-apps-with-multi-lora-support-on-rtx-ai-pcs-and-workstations)\n\n[255\\. SPINQUANT: LLM QUANTIZATION WITH LEARNED ROTATIONS](https://openreview.net/pdf?id=ogO6DGE6FZ)\n\n[256\\. Jiwon Song, Kyungseok Oh et al. “SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks.” ArXiv](https://doi.org/10.48550/arXiv.2402.09025)\n\n[257\\. SLORA | Continuum Labs](https://training.continuumlabs.ai/inference/why-is-inference-important/slora)\n\n[258\\. OPTQ: ACCURATE POST-TRAINING QUANTIZATION FOR GENERATIVE PRE-TRAINED TRANSFORMERS](https://openreview.net/pdf?id=tcbBPnfwxS)\n\n[259\\. Weile Luo, Ruibo Fan et al. “Benchmarking and Dissecting the Nvidia Hopper GPU Architecture.” 2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS)](https://doi.org/10.1109/IPDPS57955.2024.00064)\n\n[261\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[262\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[263\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[264\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[265\\. Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency, Performance, and Adversarial Robustness](https://arxiv.org/pdf/2408.04585)\n\n[266\\. Alex Wang, Amanpreet Singh et al. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” BlackboxNLP@EMNLP](https://doi.org/10.18653/v1/W18-5446)\n\n[267\\. Lecture 8: Transformers and Large Pretrained Models](https://fenix.tecnico.ulisboa.pt/downloadFile/1970943312399678/lecture_08.pdf)\n\n[268\\. Revisiting MoE and Dense Speed-Accuracy Comparisons for LLM Training](https://paperreading.club/page?id=228821)\n\n[269\\. dots.11m1 Technical Report](https://web3.arxiv.org/pdf/2506.05767)\n\n[270\\. Revisiting MoE and Dense Speed-Accuracy Comparisons ...](https://arxiv.org/html/2405.15052v1)\n\n[271\\. Transformer 架构对比：Dense、MoE 与 Hybrid-MoE 的优劣分析](https://www.ppmy.cn/devtools/146660.html)\n\n[272\\. Lecture 8 Transformer](https://sites.cs.ucsb.edu/~lilei/course/ml22fa/lectures/08_Transformer.pdf)\n\n[273\\. Transformer架构下的模型比较：Dense - 易源易彩](https://www.yicaiai.com/news/article/6798ffab4ddd79f11a641b14)\n\n[274\\. A Comprehensive Overview of Large Language Models](https://openreview.net/attachment?id=HmqIhghNF6&name=pdf)\n\n[275\\. Mixture-of-Experts in the Era of LLMs](https://courses.cs.washington.edu/courses/cse599k/24au/content/MoE.pdf)\n\n[276\\. MoE vs Dense vs Hybrid LLM Architectures](https://app.daily.dev/posts/moe-vs-dense-vs-hybrid-llm-architectures-8z05e90p7)\n\n[277\\. MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems](https://arxiv.org/pdf/2505.11415)\n\n[278\\. Young Jin Kim, A. A. Awan et al. “Scalable and Efficient MoE Training for Multitask Multilingual Models.” ArXiv](https://arxiv.org/abs/2109.10465)\n\n[279\\. LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://openreview.net/pdf?id=nZeVKeeFYf9)\n\n[280\\. Rethinking Training of Mixture-of-Experts Language Models](https://paperreading.club/page?id=220382)\n\n[281\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[282\\. Fine-Tuning Transformers Efficiently: A Survey on LoRA and Its Impact](https://www.preprints.org/manuscript/202502.1637/download/final_file)\n\n[283\\. MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications](https://openreview.net/attachment?id=uI19JapoCw&name=pdf)\n\n[284\\. When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications](http://export.arxiv.org/pdf/2310.18339)\n\n[285\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[286\\. MoR: Mixture of Ranks for Low-Rank Adaptation Tuning](https://arxiv.org/pdf/2410.13408)\n\n[287\\. Brian Lester, Rami Al-Rfou et al. “The Power of Scale for Parameter-Efficient Prompt Tuning.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/2021.emnlp-main.243)\n\n[288\\. N. Houlsby, A. Giurgiu et al. “Parameter-Efficient Transfer Learning for NLP.” ArXiv](https://arxiv.org/abs/1902.00751)\n\n[289\\. Xiang Lisa Li, Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Generation.” Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)](https://doi.org/10.18653/v1/2021.acl-long.353)\n\n[290\\. UNLEASHING THE POWER OF PRE-TRAINED LANGUAGE MODELS FOR OFFLINE REINFORCEMENT LEARNING](https://openreview.net/pdf?id=xs0fA5iSYv)\n\n[291\\. HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning](https://proceedings.neurips.cc/paper_files/paper/2024/file/123fd8a56501194823c8e0dca00733df-Paper-Conference.pdf)\n\n[292\\. \\[论文审查\\] Higher Layers Need More LoRA Experts](https://www.themoonlight.io/zh/review/higher-layers-need-more-lora-experts)\n\n[293\\. Tongxu Luo, Jiahe Lei et al. “MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2402.12851)\n\n[294\\. MAMBAPEFT: EXPLORING PARAMETER-EFFICIENT FINE-TUNING FOR MAMBA](https://arxiv.org/pdf/2411.03855)\n\n[295\\. \"Small\" LLMS](https://laramartin.net/NLP-class/slides/25-04-29_Small-LLMs.pdf)\n\n[296\\. LLM fine-tuning & evaluation](https://fpaupier.fr/assets/20240228_Practical%20insights%20for%20LLM%20fine-tuning%20and%20evaluation.pdf)\n\n[301\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[302\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[303\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[304\\. Thomas Wolf, Lysandre Debut et al. “HuggingFace's Transformers: State-of-the-art Natural Language Processing.” ArXiv](https://arxiv.org/abs/1910.03771)\n\n[305\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[306\\. ON THE EFFICIENCY OF TRANSFORMERS](https://asset.library.wisc.edu/1711.dl/L5G4PGWKGIGKX8Q/R/file-99b91.pdf)\n\n[307\\. SPINQUANT: LLM QUANTIZATION WITH LEARNED ROTATIONS](https://arxiv.org/pdf/2405.16406)\n\n[308\\. RELAXED RECURSIVE TRANSFORMERS: EFFECTIVE PARAMETER SHARING WITH LAYER-WISE LoRA](https://openreview.net/pdf?id=WwpYSOkkCt)\n\n[309\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[310\\. Efficient Multi-task LLM Quantization and Serving for Multiple LoRA Adapters](https://neurips.cc/media/neurips-2024/Slides/95811.pdf)\n\n[311\\. LCM-LoRA: Unleashing the Speed and Power of Latent Diffusion Models](https://www.luseratech.com/sd/lcm-lora-unleashing-the-speed-and-power-of-latent-diffusion-models/)\n\n[312\\. Rethinking the Impact of Heterogeneous Sub-layers in Transformers](https://openreview.net/pdf?id=qG1S5eXMzx)\n\n[313\\. NVIDIA H100, GeForce RTX4090 機械学習ベンチマーク報告書](https://www.hpc.co.jp/library/wp-content/uploads/sites/8/2022/12/NVIDIA_H100_Geforce_RTX4090_DLBenchmarkReport_20221223.pdf)\n\n[314\\. Empower Vision Applications with LoRA LMM](https://arxiv.org/pdf/2411.00915)\n\n[315\\. Deploy Diverse AI Apps with Multi-LoRA Support on RTX AI ...](https://developer.nvidia.cn/blog/deploy-diverse-ai-apps-with-multi-lora-support-on-rtx-ai-pcs-and-workstations)\n\n[316\\. Weile Luo, Ruibo Fan et al. “Benchmarking and Dissecting the Nvidia Hopper GPU Architecture.” 2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS)](https://doi.org/10.1109/IPDPS57955.2024.00064)\n\n[317\\. ELASTIC LOAD BALANCING FOR DYNAMIC LLMs](https://openreview.net/pdf?id=ic1Z7Qe9xH)\n\n[318\\. LoRA-Switch: Boosting the Efficiency of Dynamic LLM Ad...](http://arxiv.org/html/2405.17741v1)\n\n[319\\. CNS-BENCH: BENCHMARKING MODEL ROBUSTNESS UNDER CONTINUOUS NUISANCE SHIFTS](https://openreview.net/pdf/4622017ee2a88e7f71406a3e070b5c1af4d1d7e9.pdf)\n\n[321\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[322\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[323\\. Parameter Efficient Fine-Tuning for Multi-Task BERT](https://web.stanford.edu/class/cs224n/final-reports/256732792.pdf)\n\n[324\\. Scalable and Efficient MoE Training for Multitask ...](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-moe-training-for-multitask-multilingual-models/?locale=zh-cn)\n\n[325\\. W. Fedus, Barret Zoph et al. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” J. Mach. Learn. Res.](https://arxiv.org/abs/2101.03961)\n\n[326\\. Noam M. Shazeer, Azalia Mirhoseini et al. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” ArXiv](https://arxiv.org/abs/1701.06538)\n\n[327\\. Revisiting MoE and Dense Speed-Accuracy Comparisons ...](https://arxiv.org/html/2405.15052v1)\n\n[328\\. Revisiting MoE and Dense Speed-Accuracy Comparisons for LLM Training](https://paperreading.club/page?id=228821)\n\n[329\\. Quantized Mixture-of-LoRA-Experts for Low-Cost Training of Large Language Models](https://digital.wpi.edu/downloads/vt150p950?locale=en)\n\n[330\\. Dmitry Lepikhin, HyoukJoong Lee et al. “GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.” ArXiv](https://arxiv.org/abs/2006.16668)\n\n[331\\. LoRA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall24/rohan/LoRA.pdf)\n\n[332\\. Young Jin Kim, A. A. Awan et al. “Scalable and Efficient MoE Training for Multitask Multilingual Models.” ArXiv](https://arxiv.org/abs/2109.10465)\n\n[333\\. Transformer 架构对比：Dense、MoE 与 Hybrid-MoE 的优劣分析](https://www.ppmy.cn/devtools/146660.html)\n\n[334\\. Low-cost Agents with Language Perception and Dynamic Inference](https://dspace.mit.edu/bitstream/handle/1721.1/158499/pan-bpan-phd-eecs-2024-thesis.pdf?sequence=-1&isAllowed=y)\n\n[335\\. google-research/xtreme](https://github.com/google-research/xtreme)\n\n[336\\. Parameter-Efficient Adaptation of BERT using LoRA and MoE](https://web.stanford.edu/class/cs224n/final-reports/256942242.pdf)\n\n[337\\. Elastic Processing and Hardware Architectures for Machine Learning](https://escholarship.org/content/qt8m24h60t/qt8m24h60t_noSplash_e38137b817bee40b0ac564f09ab7ad22.pdf?t=rgwobr)\n\n[338\\. HMORA: MAKING LLMs MORE EFFECTIVE WITH HIERARCHICAL MIXTURE OF LoRA EXPERTS](https://openreview.net/pdf?id=lTkHiXeuDl)\n\n[341\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[342\\. Fine-Tuning Transformers Efficiently: A Survey on LoRA and Its Impact](https://www.preprints.org/manuscript/202502.1637/download/final_file)\n\n[343\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[344\\. MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications](https://openreview.net/attachment?id=uI19JapoCw&name=pdf)\n\n[345\\. A Construction Method for a Coal Mining Equipment ...](https://www.mdpi.com/2227-7390/13/10/1638)\n\n[346\\. Brian Lester, Rami Al-Rfou et al. “The Power of Scale for Parameter-Efficient Prompt Tuning.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/2021.emnlp-main.243)\n\n[347\\. N. Houlsby, A. Giurgiu et al. “Parameter-Efficient Transfer Learning for NLP.” ArXiv](https://arxiv.org/abs/1902.00751)\n\n[348\\. Xiang Lisa Li, Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Generation.” Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)](https://doi.org/10.18653/v1/2021.acl-long.353)\n\n[349\\. Mixed Text Recognition with Efficient Parameter Fine-Tuning and Transformer](https://arxiv.org/pdf/2404.12734)\n\n[350\\. mLoRA: Fine-Tuning LoRA Adapters via Highly-Efficient Pipeline Parallelism in Multiple GPUs](https://arxiv.org/pdf/2312.02515)\n\n[351\\. MoR: Mixture of Ranks for Low-Rank Adaptation Tuning](https://arxiv.org/pdf/2410.13408)\n\n[352\\. Revolutionizing Large Model Fine-Tuning: The Role of LoRA in Parameter-Efficient Adaptation](https://www.techrxiv.org/doi/pdf/10.36227/techrxiv.174015835.57150536)\n\n[353\\. MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning](https://www.clioapp.ai/research/mora)\n\n[354\\. Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://openreview.net/pdf?id=lIsCS8b6zj)"
  },
  {
    "id": 21,
    "question": "Is AI actually a general purpose technology?",
    "response": "**Research Report: An Assessment of Artificial Intelligence as a General Purpose Technology**\n\n**Date of Report:** July 26, 2025\n\n**Commissioned by:** Internal Research Mandate\n\n**Lead Researcher:** \\[Helpful Assistant\\]\n\n**Executive Summary**\n\nThis report provides a comprehensive analysis of whether Artificial Intelligence (AI), in its current state as of mid-2025, qualifies as a General Purpose Technology (GPT). A GPT is an innovation so profound that it alters the entire economic and social fabric, analogous to the steam engine, electricity, and the computer. This analysis evaluates AI against the three core criteria of a GPT: pervasive applicability across sectors, continuous technological improvement, and the capacity to spawn complementary innovations.\n\nOur research indicates that AI strongly exhibits the characteristics of an emerging GPT. Its economic impact is beginning to mirror that of historical GPTs, with significant productivity gains observed at the firm level and soaring projections for aggregate economic growth. The technology is improving at an unprecedented rate, and its applications are becoming increasingly pervasive.\n\nHowever, a critical examination reveals persistent technical limitations, particularly in the areas of true reasoning, adaptability, and cross-domain generalization, which prevent its unqualified classification as a full-fledged GPT today. While models like ChatGPT demonstrate remarkable capabilities, they often fall short of genuine, human-like understanding and struggle to generalize to truly disparate domains without specific fine-tuning.\n\nThe verdict, as of July 2025, is that AI is best understood as a **proto-GPT** or an **emerging GPT**. It is clearly on a trajectory to become one of history's most significant GPTs, and its economic effects are already being felt as if it were one. Yet, the final technical hurdles to achieving true \"general purpose\" intelligence remain, making this a period of transition, implementation, and profound potential.\n\n**1\\. Introduction: Defining the Framework for a Technological Revolution**\n\nThroughout history, a select few innovations have earned the designation of General Purpose Technology (GPT). These are not mere incremental improvements but foundational breakthroughs that provide a platform for sweeping economic and societal transformation \\[23\\]\\[25\\]. The most commonly cited examples include the steam engine, which powered the Industrial Revolution; electricity, which reshaped manufacturing and urban life; and the computer and internet, which defined the digital age \\[23\\]\\[24\\]\\[25\\].\n\nA technology is considered a GPT if it meets three defining criteria, which form the analytical backbone of this report \\[3\\]\\[4\\]\\[5\\]:\n\n1.  **General Applicability (Pervasiveness):** It can be used across a wide array of sectors and economic activities \\[3\\]\\[17\\].\n2.  **Technological Dynamism (Continuous Improvement):** It undergoes rapid and persistent improvement, often on an exponential curve \\[3\\]\\[14\\].\n3.  **Innovational Complementarities (Enabling Innovation):** Its adoption enables and inspires a wave of secondary innovations in downstream products, processes, and business models \\[3\\]\\[14\\].\n\nThe central question of this report is whether Artificial Intelligence, particularly the advanced foundation models and generative systems prominent in 2025, has met these criteria. This investigation will synthesize economic data, technical analysis, and expert commentary to determine if AI is _actually_ a GPT, or if the designation remains aspirational.\n\n**2\\. The Compelling Case for AI as a General Purpose Technology**\n\nThe argument that AI is a GPT is strong and supported by substantial evidence across all three core criteria. Economists Erik Brynjolfsson and Andrew McAfee have gone so far as to call machine learning \"the most important general-purpose technology of our era\" \\[101\\].\n\n**2.1. Criterion 1: Pervasive and General Applicability**\n\nThe pervasiveness of AI is increasingly self-evident. By 2025, AI is no longer a niche technology confined to labs or the tech sector. It is actively being deployed across a vast spectrum of industries to enhance efficiency and create new value \\[5\\].\n\n**Broad Industry Adoption:** AI is reimagining service sectors and redesigning experiences in fields as diverse as medicine, banking, education, retail, media, and manufacturing \\[2\\]\\[238\\]. Real-world deployments in 2024 and 2025 show AI integrated into healthcare diagnostics, financial fraud detection, personalized education platforms, and autonomous transportation systems \\[164\\]\\[165\\]. Large Chinese tech firms like iFlytek, for instance, have deployed AI systems at scale in both education and healthcare across dozens of provinces \\[226\\].\n\n**High Corporate Priority:** AI adoption is a strategic imperative for businesses globally. A 2025 survey found that 87% of companies identify AI as a top priority in their business plans \\[146\\]. Another found that three in four employees report their organization uses AI, with nearly half stating it is used across a broad range of tasks \\[147\\]\\[208\\].\n\n**Massive User Engagement:** Consumer-facing AI systems have achieved unprecedented scale. By August 2024, ChatGPT reportedly reached 200 million weekly active users, demonstrating its applicability to a wide range of personal and professional tasks \\[234\\]. This broad utility across different contexts is a hallmark of a GPT.\n\n**Cross-Domain Knowledge Transfer:** The most advanced models developed in 2024, with over a trillion parameters, have demonstrated \"cross-domain knowledge transfer capabilities,\" driving progress simultaneously in finance, healthcare, and manufacturing \\[171\\]\\[282\\].\n\nWhile some analyses note that adoption is still concentrated in certain areas like online retail and lacks deep industrial connections in others \\[3\\], the overall trend is one of rapid and broadening diffusion.\n\n**2.2. Criterion 2: Unprecedented Technological Dynamism**\n\nAI, particularly deep learning, is characterized by a ferocious pace of improvement, arguably faster than any previous GPT.\n\n**Rapid Model Improvement:** The evolution from one generation of foundation models to the next has brought stunning leaps in capability. The period from 2024 to 2025 saw the introduction of highly sophisticated multi-modal models like OpenAI's GPT-4o and Anthropic's Claude 3.5, which exhibit significantly improved logical reasoning, problem-solving, and the ability to process vision and audio inputs \\[81\\]\\[87\\]\\[88\\]. This rapid, endogenous improvement is a core GPT characteristic \\[14\\].\n\n**Scalability with Data and Compute:** The architecture of deep learning is inherently designed to improve with scale. The models' layered feature-learning approach allows them to seamlessly take advantage of exponential increases in computational power and data availability \\[1\\].\n\n**Hardware Acceleration:** This dynamism is fueled by a co-evolving hardware ecosystem. NVIDIA's 2024 release of the Blackwell GPU architecture, for example, provides a massive boost to both training and inference capabilities, ensuring the cycle of improvement continues \\[81\\]\\[88\\].\n\n**2.3. Criterion 3: Fostering Innovational Complementarities**\n\nAI is not just a tool for optimization; it is a catalyst for new waves of innovation. It is widely seen as an \"engine of growth\" that enables new ways of inventing \\[21\\]\\[25\\]\\[28\\].\n\n**Downstream Innovation:** The availability of powerful AI APIs has induced a higher rate of innovation in downstream sectors \\[3\\]. Entrepreneurs and incumbent firms are building new products and services on top of foundation models, from AI-powered legal research tools to automated scientific discovery platforms.\n\n**The Rise of Agentic AI:** A significant complementary innovation wave is the development of autonomous AI agents. Enabled by the improved reasoning capabilities of 2024-2025 models, these agents can break down complex goals into smaller, executable steps, paving the way for automated workflows in commerce, research, and personal assistance \\[85\\].\n\n**The Ultimate Problem-Solving Tool:** Some proponents argue AI is the _most_ general of all GPTs. Once the problem of \"intelligence\" is solved, that intelligence can be directed to solve countless other problems, from curing diseases to designing new materials \\[13\\].\n\n**3\\. The Economic Footprint: Productivity Growth and Historical Parallels**\n\nThe economic signature of a GPT is a long-run increase in aggregate productivity growth. As of 2025, the data paints a picture of an economy on the cusp of a major AI-driven productivity boom, with striking parallels drawn to the impacts of the steam engine and electricity \\[31\\]\\[34\\].\n\n**3.1. Projections and Emerging Aggregate Evidence**\n\nWhile the full impact on national statistics is still materializing, projections from leading economic institutions are overwhelmingly positive and substantial.\n\n**Macroeconomic Forecasts:** Goldman Sachs forecasts a potential 1.5 percentage point annual boost to U.S. labor productivity growth over the next decade \\[66\\]\\[191\\]\\[369\\]. Similarly, analyses by McKinsey, the IMF, and others project annual GDP growth impacts of up to 1.3%, potentially adding over 35% to the GDP of advanced economies over a decade \\[62\\]\\[122\\]. A 2024 OECD estimate suggests AI could add between 0.4 and 0.9 percentage points to annual labor productivity growth \\[370\\].\n\n**Sector-Level Evidence:** The impact is already visible in industries with high AI exposure. A 2025 PwC report found that productivity growth in AI-exposed sectors like financial services nearly quadrupled, while it stagnated in the least exposed sectors \\[358\\]. Another study found that AI-exposed industries are growing employee productivity almost five times faster than others \\[75\\].\n\n**Empirical Studies:** A July 2025 study analyzing Chinese A-share listed companies from 2010-2022 found that AI \"significantly promotes the development of new quality productive forces,\" transforming labor skill structures towards high-skilled roles \\[190\\]\\[298\\].\n\n**3.2. Micro-Level Productivity Gains**\n\nThe macroeconomic forecasts are grounded in a growing body of micro-level evidence demonstrating clear productivity gains at the task and firm level.\n\n**Controlled Experiments:** A study with consultants using GPT-4 found they finished tasks 25.1% faster and produced 40% higher quality work \\[61\\]. An NBER study showed that providing an AI tool to call center agents increased their productivity by 14% on average, with a 34% gain for novice workers \\[357\\].\n\n**Real-World Tooling:** Users of GitHub's Copilot report completing tasks up to 73% faster \\[61\\]. Microsoft reports that 71% of organizations with a generative AI-first policy are already seeing productivity increases \\[357\\].\n\n**3.3. Addressing the Productivity Paradox**\n\nDespite these impressive figures, the impact on aggregate national productivity statistics remains more modest than some expect, a phenomenon reminiscent of the \"productivity paradox\" seen with computers in the 1980s and 1990s \\[29\\]\\[22\\]. Researchers identify several reasons for this lag, arguing that implementation delays are the biggest contributor \\[371\\]. Significant time is required for businesses to reinvent processes, for workers to acquire new skills, and for complementary innovations to mature and diffuse throughout the economy. Some analyses suggest that the most significant productivity growth from AI may not materialize in aggregate data until the late 2020s \\[134\\].\n\n**4\\. The Counter-Argument: Persistent Limitations and Gaps in Generality**\n\nDespite the powerful case in its favor, a rigorous assessment reveals critical shortcomings that challenge AI's current classification as a fully realized GPT. The core of the counter-argument lies in the \"G\" of GPT—generality. Current AI, for all its power, may not be truly _general_ in the way humans are.\n\n**4.1. The Chasm Between Performance and Understanding**\n\nA primary technical limitation is the gap between AI's ability to perform tasks and its ability to genuinely understand them.\n\n**The Illusion of Reasoning:** Many researchers argue that current systems exhibit an \"illusion of reasoning\" \\[43\\]. Their impressive outputs often stem from sophisticated pattern matching across vast training datasets rather than genuine, on-the-fly causal reasoning or common sense \\[45\\]\\[56\\].\n\n**Brittleness and Narrow Specialization:** AI systems remain brittle and highly specialized. They require enormous datasets for training and specific fine-tuning for new tasks \\[42\\]\\[49\\]. They struggle to generalize across diverse domains or adapt to data outside their training distribution without catastrophic forgetting, lacking the incremental and continuous learning capabilities of humans \\[49\\]\\[51\\].\n\n**Lack of Human-like Cognition:** AI lacks fundamental aspects of human intelligence, such as intentionality, genuine creativity, curiosity, and the ability to address questions of purpose and meaning \\[44\\]\\[59\\]. This prevents them from interacting with and evolving in the world in a truly general manner \\[43\\].\n\n**4.2. Scrutinizing the Evidence for Cross-Domain Generalization**\n\nThe claim of cross-domain generalization requires careful scrutiny. While a single model like ChatGPT can be _applied_ in healthcare, education, and law, its core competency remains language processing. The available evidence for its performance does not conclusively demonstrate true generalization across fundamentally unrelated tasks.\n\n**Performance Metrics:** Quantitative metrics for ChatGPT's capabilities are impressive but often domain-constrained. For example, it can pass the U.S. Medical Licensing Exam (USMLE) with accuracies often exceeding the passing threshold of 60% \\[338\\]\\[402\\]. In a 2024 analysis, it showed 84.5% accuracy on cardiovascular-related queries \\[411\\]. While these are different tests, they all fall within the broad domain of text-based reasoning and knowledge retrieval.\n\n**The Missing Evidence:** Search queries specifically seeking a commercial AI system deployed since 2024 with over 10 million users demonstrating _measurable cross-domain generalization across three unrelated industries_ yielded no definitive examples. This suggests that while systems are widely _used_ across domains, their ability to autonomously transfer and apply skills between, for example, medical diagnostics, legal contract analysis, and educational curriculum design without domain-specific fine-tuning is not yet a documented, measurable capability at scale. Studies show that for specialized tasks, fine-tuned models often still outperform general-purpose ones like ChatGPT, highlighting a trade-off between breadth and depth \\[398\\].\n\n**4.3. The Lack of Clear Expert Consensus**\n\nWhile many prominent figures champion AI as a GPT, the search for quantitative evidence of a formal consensus among economists and AI researchers proved fruitless. Queries for surveys conducted in the first half of 2025 by the American Economic Association (AEA), the National Bureau of Economic Research (NBER), and the World Economic Forum (WEF) asking this specific question did not yield any results \\[259\\]\\[323\\]\\[377\\]. This absence of data suggests that the classification of AI as a GPT remains a subject of ongoing academic debate and interpretation, not a settled matter confirmed by professional polling. Indeed, some leading economists remain cautious about the technology's near-term disruptive potential \\[148\\].\n\n**5\\. The Path Forward: Addressing Limitations with 2024-2025 Breakthroughs**\n\nThe AI research community is acutely aware of these limitations and is actively working to overcome them. Recent breakthroughs from 2024 and 2025 point toward a future of more robust and generalizable AI.\n\n**Multi-modal Reasoning:** The latest generation of models (GPT-4o, Gemini 2.0, Claude 3.5) are inherently multi-modal, meaning they can natively process and reason across text, images, audio, and video \\[87\\]\\[88\\]. This is a crucial step toward a more holistic, human-like understanding of the world.\n\n**Improved Reasoning and Agency:** Enthusiasm is growing for autonomous AI agents built on foundation models with enhanced chain-of-thought reasoning \\[85\\]. This allows models to tackle complex, multi-step problems by breaking them down into logical sub-tasks, moving beyond simple pattern recognition toward more adaptive problem-solving.\n\n**Towards Greater Trust and Efficiency:** Significant progress is being made in explainable AI (XAI), which aims to make AI decision-making transparent and trustworthy \\[84\\]\\[86\\]. Simultaneously, research into more efficient training paradigms like self-supervised learning aims to reduce AI's reliance on massive, labeled datasets, allowing it to learn more like humans do through exploration and inference \\[83\\].\n\n**6\\. Conclusion: A Verdict for July 2025**\n\nAfter a thorough review of the available evidence, is Artificial Intelligence a General Purpose Technology? The answer, as of mid-2025, is nuanced but clear: **AI is an emerging or proto-GPT of historic significance, but it has not yet fully matured to meet all criteria.**\n\nAI's performance on the criteria of **Technological Dynamism** and **Innovational Complementarities** is extraordinary and unambiguous. The pace of improvement is breathtaking, and AI is undeniably serving as a platform for a new wave of innovation across the economy.\n\nOn the criterion of **General Applicability**, the evidence is strong and growing daily. AI is being adopted across nearly every sector, and its economic impact is beginning to mirror that of past GPTs, with compelling micro-level productivity data and soaring macroeconomic projections.\n\nHowever, the analysis reveals a critical weakness in the _technical_ interpretation of \"general.\" The persistent lack of true, human-like reasoning, adaptability, and continuous learning means that current AI systems are not yet the truly general problem-solvers that the full definition of a GPT implies. Their generalization is powerful but often superficial, reliant on the patterns within their vast training data rather than a deep, flexible understanding of the world.\n\nTherefore, AI in 2025 stands at a pivotal juncture. It is producing the economic shockwaves of a GPT while the underlying technology still strives for true generality. It is an engine of transformation that is already reshaping industries, but the final leap to Artificial General Intelligence—the point at which the \"G\" in GPT would be undisputed—remains on the horizon. The current era is best defined as the period of implementation lag, where the world is racing to adapt its processes, skills, and institutions to a technology whose ultimate potential is still unfolding.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Yann LeCun, Yoshua Bengio et al. “Deep Learning.”](https://www.semanticscholar.org/paper/2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb)\n\n[2\\. Artificial Intelligence General Purpose Technology of the 21st Century](https://www.outrightcrm.com/blog/artificial-intelligence-general-purpose-technology/)\n\n[3\\. Systems of Change: A Study on the Nature of ICT and AI and Their Impact on Industrial Trajectories](https://www.db-thueringen.de/servlets/MCRFileNodeServlet/dbt_derivate_00054655/dissertationpytkova.pdf)\n\n[4\\. EXPLORING THE 6G SOFTWARE BUSINESS ECOSYSTEM: A systematic literature review and morphological analysis approach](https://lutpub.lut.fi/bitstream/handle/10024/166620/mastersthesis_Yang_Nan.pdf?sequence=1)\n\n[5\\. THE NEXT WAVE OF INVESTMENT: EFFICIENT AI APPLICATION AS THE KEY TO SUCCESS](https://www.serafin-am.com/fileadmin/kundendaten/Publications/SERAFIN__Investment_Insights_KI_EN.pdf)\n\n[6\\. A. Turing. “Computing Machinery and Intelligence.” Mind](https://doi.org/10.1093/MIND/LIX.236.433)\n\n[7\\. The Evaluation of Artificial Intelligence as a Prediction Problem](https://schellaert.org/papers/2025_Thesis.pdf)\n\n[8\\. The impact of generative AI as a general-purpose technology](https://mitsloan.mit.edu/ideas-made-to-matter/impact-generative-ai-a-general-purpose-technology)\n\n[9\\. Artificial Intelligence and the Labor Market: A Scenario-Based Approach](https://www.federalreserve.gov/newsevents/speech/files/barr20250509a.pdf)\n\n[10\\. Artificial Intelligence as General Purpose Technology: An Empirical and Applied Analysis of its Perception](https://univda.unitesi.cineca.it/bitstream/20.500.14084/428/1/ETI_104_Guidetti_Andr%C3%A9.pdf)\n\n[11\\. Artificial Intelligence as a General-Purpose Technology](https://thebusinessgossip.com/artificial-intelligence-as-a-general-purpose-technology/)\n\n[12\\. OECD Regional Outlook 2019: Leveraging Megatrends for Cities and Rural Areas](https://www.oecd.org/content/dam/oecd/en/publications/reports/2019/03/oecd-regional-outlook-2019_g1g9f65f/9789264312838-en.pdf)\n\n[13\\. THE AI AWAKENING - PRODUCTIVITY AND THE FUTURE OF WORK](https://www.insightinvestment.com/globalassets/documents/events/summit-2024/the-ai-awakening)\n\n[14\\. Exploring Artificial Intelligence as a General Purpose Technology with Patent Data](https://oms-www.files.svdcdn.com/production/downloads/academic/Exploring-Artificial-intelligence-WP-Upload-2022-5.pdf)\n\n[15\\. Digital infrastructure investment is crucial if generative AI is to seriously boost economic growth](https://think.ing.com/downloads/pdf/article/generative-ai-investment-economic-growth)\n\n[16\\. K. Thórisson. “A New Constructivist AI: From Manual Methods to Self-Constructive Systems.”](https://doi.org/10.2991/978-94-91216-62-6_9)\n\n[17\\. Acta Scientiarum Polonorum Oeconomia](http://acta_oeconomia.sggw.pl/pdf/Acta_Oeconomia_13_3_2014.pdf)\n\n[18\\. Generally Faster: The Economic Impact of Generative AI](https://ide.mit.edu/wp-content/uploads/2024/04/Davos-Report-Draft-XFN-Copy-01112024-Print-Version.pdf?x76181)\n\n[19\\. Is Generative AI a General Purpose Pedagogical Innovation?](https://www.insidehighered.com/opinion/views/2024/11/25/understanding-generative-ai-pedagogical-innovation-opinion)\n\n[21\\. Artificial Intelligence Study (88 FR 59942)](https://downloads.regulations.gov/COLC-2023-0006-9168/attachment_1.pdf)\n\n[22\\. Artificial Intelligence as General Purpose Technology: An Empirical and Applied Analysis of its Perception](https://univda.unitesi.cineca.it/bitstream/20.500.14084/428/1/ETI_104_Guidetti_Andr%C3%A9.pdf)\n\n[23\\. The impact of generative AI as a general-purpose technology](https://mitsloan.mit.edu/ideas-made-to-matter/impact-generative-ai-a-general-purpose-technology)\n\n[24\\. AI and the Global Battle for Tech Supremacy](https://eig.org/newbazaar/ai-and-the-global-battle-for-tech-supremacy/)\n\n[25\\. OECD Employment Outlook 2023: Artificial Intelligence and the Labour Market](https://www.oecd.org/content/dam/oecd/en/publications/reports/2023/07/oecd-employment-outlook-2023_904bcef3/08785bba-en.pdf)\n\n[26\\. Economic Policy for Artificial Intelligence](https://www.nber.org/system/files/working_papers/w24690/w24690.pdf)\n\n[27\\. Agentic AI: Finance & the 'Do It For Me' Economy](https://www.citiwarrants.com/home/upload/citi_research/rsch_pdf_30305836.pdf)\n\n[28\\. THE IMPACT OF ARTIFICIAL INTELLIGENCE ON PRODUCTIVITY, DISTRIBUTION AND GROWTH: KEY MECHANISMS, INITIAL EVIDENCE AND POLICY CHALLENGES](https://www.oecd.org/content/dam/oecd/en/publications/reports/2024/04/the-impact-of-artificial-intelligence-on-productivity-distribution-and-growth_d54e2842/8d900037-en.pdf)\n\n[29\\. Artificial intelligence: a central bank's view](https://www.ecb.europa.eu/press/key/date/2024/html/ecb.sp240704_1~e348c05894.en.html)\n\n[30\\. Artificial Intelligence: A Rights-Based Blueprint for Business](https://www.bsr.org/reports/BSR-Artificial-Intelligence-A-Rights-Based-Blueprint-for-Business-Paper-02.pdf)\n\n[31\\. Impact of Artificial Intelligence Innovation Proving to Be Monumental Making Dramatic Impact on Global Economy](https://www.financialnewsmedia.com/impact-of-artificial-intelligence-innovation-proving-to-be-monumental-making-dramatic-impact-on-global-economy/)\n\n[32\\. The Business of Artificial Intelligence: What it can — and cannot — do for your organization](https://people.dsv.su.se/~perjons/AI%20and%20Healthcare/The%20Business%20of%20Artificial%20Intelligence.pdf)\n\n[33\\. Scope, CO ST RPC Scope](https://www.coloradojudicial.gov/sites/default/files/2024-07/7.26.24%20Colo.%20RPC%20Committee%20Meeting%20Appendix%20to%20AI%20Subcom%20Memo.pdf)\n\n[34\\. Study claims AI will spur growth on par with the steam engine](https://artificialintelligence-news.com/2018/09/06/study-ai-growth-steam-engine/)\n\n[35\\. The Oxford Handbook of AI Governance](https://api.pageplace.de/preview/DT0400.9780197579336_A48723079/preview-9780197579336_A48723079.pdf)\n\n[36\\. Rise Of The Machines: AI Has Arrived](https://fnarena.com/index.php/2023/11/16/rise-of-the-machines-ai-has-arrived/)\n\n[37\\. Generally Faster: The Economic Impact of Generative AI](https://ide.mit.edu/wp-content/uploads/2024/04/Davos-Report-Draft-XFN-Copy-01112024-Print-Version.pdf?x76181)\n\n[38\\. Artificial Intelligence and the Modern Productivity Paradox: A Clash of Expectations and Statistics](https://www.nber.org/system/files/chapters/c14007/c14007.pdf)\n\n[41\\. International AI Safety Report](https://italianelfuturo.com/wp-content/uploads/2025/01/International_AI_Safety_Report_2025_accessible_f_250130_073143.pdf)\n\n[42\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[43\\. Comprehensive Review of Artificial General Intelligence AGI and Agentic GenAI: Applications in Business and Finance](https://www.preprints.org/frontend/manuscript/b95ddd7d9a9b21409573603edb9e1aa5/download_pub)\n\n[44\\. Advancing the Sustainment Enterprise to Data-Driven Logistics Operations](https://alu.army.mil/alog/ARCHIVE/PB7002304FULL.pdf)\n\n[45\\. J´er´emie Sublime. “The AI Race: Why Current Neural Network-based Architectures are a Poor Basis for Artificial General Intelligence.” J. Artif. Intell. Res.](https://doi.org/10.1613/jair.1.15315)\n\n[46\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[47\\. A. Turing. “Computing Machinery and Intelligence.” Mind](https://doi.org/10.1093/MIND/LIX.236.433)\n\n[48\\. 人工智能安全标准化白皮书（2019版）](http://www.cesi.cn/images/editor/20191101/20191101115151443.pdf)\n\n[49\\. INTELIGENCIA ARTIFICIAL, naturalmente](https://www.ontsi.es/sites/ontsi/files/2020-06/InteligenciaArtificialNuriaOliver.pdf)\n\n[50\\. AI and the Future of Skills, Volume 1: Capabilities and Assessments](https://www.skillsforemployment.org/sites/default/files/2024-01/5ee71f34-en.pdf)\n\n[51\\. Artificial General Intelligence](http://repo.darmajaya.ac.id/5336/2/Springer%20-%20Artificial%20General%20Intelligence%20%28%20PDFDrive%20%29.pdf)\n\n[52\\. Can Artificial Intelligence Patents Overcome §112 Requirements?, Part 2](https://www.omm.com/media/4r0bz353/ljn213202454351melveny.pdf)\n\n[53\\. Artificial Intelligence Act](https://www.vda.de/dam/jcr:bc8fa966-941d-4a45-b288-75892bcca2d2/KI%20Verordnung_EN.pdf)\n\n[54\\. Exploring Artificial Intelligence as a General Purpose Technology with Patent Data](https://oms-www.files.svdcdn.com/production/downloads/academic/Exploring-Artificial-intelligence-WP-Upload-2022-5.pdf)\n\n[55\\. General artificial intelligence: singularity versus anti-singularity and a way out of the theoretical impasse](https://dialnet.unirioja.es/descarga/articulo/9893935.pdf)\n\n[56\\. The Technological Singularity](https://dl.natfan.io/oldbooks/thetechnologicalsingularity.pdf)\n\n[57\\. David Silver, Julian Schrittwieser et al. “Mastering the game of Go without human knowledge.” Nature](https://doi.org/10.1038/nature24270)\n\n[58\\. J. Searle. “Minds, brains, and programs.” Behavioral and Brain Sciences](https://doi.org/10.1017/S0140525X00005756)\n\n[59\\. LIMITATIONS OF ARTIFICIAL INTELLIGENCE. WHY ARTIFICIAL INTELLIGENCE CANNOT REPLACE THE HUMAN MIND](https://bibliotekanauki.pl/articles/59110932.pdf)\n\n[60\\. Theoretical Foundations of Artificial General Intelligence](https://resource.laikipia.ac.ke/sites/default/files/Theoretical%20Foundations%20of%20Artificial%20General%20Intelligence.pdf)\n\n[61\\. Artificial Intelligence Index Report 2024](https://www.bollettinoadapt.it/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_.pdf)\n\n[62\\. Artificial Intelligence and Productivity in Europe](https://www.elibrary.imf.org/downloadpdf/view/journals/001/2025/067/001.2025.issue-067-en.pdf)\n\n[63\\. Artificial Intelligence Index Report 2025: Chapter 4 Economy](https://hai-production.s3.amazonaws.com/files/hai_ai-index-report-2025_chapter4_final.pdf)\n\n[64\\. Artificial intelligence: a central bank’s view](https://www.bis.org/review/r240709c.pdf)\n\n[65\\. Artificial intelligence, labour markets and inflation](https://www.suerf.org/wp-content/uploads/2024/07/SUERF-Policy-Brief-923_-Aldasoro-et-al.pdf)\n\n[66\\. THE IMPACT OF ARTIFICIAL INTELLIGENCE ON PRODUCTIVITY, DISTRIBUTION AND GROWTH: KEY MECHANISMS, INITIAL EVIDENCE AND POLICY CHALLENGES](https://www.oecd.org/content/dam/oecd/en/publications/reports/2024/04/the-impact-of-artificial-intelligence-on-productivity-distribution-and-growth_d54e2842/8d900037-en.pdf)\n\n[67\\. Will Productivity turn global growth around?](https://research-center.amundi.com/article/will-productivity-turn-global-growth-around)\n\n[68\\. Does Artificial Intelligence Really Improve the World’s Economy? Evidence from Cross Country-level Data](https://www.atlantis-press.com/article/126005589.pdf)\n\n[69\\. OECD Economic Surveys: Chile 2025](https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/01/oecd-economic-surveys-chile-2025_049bebf1/efad96ce-en.pdf)\n\n[70\\. The Economics of Demographics: Shaping Tomorrow](https://www.oecd.org/content/dam/oecd/en/about/programmes/global-forum-on-productivity/events/chania-2024/OECD-Crete-Centre-Forum-the-Economics-of-Demographics-Alvaro-Pereira.pdf)\n\n[71\\. Artificial Intelligence and the Discovery of New Ideas: Is an Economic Growth Explosion Imminent?](https://docs.iza.org/dp16766.pdf)\n\n[72\\. AI and Growth: Where Do We Stand?](https://www.frbsf.org/wp-content/uploads/AI-and-Growth-Aghion-Bunel.pdf)\n\n[73\\. Will Artificial Intelligence increase economic growth?](https://research-center.amundi.com/files/nuxeo/dl/6c1bcc39-e695-472e-b141-54538430561d?inline=)\n\n[74\\. The Economic Implications of Artificial Intelligence](https://www.tresor.economie.gouv.fr/Articles/c29a855c-027b-44fa-b813-0cfce7679376/files/ed9240ac-7a91-42c2-b35e-d99c78d3f258)\n\n[75\\. AI-intensive Sectors Show Exponential Productivity Growth](https://yourai.pro/according-to-pwc-productivity-in-ai-intensive-sectors-is-increasing-exponentially/)\n\n[76\\. Artificial intelligence will boost US productivity, says report](http://www.cnbc.com/2016/09/29/artificial-intelligence-will-boost-us-productivity-says-report.html)\n\n[77\\. Artificial intelligence and labor market outcomes - IZA World of Labor](https://wol.iza.org/articles/artificial-intelligence-and-labor-market-outcomes/long#:~:text=AI%20is%20reshaping%20the%20labor,-%20and%20low-skilled%20employees.)\n\n[78\\. Does Artificial Intelligence Guarantee An Increase In Productivity?](https://techstory.in/artificial-intelligence-guarantee-an-increase-in-productivity/)\n\n[79\\. Shuang Luo, Wenting Lei et al. “Impact of artificial intelligence technology innovation on total factor productivity: an empirical study based on provincial panel data in China.” National Accounting Review](https://doi.org/10.3934/nar.2024008)\n\n[81\\. AI News (2024-10-22)](https://metatrend.ai/tweet-trends-pdf/2024-10-22/en/AI.pdf)\n\n[82\\. A. Esteva, Brett Kuprel et al. “Dermatologist-level classification of skin cancer with deep neural networks.” Nature](https://doi.org/10.1038/nature21056)\n\n[83\\. Achieving Artificial Superintelligence: ANI to AGI to ASI](https://deepaimind.com/articles/r3-achieving-artificial-superintelligence.pdf)\n\n[84\\. Top 10 Breakthroughs in Explainable AI in 2024](https://industrywired.com/top-10-breakthroughs-in-explainable-ai-in-2024/)\n\n[85\\. Prospecting for Performance: Data Center Networking in 2025](https://arrcus-admin.prod.unomena.io/media/documents/AvidThnik_NGI_2025_DataCenter_Networking_AI_Cloud.pdf)\n\n[86\\. 恒小花：探索人工智能时代的突破](https://news.qq.com/rain/a/20241227A07Q5W00)\n\n[87\\. Artificial Intelligence Index Report 2023](https://aiindex.stanford.edu/wp-content/uploads/2023/04/HAI_AI-Index-Report-2023_CHAPTER_2.pdf)\n\n[88\\. 2024全球AI应用趋势年度报告](https://pdf.dfcfw.com/pdf/H3_AP202502021642724409_1.pdf?1738590342000.pdf)\n\n[89\\. 2024年， AI领域将会有哪些新突破？](https://www.zhihu.com/question/635190738/answer/3382874994)\n\n[90\\. Artificial Intelligence Index Report 2024](https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024.pdf)\n\n[91\\. 《麻省理工科技评论》2024年“十大突破性技术”](https://www.tsinghua.org.cn/__local/7/D8/95/E538AB2D9F82BC0AB268DDA7228_90F9B32E_4AE91F.pdf)\n\n[92\\. 陷入芯“铜”危机](https://m.36kr.com/p/3388652077711753)\n\n[93\\. Advances in Artificial Intelligence and Computer Science: Key Trends and Future Prospects in 2024](https://media.neliti.com/media/publications/592644-advances-in-artificial-intelligence-and-5117d52b.pdf)\n\n[94\\. Top 5 Breakthroughs in AI and Machine Learning for 2024](https://www.evonence.com/blog/top-5-breakthroughs-in-ai-and-machine-learning-for-2024)\n\n[95\\. A Inteligência Artificial em 2024 – balanço e pontes para 2025](https://interactideas.pt/guia2024/guia.pdf)\n\n[96\\. Yu Li, Qizhi Pei et al. “CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through Cryptography Challenges.”](https://arxiv.org/abs/2504.19093)\n\n[97\\. THE GRIFFIN EMERGING TECHNOLOGY STARBURST 2025 EDITION . WHAT WILL YOU #DISRUPT?](https://www.311institute.com/codexes/311%20Institute%20-%202025%20A3%20Emerging%20Technology%20Starburst%20Poster.pdf)\n\n[98\\. The Limits Of AGI: Views Based On Philosophical Logic](https://hal.science/hal-04749698v1/document)\n\n[99\\. Emerging Innovative Technologies in Economies of the Future](https://rdia.gov.sa/media/z4ij3v1t/copy3-emerging-innovative-technologies-in-economies-of-the-future-v1-eng.pdf)\n\n[101\\. Consilio Symposium 2024 Insights from the Bench: Judicial Perspectives on Evolving Challenges](https://www.consilio.com/wp-content/uploads/2024/06/Symposium24-CLE11-Insights-from-the-Bench-Judicial-Perspectives-on-Evolving-Challenges.pdf)\n\n[102\\. Artificial Intelligence Index Report 2025: Chapter 4 Economy](https://hai-production.s3.amazonaws.com/files/hai_ai-index-report-2025_chapter4_final.pdf)\n\n[103\\. Trust, attitudes and use of artificial intelligence: A global study 2025](https://assets.kpmg.com/content/dam/kpmgsites/xx/pdf/2025/05/trust-attitudes-and-use-of-ai-global-report.pdf)\n\n[104\\. 机遇与风险并存：人工智能行业的全球格局、我国面临的发展困境与对策](https://m.yicai.com/news/102246615.html)\n\n[105\\. TechnoVision: Top 5 Tech Trends to Watch in 2025](https://www.capgemini.com/wp-content/uploads/2024/11/11_27_Capgemini-Top-5-Tech-Trends-2025-Press-Release.pdf)\n\n[106\\. Brandbuch Purpose Trends 2025](https://brandbuch.com/assets/purpose-trends-report-2025-by-brandbuch.pdf)\n\n[107\\. AI in 2025: Structured Strategic Insights for Decision-Makers](https://ugc.production.linktr.ee/db4b3471-a59d-492a-ab25-19d4df2c6039_AI-in-2025--Structured-Strategic-Insights-for-Decision-Makers--1-.pdf)\n\n[108\\. Global Outlook on Financing for Sustainable Development 2025: Towards a More Resilient and Inclusive Architecture](https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/02/global-outlook-on-financing-for-sustainable-development-2025_6748f647/753d5368-en.pdf)\n\n[109\\. 50 NEW Artificial Intelligence Statistics (June 2025)](https://explodingtopics.com/blog/ai-statistics#:~:text=As%20of%202025,%20as%20many,to%20utilize%20big%20data%20effectively.)\n\n[110\\. Global Artificial Intelligence Report (2025) | IDCA](https://www.idc-a.org/insights/0bKr4NJQdK5sYcAQaGZD)\n\n[111\\. In focus: AI statistics, insights and trends | Definition](https://www.thisisdefinition.com/resources/ai-statistics)\n\n[112\\. Artificial Intelligence – Global 2025 Outlook: Broadening AI capabilities will unlock new use cases](https://cuadernoborrador.com/wp-content/uploads/2025/01/outlook-artificial-intelligence-global-13jan2025-pbc_1425377-1.pdf)\n\n[113\\. The age of Intelligence: Empowering human-AI collaboration for a trusted future - A perspective on Trust, attitudes and use of artificial intelligence: A global study 2025](https://kpmg.com/kpmg-us/content/dam/kpmg/pdf/2025/trust-attitudes-artificial-intelligence-executive-summary.pdf)\n\n[114\\. Artificial Intelligence Statistics - Magnet ABA](https://www.magnetaba.com/blog/artificial-intelligence-statistics#:~:text=The%20growth%20of%20AI%20is,36.6%25%20between%202024%20and%202030.)\n\n[115\\. Top 140 Artificial Intelligence Stats for 2025](https://thunderbit.com/blog/top-artificial-intelligence-stats)\n\n[116\\. AI and big data core skills by industry 2025](https://www.statista.com/statistics/1602860/ai-and-big-data-core-skills-by-industry/)\n\n[117\\. Artificial Intelligence Industry Development Forecast](https://ojs.s-p.sg/index.php/bdai/article/download/20941/pdf)\n\n[118\\. Vanguard economic and market outlook for 2025: Global summary](https://digital-assets.vanguard.com/intl/australia/shared/documents/resources/VEMO_2025_AU_Final.pdf)\n\n[121\\. Artificial Intelligence Index Report 2024](https://www.bollettinoadapt.it/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_.pdf)\n\n[122\\. Artificial Intelligence and Productivity in Europe](https://www.elibrary.imf.org/downloadpdf/view/journals/001/2025/067/001.2025.issue-067-en.pdf)\n\n[123\\. Global Economic Futures: Productivity in 2030](https://reports.weforum.org/docs/WEF_Global_Economic_Futures_Productivity_in_2030_2025.pdf)\n\n[124\\. 全球智库半月谈](http://www.iwep.org.cn/xscg/xscg_lwybg/202503/W020250311402791316843.pdf)\n\n[125\\. ANNUAL REPORT 2024](https://file.fpts.com.vn/FileStore2/File/2025/04/03/20250403_FPT_250403_Annual_Report_2024.pdf)\n\n[126\\. OECD Economic Surveys: Chile 2025](https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/01/oecd-economic-surveys-chile-2025_049bebf1/efad96ce-en.pdf)\n\n[127\\. Global Outlook on Financing for Sustainable Development 2025: Towards a more resilient and inclusive architecture](https://www.developmentaid.org/api/frontend/cms/file/2025/02/753d5368-en.pdf)\n\n[128\\. Top 140 Artificial Intelligence Stats for 2025](https://thunderbit.com/blog/top-artificial-intelligence-stats)\n\n[129\\. Global Productivity & Economic Slowdown 2025](https://www.startus-insights.com/innovators-guide/global-productivity-and-economic-slowdown/)\n\n[130\\. ArXiv Papers Browser - Teng Wang](http://ttengwang.com/arxiv-papers.html)\n\n[131\\. Artificial intelligence, labour markets and inflation](https://www.suerf.org/wp-content/uploads/2024/07/SUERF-Policy-Brief-923_-Aldasoro-et-al.pdf)\n\n[132\\. D. Acemoglu, P. Restrepo. “Automation and New Tasks: How Technology Displaces and Reinstates Labor.” NBER Working Paper Series](https://doi.org/10.1257/JEP.33.2.3)\n\n[133\\. AI and the G20: Striking a balance between innovation and governance](https://www.globalpolicyjournal.com/sites/default/files/pdf/Stal%20-%20AI%20and%20the%20G20%20Striking%20a%20balance%20between%20innovation%20and%20governance.pdf)\n\n[134\\. Vanguard economic and market outlook for 2025: Global summary](https://digital-assets.vanguard.com/intl/australia/shared/documents/resources/VEMO_2025_AU_Final.pdf)\n\n[135\\. THE IMPACT OF ARTIFICIAL INTELLIGENCE ON PRODUCTIVITY, DISTRIBUTION AND GROWTH: KEY MECHANISMS, INITIAL EVIDENCE AND POLICY CHALLENGES](https://www.oecd.org/content/dam/oecd/en/publications/reports/2024/04/the-impact-of-artificial-intelligence-on-productivity-distribution-and-growth_d54e2842/8d900037-en.pdf)\n\n[136\\. D. Acemoglu, P. Restrepo. “The Race between Man and Machine: Implications of Technology for Growth, Factor Shares, and Employment.” American Economic Review](https://doi.org/10.1257/AER.20160696)\n\n[137\\. C. Frey, Michael A. Osborne. “The future of employment: How susceptible are jobs to computerisation?.” Technological Forecasting and Social Change](https://doi.org/10.1016/J.TECHFORE.2016.08.019)\n\n[138\\. Adopt AI Study](https://ieu-monitoring.com/wp-content/uploads/Adopt_AI__Final_study_report_0P6WutvqV0occxmADGTSP8i8X8_108555.pdf)\n\n[139\\. Artificial Intelligence and the labor market](https://www.cgi.br/media/docs/publicacoes/6/20241218181258/year-xvi-n-4-artificial-intelligence-and-the-labor-market.pdf)\n\n[141\\. Exploring Applications and Trends of Generative Artificial Intelligence](http://apjcriweb.org/content/vol10no6/10.pdf)\n\n[142\\. D. Rumelhart, Geoffrey E. Hinton et al. “Learning representations by back-propagating errors.” Nature](https://doi.org/10.1038/323533a0)\n\n[143\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[144\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[145\\. 2025年人工智能指数报告](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025_chinese_version_061325.pdf)\n\n[146\\. Global Artificial Intelligence Report (2025) | IDCA](https://idc-a.org/insights/0bKr4NJQdK5sYcAQaGZD)\n\n[147\\. Trust, attitudes and use of artificial intelligence: A global study 2025](https://assets.kpmg.com/content/dam/kpmgsites/xx/pdf/2025/05/trust-attitudes-and-use-of-ai-global-report.pdf)\n\n[148\\. Brandbuch Purpose Trends 2025](https://brandbuch.com/assets/purpose-trends-report-2025-by-brandbuch.pdf)\n\n[149\\. The Great Pivot: Global Insurance Survey 2025](https://am.gs.com/cms-assets/gsam-app/documents/insights/en/2025/am-Insurance-survey-2025.pdf)\n\n[150\\. Top tech trends of 2025: AI-powered everything](https://www.capgemini.com/wp-content/uploads/2025/01/Top-Tech-Trends-2025_Report.pdf)\n\n[151\\. 2025 Banking Priorities Executive Report](https://go.csiweb.com/rs/996-ERF-896/images/WP_NPT_MER_BankingPriorities25.pdf)\n\n[152\\. German Social Collaboration Study 2020](https://www.campana-schott.com/media/user_upload/Downloads/Brochure/EN/DSCS_2020_EN.pdf)\n\n[153\\. AI and big data core skills by industry 2025](https://www.statista.com/statistics/1602860/ai-and-big-data-core-skills-by-industry/)\n\n[154\\. 39 Digital Transformation Statistics for 2025](https://www.walkme.com/blog/digital-transformation-statistics/)\n\n[155\\. Global Sentiment Survey 2025: AI is here. What next?](https://www.opensesame.com/wp-content/uploads/2025/02/GSS-2025-Report-v-2.1-by-Donald-H-Taylor-for-OpenSesame.pdf)\n\n[156\\. Amina Adadi, M. Berrada. “Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI).” IEEE Access](https://doi.org/10.1109/ACCESS.2018.2870052)\n\n[157\\. 2025 CIO Priorities and Technology Trends](https://trendsunplugged.io/wp-content/uploads/2024/10/report.pdf)\n\n[158\\. Artificial Intelligence | AI Statistics Development](https://teksun.com/blog/surprising-ai-statistics-the-remarkable-development-of-artificial-intelligence/)\n\n[161\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[162\\. 185个真实AI应用场景案例，涵盖六大领域，全球170多个公司和组织](http://sccio.cn/uploads/20250605/8517b91e374810eb1b04ab24fcf47677.pdf)\n\n[163\\. Sustainability Report - 2024 | ABB](https://abb-bank.az/storage/uploads/files/1746681018_abb-sustainability-report-2024.pdf)\n\n[164\\. 10 Examples of Artificial Intelligence in Real-Life (2024)](https://www.geeksforgeeks.org/10-examples-of-artificial-intelligence-in-real-life-2024/)\n\n[165\\. Top 18 Artificial Intelligence (AI) Applications in 2024](https://rejolut.com/blog/top-ai-applications/)\n\n[166\\. AI应用新纪元：引爆“人工智能+消费”革命](https://aigc.idigital.com.cn/djyanbao/%E3%80%90%E5%8D%8E%E8%A5%BF%E8%AF%81%E5%88%B8%E3%80%91AI%E5%BA%94%E7%94%A8%E6%96%B0%E7%BA%AA%E5%85%83%EF%BC%9A%E5%BC%95%E7%88%86%E2%80%9C%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%2B%E6%B6%88%E8%B4%B9%E2%80%9D%E9%9D%A9%E5%91%BD-2025-03-04.pdf)\n\n[167\\. Top 11 Applications of Artificial Intelligence in 2024](https://www.appsierra.com/blog/11-artificial-intelligence-applications)\n\n[168\\. Applications of Artificial Intelligence (AI) in 2024](https://ijrpr.com/uploads/V5ISSUE9/IJRPR33082.pdf)\n\n[169\\. 传媒行业2024年回顾](https://pdf.dfcfw.com/pdf/H3_AP202412271641446526_1.pdf?1735306590000.pdf)\n\n[170\\. 2023 Global Trends in AI - WEKA](https://www.weka.io/resources/analyst-report/2023-global-trends-in-ai/)\n\n[171\\. 2024年度新质生产力视角下的人工智能人才供需与培养研究报告](http://www.csia-jpw.com/UserFiles/Article/findings/e8a71813-0fba-4bf1-aad8-9de5666f3b49.pdf)\n\n[172\\. 2024:AI加速“出圈”“扩圈”“AI+”应用全线爆发,商业化正快速落地](https://epaper.qlwb.com.cn/qlwb/PDF/20241231/A06.pdf)\n\n[173\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2024.” ArXiv](https://doi.org/10.48550/arXiv.2405.19522)\n\n[174\\. Top 10 AI Applications Exploding Across Industries in 2024](https://techovedas.com/top-10-ai-applications-exploding-across-industries-in-2024/)\n\n[175\\. ARTIFICIAL INTELLIGENCE IN ACTION: REAL-WORLD APPLICATIONS AND BENEFITS](https://www.irjmets.com/uploadedfiles/paper/issue_12_december_2024/64813/final/fin_irjmets1733137099.pdf)\n\n[176\\. 50 Mind-blowing ways AI will Reshape Our World in 2024](https://haydenjames.io/50-mind-blowing-ways-ai-will-reshape-our-world-in-2024/)\n\n[177\\. Top 20 Applications of Artificial Intelligence (AI) in 2024](https://www.geeksforgeeks.org/applications-of-ai/)\n\n[178\\. 2024年人工智能全球调查报告](https://www.bilibili.com/video/av112822130443928?t=133)\n\n[179\\. Artificial Intelligence Applications in Sustainable Life Sciences: A Global Review of Recent Developments and Implementations (2020–2024)](https://www.preprints.org/manuscript/202504.1284/v1/download)\n\n[180\\. AI for Good-Innovate for Impact Final Report 2024](http://demo.ifgict.org/wp-content/uploads/2024/09/ITU-AI-for-Good-Innovate-final-report-1.pdf)\n\n[181\\. Labor substitution effect of artificial intelligence in the era of population aging: evidence from panel data across countries and panel data at provincial level in China](https://jtp.cnki.net/bilingual/detail/html/ZKRK201806004)\n\n[182\\. 人工智能对就业规模及劳动收入的影响——来自Meta分析的证据](https://journal.cueb.edu.cn/docs/20230919125156410250.pdf)\n\n[183\\. Artificial Intelligence, Labor Demand, and Human Capital Investments](https://rkyj.ruc.edu.cn/EN/abstract/abstract3836.shtml)\n\n[184\\. Daron Acemoglu, P. Restrepo. “Robots and Jobs: Evidence from US Labor Markets.” Journal of Political Economy](https://doi.org/10.1086/705716)\n\n[185\\. C. Frey, Michael A. Osborne. “The future of employment: How susceptible are jobs to computerisation?.” Technological Forecasting and Social Change](https://doi.org/10.1016/J.TECHFORE.2016.08.019)\n\n[186\\. D. Acemoglu, P. Restrepo. “The Race between Man and Machine: Implications of Technology for Growth, Factor Shares, and Employment.” American Economic Review](https://doi.org/10.1257/AER.20160696)\n\n[187\\. Georg Graetz, Guy Michaels. “The Review of Economics and Statistics.”](https://www.semanticscholar.org/paper/f2d582c675ac69cc5da0b54d9b1fe53329f01820)\n\n[188\\. Jason Furman, Robert C. Seamans. “AI and the Economy.” Innovation Policy and the Economy](https://doi.org/10.1086/699936)\n\n[189\\. G. Damioli, Vincent Van Roy et al. “The impact of artificial intelligence on labor productivity.” Eurasian Business Review](https://doi.org/10.1007/s40821-020-00172-8)\n\n[190\\. 人工智能对新质生产力发展的影响研究](http://qks.cqu.edu.cn/cqdxskcn/article/abstract/202503006?st=article_issue)\n\n[191\\. The impact of Artificial Intelligence on productivity, distribution and growth.OECD Artificial Intelligence Papers](https://doi.org/10.1787/8d900037-en)\n\n[192\\. Christian-Andreas Schumann, Vanessa Reiher et al. “Artificial intelligence and employment.” OECD Social, Employment and Migration Working Papers](https://doi.org/10.1787/c2c1d276-en)\n\n[193\\. 人工智能发展对制造业就业的影响——基于马克思社会再生产模型的中国经验分析](http://www.ddjjyj.com/CN/article/downloadArticleFile.do?attachType=PDF&id=2225)\n\n[194\\. 人工智能、老龄化与经济增长](http://ft.newdu.com/uploads/collect/201908/20/W0201908205759218607071452.pdf)\n\n[195\\. 人工智能与劳动收入份额](https://journal.bjut.edu.cn/bjgydxxbskb/cn/article/doi/10.12120/bjutskxb202206099?viewType=HTML)\n\n[196\\. A. Georgieff, Raphaela Hyee. “Artificial Intelligence and Employment: New Cross-Country Evidence.” Frontiers in Artificial Intelligence](https://doi.org/10.3389/frai.2022.832736)\n\n[201\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[202\\. Xing Hao, Guigang Zhang et al. “Deep Learning.” Int. J. Semantic Comput.](https://doi.org/10.1142/S1793351X16500045)\n\n[203\\. D. Rumelhart, Geoffrey E. Hinton et al. “Learning representations by back-propagating errors.” Nature](https://doi.org/10.1038/323533a0)\n\n[204\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[205\\. Artificial general intelligence - Wikipedia](https://heydavid.ai/examples/agi.pdf)\n\n[206\\. 2025年人工智能指数报告](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025_chinese_version_061325.pdf)\n\n[207\\. Global Artificial Intelligence Report (2025) | IDCA](https://idc-a.org/insights/0bKr4NJQdK5sYcAQaGZD)\n\n[208\\. Trust, attitudes and use of artificial intelligence: A global study 2025](https://assets.kpmg.com/content/dam/kpmgsites/xx/pdf/2025/05/trust-attitudes-and-use-of-ai-global-report.pdf)\n\n[209\\. C. Frey, Michael A. Osborne. “The future of employment: How susceptible are jobs to computerisation?.” Technological Forecasting and Social Change](https://doi.org/10.1016/J.TECHFORE.2016.08.019)\n\n[210\\. Brandbuch Purpose Trends 2025](https://brandbuch.com/assets/purpose-trends-report-2025-by-brandbuch.pdf)\n\n[211\\. TECH COMPASS 2024](https://www.bosch.pl/media/news/sztuczna_inteligencja_btc/bosch-tech-compass-2024.pdf)\n\n[212\\. AI and big data core skills by industry 2025](https://www.statista.com/statistics/1602860/ai-and-big-data-core-skills-by-industry/)\n\n[213\\. AI Poll Compilation: Public Opinion on Reading AI (2024)](https://aiconsequences.com/ai-poll-compilation/)\n\n[214\\. Top tech trends of 2025: AI-powered everything](https://www.capgemini.com/wp-content/uploads/2025/01/Top-Tech-Trends-2025_Report.pdf)\n\n[215\\. The Path to AI Maturity 2024](https://icert.ie/wp-content/uploads/2024/09/Path-To-Maturity-2024-Report.pdf)\n\n[221\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[222\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[223\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[224\\. M. Everingham, L. Gool et al. “The Pascal Visual Object Classes (VOC) Challenge.” International Journal of Computer Vision](https://doi.org/10.1007/s11263-009-0275-4)\n\n[225\\. A. Torralba, Alexei A. Efros. “Unbiased look at dataset bias.” CVPR 2011](https://doi.org/10.1109/CVPR.2011.5995347)\n\n[226\\. 星火大模型持续升级,AI商业化加速落地- 策略研报 _ 数据中...](https://data.eastmoney.com/report/zw_strategy.jshtml?encodeUrl=rg0FGrqEwi5GZzLFcfia4hk3bn5PgRiDna16JSpyYgk%3D)\n\n[227\\. Artificial Intelligence Index Report 2024](https://40006059.fs1.hubspotusercontent-na1.net/hubfs/40006059/Stanford_HAI_2024_AI-Index-Report.pdf)\n\n[228\\. Cross-Domain Feature Augmentation for Domain Generalization](https://www.ijcai.org/proceedings/2024/0127.pdf)\n\n[229\\. Self-Challenging Improves Cross-Domain Generalization](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470120.pdf)\n\n[230\\. Analysis of the Memorization and Generalization ...](https://arxiv.org/abs/2309.10149)\n\n[231\\. 北京亚康万玮信息技术股份有限公司 2024 年年度报告摘要](https://file.finance.qq.com/finance/hs/pdf/2025/04/23/1223221017.PDF)\n\n[232\\. 36氪研究院｜2024年AIGC行业研究：多模态大模型与商业应用](https://aigcdaily.cn/news/a24p8go4bqjooq0/)\n\n[233\\. 浙江中国小商品城集团股份有限公司 2024 年年度报告](https://www.sse.com.cn/disclosure/listedinfo/announcement/c/new/2025-03-27/600415_20250327_G3J4.pdf)\n\n[234\\. Unlocking Global Value: A Business-Driven AI Implementation and Management Framework](https://www.theseus.fi/bitstream/handle/10024/885143/Salminen_Timo.pdf?sequence=2&isAllowed=y)\n\n[235\\. Cross-Domain Ensemble Distillation for Domain Generalization](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850001.pdf)\n\n[236\\. Spotlight on Artificial Intelligence](https://www.cadstudio.cz/dl/state-of-design-and-make-2024-spotlight-on-artificial-intelligence-en.pdf)\n\n[237\\. Matching-space Stereo Networks for Cross-domain Generalization](https://www.changjiangcai.com/files/msnet-3dv2020/msnet-3dv-conf20-pub.pdf)\n\n[238\\. 大模型加速从探索走向落地](https://max.book118.com/try_down/596051135104011025.pdf)\n\n[241\\. Daron Acemoglu, P. Restrepo. “Robots and Jobs: Evidence from US Labor Markets.” Journal of Political Economy](https://doi.org/10.1086/705716)\n\n[242\\. C. Frey, Michael A. Osborne. “The future of employment: How susceptible are jobs to computerisation?.” Technological Forecasting and Social Change](https://doi.org/10.1016/J.TECHFORE.2016.08.019)\n\n[243\\. D. Acemoglu, P. Restrepo. “The Race between Man and Machine: Implications of Technology for Growth, Factor Shares, and Employment.” American Economic Review](https://doi.org/10.1257/AER.20160696)\n\n[244\\. Georg Graetz, Guy Michaels. “The Review of Economics and Statistics.”](https://www.semanticscholar.org/paper/f2d582c675ac69cc5da0b54d9b1fe53329f01820)\n\n[245\\. W. Dauth, S. Findeisen et al. “German Robots - The Impact of Industrial Robots on Workers.” European Economics: Labor & Social Conditions eJournal](https://www.semanticscholar.org/paper/8756cd379467c7e6d5583f17c69f785280a7b01e)\n\n[246\\. L'intelligence artificielle va affecter 70% des emplois actuels en Chine](http://french.peopledaily.com.cn/Economie/n3/2018/0918/c31355-9501493.html)\n\n[247\\. 人工智能、企业生产率与劳动力技能匹配](http://cfrc.pbcsf.tsinghua.edu.cn/__local/B/7C/91/499E28DD79AED5D3E651A6649D4_F561E197_1A1DD8.pdf?e=.pdf)\n\n[248\\. China 2025-40 Outlook: Tech and policy foundation](https://www.dbs.com/content/article/pdf/AIO/042025/250409_insights_china.pdf)\n\n[249\\. 人工智能技术创新能拉动企业劳动力需求吗？](https://ghll.sdmu.edu.cn/__local/3/42/8D/C19A470951DC4481B896601B7C8_8AF6549B_14F17E.pdf)\n\n[250\\. AI to affect 70% of occupations as robotics complement ...](http://www.ecns.cn/news/sci-tech/2018-09-17/detail-ifyxxzwt9221228.shtml)\n\n[251\\. 人工智能对中国区域经济高质量发展影响的理论机理与实证分析——以工业机器人为例](https://www.rdfybk.com/qw/DownPdf?id=832344)\n\n[252\\. 人工智能与劳动收入份额](https://journal.bjut.edu.cn/bjgydxxbskb/cn/article/doi/10.12120/bjutskxb202206099?viewType=HTML)\n\n[253\\. 工业智能化如何重塑劳动力就业结构](https://ciejournal.ajcass.com/UploadFile/Issue/mq5tqe4j.pdf)\n\n[254\\. 人工智能对绿色经济增长的作用机制与赋能效果——产业结构优化视角](https://www.kjjb.org/CN/article/downloadArticleFile.do?attachType=PDF&id=19190)\n\n[255\\. 2025.01.07liuzhiyong-local](https://www.bilibili.com/video/av113787709489867?t=1245)\n\n[256\\. 人工智能与劳动收入份额——来自中国城市数据的经验证据](https://journal.bjut.edu.cn/bjgydxxbskb/cn/article/pdf/preview/10.12120/bjutskxb202206099.pdf)\n\n[257\\. Exploring Applications and Trends of Generative Artificial Intelligence](http://apjcriweb.org/content/vol10no6/10.pdf)\n\n[258\\. Does Artificial Intelligence Really Improve the World’s Economy? Evidence from Cross Country-level Data](https://www.atlantis-press.com/article/126005589.pdf)\n\n[259\\. Artificial Intelligence: Solving problems, growing the economy and improving our quality of life](https://www.csiro.au/-/media/D61/AI-Roadmap-assets/19-00346_DATA61_REPORT_AI-Roadmap_WEB_191111.pdf)\n\n[260\\. 2025年人工智能指数报告](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025_chinese_version_061325.pdf)\n\n[261\\. The Evaluation of Artificial Intelligence as a Prediction Problem](https://schellaert.org/papers/2025_Thesis.pdf)\n\n[262\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[263\\. Brandbuch Purpose Trends 2025](https://brandbuch.com/assets/purpose-trends-report-2025-by-brandbuch.pdf)\n\n[264\\. Trust, attitudes and use of artificial intelligence: A global study 2025](https://assets.kpmg.com/content/dam/kpmgsites/xx/pdf/2025/05/trust-attitudes-and-use-of-ai-global-report.pdf)\n\n[265\\. The Economics of Artificial Intelligence: An Agenda](https://business.columbia.edu/sites/default/files-efs/imce-uploads/Joseph_Stiglitz/The%20Economics%20of%20Artificial%20Intelligence%20-%20Chapter%2014_0.pdf)\n\n[266\\. Artificial Intelligence Statistics - Magnet ABA](https://www.magnetaba.com/blog/artificial-intelligence-statistics#:~:text=The%20growth%20of%20AI%20is,36.6%25%20between%202024%20and%202030.)\n\n[267\\. Chief Economists Outlook May 2025](https://reports.weforum.org/docs/WEF_Chief_Economists_Outlook_May_2025.pdf)\n\n[268\\. Top 140 Artificial Intelligence Stats for 2025](https://thunderbit.com/blog/top-artificial-intelligence-stats)\n\n[269\\. Technologies ranked by expected importance within 2025](https://www.statista.com/statistics/732288/worldwide-research-and-development-important-technologies/)\n\n[270\\. 机遇与风险并存：人工智能行业的全球格局、我国面临的发展困境与对策](https://m.yicai.com/news/102246615.html)\n\n[271\\. Vanguard economic and market outlook for 2025: Global summary](https://digital-assets.vanguard.com/intl/australia/shared/documents/resources/VEMO_2025_AU_Final.pdf)\n\n[272\\. Should economists be worried about artificial intelligence?](https://www.bankimpresanews.com/communication-tecnologies/2017/03/06/4262_should-economists-be-worried-about-artificial-intelligence/)\n\n[273\\. German Social Collaboration Study 2020](https://www.campana-schott.com/media/user_upload/Downloads/Brochure/EN/DSCS_2020_EN.pdf)\n\n[274\\. AI and big data core skills by industry 2025](https://www.statista.com/statistics/1602860/ai-and-big-data-core-skills-by-industry/)\n\n[275\\. Artificial Intelligence | AI Statistics Development](https://teksun.com/blog/surprising-ai-statistics-the-remarkable-development-of-artificial-intelligence/)\n\n[277\\. 2024: The Year of Responsible Generative AI](https://www.lumenova.ai/blog/2024-year-of-responsible-generative-ai/)\n\n[278\\. ChatGPT and Global Higher Education: Using Artificial Intelligence in Teaching and Learning](https://scholar.harvard.edu/sites/scholar.harvard.edu/files/roychan/files/ebook_chatgpt_and_global_higher_education.pdf)\n\n[279\\. Tiffany H. Kung, Morgan Cheatham et al. “Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models.” PLOS Digital Health](https://doi.org/10.1371/journal.pdig.0000198)\n\n[280\\. Generative AI in higher education: A cross-sector analysis ...](https://www.aimspress.com/article/doi/10.3934/steme.2025035?viewType=HTML)\n\n[281\\. E. A. V. van Dis, J. Bollen et al. “ChatGPT: five priorities for research.” Nature](https://doi.org/10.1038/d41586-023-00288-7)\n\n[282\\. 2024年度新质生产力视角下的人工智能人才供需与培养研究报告](http://www.csia-jpw.com/UserFiles/Article/findings/e8a71813-0fba-4bf1-aad8-9de5666f3b49.pdf)\n\n[283\\. Six Important eDiscovery Trends for 2024 and How to Prepare for Them](https://www.veritas.com/content/dam/www/en_us/documents/white-papers/WP_ebook_veritas_ediscovery_trends.pdf)\n\n[284\\. 2024年度C&C賞受賞者業績と略歴](https://jpn.nec.com/press/202410/images/1501-01-01.pdf)\n\n[285\\. ChatGPT在2024年的应用与潜力](https://digitalinpek.com/what-is-chatgpt-top-10-unique-ways-to-use-it-in-2024/)\n\n[286\\. Chris Stokel-Walker. “ChatGPT listed as author on research papers: many scientists disapprove.” Nature](https://doi.org/10.1038/d41586-023-00107-z)\n\n[287\\. How generative AI Is shaping the future of marketing](https://link.springer.com/content/pdf/10.1007/s11747-024-01064-3.pdf)\n\n[288\\. Is your curriculum GenAI-proof? A method for GenAI impact assessment and a case study](https://mededpublish.org/articles/15-11/v1/pdf?article_uuid=f6966354-3657-4d72-a51a-a9bf7c259844)\n\n[289\\. M. Liebrenz, R. Schleifer et al. “Generating scholarly content with ChatGPT: ethical challenges for medical publishing..” The Lancet. Digital health](https://doi.org/10.1016/s2589-7500%2823%2900019-5)\n\n[290\\. PROCEEDINGS OF THE INTERNATIONAL CONFERENCES on E-SOCIETY 2024 AND MOBILE LEARNING 2024](https://files.eric.ed.gov/fulltext/ED659933.pdf)\n\n[291\\. ChatGPT Year in Review + GenAI Look Ahead to 2024](https://www.dazzagreenwood.com/p/chatgpt-year-in-review-genai-look)\n\n[292\\. Chris Stokel-Walker. “AI bot ChatGPT writes smart essays - should professors worry?.” Nature](https://doi.org/10.1038/d41586-022-04397-7)\n\n[293\\. AI developments to track in 2025](https://www.inma.org/modules/event/2024GenAITownHall/replay/SamGuzik_INMATH24.pdf)\n\n[294\\. 2024 Outlook: Finally “Normal”](https://abundancellc.com/wp-content/uploads/2024/01/2024_Outlook_Finally_Normal_Abundance.pdf)\n\n[295\\. Cross-Domain Feature Augmentation for Domain Generalization](https://www.ijcai.org/proceedings/2024/0127.pdf)\n\n[296\\. Comprehensive AI assessment framework: Enhancing educational evaluation with ethical AI integration](https://files.eric.ed.gov/fulltext/EJ1457302.pdf)\n\n[297\\. In focus: AI statistics, insights and trends | Definition](https://www.thisisdefinition.com/resources/ai-statistics)\n\n[298\\. 人工智能对新质生产力发展的影响研究](http://qks.cqu.edu.cn/cqdxskcn/article/abstract/202503006?st=article_issue)\n\n[299\\. 人工智能如何赋能企业新质生产力](https://www.kjjb.org/article/2025/1001-7348/2025-42-7-001.htm)\n\n[300\\. Daron Acemoglu, P. Restrepo. “Robots and Jobs: Evidence from US Labor Markets.” Journal of Political Economy](https://doi.org/10.1086/705716)\n\n[301\\. D. Acemoglu, P. Restrepo. “The Race between Man and Machine: Implications of Technology for Growth, Factor Shares, and Employment.” American Economic Review](https://doi.org/10.1257/AER.20160696)\n\n[302\\. Accelerate the development of artificial intelligence to achieve a leap in productivity](https://www.lwxsd.com/en/info_view.php?tab=mynews&VID=50865)\n\n[303\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[304\\. 以社会政策健全推动乡村全面振兴长效机制](http://dbase.gslib.com.cn:8000/DRCNet.Mirror.Documents.Web/DocAttachments.aspx?AttachmentId=235827)\n\n[305\\. D. Acemoglu, P. Restrepo. “Artificial Intelligence, Automation and Work.” Alfred P. Sloan Foundation Economic Research Paper Series](https://doi.org/10.2139/ssrn.3098384)\n\n[306\\. Georg Graetz, Guy Michaels. “The Review of Economics and Statistics.”](https://www.semanticscholar.org/paper/f2d582c675ac69cc5da0b54d9b1fe53329f01820)\n\n[307\\. 人工智能赋能新质生产力：逻辑、模式及路径](https://rem.cueb.edu.cn/docs/2024-07/57a217fc4e8e4c9fa6610ca6d528a871.pdf)\n\n[308\\. PwC's 2024 AI Jobs Barometer](https://www.lexology.com/library/detail.aspx?g=bb24a363-96e5-4a18-8232-0213a14f56f9)\n\n[309\\. 人工智能发展与新质生产力提升：理论机制与实证检验](https://www.kjjb.org/CN/10.6049/kjjbydc.L2024XZ589)\n\n[310\\. Erik Brynjolfsson, Daniel Rock et al. “NBER WORKING PAPER SERIES ARTIFICIAL INTELLIGENCE AND THE MODERN PRODUCTIVITY PARADOX: A CLASH OF EXPECTATIONS AND STATISTICS.”](https://www.semanticscholar.org/paper/30d3715f9ed3f240e5d208db4f6c8ee8792edf91)\n\n[311\\. Harnessing AI-Driven Growth](http://www.circuitinsight.com/programs/54894.html)\n\n[312\\. Xiumin Li, Haojian Tang et al. “Artificial Intelligence and the New Quality Productive Forces of Enterprises: Digital Intelligence Empowerment Paths and Spatial Spillover Effects.” Systems](https://doi.org/10.3390/systems13020105)\n\n[313\\. New quality productive forces spur global development](http://en.chinadiplomacy.org.cn/2024-08/19/content_117374912.shtml)\n\n[314\\. Bloomberg Artificial Intelligence Index Methodology](https://assets.bbhub.io/professional/sites/27/Bloomberg-Artificial-Intelligence-Index-Methodology.pdf)\n\n[315\\. 人工智能促进新质生产力涌现的路径和机制研究](http://www.jishujingji.cn/technology_economics/article/abstract/J24021502)\n\n[316\\. 产学研合作如何激发数字原生企业发展新质生产力——知识编排视角下的探索性单案例研究](https://ciejournal.ajcass.com/UploadFile/Issue/201606280001/2025/1/20250122103059WU_FILE_1.pdf)\n\n[317\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[318\\. D. Rumelhart, Geoffrey E. Hinton et al. “Learning representations by back-propagating errors.” Nature](https://doi.org/10.1038/323533a0)\n\n[319\\. Exploring Applications and Trends of Generative Artificial Intelligence](http://apjcriweb.org/content/vol10no6/10.pdf)\n\n[320\\. Does Artificial Intelligence Really Improve the World’s Economy? Evidence from Cross Country-level Data](https://www.atlantis-press.com/article/126005589.pdf)\n\n[321\\. Artificial Intelligence: Solving problems, growing the economy and improving our quality of life](https://www.csiro.au/-/media/D61/AI-Roadmap-assets/19-00346_DATA61_REPORT_AI-Roadmap_WEB_191111.pdf)\n\n[322\\. 2025年人工智能指数报告](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025_chinese_version_061325.pdf)\n\n[323\\. The Evaluation of Artificial Intelligence as a Prediction Problem](https://schellaert.org/papers/2025_Thesis.pdf)\n\n[324\\. 德勤：42％的高管认为人工智能将在2年内变得“至关重要”](https://zhuanlan.zhihu.com/p/47389902)\n\n[325\\. This is what people around the world think about AI | World Economic Forum](https://www.weforum.org/agenda/2022/01/artificial-intelligence-ai-technology-trust-survey/)\n\n[326\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[327\\. Technologies ranked by expected importance within 2025](https://www.statista.com/statistics/732288/worldwide-research-and-development-important-technologies/)\n\n[328\\. Using AI and machine learning to reduce government fraud](https://intodetails.com/using-ai-and-machine-learning-to-reduce-government-fraud.html)\n\n[329\\. C. Feijóo, Y. Kwon et al. “Harnessing artificial intelligence (AI) to increase wellbeing for all: The case for a new technology diplomacy.” Telecommunications Policy](https://doi.org/10.1016/j.telpol.2020.101988)\n\n[330\\. Trust, attitudes and use of artificial intelligence: A global study 2025](https://assets.kpmg.com/content/dam/kpmgsites/xx/pdf/2025/05/trust-attitudes-and-use-of-ai-global-report.pdf)\n\n[331\\. Exploring Artificial Intelligence as a General Purpose Technology with Patent Data](https://oms-www.files.svdcdn.com/production/downloads/academic/Exploring-Artificial-intelligence-WP-Upload-2022-5.pdf)\n\n[332\\. 机遇与风险并存：人工智能行业的全球格局、我国面临的发展困境与对策](https://m.yicai.com/news/102246615.html)\n\n[333\\. Brandbuch Purpose Trends 2025](https://brandbuch.com/assets/purpose-trends-report-2025-by-brandbuch.pdf)\n\n[334\\. 首席经济学家们如何展望2024](https://www.cccme.org.cn/news/details.aspx?id=ADDB6B0449ABEAC59B1045045B9F5096&classid=E8B46837315EAAD5&xgid=F868932F64EB7AAF)\n\n[335\\. Adopt AI Study](https://ieu-monitoring.com/wp-content/uploads/Adopt_AI__Final_study_report_0P6WutvqV0occxmADGTSP8i8X8_108555.pdf)\n\n[337\\. Performance of ChatGPT in medical licensing ...](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0312771)\n\n[338\\. Tiffany H. Kung, Morgan Cheatham et al. “Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models.” PLOS Digital Health](https://doi.org/10.1371/journal.pdig.0000198)\n\n[339\\. Aidan Gilson, C. Safranek et al. “How Does ChatGPT Perform on the United States Medical Licensing Examination (USMLE)? The Implications of Large Language Models for Medical Education and Knowledge Assessment.” JMIR Medical Education](https://doi.org/10.2196/45312)\n\n[340\\. PROCEEDINGS OF THE INTERNATIONAL CONFERENCES on E-SOCIETY 2024 AND MOBILE LEARNING 2024](https://files.eric.ed.gov/fulltext/ED659933.pdf)\n\n[341\\. 2024年度C&C賞受賞者業績と略歴](https://jpn.nec.com/press/202410/images/1501-01-01.pdf)\n\n[342\\. ...systematic review and meta-analysis | BMC Medical E...](https://link.springer.com/article/10.1186/s12909-024-05944-8)\n\n[343\\. Bibliometric analysis of ChatGPT based on Scopus data: Global research trends (2022–2024)](https://icvl.eu/documents/154/a24_Mejia_Garcia_Reyes_Flores_G3hrewV.pdf)\n\n[344\\. 2024년 춘계전국학술대회 발표자료집](https://konige.kr/files/sub0203/konige202408081451370.pdf)\n\n[345\\. 2024 ASAIHL Conference, Tokyo, at Juntendo University Jun 16-17, 2024 Conference Suchedule](https://asaihl2024-juntendo.com/img/ASAIHL2024_Abstract_0611.pdf)\n\n[346\\. Performance of ChatGPT-3.5 and GPT-4 in national licensing examinations ...](https://bmcmededuc.biomedcentral.com/articles/10.1186/s12909-024-05944-8)\n\n[347\\. Yiqiu Shen, L. Heacock et al. “ChatGPT and Other Large Language Models Are Double-edged Swords..” Radiology](https://doi.org/10.1148/radiol.230163)\n\n[348\\. ChatGPT: Bullshit spewer or the end of traditional assessments in higher education?.1](https://doi.org/10.37074/jalt.2023.6.1.9)\n\n[349\\. ChatGPT在医学领域研究态势的文献计量学分析](https://yizhe.dmu.edu.cn/data/article/yxyzx/preview/pdf/7-8-lvjing.pdf)\n\n[350\\. 2024北京国际模拟联合国大会背景指南](https://bimun.org.cn/wp-content/uploads/BI24-BG-UNESCO.pdf)\n\n[351\\. A. Gandhi, MD Felista Karen Joesph et al. “Performance of ChatGPT on the India Undergraduate Community Medicine Examination: Cross-Sectional Study.” JMIR Formative Research](https://doi.org/10.2196/49964)\n\n[352\\. 臺灣博碩士論文加值系統](https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/login?o=dnclcdr&s=id%3D%22112NCKU5312001%22.&searchmode=basic)\n\n[353\\. Nino Fijačko, Lucija Gosak et al. “Can ChatGPT Pass the Life Support Exams without Entering the American Heart Association Course?.” Resuscitation](https://doi.org/10.1016/j.resuscitation.2023.109732)\n\n[354\\. AI赋能千行百业，政策孕育新机遇——2024年计算机行业策略](https://pdf.dfcfw.com/pdf/H3_AP202312201614578032_1.pdf?1703070185000.pdf)\n\n[357\\. In focus: AI statistics, insights and trends | Definition](https://www.thisisdefinition.com/resources/ai-statistics)\n\n[358\\. AI linked to a fourfold increase in productivity growth and ...](https://www.pwc.com/th/en/press-room/press-release/2025/press-release-09-07-25-en.html)\n\n[359\\. Daron Acemoglu, P. Restrepo. “Robots and Jobs: Evidence from US Labor Markets.” Journal of Political Economy](https://doi.org/10.1086/705716)\n\n[360\\. MIT Study Shows 50% Productivity gain due to AI](https://maisonmeta.io/mit-study-shows-50-productivity-gain-due-to-ai/)\n\n[361\\. D. Acemoglu, P. Restrepo. “The Race between Man and Machine: Implications of Technology for Growth, Factor Shares, and Employment.” American Economic Review](https://doi.org/10.1257/AER.20160696)\n\n[362\\. AI and Productivity Growth: Evidence from Previous Technologies](https://www.stlouisfed.org/on-the-economy/2024/apr/ai-productivity-growth-evidence-historical-development-other-technologies)\n\n[363\\. Artificial intelligence and labor market outcomes - IZA World of Labor](https://wol.iza.org/articles/artificial-intelligence-and-labor-market-outcomes/long#:~:text=AI%20is%20reshaping%20the%20labor,-%20and%20low-skilled%20employees.)\n\n[364\\. Georg Graetz, Guy Michaels. “The Review of Economics and Statistics.”](https://www.semanticscholar.org/paper/f2d582c675ac69cc5da0b54d9b1fe53329f01820)\n\n[365\\. Philippe Aghion, Benjamin F. Jones et al. “Artificial Intelligence and Economic Growth.” Kauffman: Conferences & Seminars (Topic)](https://doi.org/10.3386/W23928)\n\n[366\\. D. Acemoglu, P. Restrepo. “Artificial Intelligence, Automation and Work.” Alfred P. Sloan Foundation Economic Research Paper Series](https://doi.org/10.2139/ssrn.3098384)\n\n[367\\. Process Audit: How to prepare your team for AI](https://www.dailyclicks.net/process-audit-how-to-prepare-your-team-for-ai/)\n\n[368\\. The impact of artificial intelligence – an economic analysis](https://www.treasury.govt.nz/sites/default/files/2024-07/an24-06.pdf)\n\n[369\\. THE IMPACT OF ARTIFICIAL INTELLIGENCE ON PRODUCTIVITY, DISTRIBUTION AND GROWTH: KEY MECHANISMS, INITIAL EVIDENCE AND POLICY CHALLENGES](https://www.oecd.org/content/dam/oecd/en/publications/reports/2024/04/the-impact-of-artificial-intelligence-on-productivity-distribution-and-growth_d54e2842/8d900037-en.pdf)\n\n[370\\. Miracle or Myth? Assessing the macroeconomic productivity gains from Artificial Intelligence](https://www.oecd.org/content/dam/oecd/en/publications/reports/2024/11/miracle-or-myth-assessing-the-macroeconomic-productivity-gains-from-artificial-intelligence_fde2a597/b524a072-en.pdf)\n\n[371\\. Erik Brynjolfsson, Daniel Rock et al. “NBER WORKING PAPER SERIES ARTIFICIAL INTELLIGENCE AND THE MODERN PRODUCTIVITY PARADOX: A CLASH OF EXPECTATIONS AND STATISTICS.”](https://www.semanticscholar.org/paper/30d3715f9ed3f240e5d208db4f6c8ee8792edf91)\n\n[372\\. How Artificial Intelligence Will Transform the Economy](https://www.munichre.com/en/insights/digitalisation/how-artificial-intelligence-transforms-economy.html)\n\n[373\\. HRI Report Economic and Trade Policies of the Trump Administration and Their Effects](https://www.hitachi-hri.com/english/research/researchreport/file/vol11_04_3.pdf)\n\n[374\\. AI in the Workplace: Key Use Cases, Benefits, and Challenges](https://www.itransition.com/ai/workplace)\n\n[377\\. 2025年人工智能指数报告](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025_chinese_version_061325.pdf)\n\n[378\\. D. Rumelhart, Geoffrey E. Hinton et al. “Learning representations by back-propagating errors.” Nature](https://doi.org/10.1038/323533a0)\n\n[379\\. The Adoption of Artificial Intelligence in Firms: New Evidence for Policymaking](https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/05/the-adoption-of-artificial-intelligence-in-firms_8fab986b/f9ef33c3-en.pdf)\n\n[380\\. Artificial Intelligence: Solving problems, growing the economy and improving our quality of life](https://www.csiro.au/-/media/D61/AI-Roadmap-assets/19-00346_DATA61_REPORT_AI-Roadmap_WEB_191111.pdf)\n\n[381\\. Brandbuch Purpose Trends 2025](https://brandbuch.com/assets/purpose-trends-report-2025-by-brandbuch.pdf)\n\n[382\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[383\\. 机遇与风险并存：人工智能行业的全球格局、我国面临的发展困境与对策](https://m.yicai.com/news/102246615.html)\n\n[384\\. The Evaluation of Artificial Intelligence as a Prediction Problem](https://schellaert.org/papers/2025_Thesis.pdf)\n\n[385\\. Global Artificial Intelligence Report (2025) | IDCA](https://www.idc-a.org/insights/0bKr4NJQdK5sYcAQaGZD)\n\n[386\\. Artificial Intelligence as General Purpose Technology: An Empirical and Applied Analysis of its Perception](https://univda.unitesi.cineca.it/bitstream/20.500.14084/428/1/ETI_104_Guidetti_Andr%C3%A9.pdf)\n\n[387\\. Artificial Intelligence Index Report 2025: Chapter 4 Economy](https://hai-production.s3.amazonaws.com/files/hai_ai-index-report-2025_chapter4_final.pdf)\n\n[388\\. Global Economic Impacts Associated with Artificial Intelligence](https://www.analysisgroup.com/globalassets/content/insights/publishing/ag_full_report_economic_impact_of_ai.pdf)\n\n[389\\. The Economics of Artificial Intelligence: An Agenda](https://business.columbia.edu/sites/default/files-efs/imce-uploads/Joseph_Stiglitz/The%20Economics%20of%20Artificial%20Intelligence%20-%20Chapter%2014_0.pdf)\n\n[390\\. Artificial Intelligence: Economic Impact, Opportunities, Challenges, Implications for Policy](https://economy-finance.ec.europa.eu/document/download/10867a27-f49f-4aa9-89bb-feb752513f78_en?filename=dp210_en_artificial%20intelligence_0.pdf)\n\n[391\\. Artificial Intelligence Statistics - Magnet ABA](https://www.magnetaba.com/blog/artificial-intelligence-statistics#:~:text=The%20growth%20of%20AI%20is,36.6%25%20between%202024%20and%202030.)\n\n[392\\. John McCarthy, M. Minsky et al. “A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955.” AI Mag.](https://doi.org/10.1609/aimag.v27i4.1904)\n\n[393\\. Top 140 Artificial Intelligence Stats for 2025](https://thunderbit.com/blog/top-artificial-intelligence-stats)\n\n[397\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[398\\. A Survey on Large Language Models for Critical Societal Domains: Finance, Healthcare, and Law](https://openreview.net/pdf/2a94d2314673972d399c929a8a22e26e872091b4.pdf)\n\n[399\\. WONCA 2024 SOUTH ASIA CONFERENCE](https://woncasar2024.com/Wonca%20EBook%202024.pdf)\n\n[400\\. Congrès international francophone de pédagogie des sciences de la santé 2024 CAHIER DES COMMUNICATIONS ORALES](https://www.pedagogie-medicale.org/articles/pmed/pdf/2024/02/pmed240016s.pdf)\n\n[401\\. Performance of ChatGPT in medical licensing ...](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0312771)\n\n[402\\. Aidan Gilson, C. Safranek et al. “How Does ChatGPT Perform on the United States Medical Licensing Examination (USMLE)? The Implications of Large Language Models for Medical Education and Knowledge Assessment.” JMIR Medical Education](https://doi.org/10.2196/45312)\n\n[403\\. ChatGPT在应用开发、安全分析、心理健康诊断及银行自动化中的应用研究](https://www.preprints.org/search?search1=Chat&field1=keywords&search2=&order_by=most_downloaded)\n\n[404\\. Bibliometric analysis of ChatGPT based on Scopus data: Global research trends (2022–2024)](https://icvl.eu/documents/154/a24_Mejia_Garcia_Reyes_Flores_G3hrewV.pdf)\n\n[405\\. Advances in Economics, Management and Political Sciences](https://www.ewadirect.com/proceedings/aemps/volumes/vol/99/437.pdf)\n\n[406\\. Performance of ChatGPT in medical licensing examinations in countries worldwide: A systematic review and meta-analysis protocol](https://journals.plos.org/plosone/article/file?type=printable&id=10.1371/journal.pone.0312771)\n\n[407\\. PROCEEDINGS OF THE INTERNATIONAL CONFERENCES on E-SOCIETY 2024 AND MOBILE LEARNING 2024](https://files.eric.ed.gov/fulltext/ED659933.pdf)\n\n[408\\. ...systematic review and meta-analysis | BMC Medical E...](https://link.springer.com/article/10.1186/s12909-024-05944-8)\n\n[409\\. Yiqiu Shen, L. Heacock et al. “ChatGPT and Other Large Language Models Are Double-edged Swords..” Radiology](https://doi.org/10.1148/radiol.230163)\n\n[410\\. Chris Stokel-Walker. “ChatGPT listed as author on research papers: many scientists disapprove.” Nature](https://doi.org/10.1038/d41586-023-00107-z)\n\n[411\\. J. Kassab, Abdel Hadi El Hajjar et al. “Abstract 4128519: Performance of a Popular Large Language Model in Answering Cardiovascular Related Queries: A Systematic Review and Pooled-Analysis.” Circulation](https://doi.org/10.1161/circ.150.suppl_1.4128519)\n\n[412\\. Performance of ChatGPT-3.5 and GPT-4 in national licensing examinations ...](https://bmcmededuc.biomedcentral.com/articles/10.1186/s12909-024-05944-8)\n\n[413\\. 25+ Best ChatGPT Statistics for 2024 | Team-GPT](https://team-gpt.com/blog/chatgpt-statistics/)\n\n[414\\. Chris Stokel-Walker. “AI bot ChatGPT writes smart essays - should professors worry?.” Nature](https://doi.org/10.1038/d41586-022-04397-7)\n\n[415\\. 首届全国大模型与决策智能大会论文集](https://files.sciconf.cn/master/387/30270/202501/20250102121320_32750.pdf)"
  },
  {
    "id": 22,
    "question": "How would you advise a big nation to think about the AI stack (chips, compute, models, applications)... and how would you advise someone that's a smaller Nation differently?",
    "response": "**RESEARCH REPORT**\n\n**To:** Interested National Policy Stakeholders\n\n**From:** Expert Researcher\n\n**Date:** July 26, 2025\n\n**Subject:** National Strategies for the Artificial Intelligence Stack in the Modern Geopolitical Landscape\n\n**Executive Summary**\n\nAs of mid-2025, Artificial Intelligence is no longer an emerging technology but a foundational force reshaping global economies, military power, and societal structures. The widespread deployment and mainstream use of generative AI has solidified over the past year, marking a shift from theoretical potential to tangible impact \\[107\\]. Consequently, nations are compelled to develop coherent and robust national strategies across the entire AI stack—from the silicon in chips to the end-user applications. This report provides strategic counsel tailored to two distinct national archetypes: large, economically significant nations and smaller, more agile states. We analyze the strategic imperatives at each layer of the stack—Chips, Compute, Models, and Applications—offering differentiated advice based on a nation's scale, existing industrial base, and geopolitical posture. The counsel provided herein is derived from an extensive analysis of global trends, investments, and strategic initiatives observed through the first seven months of 2025.\n\n**1.0 Introduction: The AI Stack as the New Frontier of National Power**\n\nThe AI stack represents the hierarchical infrastructure upon which all artificial intelligence capabilities are built. A nation's strategy for this stack is a direct reflection of its global ambitions. We define the stack as follows:\n\n**Layer 1: Chips:** The specialized semiconductor hardware, including Graphics Processing Units (GPUs), Tensor Processing Units (TPUs), and custom Application-Specific Integrated Circuits (ASICs), that performs the complex calculations required for AI. This layer also includes critical components like High Bandwidth Memory (HBM) \\[1\\]\\[13\\].\n\n**Layer 2: Compute:** The infrastructure that aggregates and deploys chip-level power. This includes massive cloud data centers, on-premise servers, sovereign supercomputers, and edge computing devices \\[1\\]\\[7\\]. The availability of scalable, affordable compute is a primary determinant of a nation's AI potential \\[126\\].\n\n**Layer 3: Models:** The large, complex AI models, particularly foundational models and Large Language Models (LLMs), that are trained on vast datasets using the compute infrastructure. These models are the \"brains\" of modern AI systems.\n\n**Layer 4: Applications:** The end-user software and services that leverage AI models to perform specific tasks, create economic value, and deliver services across sectors like healthcare, finance, defense, and manufacturing \\[19\\]\\[20\\].\n\nA nation's approach to this stack cannot be monolithic. It must be tailored to its unique strengths, weaknesses, and resources. This report delineates two primary strategic paths.\n\n**2.0 Advising a Large Nation: The Pursuit of Sovereignty and Scale**\n\nFor a large nation, the strategic calculus for AI is fundamentally about achieving digital sovereignty and leveraging immense scale to establish a dominant position. However, a critical distinction exists between large nations with a mature domestic semiconductor industry and those without.\n\n**2.1 Strategy for Large Nations with an Established Semiconductor Industry (e.g., United States, China, South Korea)**\n\nFor these nations, the goal is full-stack dominance and strategic independence. The focus is on reinforcing existing advantages and insulating the national AI ecosystem from geopolitical shocks.\n\n**Chips Layer: Fortify the Foundation**\n\nThe core strategy at this layer is to accelerate domestic substitution and achieve self-reliance across the entire semiconductor supply chain \\[1\\]\\[5\\]. This is driven by both economic ambition and the reality of geopolitical tensions, such as the ongoing US export controls on advanced AI chips targeting China, which in turn has supercharged China's own self-reliance efforts \\[3\\]\\[8\\]\\[10\\]. Investment priorities for 2025 include:\n\n**Advanced Packaging:** Mastering technologies like Chip-on-Wafer-on-Substrate (CoWoS) and HBM is crucial for integrating powerful chips into functional systems \\[1\\]\\[9\\].\n\n**Custom Silicon:** Developing bespoke chips, or eXtreme Processing Units (XPUs), is a key trend, allowing hyperscalers and national entities to create hardware perfectly optimized for their specific AI workloads, reducing reliance on off-the-shelf solutions \\[17\\].\n\n**Domesticating the Full Supply Chain:** Investment must extend beyond fabrication to include Electronic Design Automation (EDA) software, wafer manufacturing, and essential materials, reducing dependencies on foreign suppliers at every stage \\[3\\]\\[9\\].\n\n**Compute Layer: Build the Castles of AI**\n\nThe race for computational supremacy has led to staggering capital expenditures. Major cloud providers like Microsoft, Google, and Amazon continue to pour billions into AI servers and GPUs \\[5\\]\\[7\\]. For a large nation, the imperative is to build and control massive, sovereign compute infrastructure.\n\n**Sovereign Supercomputers:** Ambitious projects that became operational or were significantly advanced in early 2025 exemplify this trend. The **Stargate Project**, a monumental $500 billion initiative involving OpenAI, SoftBank, and others, launched in January 2025 to construct next-generation AI data centers in the U.S. and UAE, with plans to deploy hundreds of thousands of the latest NVIDIA AI chips \\[106\\]\\[114\\]\\[115\\].\n\n**Managing Infrastructure Challenges:** Building this infrastructure is not without its challenges. As of 2025, key documented hurdles include severe compute limitations and availability, high latency, immense power consumption, and poor GPU utilization, with only 7% of organizations achieving over 85% utilization \\[144\\]\\[144\\]\\[261\\]. Energy efficiency, measured in metrics like TOPS/Watt, has become a critical benchmark, with technologies like Google's TPU showing efficiency gains of 30-80x over contemporary CPUs/GPUs \\[247\\]\\[340\\]. Power infrastructure itself is a benchmark, with data centers needing to upgrade to support racks drawing 50kW, 100kW, or even 300kW \\[146\\]\\[300\\].\n\n**Models and Applications Layers: Dictate the Intelligence Agenda**\n\nWith a secure foundation of chips and compute, these nations can focus on developing and controlling the foundational models that will define the next era of AI. The trend in 2025 is clearly towards \"sovereign AI,\" where nations develop their own large-scale models to ensure they align with national interests, languages, and values \\[48\\]\\[51\\]. This allows them to create entire ecosystems of applications, from transformative AI agents that reshape human-computer interaction to AI-driven tools for critical sectors like defense, finance, and advanced manufacturing \\[6\\]\\[19\\].\n\n**2.2 Strategy for Large Nations without an Established Semiconductor Industry (e.g., India, Brazil, Saudi Arabia, EU Nations)**\n\nFor these nations, the ambition for sovereignty is equally strong, but the path is more complex, requiring a blend of bold domestic investment and strategic international partnerships to overcome the critical gap in semiconductor fabrication.\n\n**Chips Layer: Build, Buy, and Partner**\n\nLacking domestic fabs is a significant vulnerability. The strategy must be multi-pronged:\n\n**Aggressive Public-Private Investment:** These nations must use their large economies to fund a nascent domestic industry. As seen in 2025, this is already happening: China's 475 billion semiconductor fund, Saudi Arabia's 100 billion \"Project Transcendence,\" and India's $1.25 billion commitment are massive state-led efforts to build local capacity, often in partnership with foreign firms \\[154\\]\\[265\\]\\[271\\].\n\n**Focus on Accessible Technologies:** Rather than competing on the bleeding edge immediately, these nations can focus on high-growth adjacent areas. Investing in **advanced packaging** capabilities and embracing the open-source **RISC-V architecture** are viable strategies to enter the market and build expertise without the colossal cost of a leading-edge foundry \\[67\\]\\[68\\].\n\n**Compute Layer: The PPP Imperative**\n\nWithout a domestic supply of high-end chips, Public-Private Partnerships (PPPs) are the primary mechanism for building national compute capacity. The operational frameworks for these PPPs in 2025 are becoming standardized:\n\n**National AI Missions:** Governments are establishing well-funded national strategies that guide PPPs. Examples include India’s \"IndiaAI Mission,\" Germany’s \"AI Made in Germany,\" and France's \"InvestAI\" policy, all of which explicitly fund the build-out of GPU infrastructure in collaboration with private partners like NVIDIA and local champions \\[203\\].\n\n**Co-Investment and Governance:** These partnerships are structured through various models. Governments are setting up co-investment funds, such as China's \"Governance Guidance Funds,\" to share risk with private VCs \\[268\\]. Contractual structures often involve the sharing of resources and IP, with clauses for cloud service expenditure and sometimes even equity stakes for state-backed entities \\[264\\]\\[274\\]. Governance is managed through national AI ethics boards and security institutes, often established via legislation like the U.S. \"2024 Artificial Intelligence Initiative Act\" \\[210\\]\\[272\\].\n\n**Sovereign AI Data Centers:** These partnerships are yielding tangible results. A prime example from 2025 is **Colosseum**, a massive sovereign AI data center in Italy, built through a partnership between iGenius, Vertiv, and NVIDIA, specifically to serve regulated industries under a sovereign framework \\[105\\]\\[111\\].\n\n**Models and Applications Layers: Specialize and Solve**\n\nWhile building compute, these nations should leverage it to create models tailored to their unique demographic and economic landscapes. The focus should be on:\n\n**Linguistic and Cultural Sovereignty:** Developing LLMs for local languages, as India is doing with its multi-language BharatGen model, is a crucial step in preventing digital colonization and ensuring AI serves the entire populace \\[203\\].\n\n**Targeted Application Ecosystems:** By focusing on national priorities—be it healthcare in an aging population like Japan's or smart city infrastructure in the Gulf states—these nations can foster a vibrant application layer that creates immediate economic and social value, attracting talent and further investment \\[46\\]\\[203\\].\n\n**3.0 Advising a Small Nation: The Path of Agility and Collaboration**\n\nFor a small nation, attempting to compete with large nations on scale is a path to failure. The winning strategy is not about dominance but about being smart, agile, and deeply collaborative. The goal is to become a \"niche powerhouse\" by excelling in specific areas of the AI stack while strategically partnering for the rest.\n\n**Chips Layer: Strategic Abstention**\n\nThe advice is unequivocal: do not attempt to build a leading-edge semiconductor foundry. The capital investment is prohibitive, and the geopolitical competition is too fierce. Instead, a small nation should treat advanced chips as a commodity to be accessed, not a product to be manufactured. The focus and resources should be directed higher up the stack.\n\n**Compute Layer: International Partnerships as a Lifeline**\n\nAccess to high-performance computing is the single greatest challenge. The strategy is to secure this access through a portfolio of international partnerships, a model that has gained significant traction in 2025.\n\n**Leverage Global Clouds and Networks:** For non-sensitive applications, leveraging major commercial cloud providers is the most efficient path \\[21\\]. For more strategic needs, small nations should form **alliances and consortia**. The preeminent example in 2025 is Switzerland's **International Computation and AI Network of Excellence (ICAIN)**. Launched in January 2024 and fully operational in early 2025, ICAIN is a resource-sharing network that gives members access to supercomputing, data, and expertise, harnessing collective advantage to bridge the digital divide \\[21\\]\\[82\\]\\[122\\].\n\n**Propose and Participate in Global Funds:** Advocating for initiatives like a \"Global Fund for AI\" can help pool resources and ensure that smaller countries are not locked out of the compute-heavy future of AI \\[24\\].\n\n**Acknowledge Risks:** This strategy is not without risk. Small nations must be mindful that access to compute from larger powers can be subject to geopolitical whims and export controls, which could be used as a tool of coercion \\[32\\]\\[33\\]. Diversifying partnerships is a key risk mitigation strategy.\n\n**Models Layer: Fine-Tune, Don't Build from Scratch**\n\nTraining a foundational model from the ground up is computationally prohibitive. The smart strategy is to leverage the burgeoning open-source model ecosystem.\n\n**Specialize and Adapt:** Small nations should focus their talent on **fine-tuning** powerful, pre-existing open-source models. They can adapt these models for local languages, specific cultural contexts, and niche industries where they have a competitive advantage. This approach, emphasized by countries like Finland and Estonia, allows for the creation of highly valuable, bespoke AI without the astronomical training costs \\[87\\].\n\n**Applications Layer: The Crown Jewel of the Strategy**\n\nThis is where a small nation can truly shine. By focusing its resources on the application layer, it can become a world leader in specific domains.\n\n**Niche Excellence:** Identify two or three key domestic industries (e.g., fintech, sustainable agriculture, tourism, maritime logistics) and aim to build the world's best AI applications for them.\n\n**Partnerships for Deployment:** Partner with major international technology companies to bring these applications to market. The partnership between Mexico and Microsoft to create the **PyMAlis** platform, which provides AI access and training to Mexican SMEs, is a powerful model for how a small economy can leverage a tech giant's resources for widespread AI adoption \\[86\\].\n\n**Measure Tangible Outcomes:** Success should be measured not by petaflops but by concrete results. While broad government reports quantifying GDP growth from these partnerships were scarce in the first half of 2025 \\[283\\]\\[322\\]the goal should be to produce them. The reported 40% reduction in water waste in parts of Kenya, achieved through a data analysis collaboration with India's AI4Bharat, serves as a template for the kind of verifiable, high-impact outcomes that small nations should target and publicize \\[121\\].\n\n**4.0 Conclusion: A Bifurcated Global AI Landscape**\n\nIn July 2025, the strategic pathways for success in artificial intelligence have become starkly clear, yet they are not uniform. The advice for a nation must be brutally honest about its position in the global hierarchy of scale and resources.\n\n**For large nations,** the game is one of **sovereignty and industrial might**. Their strategy must be a comprehensive, top-to-bottom pursuit of full-stack control, leveraging immense public-private investment to build impregnable fortresses of chips, compute, and proprietary models. Their success will be measured in global market share, technological leadership, and geopolitical influence.\n\n**For small nations,** the game is one of **agility and niche excellence**. Their strategy must be a surgical application of limited resources, strategically abstaining from the most capital-intensive layers of the stack. They must become masters of collaboration, leveraging international partnerships to access compute and focusing their native talent on developing world-class applications for specific, targeted domains. Their success will be measured in tangible economic and social outcomes, improved public services, and the creation of highly specialized, globally competitive industries.\n\nThe era of a one-size-fits-all approach to technology policy is over. The AI stack is the new terrain of global competition, and navigating it successfully requires a strategy that is as unique as the nation itself.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. 人工智能创新持续推进，半导体自主可控方兴未艾——电子行业2025年度投资策略](https://aigc.idigital.com.cn/djyanbao/%E3%80%90%E4%B8%AD%E5%8E%9F%E8%AF%81%E5%88%B8%E3%80%91%E7%94%B5%E5%AD%90%E8%A1%8C%E4%B8%9A2025%E5%B9%B4%E5%BA%A6%E6%8A%95%E8%B5%84%E7%AD%96%E7%95%A5%EF%BC%9A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%88%9B%E6%96%B0%E6%8C%81%E7%BB%AD%E6%8E%A8%E8%BF%9B%EF%BC%8C%E5%8D%8A%E5%AF%BC%E4%BD%93%E8%87%AA%E4%B8%BB%E5%8F%AF%E6%8E%A7%E6%96%B9%E5%85%B4%E6%9C%AA%E8%89%BE-2024-12-02.pdf)\n\n[2\\. Semiconductor Industry Outlook 2025](https://www.infosys.com/iki/documents/semiconductor-industry-outlook2025.pdf)\n\n[3\\. 美国半导体出口管制再升级，存储器价格持续回升——半导体行业月报](https://pdf.dfcfw.com/pdf/H3_AP202506131690221998_1.pdf?1749831549000.pdf)\n\n[4\\. 2025 China Strategy Outlook](https://www.cmbi.com.hk/upload/202412/20241223128045.pdf)\n\n[5\\. 半导体 2025 展望：AI 热潮将延续](https://pdf.dfcfw.com/pdf/H3_AP202412171641330122_1.pdf?1734454722000.pdf)\n\n[6\\. 自主可控和AI浪潮 电子行业 2025年度投资策略](https://pdf.dfcfw.com/pdf/H301_AP202412231641393133_1.pdf)\n\n[7\\. Global Semiconductor Market View](https://fpafinancial.com/wp-content/uploads/global-semiconductor-market-view-2025.pdf)\n\n[8\\. 美国关税政策影响终端出货增长，中国AI投资规模有望持续加码](https://pdf.dfcfw.com/pdf/H3_AP202504141656207893_1.pdf?1744619799000.pdf)\n\n[9\\. 电子行业 2025 年年度策略：关注半导体行业自主可控及 AI 终端繁荣两大主线——电子行业年度策略](https://pdf.dfcfw.com/pdf/H3_AP202412171641332220_1.pdf?1734457913000.pdf)\n\n[10\\. 2025 global semiconductor industry outlook - Deloitte](https://www2.deloitte.com/us/en/insights/industry/technology/technology-media-telecom-outlooks/semiconductor-industry-outlook.html#:~:text=The%20semiconductor%20industry%20had%20a,forecast%20of%20US$611%20billion.)\n\n[11\\. 2024 Outlook: Global Tech Investment Strategy Behind AI and Semiconductor Themes](http://researchreport.bocomgroup.com/Technology_Sector-240119e.pdf)\n\n[12\\. Vietnam Strategy 2H2024: Rising optimism but risks on the horizon](https://mbs.com.vn/media/fljb3frq/mbs_vietnam-strategy-2h24_20240701-1.pdf)\n\n[13\\. 半导体行业 2025 年年度策略报告 AI 将是强引擎，国产化有望进深水区](https://pdf.dfcfw.com/pdf/H3_AP202412161641311130_1.pdf?1734359895000.pdf)\n\n[14\\. 证券研究报告-晨会聚焦](https://file.iyanbao.com/pdf/270be-9f43f76f-8e1f-438b-bfa7-94dc0f476b22.pdf)\n\n[15\\. 野村世界業種別投資シリーズ（世界半導体株投資）](http://www.nomura-am.co.jp/news/20241226JU6A13S1.pdf)\n\n[16\\. Top 10 Semiconductor Trends in 2025 | StartUs Insights](https://www.startus-insights.com/innovators-guide/semiconductors-trends-innovation/#:~:text=This%20report%20provides%20an%20overview,more%20sustainable%20and%20efficient%20operations.)\n\n[17\\. How AI is Reviving the Semiconductor Industry in 2025](https://www.fusionww.com/insights/blog/how-ai-is-reviving-the-semiconductor-industry-in-2025)\n\n[18\\. 半导体行业2025年度投资策略:如鱼跃渊 升腾化龙_行业研究_...](https://stock.finance.sina.com.cn/stock/go.php/vReport_Show/kind/industry/rptid/788494618542/index.phtml)\n\n[19\\. Top 10 AI Investments by Country in 2025 in London, UK ...](https://iotworldmagazine.com/2025/04/23/2794/top-10-ai-investments-by-country-in-2025-in-london-uk-europe-us-canada-india-uae-saudi-asia-and-china)\n\n[20\\. 调查报告：2025年投资规划综合分析- 电子画册在线预览](https://www.yunzhan365.com/basic/22076721.html)\n\n[21\\. AI PLAYBOOK FOR SMALL STATES](https://www.imda.gov.sg/-/media/imda/files/news-and-events/media-room/media-releases/2024/09/ai-playbook-for-small-states/imda-ai-playbook-for-small-states.pdf)\n\n[22\\. The AI Export Dilemma: Three Competing Visions for U.S. Strategy](https://carnegie-production-assets.s3.amazonaws.com/static/files/Winter-Levy_AI%20Export%20Dilemma.pdf)\n\n[23\\. Muhammad Rayees, Usman Ahmad et al. “Artificial Intelligence In.”](https://www.semanticscholar.org/paper/7807218eba0e71259f8cdb7861c0466d0d490366)\n\n[24\\. Governing AI for Humanity](https://www.un.org/sites/un2.un.org/files/governing_ai_for_humanity_final_report_en.pdf)\n\n[25\\. A BLUEPRINT FOR BUILDING NATIONAL COMPUTE CAPACITY FOR ARTIFICIAL INTELLIGENCE](https://one.oecd.org/document/DSTI/CDEP/AIGO%282022%292/FINAL/en/pdf)\n\n[26\\. The Internationalization Process of Artificial Intelligence In A World of Tech Giants](https://gupea.ub.gu.se/bitstream/handle/2077/82018/IBT%202024-4.pdf?sequence=1&isAllowed=y)\n\n[27\\. Singapore joins China and the US as a key AI player, boosting Nvidia's ...](https://kr-asia.com/singapore-joins-china-and-the-us-as-a-key-ai-player-boosting-nvidias-global-revenue)\n\n[28\\. Toward International Cooperation on Foundational AI Models: An Expanded Role for Trade Agreements and International Economic Policy](https://hdsr.mitpress.mit.edu/pub/14unjde2/download/pdf)\n\n[29\\. AI in Global Development Playbook; Request for information](https://hai.stanford.edu/sites/default/files/2024-03/Stanford%20HAI%20and%20The%20Asia%20Foundation%20Response%20to%20USAID.pdf)\n\n[30\\. Building an AI ecosystem in a small nation: lessons from Singapore’s journey to the forefront of AI](https://dr.ntu.edu.sg/bitstream/10356/181327/2/s41599-024-03289-7.pdf)\n\n[31\\. 喝点VC|a16z合伙人:AI已成为地域竞赛,成为“超级中心”取...](https://news.qq.com/rain/a/20250204A05UFU00)\n\n[32\\. Chips for Peace: how the U.S. and its allies can lead on safe and beneficial AI](https://law-ai.org/chips-for-peace/)\n\n[33\\. Jason Hausenloy, Andrea Miotti et al. “Multinational AGI Consortium (MAGIC): A Proposal for International Coordination on AI.” ArXiv](https://doi.org/10.48550/arXiv.2310.09217)\n\n[41\\. A. Turing. “Computing Machinery and Intelligence.” Mind](https://doi.org/10.1093/MIND/LIX.236.433)\n\n[42\\. Trends – Artificial Intelligence](https://www.nnhem.com/Trends_Artificial_Intelligence%E5%8F%8C%E8%AF%AD%E7%BF%BB%E8%AF%91340%E9%A1%B5%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%B6%8B%E5%8A%BF%E6%8A%A5%E5%91%8A%E4%B8%AD%E6%96%87%E7%89%88.pdf)\n\n[43\\. Trends – Artificial Intelligence (AI)](https://www.madridforoempresarial.es/wp-content/uploads/2025/06/Trends_Artificial_Intelligence.pdf)\n\n[44\\. 中央企业人工智能应用场景优秀案例白皮书（2024年版）](http://www.sccio.cn/uploads/20250107/1d22aff19a78c021eb4dd71890c668af.pdf)\n\n[45\\. 2025 Futures Report](https://kpmg.com/kpmg-us/content/dam/kpmg/pdf/2025/kpmg-2025-futures-report.pdf)\n\n[46\\. Sovereign GPU Cloud for National Competitiveness in the AI Economy: A Backgrounder](https://www2.deloitte.com/content/dam/Deloitte/in/Documents/Consulting/in-consulting-nasscom-deloitte-paper-sovereign-GPU-Cloud-noexp.pdf)\n\n[47\\. 2025 Predictions: AI Finds a Reason to Tap Industry Data Lakes](https://blogs.nvidia.com/blog/industry-ai-predictions-2025/)\n\n[48\\. Digital infrastructure: Charting the course in 2025](https://assetphysics.com/digital-infrastructure-charting-the-course-in-2025/)\n\n[49\\. Considerations regarding Sovereign AI and National AI Policy](https://sovereign-ai.org/media/papers/Executive_Summery_Sovereign_AI.pdf)\n\n[50\\. AI in 2025: current initiatives and challenges in large enterprises](https://www.wavestone.com/wp-content/uploads/2025/01/ai-action-summit-report.pdf)\n\n[51\\. NVIDIA GTC 2025: Where AI Meets Real-World Impact](https://newsroom.stelia.ai/nvidia-gtc-2025-where-ai-meets-real-world-impact/)\n\n[52\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[53\\. Alejandro Barredo Arrieta, Natalia Díaz Rodríguez et al. “Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI.” Inf. Fusion](https://doi.org/10.1016/j.inffus.2019.12.012)\n\n[54\\. Sovereign AI and the Future of Data Independence](https://www.salesforce.com/au/artificial-intelligence/sovereign-ai/)\n\n[55\\. \"2025 will be the technological equivalent of the 'Big Bang ...](https://www.tahawultech.com/news/2025-will-be-the-technological-equivalent-of-the-big-bang-with-ai-at-its-epicentre-john-roese-dell-technologies/)\n\n[56\\. 关于人工智能成功案例（全球Top 10）\\_干货分享](https://www.allconfs.org/list_info_view_xueshu.asp?id=C8ADD7DECBA7438D48243ECA59277608)\n\n[61\\. Outlook 2025 Edition](https://ca.rbcwealthmanagement.com/documents/128701/0/CapitalGroup_Outlook_2025.pdf/381187bd-f60d-4ee4-96b3-a1ec84445d21)\n\n[62\\. \\[2025\\] Top 10 Global Fabless Semiconductor Companies](https://www.blackridgeresearch.com/blog/list-of-top-global-fabless-semiconductor-companies#:~:text=the%20semiconductor%20chips.-,4.2.,as%20indicated%20by%20their%20revenues.)\n\n[63\\. 2025 global semiconductor industry outlook - Deloitte](https://www.deloitte.com/us/en/insights/industry/technology/technology-media-telecom-outlooks/semiconductor-industry-outlook.html#:~:text=In%202025,%20there%20will%20likely,to%20system-level%20metrics%20like)\n\n[64\\. Global Semiconductor Market View](https://fpafinancial.com/wp-content/uploads/global-semiconductor-market-view-2025.pdf)\n\n[65\\. Semiconductor Industry Growth for 2025: Key Stats You ...](https://patentpc.com/blog/semiconductor-industry-growth-for-2025-key-stats-you-need-to-know)\n\n[66\\. 2025 Semiconductor Outlook](https://www.samsungpop.com/common.do?cmd=down&contentType=application/pdf&inlineYn=Y&saveKey=research.pdf&fileName=2020/2025011317464753K_02_01.pdf)\n\n[67\\. 半导体行业2025年度投资策略](https://pdf.dfcfw.com/pdf/H3_AP202412251641429576_1.pdf?1735159629000.pdf)\n\n[68\\. 2025年，十大半导体技术蓄势待发](http://epaper.qdcaijing.com/pc/att/202501/02/75b54864-223b-4dbe-8799-678db78a0cd6.pdf)\n\n[69\\. みずほタイ月報 May 2025 | 5月号](https://www.mizuhogroup.com/binaries/content/assets/pdf/thailand/share/geppo/geppo0525.pdf)\n\n[70\\. How to Pick the Right AI Stack for Scalable Software Development](https://www.tristatetechnology.com/blog/ultimate-ai-tech-stack-guide)\n\n[71\\. Chinas Innovationssystem „Künstliche Intelligenz“: Ein Zwischenstand.](https://www.aies.at/download/2021/AIES-Fokus-2021-13.pdf)\n\n[72\\. 2024 KPMG Global Semiconductor Industry Outlook](https://assets.kpmg.com/content/dam/kpmg/sg/pdf/2024/03/global-semiconductor-industry-outlook.pdf)\n\n[73\\. Artificial Intelligence: What to expect in 2025](https://www.dws.com/AssetDownload/Index?assetGuid=2cec319c-e3ea-44e8-b627-50367093008b&consumer=E-Library)\n\n[74\\. AI Progress in 2025: What's Happened and What's Next](https://www.digitalbricks.ai/blog-posts/ai-progress-in-2025-whats-happened-and-whats-next)\n\n[75\\. SILICON SEMICONDUCTOR](https://magazines.angel.digital/magazines/Silicon_Semiconductor_Issue_8_2024.pdf)\n\n[76\\. AI Agent Infrastructure Stack for Agentic Systems](https://www.xenonstack.com/blog/ai-agent-infrastructure-stack)\n\n[77\\. 全球科创观察 2023年第17期](https://www.beijingfintech.ac.cn/statics/2023/05/70a15308845375f60ddd.pdf)\n\n[78\\. AI on the Edge](https://prod-www.zebra.com/content/dam/zebra_dam/en/presentation/customer-facing/zebra-devcon2023-presentation-customer-facing-al-on-the-edge-stuart-hubbard-en-us.pdf)\n\n[79\\. The Ultimate AI Coding Stack Every Developer Should ...](https://dev.to/devtechinsights/the-ultimate-ai-coding-stack-every-developer-should-know-in-2025-1j8n)\n\n[80\\. Top 10 Semiconductor Trends in 2025 | StartUs Insights](https://www.startus-insights.com/innovators-guide/semiconductors-trends-innovation/#:~:text=This%20report%20provides%20an%20overview,more%20sustainable%20and%20efficient%20operations.)\n\n[81\\. Building an AI ecosystem in a small nation: lessons from Singapore’s journey to the forefront of AI](https://dr.ntu.edu.sg/bitstream/10356/181327/2/s41599-024-03289-7.pdf)\n\n[82\\. AI PLAYBOOK FOR SMALL STATES](https://www.imda.gov.sg/-/media/imda/files/news-and-events/media-room/media-releases/2024/09/ai-playbook-for-small-states/imda-ai-playbook-for-small-states.pdf)\n\n[83\\. Big Data and Artificial Intelligence in Modeling International Socio-Economic Processes](https://bdaim.uwed.uz/static/proceding.pdf)\n\n[84\\. 算力焦虑背后的认知博弈](https://www.hunantoday.cn/news/xhn/202507/29988547.html)\n\n[85\\. The 2025 Global AI Adoption Report: Is Your Country on This List?](https://www.allaboutai.com/resources/ai-statistics/global-ai-adoption/#:~:text=Key%20Findings:%20Global%20AI%20Race%20in%202025,-Here%20are%20the&text=%F0%9F%8C%8FRegional%20Adoption%20Leaders:%20China,limited%20growth%20over%20five%20years.)\n\n[86\\. Navigating Artificial Intelligence Landscape in APEC: Balancing Development and Oversight](https://prod-statistics.apec.org/apecapi/publication/getfile?publicationId=96a5d4b5-6f1e-4e53-959a-815098873f35)\n\n[87\\. 108 small nations band together to share AI lessons globally](https://www.zdnet.com/article/small-nations-around-the-world-band-together-to-share-ai-lessons/)\n\n[88\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[89\\. 江苏教育考试科研月报](https://www.jseea.cn/webfile/upload/2025/05-07/14-26-050650-1849091977.pdf)\n\n[90\\. 2025 APA Foresight Trend Report for Planners](https://planning-org-uploaded-media.s3.amazonaws.com/publication/download_pdf/2025-Trend-Report-for-Planners.pdf)\n\n[91\\. Artificial Intelligence: Shaping a Future New Zealand](https://aiforum.org.nz/wp-content/uploads/2018/07/AI-Report-2018_web-version.pdf)\n\n[92\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2024.” ArXiv](https://doi.org/10.48550/arXiv.2405.19522)\n\n[93\\. Government of Canada opens applications for the AI Compute ...](https://www.newswire.ca/news-releases/government-of-canada-opens-applications-for-the-ai-compute-access-fund-875863402.html#:~:text=The%20Canadian%20Sovereign%20AI%20Compute%20Strategy,%20including%20the%20Access%20Fund,small%20and%20medium-sized%20businesses)\n\n[94\\. RÉALITÉS INDUSTRIELLES](https://annales-des-mines.org/wp-content/uploads/2025/04/RI_2025_05_webcouv.pdf)\n\n[95\\. OECD Artificial Intelligence Review of Egypt](https://www.oecd.org/content/dam/oecd/en/publications/reports/2024/05/oecd-artificial-intelligence-review-of-egypt_3c437131/2a282726-en.pdf)\n\n[96\\. MCP引领AGENT互联网新时代，连接旧世界和AI新世界的钥匙，2025年第二届AIGC上海开发者大会](https://b23.tv/BV1P4j7z2ETo?t=301)\n\n[97\\. IFZ FinTech Study 2025: An Overview of Swiss and Liechtenstein FinTech](https://www.handelszeitung.ch/sites/default/files/media/document/ifz_fintech_studie_2025.pdf)\n\n[98\\. The Global Cooperation Barometer 2025 Second Edition](https://reports.weforum.org/docs/WEF_Global_Cooperation_Barometer_2025.pdf)\n\n[99\\. Bart Verheij, M. Wiering. “Artificial Intelligence.” Communications in Computer and Information Science](https://doi.org/10.1007/978-3-319-76892-2)\n\n[101\\. Sovereign AI](https://atos.net/en/lp/sovereign-ai)\n\n[102\\. Sovereign AI | Oracle Qatar](https://www.oracle.com/qa/cloud/sovereign-ai/)\n\n[103\\. Interim report January - March 2025](https://attachment.news.eu.nasdaq.com/a5a1f0ab8e0ff9017a6561cde50744f2b)\n\n[104\\. July 2025 AI Startup Spotlight: 10 Breakthrough Innovators](https://www.aibmag.com/2025/07/17/ai-startup-spotlight/)\n\n[105\\. iGenius launches Colosseum, a giant sovereign AI data ...](https://pageoneasia.com/technology/igenius-launches-colosseum-a-giant-sovereign-ai-data-center)\n\n[106\\. The Stargate Project and AI play President Donald Trump unveile...](https://xueqiu.com/7367587733/321793626)\n\n[107\\. Latest Developments in AI (June–July 2025)](https://ts2.tech/en/latest-developments-in-ai-june-july-2025/)\n\n[108\\. Sovereign Artificial Intelligence — Bloomsbury Intelligence and ...](https://bisi.org.uk/reports/sovereign-artificial-intelligence)\n\n[109\\. From Chips to Systems: How AI Is Revolutionizing Compute and Infrastructure](https://www.williamblair.com/-/media/downloads/eqr/2025/williamblair_how-ai-is-revolutionizing-compute-and-infrastructure.pdf)\n\n[110\\. AI Chips in 2025: The end of “more GPUs is all you need”?](https://linkeddataorchestration.com/2025/01/29/ai-chips-in-2025-the-end-of-more-gpus-is-all-you-need/)\n\n[111\\. iGenius, Vertiv and NVIDIA Launch AI Supercomputer ...](https://www.techerati.com/news-hub/igenius-vertiv-and-nvidia-launch-ai-supercomputer-colosseum/)\n\n[112\\. How AI Infrastructure Will Redefine National Sovereignty](https://builtin.com/articles/ai-infrastructure-national-sovereignty)\n\n[113\\. Getting Started with Grass](https://wynd-network.gitbook.io/grass-docs)\n\n[114\\. ASA Community](https://community.amstat.org/scasa/newsroom)\n\n[115\\. OpenAI计划2025年前部署超百万GPU,启动百倍扩容(全文)\\_人工智能...](https://ai.zol.com.cn/1017/10176363_all.html)\n\n[116\\. AI Infrastructure Companies to Watch in 2025 Today](https://ossisto.com/blog/ai-infrastructure-companies/)\n\n[117\\. 通信行业 2025 年 6 月投资策略 高速光模块景气度持续，商业航天发展加速](https://pdf.dfcfw.com/pdf/H3_AP202506021683623704_1.pdf?1748939352000.pdf=)\n\n[118\\. 2025年人工智能领域的重要动态与重大事件回顾 - 月光AI博客](https://blog.moontak.com/id/563688/)\n\n[119\\. 2025年：AI应用和自主可控将持续驱动半导体周期上行](https://pdf.dfcfw.com/pdf/H3_AP202412251641422672_1.pdf?1735125567000.pdf)\n\n[121\\. Big Data and Artificial Intelligence in Modeling International Socio-Economic Processes](https://bdaim.uwed.uz/static/proceding.pdf)\n\n[122\\. AI PLAYBOOK FOR SMALL STATES](https://www.imda.gov.sg/-/media/imda/files/news-and-events/media-room/media-releases/2024/09/ai-playbook-for-small-states/imda-ai-playbook-for-small-states.pdf)\n\n[123\\. The 2025 Global AI Adoption Report: Is Your Country on This List?](https://www.allaboutai.com/resources/ai-statistics/global-ai-adoption/#:~:text=Key%20Findings:%20Global%20AI%20Race%20in%202025,-Here%20are%20the&text=%F0%9F%8C%8FRegional%20Adoption%20Leaders:%20China,limited%20growth%20over%20five%20years.)\n\n[124\\. THE NATIONAL GUIDELINES ON AI GOVERNANCE & ETHICS](https://cdo.unimas.my/media/attachments/2024/09/23/national-guidelines-of-aige.pdf)\n\n[125\\. The Adoption of Artificial Intelligence in Firms: New Evidence for Policymaking](https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/05/the-adoption-of-artificial-intelligence-in-firms_8fab986b/f9ef33c3-en.pdf)\n\n[126\\. Blueprint for Intelligent Economies: AI Competitiveness through Regional Collaboration](https://reports.weforum.org/docs/WEF_A_Blueprint_for_Intelligent_Economies_2025.pdf)\n\n[127\\. Digital Transformation in the Age of Artificial Intelligence: Empowering SMEs in the BRICS for a competitive future](https://brics.br/pt-br/documentos/economia-financas-comercio-e-infraestrutura/250504-report-digital-transformation-in-the-age-of-ai.pdf/@@download/file)\n\n[128\\. Науковий вісник Ужгородського національного університету](http://www.visnyk-econom.uzhnu.uz.ua/archive/31_2020ua/31_2020.pdf)\n\n[129\\. 108 small nations band together to share AI lessons globally](https://www.zdnet.com/article/small-nations-around-the-world-band-together-to-share-ai-lessons/)\n\n[130\\. Artificial Intelligence – Global 2025 Outlook: Broadening AI capabilities will unlock new use cases](https://cuadernoborrador.com/wp-content/uploads/2025/01/outlook-artificial-intelligence-global-13jan2025-pbc_1425377-1.pdf)\n\n[131\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[132\\. 2025 \"人工智能+\" 行业发展蓝皮书](https://www.acem.sjtu.edu.cn/ueditor/jsp/upload/file/20250427/1745731689854071357.pdf)\n\n[133\\. Hanna Prokhazka, Olena D. Melnyk. “Implementation of AI in international law and administrative law (in the context of human rights protection).” Revista Amazonia Investiga](https://doi.org/10.34069/ai/2023.67.07.6)\n\n[134\\. Artificial Intelligence for Efficiency, Sustainability and Inclusivity in TradeTech](https://reports.weforum.org/docs/WEF_Artificial_Intelligence_for_Efficiency_Sustainability_and_Inclusivity_in_TradeTech_2025.pdf)\n\n[135\\. Big challenge for Germany amid changing world order](https://jammuuniversity.ac.in/sites/default/files/inline-files/ALL%20ENGLISH%20EDITORIAL%2026-02-2025.pdf)\n\n[136\\. 小国将与大国分享影响力，人工智能将颠覆传统地缘政治？](https://www.jfdaily.com/news/detail?id=98244)\n\n[137\\. 斯坦福报告显示：中美大模型质量差距缩小至0.3%](https://cj.sina.cn/articles/view/1654203637/629924f5020013kps?froms=ggmp)\n\n[138\\. Building an AI ecosystem in a small nation: lessons from Singapore’s journey to the forefront of AI](https://dr.ntu.edu.sg/bitstream/10356/181327/2/s41599-024-03289-7.pdf)\n\n[141\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[142\\. Sovereign AI](https://atos.net/en/lp/sovereign-ai)\n\n[143\\. Navigating the Challenges of AI Infrastructure Design: Balancing Power, Latency, Reliability, and Data Requirements](https://www.f5.com/es_es/resources/white-papers/overcoming-ai-infrastructure-challenges-balancing-power-latency-reliability-and-data-requirements)\n\n[144\\. The State of AI Infrastructure at Scale 2024: Unveiling Future Landscapes, Key Insights, and Business Benchmarks](https://ai-infrastructure.org/wp-content/uploads/2024/03/The-State-of-AI-Infrastructure-at-Scale-2024.pdf)\n\n[145\\. AI infrastructure in 2025 | Project Stargate AI](https://lumenalta.com/insights/ai-infrastructure-in-2025)\n\n[146\\. 2025 AI Infrastructure: Key Insights from Uptime Institute ...](https://www.nextdc.com/blog/uptime-institute-ai-infrastructure-survey-2025)\n\n[147\\. Interim report January - March 2025](https://attachment.news.eu.nasdaq.com/a5a1f0ab8e0ff9017a6561cde50744f2b)\n\n[148\\. 2025年人工智能指数报告](https://pdf.dfcfw.com/pdf/H3_AP202506201694220072_1.pdf?1750413587000.pdf)\n\n[149\\. On-Premises AI Infrastructure Balances Innovation and Security](https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/docs/vmw-idc-us51604323.pdf)\n\n[150\\. AI Workloads in 2025: Challenges and Best Practices](https://www.koombea.com/blog/ai-workloads/)\n\n[151\\. The state of datacenter modernization in an AI-driven world](https://www.amd.com/content/dam/amd/en/documents/epyc-business-docs/white-papers/state-data-center-modernization-ai-451-research-discovery-report.pdf)\n\n[153\\. The Global Rise of Public AI](https://cdn.vanderbilt.edu/vu-URL/wp-content/uploads/sites/412/2025/05/05220054/The-Global-Rise-of-Public-AI.pdf)\n\n[154\\. \\[2025\\] Top 10 Global Fabless Semiconductor Companies](https://www.blackridgeresearch.com/blog/list-of-top-global-fabless-semiconductor-companies#:~:text=the%20semiconductor%20chips.-,4.2.,as%20indicated%20by%20their%20revenues.)\n\n[155\\. SPOT ISSUE](https://www.ktb.co.kr/common/download.jspx?cmd=viewPDF&path=/attach_file/RESEARCH/131089/1/20250625_B4530_hyyy_112.pdf)\n\n[156\\. Big Data and Artificial Intelligence in Modeling International Socio-Economic Processes](https://bdaim.uwed.uz/static/proceding.pdf)\n\n[157\\. Mobilising ASEAN Capital Markets for Sustainable Growth](https://www.oecd.org/content/dam/oecd/en/publications/reports/2024/05/mobilising-asean-capital-markets-for-sustainable-growth_67982c66/196b5bde-en.pdf)\n\n[158\\. Anthropic Comment on the Framework for Artificial Intelligence Diffusion](https://www-cdn.anthropic.com/7449887b6715e3a35f362b1301e5b5d8a6b116e5.pdf)\n\n[159\\. Artificial Intelligence: What to expect in 2025](https://www.dws.com/AssetDownload/Index?assetGuid=2cec319c-e3ea-44e8-b627-50367093008b&consumer=E-Library)\n\n[160\\. 维护国际和平与安全](https://digitallibrary.un.org/record/4070651/files/S_PV.9821-ZH.pdf)\n\n[161\\. Outlook 2025 Edition](https://www.capitalgroup.com/content/dam/cgc/shared-content/documents/webinar/2025_Outlook_report.pdf)\n\n[162\\. Permanent Mission of the Russian Federation to the United ...](https://russiaun.ru/en/news/19122024)\n\n[163\\. Mapping U.S. Multinationals' Global AI R&D Activity](https://cset.georgetown.edu/wp-content/uploads/CSET-Mapping-U.S.-Multinationals-Global-AI-RD-Activity-1.pdf)\n\n[164\\. National Strategy for Artificial Intelligence](https://www.niti.gov.in/sites/default/files/2023-03/National-Strategy-for-Artificial-Intelligence.pdf)\n\n[165\\. RÉALITÉS INDUSTRIELLES](https://annales-des-mines.org/wp-content/uploads/2025/04/RI_2025_05_webcouv.pdf)\n\n[166\\. Generative AI in the BRICS+ Countries: Trends and Outlook](https://www.yakovpartners.com/upload/iblock/a35/ft76bknzh09znv71qpvyr9k7q7c3wsxc/210125_generative_AI_BRICS_ENG.pdf)\n\n[167\\. Chinas Innovationssystem „Künstliche Intelligenz“: Ein Zwischenstand.](https://www.aies.at/download/2021/AIES-Fokus-2021-13.pdf)\n\n[168\\. 英伟达的进阶之路](https://finance.sina.com.cn/roll/2024-06-30/doc-incappte3841255.shtml?cre=tianyi&mod=pchp&loc=2&r=0&rfunc=67&tj=cxvertical_pc_hp&tr=12)\n\n[169\\. APPLYING AI IN KEY EUROPEAN INDUSTRIES](https://www.sitra.fi/wp/wp-content/uploads/2025/03/sitra-applying-ai-in-key-european-industries.pdf)\n\n[170\\. 科技巨头时代的超级大国，法国和印度共商2025AI人工智能 大计](https://www.bilibili.com/video/av113990948619445?t=1340)\n\n[171\\. Funding the Future: Global Investment Strategies in AI](https://trendsresearch.org/insight/funding-the-future-global-investment-strategies-in-ai/?srsltid=AfmBOopF6gafxj45P-7dWp22ISG6_YB3dAI2jk_fbsFKt_Nmp_mxOw8V)\n\n[173\\. ...with Cognizant to boost the adoption of generative ...](https://techbullion.com/microsoft-partnered-with-cognizant-to-boost-the-adoption-of-generative-ai/)\n\n[174\\. Big Data and Artificial Intelligence in Modeling International Socio-Economic Processes](https://bdaim.uwed.uz/static/proceding.pdf)\n\n[175\\. Global Outlook on Financing for Sustainable Development 2025: Towards a More Resilient and Inclusive Architecture](https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/02/global-outlook-on-financing-for-sustainable-development-2025_6748f647/753d5368-en.pdf)\n\n[176\\. The 2025 Global AI Adoption Report: Is Your Country on This List?](https://www.allaboutai.com/resources/ai-statistics/global-ai-adoption/#:~:text=Key%20Findings:%20Global%20AI%20Race%20in%202025,-Here%20are%20the&text=%F0%9F%8C%8FRegional%20Adoption%20Leaders:%20China,limited%20growth%20over%20five%20years.)\n\n[177\\. AI PLAYBOOK FOR SMALL STATES](https://www.imda.gov.sg/-/media/imda/files/news-and-events/media-room/media-releases/2024/09/ai-playbook-for-small-states/imda-ai-playbook-for-small-states.pdf)\n\n[178\\. Blueprint for Intelligent Economies: AI Competitiveness through Regional Collaboration](https://reports.weforum.org/docs/WEF_A_Blueprint_for_Intelligent_Economies_2025.pdf)\n\n[179\\. A Compute Agenda for India](https://cyberbrics.info/wp-content/uploads/2024/07/A-Compute-Agenda-for-India.pdf)\n\n[180\\. AI in small businesses – current and potential applications and issues for adoption](https://figshare.swinburne.edu.au/ndownloader/files/54910736)\n\n[181\\. Artificial Intelligence White Paper 2019](https://www.ipa.go.jp/publish/wp-ai/qv6pgp0000000w5z-att/000088599.pdf)\n\n[182\\. The Global Cooperation Barometer 2025 Second Edition](https://reports.weforum.org/docs/WEF_Global_Cooperation_Barometer_2025.pdf)\n\n[183\\. Developing Countries' Business Participation in the AI Economy](https://www.cigionline.org/documents/3432/Ai-economy-Lippoldt.pdf)\n\n[184\\. R. Vinuesa, Hossein Azizpour et al. “The role of artificial intelligence in achieving the Sustainable Development Goals.” Nature Communications](https://doi.org/10.1038/s41467-019-14108-y)\n\n[185\\. ISO - Computing](https://msb.isolutions.iso.org/foresight/computing.html)\n\n[186\\. Essential Business Stats & Trends in 2025](https://www.intuition.com/wp-content/uploads/2024/11/Essential-business-stats-trends-in-2025-compressed-1.pdf)\n\n[187\\. National Program for Artificial Intelligence](https://innovationisrael.org.il/en/wp-content/uploads/sites/3/2025/05/AI-National-Program-ENG-14-5-25.pdf)\n\n[188\\. The Commonwealth Cyber Journal](https://production-new-commonwealth-files.s3.eu-west-2.amazonaws.com/s3fs-public/2024-06/commonwealth-cybercrime-journal_updf.pdf)\n\n[189\\. 电子周跟踪:微软等联手启动AI合作 阿里QWEN2.5再登全球开...](https://stock.finance.sina.com.cn/stock/go.php/vReport_Show/kind/industry/rptid/780453214260/index.phtml)\n\n[190\\. Науковий вісник Ужгородського національного університету](http://www.visnyk-econom.uzhnu.uz.ua/archive/31_2020ua/31_2020.pdf)\n\n[191\\. Trust, attitudes and use of artificial intelligence: A global study 2025](https://assets.kpmg.com/content/dam/kpmg/au/pdf/2025/trust-in-ai-global-insights-2025-full-report.pdf)\n\n[192\\. 小国将与大国分享影响力，人工智能将颠覆传统地缘政治？](https://www.jfdaily.com/news/detail?id=98244)\n\n[193\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[194\\. KENYA ARTIFICIAL INTELLIGENCE STRATEGY 2025-2030](https://www.ict.go.ke/sites/default/files/2025-03/Kenya%20AI%20Strategy%202025%20-%202030.pdf)\n\n[195\\. Interim report January - March 2025](https://attachment.news.eu.nasdaq.com/a5a1f0ab8e0ff9017a6561cde50744f2b)\n\n[196\\. AI Infrastructure for the Agentic Era](https://www.cisco.com/c/dam/en_us/solutions/artificial-intelligence/ai-infrastructure.pdf?dtid=osscdc000283&linkclickid=srch)\n\n[197\\. AI Infrastructure in 2025: Balancing Datacenter and Cloud investments](https://www.intel.com/content/dam/www/central-libraries/us/en/documents/2025-02/idc-ai-infrastructure-balancing-dc-and-cloud-investments-brief.pdf)\n\n[198\\. AI infrastructure in 2025 | Project Stargate AI](https://lumenalta.com/insights/ai-infrastructure-in-2025)\n\n[199\\. Innovations in Enterprise AI Infrastructure: The Evolution of Scalable Architectures](https://www.irjmets.com/uploadedfiles/paper/volume_7/issue_3_march_2025/69528/1742192710.docx)\n\n[200\\. ...$ $中际旭创(SZ300308)$原文网页链接一、光模块... - 雪球](https://xueqiu.com/1615078503/335072541)\n\n[201\\. 2025年のAIインフラストラクチャ：データセンターとクラウドに対する投資バランス](https://www.intel.co.jp/content/dam/www/central-libraries/jp/ja/documents/2025-04/idc-ai-infrastructure-balancing-dc-and-cloud-investments-brief-jp.pdf)\n\n[203\\. SPOT ISSUE](https://www.ktb.co.kr/common/download.jspx?cmd=viewPDF&path=/attach_file/RESEARCH/131089/1/20250625_B4530_hyyy_112.pdf)\n\n[204\\. National Artificial Intelligence Strategy 2024-2026](https://www.mots.gov.zm/wp-content/uploads/2025/02/Zambia-Ai-Strategy-Book-option-2.pdf)\n\n[205\\. 一文读懂G20的AI治理路线图!](https://mp.weixin.qq.com/s?__biz=MzU0NDkzNTYxOQ%3D%3D&mid=2247491441&idx=1&sn=531e2571d7217646df47dcb0f3ddf2f6&chksm=fa4f2b03f4f6b425a48b08889737f1b1ffbfbf48b372db5dbdb7df757e7590c91d27f03dd9b4&scene=27)\n\n[206\\. The AGI Landscape in 2025: Competition, Governance, and Emerging Paradigms](https://datatunnel.io/wp-content/uploads/2025/04/Publication-AGI-2025-Race-Control-and-the-Future-of-Intelligence.pdf)\n\n[207\\. Anna Jobin, M. Ienca et al. “The global landscape of AI ethics guidelines.” Nature Machine Intelligence](https://doi.org/10.1038/s42256-019-0088-2)\n\n[208\\. The Global Rise of Public AI](https://cdn.vanderbilt.edu/vu-URL/wp-content/uploads/sites/412/2025/05/05220054/The-Global-Rise-of-Public-AI.pdf)\n\n[209\\. D. Greene, A. Hoffmann et al. “Better, Nicer, Clearer, Fairer: A Critical Assessment of the Movement for Ethical Artificial Intelligence and Machine Learning.” Hawaii International Conference on System Sciences](https://doi.org/10.24251/HICSS.2019.258)\n\n[210\\. 2025 \"人工智能+\" 行业发展蓝皮书](http://www.sccio.cn/uploads/20250522/8696496143e2c9e662e2e45890c9c1b4.pdf)\n\n[211\\. Thilo Hagendorff. “The Ethics of AI Ethics: An Evaluation of Guidelines.” Minds and Machines](https://doi.org/10.1007/s11023-020-09517-8)\n\n[212\\. L. Floridi, Josh Cowls et al. “AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations.” Minds and Machines](https://doi.org/10.1007/s11023-018-9482-5)\n\n[213\\. A policy framework on AI usage in developing countries and its impact](https://gjeta.com/sites/default/files/GJETA-2024-0192.pdf)\n\n[214\\. Strategic Research and Innovation Agenda 2021](https://efecs.eu/publication/download/ecs-sria-2021.pdf)\n\n[215\\. Public & Private Partnerships Key to Maximizing Sovereign ...](https://www.dell.com/en-sg/blog/public-and-private-partnership-is-the-key-to-maximizing-sovereign-ai-opportunities/)\n\n[216\\. Funding the Future: Global Investment Strategies in AI](https://trendsresearch.org/insight/funding-the-future-global-investment-strategies-in-ai/?srsltid=AfmBOooeF4WuOJ4LdAtgUREbawYF-PBMxnSiUjn4Wfys3COGBcpdychq)\n\n[217\\. REWIRE FOR SUCCESS: BOOSTING INDIA'S AIQ](https://www.accenture.com/content/dam/accenture/final/a-com-migration/r3-3/pdf/pdf-153/accenture-ai-for-economic-growth-india.pdf)\n\n[218\\. Futures of Global AI Governance: Co-Creating an Approach for Transforming Economies and Societies](https://www.oecd.org/content/dam/oecd/en/about/programmes/strategic-foresight/GSG%20Background%20Note_GSG%282024%291en.pdf/_jcr_content/renditions/original./GSG%20Background%20Note_GSG%282024%291en.pdf)\n\n[219\\. Corinne Cath, Sandra Wachter et al. “Artificial Intelligence and the ‘Good Society’: the US, EU, and UK approach.” Science and Engineering Ethics](https://doi.org/10.1007/s11948-017-9901-7)\n\n[220\\. GROWING TOGETHER BUILDING PUBLIC-PRIVATE PARTNERSHIPS TO BOOST THE AI REVOLUTION](https://innovitsf.com/wp-content/uploads/2024/11/Innovit-G7-Growing-Together-Agenda-D5.pdf)\n\n[223\\. ...with Cognizant to boost the adoption of generative ...](https://techbullion.com/microsoft-partnered-with-cognizant-to-boost-the-adoption-of-generative-ai/)\n\n[224\\. Global Outlook on Financing for Sustainable Development 2025: Towards a more resilient and inclusive architecture](https://www.developmentaid.org/api/frontend/cms/file/2025/02/753d5368-en.pdf)\n\n[225\\. The 2025 Global AI Adoption Report: Is Your Country on This List?](https://www.allaboutai.com/resources/ai-statistics/global-ai-adoption/#:~:text=Key%20Findings:%20Global%20AI%20Race%20in%202025,-Here%20are%20the&text=%F0%9F%8C%8FRegional%20Adoption%20Leaders:%20China,limited%20growth%20over%20five%20years.)\n\n[226\\. BEYOND BITS AND ALGORITHMS: REDEFINING BUSINESSES AND FUTURE OF WORK](http://www.ef.uni-lj.si/media/document_files/enote/IMB/PKP2023_book.pdf)\n\n[227\\. 人工智能赋能未来产业发展探析](https://journals.istic.ac.cn/qqkjjjlw/ch/reader/create_pdf.aspx?file_no=201811-12001&year_id=2018&quarter_id=11-12&falg=1)\n\n[228\\. AI in Numbers: Market Growth, Emerging Trends, and What’s Next for Artificial Intelligence](https://www.procurri.com/wp-content/uploads/2025/03/Procurri-Knowledge-Hub-AI-in-Numbers-Market-Growth-Emerging-Trends-and-Whats-Next-for-Artificial-Intelligence.pdf)\n\n[229\\. Adopt AI Study](https://ieu-monitoring.com/wp-content/uploads/Adopt_AI__Final_study_report_0P6WutvqV0occxmADGTSP8i8X8_108555.pdf)\n\n[230\\. World Employment and Social Outlook: September 2024 Update](https://www.ilo.org/media/589756/download)\n\n[231\\. 55 Fascinating AI Statistics And Trends For 2025](https://dataprot.net/blog/ai-statistics/)\n\n[232\\. BÁO CÁO THƯỜNG NIÊN 2024](https://fpt.com/-/media/project/fpt-corporation/fpt/ir/information-disclosures/year-report/2025/april/20250402---fpt---bao-cao-thuong-nien-nam-2024.pdf)\n\n[233\\. Essential Business Stats & Trends in 2025](https://www.intuition.com/wp-content/uploads/2024/11/Essential-business-stats-trends-in-2025-compressed-1.pdf)\n\n[234\\. IMD World Digital Competitiveness Ranking 2023](https://www.imd.org/wp-content/uploads/2023/12/Digital_2023.pdf)\n\n[235\\. 普华永道报告分析人工智能对产业的影响和机遇](http://www.casisd.cn/zkcg/ydkb/kjzcyzxkb/2017/201709/201709/t20170908_4857646.html)\n\n[236\\. The Adoption of Artificial Intelligence in Firms: New Evidence for Policymaking](https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/05/the-adoption-of-artificial-intelligence-in-firms_8fab986b/f9ef33c3-en.pdf)\n\n[237\\. ICMA Quarterly Report](https://www.icmagroup.org/assets/documents/Regulatory/Quarterly_Reports/ICMA-Quarterly-Report-Q2-2024.pdf)\n\n[238\\. К.Ю. Корсунова. “ШТУЧНИЙ ІНТЕЛЕКТ У КОНТЕНТ-МАРКЕТИНГУ: ФОРМУВАННЯ МАЙБУТНЬОГО ЦИФРОВОЇ СТРАТЕГІЇ.” Journal of Strategic Economic Research](https://doi.org/10.30857/2786-5398.2024.1.8)\n\n[239\\. AI in information technology and its future in the United States.](https://www.semanticscholar.org/paper/302ada4d94b0fff04ec767e94be9186d2256c869)\n\n[240\\. 高盛：2025年全球人工智能投资有望增至2000亿美元](https://finance.sina.com.cn/stock/usstock/c/2023-08-03/doc-imzexiix6581840.shtml?cre=tianyi&mod=pchp&loc=20&r=0&rfunc=69&tj=cxvertical_pc_hp&tr=12)\n\n[241\\. Artificial Intelligence Index Report 2025: Chapter 4 Economy](https://hai-production.s3.amazonaws.com/files/hai_ai-index-report-2025_chapter4_final.pdf)\n\n[242\\. 厦门弘信电子科技集团股份有限公司2024年年度报告](http://notice.10jqka.com.cn/api/pdf/98728743cd4887dd_1743320304/%E5%BC%98%E4%BF%A1%E7%94%B5%E5%AD%90%EF%BC%9A2024%E5%B9%B4%E5%B9%B4%E5%BA%A6%E6%8A%A5%E5%91%8A.pdf)\n\n[243\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[244\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[245\\. Q2 2025: Nebius AI Cloud updates](https://nebius.com/blog/posts/q2-2025-nebius-ai-cloud-updates)\n\n[246\\. Deep Learning Benchmarks 2025](https://www.byteplus.com/en/topic/466080)\n\n[247\\. N. Jouppi, C. Young et al. “In-datacenter performance analysis of a tensor processing unit.” 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)](https://doi.org/10.1145/3079856.3080246)\n\n[248\\. Roy Schwartz, Jesse Dodge et al. “Green AI.”](https://arxiv.org/abs/1907.10597)\n\n[249\\. David A. Patterson, Joseph Gonzalez et al. “Carbon Emissions and Large Neural Network Training.” ArXiv](https://arxiv.org/abs/2104.10350)\n\n[250\\. 2025 AI Infrastructure: Key Insights from Uptime Institute ...](https://www.nextdc.com/blog/uptime-institute-ai-infrastructure-survey-2025)\n\n[251\\. Interim report January - March 2025](https://attachment.news.eu.nasdaq.com/a5a1f0ab8e0ff9017a6561cde50744f2b)\n\n[252\\. Q2FY25 Earnings Results Conference Call](https://s2.q4cdn.com/951347115/files/doc_financials/2025/q2/Q2FY25-Prepared-Remarks-IR.pdf)\n\n[253\\. 2025 Futures Report](https://kpmg.com/kpmg-us/content/dam/kpmg/pdf/2025/kpmg-2025-futures-report.pdf)\n\n[254\\. 2024大模型十大趋势 走进“机器外脑”时代](http://www.ecconsortium.org/Uploads/file/20240830/1725004885522204.pdf)\n\n[255\\. Training Math Reasoning Model with Reinforcement ...](https://research.nvidia.com/labs/adlr/acemath_rl/)\n\n[256\\. 美国AI基础设施革命与AGI时间窗口推演（2025-2030）](https://www.cnsunrun.com/blog-details/210)\n\n[257\\. Artificial Intelligence Index Report 2025](https://hai-production.s3.amazonaws.com/files/hai_ai-index-report-2025_chapter1_final.pdf)\n\n[258\\. DeepSeek 火出圈，重视 AIDC 产业链及端侧 AI 投资机会](https://file.iyanbao.com/pdf/6ee20-5608d1fe-e9dd-441e-9012-f0630048bde4.pdf)\n\n[259\\. Sovereign AI](https://atos.net/en/lp/sovereign-ai)\n\n[260\\. I Reviewed Top 6 Generative AI Infrastructure Tools of 2025](https://learn.g2.com/best-generative-ai-infrastructure-software?hsLang=en)\n\n[261\\. 2025 AI infrastructure: What the data is telling leaders now - CIO](https://www.cio.com/article/4017696/2025-ai-infrastructure-what-the-data-is-telling-leaders-now.html)\n\n[263\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[264\\. Partnerships Between Cloud Service Providers and AI Developers: FTC Staff Report on AI Partnerships & Investments 6(b) Study](https://www.ftc.gov/system/files/ftc_gov/pdf/p246201_aipartnerships6breport_redacted_0.pdf)\n\n[265\\. 西城园协会金科联盟 | 斯坦福大学发布《2025 年人工智能指...](https://mp.weixin.qq.com/s?__biz=MzU2NjQxMzI5OQ%3D%3D&mid=2247706128&idx=8&sn=6f87a1678a1b86f7eabb8cfe96456cfa&chksm=fdc64d940c859e42876261f7929dca04d2c1c076b692ff510f5521f8c61ba5a1a15f50d73b46&scene=27)\n\n[266\\. 人工智能芯片2023-2033](https://www.idtechex.com/zh/research-report/ai-chips/937)\n\n[267\\. Joy Buolamwini, Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” FAT](https://www.semanticscholar.org/paper/18858cc936947fc96b5c06bbe3c6c2faa5614540)\n\n[268\\. Generative AI in the BRICS+ Countries: Trends and Outlook](https://www.yakovpartners.com/upload/iblock/a35/ft76bknzh09znv71qpvyr9k7q7c3wsxc/210125_generative_AI_BRICS_ENG.pdf)\n\n[269\\. Outlook 2025 Edition](https://www.lwm-midland.com/files/75642/Econ%20Market%2012.23.pdf)\n\n[270\\. SPOT ISSUE](https://www.ktb.co.kr/common/download.jspx?cmd=viewPDF&path=/attach_file/RESEARCH/131089/1/20250625_B4530_hyyy_112.pdf)\n\n[271\\. 斯坦福大学全球AI发展全景洞察](https://www.360doc.cn/article/47115229_1156957943.html)\n\n[272\\. Public-Private Partnerships for AI Governance: Encouraging Cooperation Between Stakeholders](https://hackernoon.com/public-private-partnerships-for-ai-governance-encouraging-cooperation-between-stakeholders?source=rss)\n\n[273\\. AI in 2025: current initiatives and challenges in large enterprises](https://www.wavestone.com/wp-content/uploads/2025/01/ai-action-summit-report.pdf)\n\n[274\\. The Global Rise of Public AI](https://cdn.vanderbilt.edu/vu-URL/wp-content/uploads/sites/412/2025/05/05220054/The-Global-Rise-of-Public-AI.pdf)\n\n[275\\. 美国2019《国家人工智能战略》报告摘编](https://www.secrss.com/articles/12198)\n\n[276\\. The AGI Landscape in 2025: Competition, Governance, and Emerging Paradigms](https://datatunnel.io/wp-content/uploads/2025/04/Publication-AGI-2025-Race-Control-and-the-Future-of-Intelligence.pdf)\n\n[277\\. Public & Private Partnerships Key to Maximizing Sovereign ...](https://www.dell.com/en-sg/blog/public-and-private-partnership-is-the-key-to-maximizing-sovereign-ai-opportunities/)\n\n[278\\. Anna Jobin, M. Ienca et al. “The global landscape of AI ethics guidelines.” Nature Machine Intelligence](https://doi.org/10.1038/s42256-019-0088-2)\n\n[279\\. Matthew U. Scherer. “Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies.” Harvard Journal of Law & Technology](https://doi.org/10.2139/SSRN.2609777)\n\n[280\\. L. Floridi, Josh Cowls et al. “AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations.” Minds and Machines](https://doi.org/10.1007/s11023-018-9482-5)\n\n[283\\. The transformative role of AI in reshaping employment trends across South Asia](https://revecon.ro/sites/default/files/2024-2-4.pdf)\n\n[284\\. BEYOND BITS AND ALGORITHMS: REDEFINING BUSINESSES AND FUTURE OF WORK](http://www.ef.uni-lj.si/media/document_files/enote/IMB/PKP2023_book.pdf)\n\n[285\\. Big Data and Artificial Intelligence in Modeling International Socio-Economic Processes](https://bdaim.uwed.uz/static/proceding.pdf)\n\n[286\\. The mirror for (artificial) intelligence: Working in whose reflection?](https://www.econstor.eu/bitstream/10419/213436/1/1688602348.pdf)\n\n[287\\. UNLOCKING THE AI GROWTH MULTIPLIER](https://www.bny.com/assets/corporate/documents/pdf/insights/unlocking-the-ai-growth-multiplier.pdf)\n\n[288\\. AI in Numbers: Market Growth, Emerging Trends, and What’s Next for Artificial Intelligence](https://www.procurri.com/wp-content/uploads/2025/03/Procurri-Knowledge-Hub-AI-in-Numbers-Market-Growth-Emerging-Trends-and-Whats-Next-for-Artificial-Intelligence.pdf)\n\n[289\\. Essential Business Stats & Trends in 2025](https://www.intuition.com/wp-content/uploads/2024/11/Essential-business-stats-trends-in-2025-compressed-1.pdf)\n\n[290\\. 产业孵化中AI业务的对接](https://s3.cn-north-1.amazonaws.com.cn/aws-summit-2017-beijing/4_%E4%BA%A7%E4%B8%9A%E5%AD%B5%E5%8C%96%E4%B8%AD+AI+%E4%B8%9A%E5%8A%A1%E7%9A%84%E5%AF%B9%E6%8E%A5.pdf)\n\n[291\\. The diverse economic impacts of artificial intelligence](https://research-center.amundi.com/files/nuxeo/dl/13355d1c-2ecd-4fbf-8a53-8d53c3338c56?inline=)\n\n[292\\. AI产业迈入成长期，坚定中国AI产业发展信心](https://pdf.dfcfw.com/pdf/H3_AP202504091653771051_1.pdf?1744219458000.pdf)\n\n[293\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[294\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[295\\. K. Simonyan, Andrew Zisserman. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” CoRR](https://arxiv.org/abs/1409.1556)\n\n[296\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[297\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[298\\. Q2 2025: Nebius AI Cloud updates](https://nebius.com/blog/posts/q2-2025-nebius-ai-cloud-updates)\n\n[299\\. 高质量大模型基础设施研究报告（2024年）](https://www.caict.ac.cn/kxyj/qwfb/ztbg/202501/P020250116364238709525.pdf)\n\n[300\\. 2025 AI Infrastructure: Key Insights from Uptime Institute ...](https://www.nextdc.com/blog/uptime-institute-ai-infrastructure-survey-2025)\n\n[301\\. Interim report January - March 2025](https://attachment.news.eu.nasdaq.com/a5a1f0ab8e0ff9017a6561cde50744f2b)\n\n[302\\. Cisco's AI Momentum Driving Infrastructure Deals To The Tune Of ...](https://www.crn.com/news/networking/2025/cisco-s-ai-momentum-driving-infrastructure-deals-to-the-tune-of-700m-in-2025-ceo#:~:text=Cisco%20reported%20AI%20infrastructure%20orders,orders%20in%202025,%20Robbins%20said.)\n\n[303\\. Second Quarter FY 2025 Quarterly Update](https://www.infineon.com/dgdl/2025-05-08-q2-fy25-investor-presentation-v01-00-en.pdf?fileId=8ac78c8b96295ebc0196abff795a00b1)\n\n[304\\. Five Forces Reshaping AI Infrastructure in 2025](https://ai-infra-summit.com/blog/five-forces-reshaping-ai-infrastructure-2025)\n\n[305\\. AI Progress in 2025: What's Happened and What's Next](https://www.digitalbricks.ai/blog-posts/ai-progress-in-2025-whats-happened-and-whats-next)\n\n[306\\. I Reviewed Top 6 Generative AI Infrastructure Tools of 2025](https://learn.g2.com/best-generative-ai-infrastructure-software?hsLang=en)\n\n[307\\. Yann LeCun, Yoshua Bengio et al. “Deep Learning.”](https://www.semanticscholar.org/paper/2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb)\n\n[308\\. Artificial Intelligence Index Report 2025 Policy Highlights](https://hai-production.s3.amazonaws.com/files/hai-ai-index-2025-policy-highlights.pdf)\n\n[309\\. Artificial Intelligence Index Report 2025: Chapter 6 - Policy and Governance](https://hai-production.s3.amazonaws.com/files/hai_ai-index-report-2025_chapter6_final.pdf)\n\n[310\\. 西城园协会金科联盟 | 斯坦福大学发布《2025 年人工智能指...](https://mp.weixin.qq.com/s?__biz=MzU2NjQxMzI5OQ%3D%3D&mid=2247706128&idx=8&sn=6f87a1678a1b86f7eabb8cfe96456cfa&chksm=fdc64d940c859e42876261f7929dca04d2c1c076b692ff510f5521f8c61ba5a1a15f50d73b46&scene=27)\n\n[311\\. Matthew U. Scherer. “Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies.” Harvard Journal of Law & Technology](https://doi.org/10.2139/SSRN.2609777)\n\n[312\\. Partnerships Between Cloud Service Providers and AI Developers: FTC Staff Report on AI Partnerships & Investments 6(b) Study](https://www.ftc.gov/system/files/ftc_gov/pdf/p246201_aipartnerships6breport_redacted_0.pdf)\n\n[313\\. Public AI: Making AI work for everyone, by everyone](https://assets.mofoprod.net/network/documents/Public_AI_Mozilla.pdf)\n\n[314\\. 斯坦福大学全球AI发展全景洞察](https://www.360doc.cn/article/47115229_1156957943.html)\n\n[315\\. SPOT ISSUE](https://www.ktb.co.kr/common/download.jspx?cmd=viewPDF&path=/attach_file/RESEARCH/131089/1/20250625_B4530_hyyy_112.pdf)\n\n[316\\. Solutions: Infrastructure - Public-Private Partnerships](https://connectednation.org/press-releases/solutions-infrastructure-public-private-partnerships)\n\n[317\\. 人工智能芯片2023-2033](https://www.idtechex.com/zh/research-report/ai-chips/937)\n\n[318\\. 斯坦福：2025年人工智能AI指数报告](https://finance.sina.com.cn/tech/roll/2025-04-09/doc-inespimz3126718.shtml)\n\n[321\\. R. Vinuesa, Hossein Azizpour et al. “The role of artificial intelligence in achieving the Sustainable Development Goals.” Nature Communications](https://doi.org/10.1038/s41467-019-14108-y)\n\n[322\\. 报告：美、中两国居全球计算力指数排名前二](https://finance.sina.com.cn/chanjing/cyxw/2022-03-17/doc-imcwipih9107641.shtml)\n\n[323\\. Artificial Intelligence Index Report 2025: Chapter 6 - Policy and Governance](https://hai-production.s3.amazonaws.com/files/hai_ai-index-report-2025_chapter6_final.pdf)\n\n[324\\. Job Creation and Local Economic Development 2024: The Geography of Generative AI](https://www.oecd.org/content/dam/oecd/en/publications/reports/2024/11/job-creation-and-local-economic-development-2024_01a245c1/83325127-en.pdf)\n\n[325\\. Artificial Intelligence Index Report 2025: Chapter 4 Economy](https://hai-production.s3.amazonaws.com/files/hai_ai-index-report-2025_chapter4_final.pdf)\n\n[326\\. 埃森哲：2024生成式AI时代的工作模式](https://mip.sgpjbg.com/baogao/169510.html)\n\n[327\\. March Macro Brief: The geopolitics of AI](https://www.accenture.com/content/dam/accenture/final/accenture-com/document-3/Accenture-Strategy-Macro-Foresight-Brief-March-2025.pdf)\n\n[328\\. The Global Cooperation Barometer 2025 Second Edition](https://reports.weforum.org/docs/WEF_Global_Cooperation_Barometer_2025.pdf)\n\n[329\\. GDP growth (annual).](https://doi.org/10.1787/72d6876b-en)\n\n[330\\. Artificial intelligence doesn’t create, or destroy jobs. It transforms them](https://www.bbva.com/en/artificial-intelligence-doesnt-create-destroy-jobs-transforms/)\n\n[331\\. Future of International Cooperation Report 2024: The Innovation Imperative: Tech-Governance, Development & Security at a Crossroads](https://www.stimson.org/wp-content/uploads/2024/09/FIC-24_Web_4.10PM.pdf)\n\n[339\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[340\\. N. Jouppi, C. Young et al. “In-datacenter performance analysis of a tensor processing unit.” 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)](https://doi.org/10.1145/3079856.3080246)\n\n[341\\. Roy Schwartz, Jesse Dodge et al. “Green AI.”](https://arxiv.org/abs/1907.10597)\n\n[342\\. Tan Yigitcanlar, K. Desouza et al. “Contributions and Risks of Artificial Intelligence (AI) in Building Smarter Cities: Insights from a Systematic Review of the Literature.” Energies](https://doi.org/10.3390/en13061473)\n\n[343\\. 公用环保 202502 第 3 期 美国成立国家能源委员会，AI 如何赋能电力行业](https://file.iyanbao.com/pdf/29dd5-7341a059-0bbc-46c1-aaa1-98817d5ec4b6.pdf)\n\n[344\\. 2025 AI Infrastructure: Key Insights from Uptime Institute ...](https://www.nextdc.com/blog/uptime-institute-ai-infrastructure-survey-2025)\n\n[345\\. 美国AI基础设施革命与AGI时间窗口推演（2025-2030）](https://www.cnsunrun.com/blog-details/210)\n\n[346\\. KENYA ARTIFICIAL INTELLIGENCE STRATEGY 2025-2030](https://www.ict.go.ke/sites/default/files/2025-03/Kenya%20AI%20Strategy%202025%20-%202030.pdf)\n\n[347\\. Yang Zhao, Chaobo Zhang et al. “A review of data mining technologies in building energy systems: Load prediction, pattern identification, fault detection and diagnosis.”](https://doi.org/10.1016/j.enbenv.2019.11.003)\n\n[348\\. AI Infrastructure in 2025: Balancing Datacenter and Cloud investments](https://www.intel.com/content/dam/www/central-libraries/us/en/documents/2025-02/idc-ai-infrastructure-balancing-dc-and-cloud-investments-brief.pdf)\n\n[349\\. Powering artificial intelligence: A study of AI's environmental footprint—today and tomorrow](https://www2.deloitte.com/content/dam/Deloitte/fr/Documents/explore/powering-artificial-intelligence.pdf)\n\n[350\\. Interim report January - March 2025](https://attachment.news.eu.nasdaq.com/a5a1f0ab8e0ff9017a6561cde50744f2b)\n\n[351\\. Artificial intelligence and national security in 2025](https://www.ditchley.com/sites/default/files/AI%20and%20national%20security%20in%202025%20TOR.pdf)\n\n[352\\. AI Infrastructure for the Agentic Era](https://www.cisco.com/c/dam/en_us/solutions/artificial-intelligence/ai-infrastructure.pdf?dtid=osscdc000283&linkclickid=srch)\n\n[353\\. DEPLOYING AI/ML IN MICROSERVICES: A COMPARATIVE STUDY OF TOOLS AND METHODOLOGIES WITH SCALABILITY ANALYSIS AND CASE STUDIES OF LEADING COMPANIES](https://tianjindaxuexuebao.com/dashboard/uploads/6.14997638.pdf)\n\n[354\\. 2025年のAIインフラストラクチャ：データセンターとクラウドに対する投資バランス](https://www.intel.co.jp/content/dam/www/central-libraries/jp/ja/documents/2025-04/idc-ai-infrastructure-balancing-dc-and-cloud-investments-brief-jp.pdf)\n\n[355\\. Oliver E. Williamson. “The Economic Institutions of Capitalism.” The Antitrust Bulletin](https://doi.org/10.1177/0003603X8603100308)\n\n[356\\. Is the Spotlight Shifting? Will the Chinese Dominate the AI ...](https://eu.36kr.com/en/p/3392935258933636)\n\n[357\\. Artificial Intelligence Index Report 2025 Policy Highlights](https://hai-production.s3.amazonaws.com/files/hai-ai-index-2025-policy-highlights.pdf)\n\n[358\\. Matthew U. Scherer. “Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies.” Harvard Journal of Law & Technology](https://doi.org/10.2139/SSRN.2609777)\n\n[359\\. 西城园协会金科联盟 | 斯坦福大学发布《2025 年人工智能指...](https://mp.weixin.qq.com/s?__biz=MzU2NjQxMzI5OQ%3D%3D&mid=2247706128&idx=8&sn=6f87a1678a1b86f7eabb8cfe96456cfa&chksm=fdc64d940c859e42876261f7929dca04d2c1c076b692ff510f5521f8c61ba5a1a15f50d73b46&scene=27)\n\n[360\\. 斯坦福大学全球AI发展全景洞察](https://www.360doc.cn/article/47115229_1156957943.html)\n\n[361\\. G. Hodge. “The risky business of public–private partnerships.” Australian Journal of Public Administration](https://doi.org/10.1111/J.1467-8500.2004.00400.X)\n\n[362\\. H. Van Ham, J. Koppenjan. “BUILDING PUBLIC-PRIVATE PARTNERSHIPS: Assessing and managing risks in port development.” Public Management Review](https://doi.org/10.1080/14616670110070622)\n\n[363\\. Artificial Intelligence Index Report 2025: Chapter 6 - Policy and Governance](https://hai-production.s3.amazonaws.com/files/hai_ai-index-report-2025_chapter6_final.pdf)\n\n[364\\. SPOT ISSUE](https://www.ktb.co.kr/common/download.jspx?cmd=viewPDF&path=/attach_file/RESEARCH/131089/1/20250625_B4530_hyyy_112.pdf)\n\n[365\\. USI | SiP Modules, Electronic Devices Design and ...](https://www.usiglobal.com/en)\n\n[366\\. Avis n° 24-A-05 du 28 juin 2024 relatif au fonctionnement concurrentiel du secteur de l'intelligence artificielle générative](https://www.autoritedelaconcurrence.fr/sites/default/files/integral_texts/2024-06/avisIA.pdf)\n\n[367\\. AI Index 2025 주요 내용과 시사점](https://www.spri.kr/download/23630)\n\n[368\\. Wacker demos EV battery gear, AI cooling tech at supply ...](http://www.china.org.cn/2025-07/21/content_117988250.shtml)\n\n[369\\. Model contractual clauses for the public procurement of Non-High-Risk AI ('MCC-AI-High-Light')](https://public-buyers-community.ec.europa.eu/system/files/2025-03/20250226%20Model%20Clauses%20Light%20Version_proofread.docx)\n\n[370\\. Partnerships Between Cloud Service Providers and AI Developers: FTC Staff Report on AI Partnerships & Investments 6(b) Study](https://www.ftc.gov/system/files/ftc_gov/pdf/p246201_aipartnerships6breport_redacted_0.pdf)\n\n[371\\. 斯坦福：2025年人工智能AI指数报告](https://finance.sina.com.cn/tech/roll/2025-04-09/doc-inespimz3126718.shtml)"
  },
  {
    "id": 23,
    "question": "How might the development of 'molecular psychology' through advanced neurochemical manipulation reshape our understanding of both human consciousness and machine intelligence?",
    "response": "**Research Report**\n\n**To:** Interested Parties\n\n**From:** Expert Researcher\n\n**Date:** July 26, 2025\n\n**Subject:** The Emergence of Molecular Psychology and its Dual Impact on Consciousness and Machine Intelligence\n\n**1.0 Introduction: The Dawn of Molecular Psychology**\n\nAs of 2025, neuroscience is undergoing a profound transformation, marked by the rise of a discipline we can term 'molecular psychology'. This field moves beyond correlational studies of brain activity to the direct, causal manipulation of neural circuits through advanced neurochemical and genetic engineering. Its core premise is that by precisely controlling the chemical messengers and signaling pathways in the brain, we can deconstruct complex phenomena like behavior, emotion, and consciousness itself into their fundamental molecular and circuit-level components.\n\nThis report will analyze the current state of molecular psychology, drawing exclusively on research and developments documented up to the present date. We will examine its two most significant frontiers. First, how its powerful toolkit is beginning to reshape our understanding of human consciousness, moving from broad pharmacological effects to targeted circuit-based interventions. Second, how insights derived from these neurochemical manipulations are creating powerful, albeit nascent, analogies and direct inspirations for the architecture of advanced machine intelligence, particularly in the domain of neural transformers. This analysis will reveal a field characterized by powerful new capabilities, significant progress in animal models, a critical translational gap to human studies, and a burgeoning, reciprocal dialogue between neuroscience and artificial intelligence.\n\n**2.0 The Methodological Arsenal of Molecular Psychology**\n\nThe power of molecular psychology lies in its increasingly sophisticated set of tools for perturbing and observing neural systems with unprecedented precision.\n\n**2.1 Precision Targeting: Chemogenetics and Optogenetics**\n\nThe vanguard of neurochemical manipulation is led by chemogenetics and optogenetics, techniques highlighted as central to major research efforts like the BRAIN 2025 initiative \\[1\\].\n\n**Chemogenetics:** This technique utilizes genetically engineered receptors that are inert to endogenous molecules but can be activated by specific, otherwise inert \"designer drugs.\" The most prominent example is DREADD (Designer Receptors Exclusively Activated by Designer Drugs) technology, which allows researchers to non-invasively turn specific neuron populations on or off with high temporal and spatial resolution \\[1\\]. This method is fundamental for establishing causal links between specific cell types and observed behaviors or physiological states \\[1\\].\n\n**Optogenetics:** While leveraging light rather than chemicals for activation, optogenetics is a key complementary tool for circuit manipulation. It involves genetically introducing light-sensitive proteins into neurons, allowing for their activation or inhibition with millisecond-scale precision, enabling deep causality studies \\[1\\].\n\n**2.2 Diversifying the Toolkit: Advanced Neuromodulation and Imaging**\n\nBeyond these headline techniques, the field employs a range of other methods for chemical neuromodulation. These include microinjection, the release of chemicals from electrode coatings or nanoparticles, and even the _in-situ_ electrochemical synthesis of neuromodulators at a target site \\[1\\]. These methods offer distinct advantages in terms of the variety of substances that can be used and the potential for programmable, quantitatively precise release profiles \\[1\\].\n\nCrucially, translating findings from these manipulation techniques to humans requires non-invasive observation methods. Here, Positron Emission Tomography (PET) is a vital tool, providing a window into dynamic neurochemical events like neurotransmitter release and receptor binding in the living human brain. It offers a level of chemical specificity that other techniques, such as MR spectroscopy, currently lack \\[1\\]. This suite of tools, from genetic manipulation to advanced imaging, forms the bedrock of molecular psychology's approach to dissecting brain function.\n\n**3.0 Reshaping the Understanding of Human Consciousness**\n\nThe ultimate goal for many in this field is to unravel the biological basis of subjective experience. Molecular psychology approaches this by attempting to link precise neurochemical interventions to measurable changes in consciousness.\n\n**3.1 The Enduring Challenge: Quantifying Subjective Experience**\n\nA primary obstacle is the measurement of consciousness itself. Research between 2020 and 2025 has relied on a multi-modal approach to capture facets of subjective experience, combining introspective reports with objective physiological and behavioral data.\n\n**Self-Report Inventories:** These remain a cornerstone for assessing subjective states. Prominent examples include the **Phenomenology of Consciousness Inventory (PCI)**, a comprehensive 53-item scale measuring 12 dimensions of experience like self-awareness, rationality, and emotion \\[53\\]\\[116\\]; the **Profile of Moods States (POMS)**, which quantifies eight distinct mood states \\[70\\]; and various **Visual Analogue Scales (VAS)** used to rate singular dimensions like \"high\" or \"drowsiness\" \\[70\\]\\[120\\]. Other specialized scales like the Subjective Exercise Experience Scale (SEES) and the Differentiated Emotion Scale (DES) are also employed \\[67\\]\\[59\\].\n\n**Objective and Physiological Correlates:** To ground these subjective reports, researchers heavily utilize neuroimaging and physiological measures. Functional Magnetic Resonance Imaging (fMRI) is used to track neural activity associated with subjective states like fear or reward anticipation \\[63\\]\\[66\\]. Electroencephalography (EEG) and Magnetoencephalography (MEG) measure the brain's electrical and magnetic activity, respectively \\[66\\]. Peripheral measures like Skin Conductance Response (GSR), eye-tracking, and automated Facial Action Coding (FACS) provide objective data on arousal, attention, and emotional expression \\[66\\]\\[59\\].\n\n**Behavioral and Experimental Paradigms:** Specific tasks are used to probe cognitive functions related to consciousness. These include attentional capture paradigms, dual-task paradigms, and visual search tasks that quantify how attention is allocated and influenced by emotional or motivational states \\[52\\]\\[59\\]\\[68\\].\n\n**3.2 The Current Frontier: A Chasm Between Animal Models and Human Insight**\n\nWhile the toolkit for measuring consciousness is extensive, a significant gap exists in the application of advanced manipulation techniques. As of 2025, the literature demonstrates that sophisticated chemogenetic tools like DREADDs are overwhelmingly used in **animal models**. Studies published between 2020 and 2025 show DREADDs being used to causally link specific neural circuits to working memory in non-human primates \\[202\\], anxiety in macaques \\[201\\], fear memory in mice \\[194\\], and feeding behavior in rodents \\[191\\]\\[194\\]. These studies are essential for building foundational, circuit-level models of behavior.\n\nHowever, a review of peer-reviewed publications from 2020-2025 reveals a critical absence of human studies using DREADD technology to investigate higher-order cognitive functions like metacognition—our ability to monitor our own cognitive states \\[186\\]\\[190\\]\\[190\\]. Queries for human chemogenetic studies measuring metacognitive accuracy, confidence judgments, or post-decision wagering tasks have yielded no results \\[91\\]\\[99\\]\\[141\\]. Researchers explicitly state that establishing the reliability of these tools in non-human primates is an essential, ongoing step _before_ they can be translated for clinical use in humans \\[201\\]. Therefore, the promise of using DREADDs to, for example, selectively enhance or impair metacognitive awareness in a human subject and measure the change with confidence ratings remains, for now, a future prospect rather than a current reality.\n\n**3.3 Unresolved Methodological Gaps**\n\nFurthermore, even within existing human studies using more traditional pharmacological manipulations, there is no evidence that standard measurement tools are being specifically adapted. Searches for modifications to instruments like the Phenomenology of Consciousness Inventory (PCI) or Visual Analog Scales (VAS) tailored to studies of specific neurotransmitter systems, such as serotonin or dopamine pathways, have come up empty \\[112\\]\\[120\\]\\[152\\]. This suggests that while we can manipulate neurochemistry and measure subjective experience, the field has not yet developed a practice of refining its psychometric instruments to capture the specific phenomenological signatures of targeted neurochemical changes.\n\n**4.0 Informing the Development of Machine Intelligence**\n\nThe precise, mechanistic understanding of the brain pursued by molecular psychology has created a fertile ground for dialogue with the field of artificial intelligence. The insights are not just philosophical; they are beginning to inform the very architecture of machine learning systems.\n\n**4.1 Architectural Parallels: The Brain as a Blueprint for Transformers**\n\nThe rise of the Transformer neural network architecture, first introduced in 2017's \"Attention is All You Need,\" has revealed striking parallels with biological brain function \\[31\\].\n\n**Attention Mechanisms:** The core innovation of the Transformer is its self-attention mechanism, which allows it to weigh the importance of different parts of an input sequence to process it in parallel and capture global dependencies \\[37\\]\\[41\\]. This is seen as a computational analogue to biological attention, where the brain selectively processes and integrates vast amounts of sensory information.\n\n**Memory and Learning:** The \"in-context learning\" ability of large language models, where a model can perform a new task based on examples in its prompt without explicit retraining, has been directly compared to human episodic memory \\[35\\].\n\n**Parallel Processing:** Theories of consciousness that posit the interaction of multiple, parallel modular processors find an echo in the Transformer's ability to operate on different parts of an input sequence or memory in parallel \\[40\\]\\[48\\].\n\n**Structural Homology:** Research has even found that neural representations within the hippocampus, a brain region critical for memory and spatial navigation, mirror the architecture of transformers, suggesting a deep functional similarity in how both systems represent information \\[46\\].\n\n**4.2 From Analogy to Implementation: A Nascent Integration**\n\nWhile these parallels are compelling, the crucial question is whether neurochemical insights are being directly implemented to improve AI. The evidence here is mixed, showing early promise but a significant lag in widespread application within the most advanced architectures.\n\nA key piece of positive evidence is the development of a \"neuromodulatory tuning\" method for machine learning. Inspired directly by the action of modulatory neurotransmitters, this technique involves manipulating the pre-trained weights of a network via a supplemental bias input, allowing the network to rapidly learn new tasks \\[43\\]. This is a concrete example of a neurochemical principle being computationally implemented to enhance AI flexibility.\n\nHowever, this appears to be an early-stage development rather than a widespread trend. A systematic search of literature published since 2023 reveals no concrete implementations of other well-understood neurochemical processes, such as **dopamine-based reward prediction error (RPE) algorithms**, directly within Transformer neural network architectures \\[169\\]\\[185\\]\\[218\\]. While the temporal difference learning framework, which models dopaminergic RPEs, is a cornerstone of reinforcement learning \\[169\\]\\[180\\]its specific integration into the attention-based structure of modern Transformers is not documented in the provided sources. The conversation is rich with analogies \\[83\\], but the direct, computational translation of dynamic, neurotransmitter-level processes into state-of-the-art AI systems remains a frontier.\n\n**5.0 Synthesis and Future Outlook**\n\nAs of July 2025, molecular psychology stands as a field of immense potential, defined by a powerful toolkit for causal manipulation of the brain. This has fundamentally reshaped our ability to link specific neural circuits to behavior in animal models. However, its impact on the direct study of human consciousness is constrained by a significant translational gap; the most precise tools, like DREADDs, are not yet in human use for investigating higher-order cognition, and the psychometric instruments used to measure subjective experience have not yet been refined to capture the nuances of specific neurochemical manipulations.\n\nSimultaneously, the mechanistic view of the brain championed by molecular psychology has fostered a deep and inspiring dialogue with artificial intelligence. The architectural principles of the brain, including attention and parallel processing, are mirrored in the dominant Transformer models. Yet, this dialogue is still more conceptual than concrete. While isolated examples of implementing neuromodulatory principles exist, the direct integration of dynamic neurochemical algorithms—such as dopamine RPE—into these advanced AI architectures is not yet a documented, mainstream practice.\n\nThe path forward for the next five years is therefore clear. The central challenge for understanding consciousness will be to safely and ethically bridge the translational chasm, bringing tools like chemogenetics to bear on human studies of subjective experience and developing more sensitive, tailored measurement scales. For machine intelligence, the challenge is to move beyond analogy to implementation, computationally instantiating the dynamic, adaptive, and efficient processing strategies honed by billions of years of evolution, as revealed by the ongoing work of molecular psychology. The degree to which these two challenges are met will determine how profoundly this new science can unify our understanding of intelligence, both biological and artificial.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Brain Stimulation and Imaging - NEUROLOGY-2025](https://neurology.global-summit.com/events-list/brain-stimulation-and-imaging)\n\n[2\\. The University of Tokyo Faculty of Medicine Graduate School of Medicine PROSPECTUS 2023-2024](https://www.m.u-tokyo.ac.jp/information/pamphlet/Prospectus_2023-2024e.pdf)\n\n[3\\. 2025神经科学、认知与科学教育国际会议（NCSE 2025）](https://www.allconfs.org/list_info_view.asp?id=668C3C079D4182D5E18867F9EA089AFD)\n\n[4\\. BRAIN 2025: A SCIENTIFIC VISION](https://braininitiative.nih.gov/sites/default/files/documents/brain2025_508c_2.pdf)\n\n[5\\. mp.weixin.qq.com/s?\\__biz=MzI5NDU2MDc0OA==&mid=22475104...](https://mp.weixin.qq.com/s?__biz=MzI5NDU2MDc0OA%3D%3D&mid=2247510478&idx=1&sn=b5ad6535187cbd3ac7c384aff16fac57&chksm=ed18d092a162f38e46ffa6a5ab7e4a2f14983d52755b15b01e33998666034386272840165154&scene=27)\n\n[6\\. Neurochemistry - Recent articles and discoveries](https://link.springer.com/subjects/neurochemistry)\n\n[7\\. Neurology Conference 2025 | Neurology Conference | Neu...](https://neuromuscular.neuroconferences.com/events-list/neurology)\n\n[8\\. NATIONAL ACADEMY OF SCIENTIST EDUCATION YEARBOOK 2024/25](https://www.edu-sci.org/downloads/NASE_Yearbook_2024-25.pdf)\n\n[9\\. THE BRAIN INITIATIVE AND NEUROETHICS: ENABLING AND ENHANCING NEUROSCIENCE ADVANCES FOR SOCIETY](https://acd.od.nih.gov/documents/presentations/10212019bnsroadmap.pdf)\n\n[10\\. Ritchie Chen, Gabriela Romero et al. “Wireless magnetothermal deep brain stimulation.” Science](https://doi.org/10.1126/science.1261821)\n\n[11\\. D. Urban, B. Roth. “DREADDs (designer receptors exclusively activated by designer drugs): chemogenetic tools with therapeutic utility..” Annual review of pharmacology and toxicology](https://doi.org/10.1146/annurev-pharmtox-010814-124803)\n\n[12\\. B. Roth. “DREADDs for Neuroscientists.” Neuron](https://doi.org/10.1016/j.neuron.2016.01.040)\n\n[13\\. General neurochemical techniques](https://www.amazon.com/dp/089603075X)\n\n[14\\. S. Sternson, B. Roth. “Chemogenetic tools to interrogate brain functions..” Annual review of neuroscience](https://doi.org/10.1146/annurev-neuro-071013-014048)\n\n[15\\. The Future of Neurophysiology in Neuropsychopharmacology](https://www.nature.com/articles/1395337.pdf)\n\n[16\\. A Research on the Current and Future Applications of Neuroimaging Techniques in Neuroscience Research](https://actascientific.com/ASAT/pdf/ASAT-03-0124.pdf)\n\n[17\\. 心理健康研究进展- 许培扬的博文](https://wap.sciencenet.cn/blog-280034-1479635.html)\n\n[18\\. 智能时代的脑科学与类脑智能研究](https://www.cns.org.cn/upload/file/20240530/105607_35432.pdf)\n\n[21\\. 农业农村部印发《全国乡村产业发展规划（2020-2025年）》\\[J\\].西部旅游,2020.](https://s.wanfangdata.com.cn/paper?q=%E5%86%9C%E4%B8%9A%E5%86%9C%E6%9D%91%E9%83%A8%E5%8D%B0%E5%8F%91%E3%80%8A%E5%85%A8%E5%9B%BD%E4%B9%A1%E6%9D%91%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E8%A7%84%E5%88%92%EF%BC%882020-2025%E5%B9%B4%EF%BC%89%E3%80%8B)\n\n[22\\. 梁明.农业农村部《全国乡村产业发展规划（2020-2025年）》（节选）解读\\[J\\].农村实用技术,2020.](https://s.wanfangdata.com.cn/paper?q=%E5%86%9C%E4%B8%9A%E5%86%9C%E6%9D%91%E9%83%A8%E3%80%8A%E5%85%A8%E5%9B%BD%E4%B9%A1%E6%9D%91%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E8%A7%84%E5%88%92%EF%BC%882020-2025%E5%B9%B4%EF%BC%89%E3%80%8B%EF%BC%88%E8%8A%82%E9%80%89%EF%BC%89%E8%A7%A3%E8%AF%BB)\n\n[23\\. 农业农村部印发《全国乡村产业发展规划（2020-2025年）》以产业发展促乡村全面振兴\\[J\\].农经,2020.](https://s.wanfangdata.com.cn/paper?q=%E5%86%9C%E4%B8%9A%E5%86%9C%E6%9D%91%E9%83%A8%E5%8D%B0%E5%8F%91%E3%80%8A%E5%85%A8%E5%9B%BD%E4%B9%A1%E6%9D%91%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E8%A7%84%E5%88%92%EF%BC%882020-2025%E5%B9%B4%EF%BC%89%E3%80%8B%E4%BB%A5%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E4%BF%83%E4%B9%A1%E6%9D%91%E5%85%A8%E9%9D%A2%E6%8C%AF%E5%85%B4)\n\n[24\\. 《全国乡村产业发展规划（2020-2025年）》印发\\[J\\].云南农业,2020.](https://s.wanfangdata.com.cn/paper?q=%E3%80%8A%E5%85%A8%E5%9B%BD%E4%B9%A1%E6%9D%91%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E8%A7%84%E5%88%92%EF%BC%882020-2025%E5%B9%B4%EF%BC%89%E3%80%8B%E5%8D%B0%E5%8F%91)\n\n[25\\. 沈立宏.规划引领产业振兴新画卷——农业农村部印发《全国乡村产业发展规划（2020-2025年）》\\[J\\].农村工作通讯,2020.](https://www.cqvip.com/search?k=%E8%A7%84%E5%88%92%E5%BC%95%E9%A2%86%E4%BA%A7%E4%B8%9A%E6%8C%AF%E5%85%B4%E6%96%B0%E7%94%BB%E5%8D%B7%E2%80%94%E2%80%94%E5%86%9C%E4%B8%9A%E5%86%9C%E6%9D%91%E9%83%A8%E5%8D%B0%E5%8F%91%E3%80%8A%E5%85%A8%E5%9B%BD%E4%B9%A1%E6%9D%91%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E8%A7%84%E5%88%92%EF%BC%882020-2025%E5%B9%B4%EF%BC%89%E3%80%8B)\n\n[26\\. 杨金沙.基于《法治社会建设实施纲要（2020-2025年）》谈高职大学生法治观教育\\[J\\].法制与社会,2021.](https://s.wanfangdata.com.cn/paper?q=%E5%9F%BA%E4%BA%8E%E3%80%8A%E6%B3%95%E6%B2%BB%E7%A4%BE%E4%BC%9A%E5%BB%BA%E8%AE%BE%E5%AE%9E%E6%96%BD%E7%BA%B2%E8%A6%81%EF%BC%882020-2025%E5%B9%B4%EF%BC%89%E3%80%8B%E8%B0%88%E9%AB%98%E8%81%8C%E5%A4%A7%E5%AD%A6%E7%94%9F%E6%B3%95%E6%B2%BB%E8%A7%82%E6%95%99%E8%82%B2)\n\n[27\\. 方世荣,孙思雨.论公众参与法治社会建设及其引导\\[J\\].行政法学研究,2021.](https://s.wanfangdata.com.cn/paper?q=%E8%AE%BA%E5%85%AC%E4%BC%97%E5%8F%82%E4%B8%8E%E6%B3%95%E6%B2%BB%E7%A4%BE%E4%BC%9A%E5%BB%BA%E8%AE%BE%E5%8F%8A%E5%85%B6%E5%BC%95%E5%AF%BC)\n\n[28\\. 胡建淼.在新的起点上全面推进法治中国建设——认真学习贯彻《法治中国建设规划（2020-2025年）》\\[J\\].中国律师,2021.](https://s.wanfangdata.com.cn/paper?q=%E5%9C%A8%E6%96%B0%E7%9A%84%E8%B5%B7%E7%82%B9%E4%B8%8A%E5%85%A8%E9%9D%A2%E6%8E%A8%E8%BF%9B%E6%B3%95%E6%B2%BB%E4%B8%AD%E5%9B%BD%E5%BB%BA%E8%AE%BE%E2%80%94%E2%80%94%E8%AE%A4%E7%9C%9F%E5%AD%A6%E4%B9%A0%E8%B4%AF%E5%BD%BB%E3%80%8A%E6%B3%95%E6%B2%BB%E4%B8%AD%E5%9B%BD%E5%BB%BA%E8%AE%BE%E8%A7%84%E5%88%92%EF%BC%882020-2025%E5%B9%B4%EF%BC%89%E3%80%8B)\n\n[29\\. 本刊.《法治中国建设规划（2020-2025年）》对新发展阶段调解工作提出要求\\[J\\].人民调解,2021.](https://s.wanfangdata.com.cn/paper?q=%E3%80%8A%E6%B3%95%E6%B2%BB%E4%B8%AD%E5%9B%BD%E5%BB%BA%E8%AE%BE%E8%A7%84%E5%88%92%EF%BC%882020-2025%E5%B9%B4%EF%BC%89%E3%80%8B%E5%AF%B9%E6%96%B0%E5%8F%91%E5%B1%95%E9%98%B6%E6%AE%B5%E8%B0%83%E8%A7%A3%E5%B7%A5%E4%BD%9C%E6%8F%90%E5%87%BA%E8%A6%81%E6%B1%82)\n\n[30\\. 陈益,齐凤平,李锐等.钢铁企业合规管理体系的构建与实施 附视频\\[J\\].企业改革与管理,2024.](https://s.wanfangdata.com.cn/paper?q=%E9%92%A2%E9%93%81%E4%BC%81%E4%B8%9A%E5%90%88%E8%A7%84%E7%AE%A1%E7%90%86%E4%BD%93%E7%B3%BB%E7%9A%84%E6%9E%84%E5%BB%BA%E4%B8%8E%E5%AE%9E%E6%96%BD)\n\n[31\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[32\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[33\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[34\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[35\\. Linking In-context Learning in Transformers to Human ...](https://openreview.net/forum?id=AYDBFxNon4&noteId=o0XCTj7ZSu)\n\n[36\\. The Chinese Room and Creating Consciousness: How Recent Strides in AI Technology Revitalize a Classic Debate](https://scholar.umw.edu/cgi/viewcontent.cgi?article=1652&context=student_research)\n\n[37\\. A Neural Architecture Predictor based on GNN-Enhanced Transformer](https://proceedings.mlr.press/v238/xiang24a/xiang24a.pdf)\n\n[38\\. G. Tononi, M. Boly et al. “Integrated information theory: from consciousness to its physical substrate.” Nature Reviews Neuroscience](https://doi.org/10.1038/nrn.2016.44)\n\n[39\\. Join our European research networks 2021](https://www.ffg.at/sites/default/files/downloads/Livre-COST-Action-Booklet-220401-1-1.pdf)\n\n[40\\. Systematic Generalization in Connectionist Models](https://sonar.ch/documents/326205/files/2023INF013.pdf)\n\n[41\\. Motif2Mol: Prediction of New Active Compounds Based on ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC10216779/)\n\n[42\\. Consciousness Explained by Daniel Dennett (Chapter Summaries)](https://pages.uoregon.edu/donovan/writings/Chapter%207%20summary.pdf)\n\n[43\\. T. Barton, Hao Yu et al. “Towards Low-Power Machine Learning Architectures Inspired by Brain Neuromodulatory Signalling.” Journal of Low Power Electronics and Applications](https://doi.org/10.3390/jlpea12040059)\n\n[44\\. What is a transformer model architecture and why was it ...](https://www.designgurus.io/answers/detail/what-is-a-transformer-model-architecture-and-why-was-it-a-breakthrough-for-nlp-tasks)\n\n[45\\. Consciousness in Machines: A Critical Exploration](https://www.ijmra.in/v7i12/Doc/18.pdf)\n\n[46\\. Victoria Violet Hoyle. “The Phenomenology of Machine: A Comprehensive Analysis of the Sentience of the OpenAI-o1 Model Integrating Functionalism, Consciousness Theories, Active Inference, and AI Architectures.”](https://arxiv.org/abs/2410.00033)\n\n[47\\. A Modular Architecture with Generalized Cross-Attention](https://arxiv.org/html/2501.00823v1)\n\n[48\\. Canadian Undergraduate Journal of Cognitive Science](https://cujcs.ca/wp-content/uploads/2021/12/CUJCS2020_digital.pdf)\n\n[49\\. Daniel Pouzzner. “Control of Functional Connectivity in Cerebral Cortex by Basal Ganglia Mediated Synchronization.” arXiv: Neurons and Cognition](https://arxiv.org/abs/1708.00779)\n\n[51\\. N. Baumann, R. Kaschel et al. “Striving for unwanted goals: stress-dependent discrepancies between explicit and implicit achievement motives reduce subjective well-being and increase psychosomatic symptoms..” Journal of personality and social psychology](https://doi.org/10.1037/0022-3514.89.5.781)\n\n[52\\. EndNote](https://journal.psych.ac.cn/xlkxjz/CN/article/getTxtFile.do?fileType=EndNote&id=6952)\n\n[53\\. The Instruments - Quantifying Consciousness](https://quantifyingconsciousness.com/instruments)\n\n[54\\. 第二轮修改 修改说明](https://jps.ecnu.edu.cn/CN/article/downloadArticleFile.do?attachType=PSFJ&id=11439)\n\n[55\\. 农业农村部印发《全国乡村产业发展规划（2020-2025年）》\\[J\\].新农业,2020.](https://www.cqvip.com/search?k=%E5%86%9C%E4%B8%9A%E5%86%9C%E6%9D%91%E9%83%A8%E5%8D%B0%E5%8F%91%E3%80%8A%E5%85%A8%E5%9B%BD%E4%B9%A1%E6%9D%91%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E8%A7%84%E5%88%92%EF%BC%882020-2025%E5%B9%B4%EF%BC%89%E3%80%8B)\n\n[56\\. 上海交通大学中国法与社会研究院](http://www.socio-legal.sjtu.edu.cn/wxzy/info.aspx?itemid=2656)\n\n[57\\. 《实验心理学》（课程代码： 02108）课程考试大纲](https://wjzy.fjnu.edu.cn/_upload/article/files/8d/25/ef880dba447588dfbd6a6ba8069b/41af052f-5e45-4179-b4c9-6da0d9da8333.docx)\n\n[58\\. 实验心理学](https://www.chinaooc.cn/upload/ylbkattach/20200107/20200107102147223556.pdf)\n\n[59\\. 心理学考研每日一练](https://new.qq.com/rain/a/20210721A0BKXD00)\n\n[60\\. 《全国乡村产业发展规划（2020-2025年）》印发\\[J\\].云南农业,2020.](https://s.wanfangdata.com.cn/paper?q=%E3%80%8A%E5%85%A8%E5%9B%BD%E4%B9%A1%E6%9D%91%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E8%A7%84%E5%88%92%EF%BC%882020-2025%E5%B9%B4%EF%BC%89%E3%80%8B%E5%8D%B0%E5%8F%91)\n\n[61\\. 焦宇薇.初中《道德与法治》课中法治教育现状及对策研究\\[D\\].西南大学,2021.](https://s.wanfangdata.com.cn/paper?q=%E5%88%9D%E4%B8%AD%E3%80%8A%E9%81%93%E5%BE%B7%E4%B8%8E%E6%B3%95%E6%B2%BB%E3%80%8B%E8%AF%BE%E4%B8%AD%E6%B3%95%E6%B2%BB%E6%95%99%E8%82%B2%E7%8E%B0%E7%8A%B6%E5%8F%8A%E5%AF%B9%E7%AD%96%E7%A0%94%E7%A9%B6)\n\n[62\\. Emotion und Zielverfolgung Eine kritische Auseinandersetzung mit dem Selbstregulationsmodell von Charles S. Carver und Michael F. Scheier](https://epub.ub.uni-greifswald.de/files/1280/Krohn_Dissertation_19032014.pdf)\n\n[63\\. 《这样看大脑》:会抑郁,会快乐,拥有不完美的大脑才是完整的人](https://weibo.com/ttarticle/p/show?id=2309405168430860337203)\n\n[64\\. Reality Bubble](https://everything2.com/title/Reality+bubble?showwidget=showCs1299157)\n\n[65\\. 农业农村部印发《全国乡村产业发展规划（2020-2025年）》以产业发展促乡村全面振兴\\[J\\].农经,2020.](https://s.wanfangdata.com.cn/paper?q=%E5%86%9C%E4%B8%9A%E5%86%9C%E6%9D%91%E9%83%A8%E5%8D%B0%E5%8F%91%E3%80%8A%E5%85%A8%E5%9B%BD%E4%B9%A1%E6%9D%91%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E8%A7%84%E5%88%92%EF%BC%882020-2025%E5%B9%B4%EF%BC%89%E3%80%8B%E4%BB%A5%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E4%BF%83%E4%B9%A1%E6%9D%91%E5%85%A8%E9%9D%A2%E6%8C%AF%E5%85%B4)\n\n[66\\. 神经营销学：通过神经科学了解消费者行为](https://www.affmu.com/zh-CN/neuromarketing-understanding-consumer-behavior-through-neuroscience.html)\n\n[67\\. 化学实验对学生的影响精选(九篇)](https://www.21ks.net/haowen/265108.html)\n\n[68\\. 第一部分考研真题精选_朱滢《实验心理学》（第4版）配套题库 ...](https://m.chuangshi.qq.com/read/27057014/2)\n\n[69\\. 上亿预算打水漂，广告投放越来越难怎么办？](https://zhuanlan.zhihu.com/p/35842661)\n\n[70\\. E. Theunissen, N. Hutten et al. “Neurocognition and subjective experience following acute doses of the synthetic cannabinoid JWH‐018: a phase 1, placebo‐controlled, pilot study.” British Journal of Pharmacology](https://doi.org/10.1111/bph.14066)\n\n[71\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[72\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[73\\. J. Schmidhuber. “Deep learning in neural networks: An overview.” Neural networks : the official journal of the International Neural Network Society](https://doi.org/10.1016/j.neunet.2014.09.003)\n\n[74\\. P. Angeline, G. M. Saunders et al. “An evolutionary algorithm that constructs recurrent neural networks.” IEEE transactions on neural networks](https://doi.org/10.1109/72.265960)\n\n[75\\. Optimal artificial neural network architecture design for modeling an ...](https://www.sciencedirect.com/science/article/pii/S0098135422001880)\n\n[76\\. V. W. Porto, David B. Fogel. “Clinical applications of artificial neural networks: Evolving artificial neural networks.”](https://doi.org/10.1017/CBO9780511543494.010)\n\n[77\\. Game Changers 2025](https://trendsunplugged.io/wp-content/uploads/2025/01/CB-INSIGHTS-Game-Changers-2025_CAIG.pdf)\n\n[78\\. Deep Reinforcement Learning for Long-Short Portfolio Optimization](https://arxiv.org/pdf/2012.13773)\n\n[79\\. OFA³: Automatic Selection of the Best Non-dominated Sub-networks for Ensembles](https://openreview.net/pdf/e2655b38ab3c1ff93f532669cf00af80c1d41cb4.pdf)\n\n[80\\. Conférence Nationale d'Intelligence Artificielle Année 2023](https://afia.asso.fr/wp-content/uploads/2023/12/Ouvrage_promotion_AFIA_2023_HAL.pdf)\n\n[81\\. AI in Mapping Neural Pathways for Neuroscience](https://trendsresearch.org/insight/ai-in-mapping-neural-pathways-for-neuroscience/?srsltid=AfmBOorf66-jCKvZ8FlLHaPk4eht3VA_UmbyBTkn-2c16h5gbccxAZPo)\n\n[82\\. Instrumentalization of machine learning in architectural design](https://akjournals.com/downloadpdf/view/journals/1848/aop/article-10.1556-1848.2025.00943/article-10.1556-1848.2025.00943.pdf)\n\n[83\\. 2023 ACTIVITY REPORT Project-Team MIND](https://radar.inria.fr/rapportsactivite/RA2023/mind/MIND-RA-2023.pdf)\n\n[84\\. Innovation focus in 2023 - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC10794114/#:~:text=Review%20of%20scientific%20breakthroughs%20in,editorial%20team%20%28Figure%201%29.)\n\n[85\\. THE EVOLUTION AND ARCHITECTURE OF ARTIFICIAL INTELLIGENCE: FROM STATISTICAL FOUNDATIONS TO DEEP LEARNING](https://www.irjmets.com/uploadedfiles/paper/issue_2_february_2025/68226/final/fin_irjmets1740679491.pdf)\n\n[86\\. Readings in AI 2023](https://www.zhaw.ch/storage/engineering/institute-zentren/cai/user_upload/Readings-in-AI-2023_final.pdf)\n\n[87\\. Meta-optical Front-end for Imaging and Computing](http://labs.ece.uw.edu/amlab/Thesis/Luocheng_Huang_Thesis.pdf)\n\n[88\\. Metasurfaces designed by a bidirectional deep neural network and iterative algorithm for generating quantitative field distributions](https://www.light-am.com/article/pdf/preview/LAM2022090031.pdf)\n\n[89\\. АКТУАЛЬНЫЕ ПРОБЛЕМЫ МОНУМЕНТАЛЬНОГО ИСКУССТВА](http://publish.sutd.ru/docs/content/sb_aktproblmonumisk_2023.pdf)\n\n[90\\. 2023年脑机接口领域发展态势](https://lifescience.sinh.ac.cn/webadmin/upload/20240204144601_3737_1171.pdf)\n\n[91\\. J. Flavell. “Metacognition and Cognitive Monitoring: A New Area of Cognitive-Developmental Inquiry..” American Psychologist](https://doi.org/10.1037/0003-066X.34.10.906)\n\n[92\\. Gregory Schraw, R. Dennison. “Assessing metacognitive awareness.” Contemporary Educational Psychology](https://doi.org/10.1006/CEPS.1994.1033)\n\n[93\\. Blaine N. Armbruster, Xiang Li et al. “Evolving the lock to fit the key to create a family of G protein-coupled receptors potently activated by an inert ligand.” Proceedings of the National Academy of Sciences](https://doi.org/10.1073/pnas.0700293104)\n\n[94\\. S. Deprez, F. Amant et al. “Longitudinal assessment of chemotherapy-induced structural changes in cerebral white matter and its correlation with impaired cognitive functioning..” Journal of clinical oncology : official journal of the American Society of Clinical Oncology](https://doi.org/10.1200/JCO.2011.36.8571)\n\n[95\\. Chemotherapy-Induced Cognitive Impairments: A Systematic and Updated Review](https://globalresearchonline.net/journalcontents/v63-1/40.pdf)\n\n[96\\. The GEM Research Journal](https://depedelsalvadorcity.net/wp-content/uploads/2022/01/GEM-RESEARCH-JOURNAL_2021-1.pdf)\n\n[97\\. Effects of improving metacognitive awareness on emotional regulation and concentration in high school students](https://www.syncsci.com/journal/ADEP/article/download/ADEP.2024.01.002/978)\n\n[98\\. T. L. Briones, Julie Woods. “Chemotherapy-induced cognitive impairment is associated with decreases in cell proliferation and histone modifications.” BMC Neuroscience](https://doi.org/10.1186/1471-2202-12-124)\n\n[99\\. Tavonga Tawanda, Awelani V. Mudau. “The influence of indigenous knowledge on chemistry metacognition.” F1000Research](https://doi.org/10.12688/f1000research.131685.4)\n\n[100\\. The neural correlates of metacognitive awareness](https://www.maxwell.vrac.puc-rio.br/61730/61730.PDF)\n\n[101\\. The impact of meta-cognitive skills on students learning](https://www.allresearchjournal.com/archives/2025/vol11issue2/PartD/11-2-56-269.pdf)\n\n[102\\. Prevalence of cognitive impairment following chemotherapy treatment for breast cancer: A systematic review and meta-analysis](https://www.medrxiv.org/content/10.1101/2021.08.17.21262190v1.full.pdf)\n\n[103\\. T. Wolf, M. Doherty et al. “The Feasibility of Using Metacognitive Strategy Training to Improve Cognitive Performance and Neural Connectivity in Women with Chemotherapy-Induced Cognitive Impairment.” Oncology](https://doi.org/10.1159/000447744)\n\n[111\\. M. Massimini, F. Ferrarelli et al. “Breakdown of Cortical Effective Connectivity During Sleep.” Science](https://doi.org/10.1126/SCIENCE.1117256)\n\n[112\\. A. Casali, O. Gosseries et al. “A Theoretically Based Index of Consciousness Independent of Sensory Processing and Behavior.” Science Translational Medicine](https://doi.org/10.1126/scitranslmed.3006294)\n\n[113\\. S. Casarotto, A. Comanducci et al. “Stratification of unresponsive patients by an independently validated index of brain complexity.” Annals of Neurology](https://doi.org/10.1002/ana.24779)\n\n[114\\. S. Dehaene, J. Changeux. “Experimental and Theoretical Approaches to Conscious Processing.” Neuron](https://doi.org/10.1016/j.neuron.2011.03.018)\n\n[115\\. S. Sarasso, M. Boly et al. “Consciousness and Complexity during Unresponsiveness Induced by Propofol, Xenon, and Ketamine.” Current Biology](https://doi.org/10.1016/j.cub.2015.10.014)\n\n[116\\. Hypnagogia, psychedelics, and sensory deprivation: the mythic structure of dream-like experiences](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1498677/pdf)\n\n[117\\. 个体化舒适护理在 PCI 患者中的应用效果](https://www.yyqyweb.com/public/static/index/uploads/att/20250212/ac2329fcf92258827af5134de4331a62.pdf)\n\n[118\\. Human Brain Project Summit 2023 Book of Abstracts](https://summit2023.humanbrainproject.eu/wp-content/uploads/2023/03/Book_of_Abstracts_HBP_Summit_2023.pdf)\n\n[119\\. Does Cognitive Load Affect Measures of Consciousness?](https://www.mdpi.com/2076-3425/14/9/919)\n\n[120\\. A Review of Mulligan's SNAG Technique for Managing Non-Specific Low Back Pain](https://africanjournalofbiomedicalresearch.com/index.php/AJBR/article/download/2552/2068/4617)\n\n[121\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[122\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[123\\. Sepp Hochreiter, J. Schmidhuber. “Long Short-Term Memory.” Neural Computation](https://doi.org/10.1162/neco.1997.9.8.1735)\n\n[124\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[125\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[126\\. Transformer models in biomedicine](https://pubmed.ncbi.nlm.nih.gov/39075407/)\n\n[127\\. Neural Machine Translation](https://cc.bingj.com/cache.aspx?q=%2BNeural+machine+translation+wikipedia&d=4545815045418417&mkt=en-US&setlang=en-US&w=81BQTMCjuG8TMqeOR31qTVMmLDHLG7Ri)\n\n[128\\. WMT 2023](http://www2.statmt.org/wmt23/2023.wmt-1.pdf)\n\n[129\\. Learning Time-Invariant Representations for Individual Neurons from Population Dynamics](https://proceedings.neurips.cc/paper_files/paper/2023/file/9032e5c9ec394ce768a2fa9bdc56af6c-Paper-Conference.pdf)\n\n[130\\. Noteworthy Developments in the Korean Journal of Radiology in 2023 and for 2024](https://kjronline.org/src/PDFs/kjr-v25n1.pdf)\n\n[131\\. Conformational dynamics of a nicotinic receptor neurotransmitter site](https://elifesciences.org/articles/92418.pdf)\n\n[132\\. Transformers for Neuroimage Segmentation: Scoping Review](https://www.jmir.org/2025/1/e57723/PDF)\n\n[133\\. Applied and Computational Engineering: Proceedings of the 2023 International Conference on Machine Learning and Automation](https://www.ewadirect.com/media/var/media/upload/vol_pdf/ace/37.pdf)\n\n[134\\. 2023년 ICT R&D 기술예고](https://iitp.kr/resources/file/2023%EB%85%84%20ICR%20RnD%20%EA%B8%B0%EC%88%A0%EC%98%88%EA%B3%A0.pdf)\n\n[135\\. NCM 2023 Abstract Book](https://ncm-society.org/wp-content/uploads/2023/04/NCMAbstractBook2023.pdf)\n\n[136\\. Proceedings of the Austrian Robotics Workshop 2025](https://www.fh-salzburg.ac.at/fileadmin/fhs_daten/departments/information-technologies/documents/ARW2025_Proceedings_final_kl.pdf)\n\n[137\\. 1D-CONVOLUTION NEURAL NETWORK FOR TRANSFORMER CORE FAULT IDENTIFICATION](https://romanpub.com/resources/ijaet20v5-4-2023-70.pdf)\n\n[138\\. 【源头活水】NeurIPS 2023 | 结合脉冲神经网络和Transform...](https://cloud.tencent.com.cn/developer/article/2373230)\n\n[141\\. Diagnostic performance of metagenomic next-generation ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC10797782/)\n\n[142\\. S. Fleming, H. Lau. “How to measure metacognition.” Frontiers in Human Neuroscience](https://doi.org/10.3389/fnhum.2014.00443)\n\n[143\\. Roozbeh Kiani, M. Shadlen. “Representation of Confidence Associated with a Decision by Neurons in the Parietal Cortex.” Science](https://doi.org/10.1126/science.1169405)\n\n[144\\. T. Pleskac, J. Busemeyer. “Two-stage dynamic signal detection: a theory of choice, decision time, and confidence..” Psychological review](https://doi.org/10.1037/a0019737)\n\n[145\\. R. G. E. R. Torres, M. G. Gainetdinov et al. “References and Notes Supporting Online Material Relating Introspective Accuracy to Individual Differences in Brain Structure.”](https://www.semanticscholar.org/paper/9cba385fd82cfe0491e37a07cdc8b89abf246b72)\n\n[146\\. Metacognitive Confidence Can Increase but Also Decrease Performance in Academic Settings](https://pablobrinol.com/wp-content/uploads/2021/09/Moreno2021_Article_MetacognitiveConfidenceCanIncr.pdf)\n\n[147\\. Brian Maniscalco, H. Lau. “A signal detection theoretic approach for estimating metacognitive sensitivity from confidence ratings.” Consciousness and Cognition](https://doi.org/10.1016/j.concog.2011.09.021)\n\n[148\\. ANNUAL SCIENTIFIC REPORT 2020](https://idibgi.org/wp-content/uploads/2021/11/Memoria_Idibgi_2020.pdf)\n\n[149\\. Confidence judgments interfere with perceptual decision making - Nature](https://www.nature.com/articles/s41598-024-64575-7)\n\n[150\\. 2020 Scientific Report](https://www.incliva.es/wp-content/uploads/2021/documentacion/Memoria%202020.pdf)\n\n[151\\. 2025](http://www.bbmi.zju.edu.cn/bbmi/2025/list.htm)\n\n[152\\. Chemogenetic Tools and their Use in Studies of Neuropsychiatric Disorders](https://www.biomed.cas.cz/physiolres/pdf/73/73_S449.pdf)\n\n[153\\. The Relationship between Metacognitive Ability and Metacognitive Accuracy](https://rd.springer.com/article/10.1007/s11409-020-09232-w?error=cookies_not_supported&code=18144384-cf22-41d5-82b4-53ca186865f9)\n\n[154\\. A Review of Metacognition: Implications for Teaching and Learning](https://repository.nie.edu.sg/bitstreams/6c086103-8780-44fc-b978-c0bf9c9d3a1b/download)\n\n[158\\. W. Schultz, P. Dayan et al. “A Neural Substrate of Prediction and Reward.” Science](https://doi.org/10.1126/SCIENCE.275.5306.1593)\n\n[159\\. S. Chamberlain, U. Müller et al. “Neurochemical Modulation of Response Inhibition and Probabilistic Learning in Humans.” Science](https://doi.org/10.1126/SCIENCE.1121218)\n\n[160\\. M. Crockett, L. Clark et al. “Reconciling the Role of Serotonin in Behavioral Inhibition and Aversion: Acute Tryptophan Depletion Abolishes Punishment-Induced Inhibition in Humans.” The Journal of Neuroscience](https://doi.org/10.1523/JNEUROSCI.2513-09.2009)\n\n[161\\. R. Cools, A. Roberts et al. “Serotoninergic regulation of emotional and behavioural control processes.” Trends in Cognitive Sciences](https://doi.org/10.1016/j.tics.2007.10.011)\n\n[162\\. A. Bari, D. Theobald et al. “Serotonin Modulates Sensitivity to Reward and Negative Feedback in a Probabilistic Reversal Learning Task in Rats.” Neuropsychopharmacology](https://doi.org/10.1038/npp.2009.233)\n\n[163\\. 2025 — Stanford 2025](http://www.stanford2025.com/2025)\n\n[164\\. AI/ML methodologies and the future-will they be successful ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC11974048/)\n\n[165\\. Understanding the Biology of Non-Suicidal Self-Injury: A Neurobiological Perspective on Emotion Regulation and Reward Processing](https://fse.studenttheses.ub.rug.nl/34662/1/mBMS2025DrenthE-2.pdf)\n\n[166\\. Human Brain Project Summit 2023 Book of Abstracts](https://summit2023.humanbrainproject.eu/wp-content/uploads/2023/03/Book_of_Abstracts_HBP_Summit_2023.pdf)\n\n[167\\. Researchers Make First Serotonin Measurements in Humans](https://neurosciencenews.com/serotonin-measured-humans-8921/)\n\n[168\\. W. Schultz, P. Dayan et al. “A Neural Substrate of Prediction and Reward.” Science](https://doi.org/10.1126/SCIENCE.275.5306.1593)\n\n[169\\. 2023 URI Summer Research and Innovation Symposium Book of Abstracts](https://research.njit.edu/uri/sites/research.uri/files/FINAL%20DRAFT%202023%20URI%20Symposium%20Book%20of%20Abstracts%20Update%208.24.23.pdf)\n\n[170\\. The Dopamine Prediction Error: Contributions to Associative Models of Reward Learning](https://pubmed.ncbi.nlm.nih.gov/28275359/)\n\n[171\\. Dopamine reward prediction error coding](https://pubmed.ncbi.nlm.nih.gov/27069377/)\n\n[172\\. Dopamine transients encode reward prediction errors independent of learning rates](https://www.biorxiv.org/content/10.1101/2024.04.18.590090v2.full.pdf)\n\n[173\\. Dopamine prediction error responses integrate subjecti...](https://www.ncbi.nlm.nih.gov/pubmed/24453218)\n\n[174\\. Reward prediction error neurons implement an efficient ...](https://www.nature.com/articles/s41593-024-01671-x)\n\n[175\\. An overview of reward prediction error and its links with dopamine](https://www.ewadirect.com/proceedings/tns/article/view/14289/pdf)\n\n[176\\. Neural Circuitry of Reward Prediction Error](https://pubmed.ncbi.nlm.nih.gov/28441114/?dopt=Abstract)\n\n[177\\. Building and breaking the chain: A model of reward prediction error integration and segmentation of memory](https://clewettlab.psych.ucla.edu/wp-content/uploads/sites/16/2025/04/rouhani2024_jocn_preprint.pdf)\n\n[178\\. Dopaminergic prediction errors in the ventral tegmental area reflect a multithreaded predictive model](https://irp.nida.nih.gov/hot-off-the-press-may-2023/)\n\n[179\\. A vector reward prediction error model explains dopaminergic heterogeneity](http://www.princeton.edu/~ndaw/lewd2022.pdf)\n\n[180\\. Expected reward value and reward prediction errors reinforce but also interfere with human time perception.](https://www.biorxiv.org/content/10.1101/2024.04.17.589985v1.full.pdf)\n\n[181\\. Dopamine neurons report an error in the temporal prediction of reward during learning](https://pubmed.ncbi.nlm.nih.gov/10195164/?dopt=Abstract)\n\n[182\\. P. Read, Montague et al. “A framework for mesencephalic dopamine systems based on predictive Hebbian learning.” Journal of Neuroscience](https://doi.org/10.1523/JNEUROSCI.16-05-01936.1996)\n\n[183\\. The temporal precision of reward prediction in dopamine neurons](https://pubmed.ncbi.nlm.nih.gov/18660807/?dopt=Abstract)\n\n[184\\. Ian Cone, C. Clopath et al. “Learning to express reward prediction error-like dopaminergic activity requires plastic representations of time.” Nature Communications](https://doi.org/10.1038/s41467-024-50205-3)\n\n[185\\. GPCR-Based Genetically Encoded Fluorescent Indicators – Development and Applications in Neuromodulator Research](https://www.zora.uzh.ch/id/eprint/270790/1/xuzhou-thesis.pdf)\n\n[186\\. W. Schultz, P. Apicella et al. “Responses of monkey dopamine neurons to reward and conditioned stimuli during successive steps of learning a delayed response task.” Journal of Neuroscience](https://doi.org/10.1523/JNEUROSCI.13-03-00900.1993)\n\n[188\\. Chemogenetics for cell-type-specific modulation of signalling and neuronal activity | Nature Reviews Methods Primers](https://www.nature.com/articles/s43586-023-00276-1)\n\n[189\\. A humanized Gs-coupled DREADD for circuit and behavior modulation](https://www.frontiersin.org/journals/cellular-neuroscience/articles/10.3389/fncel.2025.1577117/pdf)\n\n[190\\. Longitudinal assessment of DREADD expression and efficacy in the monkey brain](https://www.biorxiv.org/content/10.1101/2024.12.26.630299v1.full.pdf)\n\n[191\\. DREADD: A Chemogenetic GPCR Signaling Platform - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC4368861/)\n\n[192\\. Blaine N. Armbruster, Xiang Li et al. “Evolving the lock to fit the key to create a family of G protein-coupled receptors potently activated by an inert ligand.” Proceedings of the National Academy of Sciences](https://doi.org/10.1073/pnas.0700293104)\n\n[193\\. Systemic IRNA chemogenetic technology](https://www.biorxiv.org/content/10.1101/2023.09.11.557081v1.full.pdf)\n\n[194\\. Hu Zhu, B. Roth. “DREADD: A Chemogenetic GPCR Signaling Platform.” International Journal of Neuropsychopharmacology](https://doi.org/10.1093/ijnp/pyu007)\n\n[195\\. B. Roth. “DREADDs for Neuroscientists.” Neuron](https://doi.org/10.1016/j.neuron.2016.01.040)\n\n[196\\. Characterization of DREADD receptor expression and function in rhesus macaques trained to discriminate ethanol | Neuropsychopharmacology](https://www.nature.com/articles/s41386-021-01181-5)\n\n[197\\. Jean‐Marc Guettier, D. Gautam et al. “A chemical-genetic approach to study G protein regulation of β cell function in vivo.” Proceedings of the National Academy of Sciences](https://doi.org/10.1073/pnas.0906593106)\n\n[198\\. Martin R. Silic, Guangjun Zhang. “Tissue-specific modification of cellular bioelectrical activities using the chemogenetic tool, DREADD, in zebrafish.” bioRxiv](https://doi.org/10.1101/2021.06.22.449481)\n\n[199\\. Chemogenetic Technique Turns Mouse Behavior On and Off](https://neurosciencenews.com/chemogenetics-neurobehavior-dreadd-2020/)\n\n[200\\. Growing Awareness](https://www.awarenessjournals.com/public/img/pdf/awareness-jan-journal-2025.pdf)\n\n[201\\. Applications of chemogenetics in non-human primates](https://www.binasss.sa.cr/jun22/24.pdf)\n\n[202\\. A head-to-head comparison of two DREADD agonists for suppressing operant behavior in rats via VTA dopamine neuron inhibition](https://escholarship.org/content/qt9bd7w5jj/qt9bd7w5jj.pdf)\n\n[203\\. Chemogenetics - an overview | ScienceDirect Topics](https://www.sciencedirect.com/topics/biochemistry-genetics-and-molecular-biology/chemogenetics)\n\n[204\\. Juan L. Gomez, J. Bonaventura et al. “Chemogenetics revealed: DREADD occupancy and activation via converted clozapine.” Science](https://doi.org/10.1126/science.aan2475)\n\n[208\\. M. Crockett, L. Clark et al. “Reconciling the Role of Serotonin in Behavioral Inhibition and Aversion: Acute Tryptophan Depletion Abolishes Punishment-Induced Inhibition in Humans.” The Journal of Neuroscience](https://doi.org/10.1523/JNEUROSCI.2513-09.2009)\n\n[209\\. J. Scholl, Nils Kolling et al. “Beyond negative valence: 2-week administration of a serotonergic antidepressant enhances both reward and effort learning signals.” PLoS Biology](https://doi.org/10.1371/journal.pbio.2000756)\n\n[210\\. Saori C. Tanaka, K. Shishida et al. “Serotonin Affects Association of Aversive Outcomes to Past Actions.” The Journal of Neuroscience](https://doi.org/10.1523/JNEUROSCI.2799-09.2009)\n\n[211\\. J. Hornung. “The human raphe nuclei and the serotonergic system.” Journal of Chemical Neuroanatomy](https://doi.org/10.1016/j.jchemneu.2003.10.002)\n\n[212\\. K. Miyazaki, Katsuhiko Miyazaki et al. “Optogenetic Activation of Dorsal Raphe Serotonin Neurons Enhances Patience for Future Rewards.” Current Biology](https://doi.org/10.1016/j.cub.2014.07.041)\n\n[213\\. Serotonin concentration enhancers at clinically relevant doses reduce ...](https://www.nature.com/articles/s41398-018-0178-7)\n\n[214\\. Design and development of nanofilter interface for detection of small-molecule biomarker using potentiometric biosensor](https://repository.dl.itc.u-tokyo.ac.jp/record/2006358/files/A37623.pdf)\n\n[215\\. Neuron 影响因子14.700分，是几区，2023-2024年期刊投稿 ...](http://letpub.com.cn/index.php?page=journalapp&view=detail&journalid=6169)\n\n[216\\. IF 4.8，2区综述期刊，国人发文第三，审稿快，巨好投的医学SCI ...](https://www.tenuosci.com/site/detail/8499744f74644c25bd4c8f1bc7089b3b.html)\n\n[217\\. J. Bassaganya-Riera, E. Berry et al. “Goals in Nutrition Science 2020-2025.” Frontiers in Nutrition](https://doi.org/10.3389/fnut.2020.606378)\n\n[218\\. Dopamine reward prediction error coding](https://pubmed.ncbi.nlm.nih.gov/27069377/)\n\n[219\\. Dopamine prediction error responses integrate subjecti...](https://www.ncbi.nlm.nih.gov/pubmed/24453218)\n\n[220\\. The Dopamine Prediction Error: Contributions to Associative Models of Reward Learning](https://pubmed.ncbi.nlm.nih.gov/28275359/)\n\n[221\\. Building and breaking the chain: A model of reward prediction error integration and segmentation of memory](https://clewettlab.psych.ucla.edu/wp-content/uploads/sites/16/2025/04/rouhani2024_jocn_preprint.pdf)\n\n[222\\. 2023 Peer-Reviewed Publications](https://assets-global.website-files.com/61db64b3baa3181712b020d5/6632b31de8220ac3c6154476_Untitled1.pdf)\n\n[223\\. R. S. Sutton, A. Barto. “Reinforcement Learning: An Introduction.” IEEE Trans. Neural Networks](https://doi.org/10.1109/TNN.1998.712192)\n\n[224\\. 2023 URI Summer Research and Innovation Symposium Book of Abstracts](https://research.njit.edu/uri/sites/research.uri/files/FINAL%20DRAFT%202023%20URI%20Symposium%20Book%20of%20Abstracts%20Update%208.24.23.pdf)\n\n[225\\. Dopamine transients encode reward prediction errors independent of learning rates](https://www.biorxiv.org/content/10.1101/2024.04.18.590090v2.full.pdf)\n\n[226\\. Dopaminergic prediction errors in the ventral tegmental area reflect a multithreaded predictive model](https://irp.nida.nih.gov/hot-off-the-press-may-2023/)\n\n[227\\. NATURE NEUROSCIENCE](https://www.peeref.com/zh/journals/6066/nature-neuroscience)\n\n[228\\. A distributional code for value in dopamine-based reinforcement learning | Nature](https://www.nature.com/articles/s41586-019-1924-6)\n\n[229\\. Explaining dopamine through prediction errors and beyond](https://www.nature.com/articles/s41593-024-01705-4)\n\n[230\\. Neural Circuitry of Reward Prediction Error](https://pubmed.ncbi.nlm.nih.gov/28441114/?dopt=Abstract)\n\n[231\\. A vector reward prediction error model explains dopaminergic heterogeneity](http://www.princeton.edu/~ndaw/lewd2022.pdf)\n\n[232\\. Dopamine errors drive excitatory and inhibitory components of backward conditioning in an outcome-specific manner](https://sharpelab.psych.ucla.edu/wp-content/uploads/sites/185/2022/08/Seitz-et-al.-2022.-Current-Biology.pdf)\n\n[233\\. Expected reward value and reward prediction errors reinforce but also interfere with human time perception.](https://www.biorxiv.org/content/10.1101/2024.04.17.589985v1.full.pdf)\n\n[234\\. Reward signaling by dopamine neurons](https://pubmed.ncbi.nlm.nih.gov/11488395/)\n\n[235\\. An overview of reward prediction error and its links with dopamine](https://www.ewadirect.com/proceedings/tns/article/view/14289/pdf)\n\n[238\\. J. Flavell. “Metacognition and Cognitive Monitoring: A New Area of Cognitive-Developmental Inquiry..” American Psychologist](https://doi.org/10.1037/0003-066X.34.10.906)\n\n[239\\. Joint contributions of metacognition and self-beliefs to ... - Nature](https://www.nature.com/articles/s41598-021-97958-1)\n\n[240\\. Lion Schulz, S. Fleming et al. “Metacognitive Computations for Information Search: Confidence in Control.” bioRxiv](https://doi.org/10.1101/2021.03.01.433342)\n\n[241\\. SNE 2024 Abstract Proceedings](https://neuroeconomics.org/wp-content/uploads/2024/10/SNE2024_Abstract-Proceedings-1.pdf)\n\n[242\\. S. Fleming, H. Lau. “How to measure metacognition.” Frontiers in Human Neuroscience](https://doi.org/10.3389/fnhum.2014.00443)\n\n[243\\. Self-Evaluation of Decision-Making: A General Bayesian Framework for Metacognitive Computation](https://www.princeton.edu/~ndaw/fd17.pdf)\n\n[244\\. M. Wierzchoń, E. Wronka et al. “Post-Decision Wagering Affects Metacognitive Awareness of Emotional Stimuli: An Event Related Potential Study.” PLoS ONE](https://doi.org/10.1371/journal.pone.0159516)\n\n[245\\. How Do Humans Give Confidence? A Comprehensive Comparison of Process Models of Perceptual Metacognition](https://rahnevlab.gatech.edu/documents/papers/Shekhar_2023.pdf)\n\n[246\\. N. Yeung, C. Summerfield. “Metacognition in human decision-making: confidence and error monitoring.” Philosophical Transactions of the Royal Society B: Biological Sciences](https://doi.org/10.1098/rstb.2011.0416)\n\n[247\\. T. Pleskac, J. Busemeyer. “Two-stage dynamic signal detection: a theory of choice, decision time, and confidence..” Psychological review](https://doi.org/10.1037/a0019737)\n\n[248\\. 元认知中自信心对联合决策的预测作用](https://www.chinaxiv.org/user/download.htm?uuid=ae665d77dec949a185ea0675edf04a80)\n\n[249\\. Confidence in consciousness research](https://philpapers.org/archive/MICCIC-3.pdf)\n\n[250\\. The neural correlates of metacognitive awareness](https://www.maxwell.vrac.puc-rio.br/61730/61730.PDF)"
  },
  {
    "id": 24,
    "question": "How might the relationship between web standards and creative expression evolve if AI agents can automatically adapt experiences across different presentation layers (DOM, 3D, AR)?",
    "response": "**Research Report: The Symbiotic Evolution of Web Standards and Creative Expression in the Age of Adaptive AI**\n\n**Date:** July 26, 2025\n\n**Author:** Expert Researcher\n\n**1.0 Introduction: The Web's Next Metamorphosis**\n\nFor decades, the relationship between web standards and creative expression has been defined by a clear, prescriptive hierarchy. Creators—designers and developers—have worked within the constraints of standards like HTML, CSS, and JavaScript to craft experiences for a specific presentation layer, primarily the Document Object Model (DOM) rendered on a 2D screen. The World Wide Web Consortium (W3C) has provided the stable, interoperable foundation upon which this creativity is built, defining the rules of the canvas \\[50\\]\\[53\\].\n\nAs of mid-2025, we are witnessing a fundamental paradigm shift. The canvas is no longer a static, two-dimensional plane. The proliferation of immersive technologies like Augmented Reality (AR) and Virtual Reality (VR), accessible through the open web via WebXR, has expanded the creative palette to three dimensions and beyond \\[33\\]\\[34\\]\\[52\\]. Concurrently, the rise of sophisticated Artificial Intelligence (AI) agents is fundamentally altering the creative process itself. These agents promise to act as intermediaries, automatically adapting a single source of creative intent across a multitude of presentation layers—from a traditional desktop DOM to a mobile interface, and further into a fully immersive 3D or AR scene \\[252\\]\\[259\\]\\[264\\].\n\nThis report investigates the profound evolution of the relationship between web standards and creative expression in light of this new reality. It explores how AI-driven content adaptation is forcing a re-evaluation of what a \"web standard\" is and what \"creative expression\" means. We will analyze the emerging technical foundations, the critical challenge of preserving creative intent during transformation, the quantifiable impact on design workflows, and the necessary evolution of standards bodies like the W3C to shepherd this new, dynamic, and multi-layered web. The core tension is shifting from how to precisely control layout on one canvas to how to express semantic intent that an AI can interpret and manifest meaningfully on any canvas.\n\n**2.0 The Emerging Technological and Standards Ecosystem**\n\nThe vision for an AI-mediated, multi-layered web is not speculative; it is being actively built upon a foundation of existing and emerging technologies and standards. This ecosystem is characterized by a push towards immersive experiences and an increasing reliance on AI to manage complexity.\n\n**2.1 The Guiding Vision: AI-Driven Dynamic Adaptation**\n\nThe primary driver of this evolution is a widely held vision for the dynamic adaptation of content, particularly in immersive environments \\[1\\]\\[8\\]. Research from 2024 and 2025 articulated a future where AI, leveraging computer vision, large language models (LLMs), and reinforcement learning, dynamically adjusts AR content based on a deep understanding of the user and their environment \\[2\\]\\[3\\]\\[3\\]. The goal is to move beyond static overlays and create experiences that are personalized, contextually aware, and optimized for user engagement while minimizing cognitive load \\[2\\]\\[8\\]. This involves intelligent content placement, managing the distribution of information between immersive projections and traditional displays, and even generating 3D models and interactive worlds in real-time \\[32\\]\\[312\\]. This vision necessitates a web architecture where content is fluid and presentation is a function of context, managed by an AI.\n\n**2.2 The W3C's Foundational Role: Building Bridges Between Worlds**\n\nWhile the search for a single W3C specification for \"AI-mediated content transformation\" proves fruitless \\[62\\], W3C query), the W3C's Immersive Web Working Group (IWG) has been methodically laying the groundwork for this new reality since the early 2020s \\[52\\]\\[101\\]\\[105\\]. Their work on WebXR provides the essential APIs for browsers to interact with VR and AR hardware \\[34\\]\\[35\\].\n\nOf critical importance is the **WebXR DOM Overlays Module** \\[49\\]\\[103\\]\\[225\\]. This specification, which entered the First Public Working Draft stage in 2021, directly addresses the need to bridge the 2D and 3D worlds. It defines a mechanism to display interactive 2D web content—standard HTML and CSS—as a transparent layer within an immersive WebXR session \\[103\\], W3C DOM Overlays query). This is crucial for user interfaces like heads-up displays (HUDs), configuration menus, and informational panels (W3C DOM Overlays query). The existence of this module acknowledges that not all content needs to be re-rendered in complex 3D; it creates a direct, standardized link between the legacy DOM and the new immersive scene graph. It is the conduit through which an AI could synchronize content, for example, updating a 2D product description in a DOM overlay when a user interacts with its 3D representation in the AR scene.\n\nThe IWG's work extends to other vital modules that create a richer data stream for an AI to act upon, including APIs for Hand Input, Anchors for locking virtual objects to real-world locations, and Lighting Estimation for realistic rendering \\[49\\]\\[51\\]. While meeting minutes from 2023 do not yet show explicit discussions on \"AI-driven content transformation mechanisms\" \\[104\\], the creation of these foundational APIs is a necessary prerequisite. Furthermore, a March 2024 W3C report, \"AI & the Web: Understanding and managing the impact of Machine Learning models on the Web,\" signals the organization's high-level commitment to analyzing the systemic impact of AI and the role standardization must play in managing it \\[62\\].\n\n**2.3 The Rendering Stack: WebGL, Three.js, and the Scene Graph**\n\nThe actual rendering of these multi-layered experiences relies on a mature stack. WebGL provides the low-level API for 2D and 3D graphics rendering in the browser \\[64\\]\\[67\\]. Libraries like **Three.js** abstract away much of WebGL's complexity, providing a high-level, open-source API for creating and managing 3D scenes \\[122\\]\\[123\\]\\[128\\]. The core organizational principle in Three.js is the **scene graph**, a hierarchical tree structure of objects \\[194\\].\n\nCritically, Three.js includes a **CSS3DRenderer**, which translates the position, rotation, and scale of objects in the scene graph into CSS 3D transforms, allowing standard HTML elements to be integrated and manipulated within a 3D space \\[126\\]\\[243\\]\\[358\\]. This technical capability is the practical implementation that underpins the concept of transforming DOM-based layouts into 3D scenes. The challenge, therefore, is not whether it _can_ be done, but how it can be done _meaningfully_ and _automatically_ by an AI.\n\n**3.0 The Transformation Challenge: Preserving Creative Intent Across Layers**\n\nThe core technical and philosophical challenge in this new paradigm is the preservation of creative intent. When an AI agent transforms a meticulously crafted 2D responsive layout into a 3D immersive scene, how does it maintain the original design's purpose, hierarchy, emotional tone, and usability? The search results reveal both significant hurdles and promising solutions centered on semantics and constraints.\n\n**3.1 Technical Limitations of AI as a Creative Interpreter**\n\nCurrent AI agents, for all their power, face inherent limitations in understanding the nuances of human creativity. A primary concern is their reliance on existing training data, which can lead to **homogenized outputs** and a lack of true originality, essentially averaging past designs rather than inventing new ones \\[24\\]. Furthermore, AI systems struggle to comprehend complex **human emotions and cultural contexts**, which are often central to creative intent and achieving deep user resonance \\[24\\]. Their difficulty in encoding **common sense knowledge** makes it hard for them to make sound decisions in open-ended creative tasks, where the \"rules\" are implicit and context-dependent \\[21\\]. These limitations mean that a purely automated transformation risks producing a flattened, generic, or contextually inappropriate version of the original creative vision, failing to replace the nuanced thinking of a human designer \\[24\\].\n\n**3.2 From Pixels to Purpose: The Rise of Constraint-Based Conversion**\n\nThe most promising approach to overcoming these limitations lies in shifting the translation process from a literal, geometric one to a semantic, constraint-based one. Instead of the AI trying to guess the \"why\" behind a layout, the creator provides the \"why\" through declarative rules, and the AI solves for the \"how.\"\n\nThis involves representing design requirements as a set of **constraints**—rules that define the relationships between objects \\[69\\]\\[81\\]. For example, rather than specifying a button is at (x=100, y=200), a constraint would declare that the button must \"remain to the right of the main image\" and \"be accessible to the user's primary hand.\" This declarative formalism allows for flexible adaptation while maintaining correctness and intent \\[69\\]\\[77\\]\\[78\\].\n\nWe are seeing this principle being implemented in practice. The translation of 2D layout systems like CSS Flexbox and Grid into 3D space is a key battleground. While the search results show a lack of ready-made open-source converters for CSS Grid to Three.js (CSS Grid to Three.js query), the path forward is clear. Libraries like @react-three/flex have emerged that successfully bring the semantic rules of CSS Flexbox—properties like justifyContent and alignItems—to 3D scenes in React Three Fiber \\[181\\]. This allows developers to arrange 3D objects using familiar, powerful layout primitives, demonstrating that the semantics of a 2D layout system can be preserved and applied meaningfully in a 3D scene graph \\[181\\].\n\nThe next logical step, which AI is poised to take, is to automatically parse a standard CSS file, extract these semantic Flexbox or Grid rules, and apply them as constraints within a 3D environment using a constraint solver, perhaps even a genetic algorithm for complex arrangements \\[189\\]\\[189\\]\\[192\\]. The creator's role thus evolves from dictating pixels to defining relationships and priorities.\n\n**4.0 The Quantifiable Impact on Design and Creative Expression**\n\nThe shift towards AI-mediated, multi-layered web experiences is not just a technical curiosity; it is having a measurable impact on the design industry. Data from studies and reports published between 2024 and 2025 paint a picture of a profession in rapid transition, marked by significant gains in efficiency and a re-evaluation of the designer's role and satisfaction.\n\n**4.1 A Surge in Productivity and Efficiency**\n\nThe adoption of AI tools in design workflows has led to dramatic, quantifiable improvements. Organizations adopting AI-driven design systems reported a **45% average reduction in design implementation cycles** and a **62% reduction in UI development time** \\[307\\]\\[249\\]. Component reuse, a cornerstone of efficient design systems, saw increases of up to **134%** \\[249\\]\\[307\\]. For individual designers and developers, 70% reported faster prototype creation \\[306\\]. These efficiency gains free up creators from repetitive tasks, allowing them to focus on higher-level creative thinking and problem-solving \\[253\\].\n\n**4.2 Enhancing User Experience and Design Quality**\n\nThe impact extends beyond the design team to the end-user. AI-driven adaptation and personalization are demonstrably improving the quality of web experiences. Reports cite a **127% increase in user engagement metrics** and an **89% enhancement in overall user satisfaction scores** for platforms using AI-driven systems \\[249\\]\\[307\\]. First-time user task completion rates improved by as much as **76%** \\[249\\]\\[307\\]. AI's ability to analyze user data and optimize layouts has also led to a **67% reduction in error rates** and improved UI readability by 22% \\[307\\]\\[306\\]. Furthermore, AI tools are significantly improving accessibility, with a reported **25% increase in the creation of inclusive designs** and an 89% reduction in accessibility-related issues \\[306\\]\\[307\\].\n\n**4.3 The Designer's Perspective: Satisfaction and Evolving Role**\n\nFor designers themselves, the picture is largely positive, though nuanced. A 2025 \"State of the Designer\" report found that **41% of designers reported increased job satisfaction** due to AI, compared to only 15% who were less satisfied \\[367\\]. A separate survey found that **76% of UX professionals hold a positive attitude** towards AI's potential to enhance their work \\[253\\]. Studies of designers using generative AI tools found that perceived usefulness and aesthetics were significant drivers of their satisfaction \\[255\\]\\[375\\].\n\nHowever, this optimism is tempered by the recognition that AI's impact is not yet universal. The same 2025 report found that **61% of designers felt the impact of AI on their workflow was still slight or non-existent**, suggesting adoption and integration are ongoing processes \\[367\\]. Concerns remain around creative control, algorithmic bias, and privacy \\[253\\]. The consensus is that AI is a powerful assistant that excels at data-intensive and routine tasks, but that core creative and strategic design thinking remains a uniquely human domain \\[253\\]. The designer's role is shifting from a \"maker\" of artifacts to a \"conductor\" of systems, a \"curator\" of AI-generated options, and a \"guardian\" of creative and ethical intent \\[97\\].\n\n**5.0 The Future of Web Standards: From Prescription to Semantic Intent**\n\nThis ongoing revolution necessitates a corresponding evolution in the nature and purpose of web standards. The traditional model of specifying precise rendering behavior is ill-suited for a world where the final presentation is dynamically generated by an AI based on context.\n\n**5.1 The Need for a New Layer of Standardization**\n\nThe future of web standards in an AI-mediated world will likely involve a move up the abstraction ladder. Instead of focusing solely on the presentation layer, standards will need to emerge that govern the **semantic intent layer**. This means developing standardized vocabularies for creators to express:\n\n**Content Hierarchy and Priority:** What is the most important information? What is secondary?\n\n**Relational Constraints:** How should elements behave relative to each other, to the user, and to the environment?\n\n**Functional and Emotional Intent:** What is the purpose of this interface? Is it for focused work, casual discovery, or urgent notification? What is the desired emotional tone?\n\n**Adaptation Rules and Boundaries:** Under what conditions can the layout change? What transformations are permissible, and which are forbidden to preserve brand identity or usability?\n\nSuch standards would not tell a user agent _how_ to render a scene, but rather provide the AI agent with the structured, machine-readable brief it needs to make intelligent decisions. The alt attribute for images is a primitive ancestor of this idea; the future will require a far richer, more comprehensive grammar for design intent.\n\n**5.2 The W3C's Trajectory and the Path Forward**\n\nThe W3C is already on this path, even if nascently. The \"AI & the Web\" report from 2024 is a clear acknowledgment of the need for strategic thinking in this area \\[62\\]. The development of the DOM Overlays Module shows a pragmatic approach to bridging old and new paradigms \\[49\\]\\[103\\].\n\nThe logical next steps for the Immersive Web Working Group and other relevant W3C bodies would be to:\n\n1.  **Standardize Constraint Primitives:** Move beyond the specific implementations seen in libraries like @react-three/flex \\[181\\] and develop a standard set of layout and relational constraints that can be applied across DOM, WebGL, and WebXR contexts.\n2.  **Develop an \"Intent\" Vocabulary:** Begin the process of creating a standardized way to embed semantic and creative intent directly into web documents, perhaps as a new set of attributes or a dedicated metadata block.\n3.  **Define APIs for AI-Synchronization:** While the DOM Overlays module provides the bridge \\[225\\], future specifications will be needed to define the APIs that govern how an AI agent can safely and predictably synchronize content and state between these different presentation layers \\[223\\]\\[227\\].\n\nThis evolution mirrors past shifts in web development. Just as CSS separated content (HTML) from presentation, a new \"Intent\" layer will separate the core creative purpose from its ultimate, context-dependent manifestation.\n\n**6.0 Conclusion**\n\nThe relationship between web standards and creative expression is undergoing its most significant evolution since the advent of CSS. The rise of AI agents capable of adapting experiences across the 2D DOM, 3D scenes, and immersive AR environments is dissolving the concept of a single, fixed canvas. This is forcing a fundamental change in the roles of all actors.\n\nFor **creators**, the focus is shifting from direct, pixel-perfect manipulation to the high-level articulation of semantic intent, constraints, and creative goals. As validated by numerous 2024-2025 industry reports, this is leading to massive efficiency gains and a renewed focus on strategic design, though it requires new skills and a new mindset \\[249\\]\\[307\\]\\[367\\].\n\nFor **web standards**, the imperative is to evolve from prescriptive rules for a single presentation layer to a more abstract, descriptive framework that enables interoperable communication of intent to AI intermediaries. The foundational work by the W3C's Immersive Web Working Group, particularly on WebXR and DOM Overlays, has paved the way \\[49\\]\\[52\\]. The future, however, demands new standards for semantic constraints and creative intent vocabularies.\n\nThe ultimate outcome of this evolution will be a web that is more intelligent, adaptive, personal, and accessible \\[252\\]\\[259\\]. It will allow creative expression to transcend the boundaries of the device and the screen, becoming a fluid conversation between the creator's vision, the user's context, and the AI's interpretive power. The success of this new era hinges on the symbiotic development of both the creative tools that empower designers to express this intent and the open standards that ensure it can be understood and rendered faithfully, anywhere.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments](https://arxiv.org/pdf/2504.16562)\n\n[2\\. A Vision for AI-Driven Adaptation of Dynamic AR Content to ...](https://arxiv.org/abs/2504.16562)\n\n[3\\. Applied and Computational Engineering: Proceedings of the 2nd International Conference on Machine Learning and Automation](https://www.ewadirect.com/media/var/media/upload/vol_pdf/ace/102.pdf)\n\n[4\\. Geomatics and Information Science of Wuhan University](http://ch.whu.edu.cn/indexen.htm)\n\n[5\\. A Systematic Survey on the Integration of Artificial Intelligence with Augmented and Virtual Reality Technologies](https://mkscienceset.com/articles_file/851-_article1746525480.pdf)\n\n[6\\. Promises and Pitfalls of AI-Driven Content Creation in Academia](https://theacse.com/2025/slides/Hong.pptx)\n\n[7\\. Innovative Applications of Artificial Intelligence in New Media Content Creation and Dissemination](https://fhssjournal.org/index.php/ojs/article/download/57/55)\n\n[8\\. \\[论文审查\\] A Vision for AI-Driven Adaptation of Dynamic AR ...](https://www.themoonlight.io/zh/review/a-vision-for-ai-driven-adaptation-of-dynamic-ar-content-to-users-and-environments)\n\n[9\\. AI-Driven Adaptive Content Marketing: Automating Strategy Adjustments for Enhanced Consumer Engagement](https://www.ijfmr.com/papers/2024/5/27940.pdf)\n\n[10\\. Augmented Reality Applications for on-Site Interpretation of Archaeological Remains](https://easychair.org/publications/preprint/RR4X/open)\n\n[19\\. OpenAI Josh Achiam, Steven Adler et al. “GPT-4 Technical Report.”](https://arxiv.org/abs/2303.08774)\n\n[20\\. Saleema Amershi, Daniel S. Weld et al. “Guidelines for Human-AI Interaction.” Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems](https://doi.org/10.1145/3290605.3300233)\n\n[21\\. Agent AI Towards a Holistic Intelligence](https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AgentAI_p.pdf)\n\n[22\\. F. S. D. Sio, J. Hoven. “Meaningful Human Control over Autonomous Systems: A Philosophical Account.” Frontiers Robotics AI](https://doi.org/10.3389/frobt.2018.00015)\n\n[23\\. Prashik Buddhaghosh Bansod. “Distinguishing Autonomous AI Agents from Collaborative Agentic Systems: A Comprehensive Framework for Understanding Modern Intelligent Architectures.”](https://arxiv.org/abs/2506.01438)\n\n[24\\. 中华纸业](http://www.cppi.cn/skin/zazhi/20244507575.pdf)\n\n[25\\. 互联网电商 大模型加速迭代，聚焦AI应用落地](https://bigdata-s3.wmcloud.com/researchreport/2023-12/0ac40d7eaa934d0079931fad224f03d9.pdf)\n\n[26\\. Challenges in Human-Agent Communication](https://www.microsoft.com/en-us/research/uploads/prod/2024/12/HCAI_Agents.pdf)\n\n[27\\. Animation Global Magazine](https://awnchina.cn/AG2025_SCH)\n\n[29\\. Integration of AI and XR in Optimizing Human-Centered Architectural Spaces: A Scoping Review](https://www.propulsiontechjournal.com/index.php/journal/article/download/6558/4284/11273)\n\n[30\\. Comparative Analysis of the Performance of ARCore and WebXR APIs for AR Applications](https://www.diva-portal.org/smash/get/diva2:1782862/FULLTEXT02.pdf)\n\n[31\\. The Dawn of a New Era: AI and Web Development in 2023](https://makitsol.com/the-dawn-of-a-new-era-ai-and-web-development-in-2023/)\n\n[32\\. SURF Tech Trends 2023](https://www.surf.nl/files/2023-02/sf_trendrapport_v10_compressed.pdf)\n\n[33\\. Upcoming Trends in Virtual Reality World in 2023](https://rowdytech.com/technologies/upcoming-trends-in-virtual-reality/)\n\n[34\\. IT Innovationen Band 32 Januar 2024](https://www.hs-esslingen.de/fileadmin/media/Fakultaeten/it/SERVICE/IT-Innovationen/IT-Innovationen-Band32_WS2324.pdf)\n\n[35\\. Jan 20, 2023](http://www.bimant.com/blog/ar-dev-with-threejs-webxr/)\n\n[36\\. Artificial Intelligence (AI) & Extended Reality (XR)](https://www.circuitstream.com/blog/artificial-intelligence-%28ai%29-extended-reality-%28xr%29-exploring-their-limitless-potential)\n\n[37\\. A Domain-Specific Visual Modeling Language for Augmented Reality Applications Using WebXR](https://www.unifr.ch/inf/digits/en/assets/public/files/research/papers/Muff_Fill_2023_Augmented_Reality_Modeling_Language.pdf)\n\n[38\\. WebXR Device API](https://www.thoughtworks.com/en-br/radar/languages-and-frameworks/webxr-device-api)\n\n[39\\. Augmented Reality für die Visualisierung von WindparksAugmented Reality for the Visualization of Wind Farms](https://ths.rwth-aachen.de/wp-content/uploads/sites/4/Thesis_Qu.pdf)\n\n[40\\. WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences](https://keg.cs.tsinghua.edu.cn/jietang/publications/KDD23-Liu-et-al-WebGLM.pdf)\n\n[41\\. 23rd ICTer International Conference (ICTer) - 2023](https://icter.lk/downloads/ICTer2023-Book-of-Abstracts.pdf)\n\n[42\\. How AI Is Transforming Training And Development In 2025](https://groovetechnology.com/blog/software-development/ai-for-training-and-development/)\n\n[43\\. AR and VR Using the WebXR API](https://download.e-bookshelf.de/download/0014/2525/82/L-G-0014252582-0048892655.pdf)\n\n[49\\. W3C standards and drafts](https://www.w3.org/TR/?tags%5B0%5D=webapi)\n\n[50\\. Document Object Model Specification](https://www.w3.org/TR/1998/WD-DOM-19980416/)\n\n[51\\. \\*PROPOSED\\* Immersive Web Working Group Charter - World Wide Web ...](https://www.w3.org/2020/03/proposed-immersive-web-wg-charter.html)\n\n[52\\. An Open, Cross-Platform, Web-Based Metaverse Using WebXR and A-Frame](https://arxiv.org/pdf/2408.13520)\n\n[53\\. Document Object Model (DOM) Level 3 Events Specification](https://www.w3.org/TR/2003/NOTE-DOM-Level-3-Events-20031107/DOM3-Events.pdf)\n\n[54\\. XML-related Activities at the W3C](https://www.xml.com/pub/a/ws/2001/01/03/w3c.html)\n\n[55\\. Raghav Sethi, A. Plesch et al. “Integrating XR Content in X3DOM: Supporting Navigation and Custom Functions in X3D Scenes.” Proceedings of the 28th International ACM Conference on 3D Web Technology](https://doi.org/10.1145/3611314.3615918)\n\n[56\\. M. Grosz. “World Wide Web Consortium.”](https://doi.org/10.1163/EJ.9789004163300.I-1081.558)\n\n[57\\. Document Object Model (DOM) Level 2 HTML Specification](http://www.w3.org/TR/2001/WD-DOM-Level-2-HTML-20011207/DOM2-HTML.pdf)\n\n[58\\. Document Object Model (DOM) Level 2 Specification](https://www.w3.org/TR/WD-DOM-Level-2/)\n\n[59\\. Proceedings of the 13th International Conference on Applied Internet and Information Technologies AIIT 2023](http://www.tfzr.rs/aiit/files/Proceedings_AIIT2023.pdf)\n\n[60\\. Rendering Volumetric Data in WebGL with WebXR](https://users.soe.ucsc.edu/~pang/261/f21/projects/maxim/kuznetsov-volume-rendering.pdf)\n\n[61\\. Web Media API Snapshot 2023](https://cdn.cta.tech/cta/media/media/resources/standards/pdfs/cta-5000-f-final.pdf)\n\n[62\\. Reports | W3C](https://www.w3.org/reports/)\n\n[63\\. Lin Geng Foo, H. Rahmani et al. “AI-Generated Content (AIGC) for Various Data Modalities: A Survey.”](https://arxiv.org/abs/2308.14177)\n\n[64\\. Revolutionizing Occupational Health and Safety (OHS) Education: Embracing Immersive Technologies and WebXR](https://www.afjbs.com/uploads/paper/1a23ddf6cb0cbecdb8e02f8d9783be3d.pdf)\n\n[65\\. Overview of the W3C](https://www.cni.org/wp-content/uploads/2013/06/Denenberg-W3C2000Stf.pdf)\n\n[66\\. The World Wide Web Consortium Issues DOM Level 1 as a W3C Recommendation](https://www.w3.org/Press/1998/DOM-REC.html.en)\n\n[67\\. Advancements and Challenges in 360° Augmented Reality Video Streaming: A Comprehensive Review](https://www.warse.org/IJCCN/static/pdf/file/ijccn011312024.pdf)\n\n[69\\. Artificial intelligence in design '91](https://johngero.com/publications/1991/91GeroAID91.pdf)\n\n[70\\. Technical Communications of the 34th International Conference on Logic Programming](https://drops.dagstuhl.de/storage/01oasics/oasics-vol064-iclp2018/OASIcs.ICLP.2018/OASIcs.ICLP.2018.pdf)\n\n[71\\. I. Sutherland. “Sketchpad a Man-Machine Graphical Communication System.” Simulation](https://doi.org/10.1177/003754976400200514)\n\n[72\\. Открытые семантические технологии проектирования интеллектуальных систем / Open Semantic Technologies for Intelligent Systems](http://proc.ostis.net/proc/Proceedings%20OSTIS-2018.pdf)\n\n[73\\. Constraint-basierte Verarbeitung graphischen Wissens](https://publikationen.sulb.uni-saarland.de/bitstream/20.500.11880/24878/1/RR_91_35.pdf)\n\n[74\\. Automated Identification and Representation of Design Constraints](https://mediatum.ub.tum.de/doc/1730539/vr30c5iwxtfol9egymk4j2d81.2023_BA_Khromykh_opt.pdf)\n\n[75\\. Artificial Intelligence in Engineering Design VOLUME I DESIGN REPRESENTATION AND MODELS OF ROUTINE DESIGN](http://ndl.ethernet.edu.et/bitstream/123456789/36000/1/Artificial%20Intelligence%20in%20Engineering%20Design%20VOLUME%20I.pdf)\n\n[76\\. A. Borning. “The Programming Language Aspects of ThingLab, a Constraint-Oriented Simulation Laboratory.” ACM Trans. Program. Lang. Syst.](https://doi.org/10.1145/357146.357147)\n\n[77\\. Sitt Sen Chok, K. Marriott et al. “Constraint-based diagram beautification.” Proceedings 1999 IEEE Symposium on Visual Languages](https://doi.org/10.1109/VL.1999.795870)\n\n[78\\. Thomas Triebsees. “Constraint-based Model Transformation: Tracing the Preservation of Semantic Properties.” J. Softw.](https://doi.org/10.4304/JSW.2.3.19-29)\n\n[79\\. The 14th Irish Conference on Artificial Intelligence & Cognitive Science AICS 2003 Proceedings](https://publications.scss.tcd.ie/tech-reports/reports.03/TCD-CS-2003-42.pdf)\n\n[80\\. M. Stumptner, G. Friedrich et al. “Generative constraint-based configuration of large technical systems.” Artificial Intelligence for Engineering Design, Analysis and Manufacturing](https://doi.org/10.1017/S0890060498124046)\n\n[81\\. J. Cha, I. Lee et al. “A constraint-based inference system for satisfying design constraints.” KSME International Journal](https://doi.org/10.1007/BF03184441)\n\n[82\\. PCB Layout](https://f-smartcloud.com/raitek/wp-content/uploads/2021/04/Xpedition-PCB-Layout.pdf)\n\n[83\\. C. V. Wyk. “A High-Level Language for Specifying Pictures.” ACM Trans. Graph.](https://doi.org/10.1145/357299.357303)\n\n[84\\. Steven K. Feiner. “A grid-based approach to automating display layout.”](https://www.semanticscholar.org/paper/cfa7f09e585a098beba3c4bc55b04f2d74ed2c1b)\n\n[85\\. Stéphane Sanchez, O. Roux et al. “Constraint-Based 3 D-Object Layout using a Genetic Algorithm.”](https://www.semanticscholar.org/paper/86b0e2d30b2e3e632becbc0cb5bfa8b4e3156724)\n\n[86\\. S. Neurohr, A. Krr et al. “Integrating Ai and Constraint Programming Techniques for Multimedia Directory Publishing.”](https://www.semanticscholar.org/paper/6c2bfde25b9f514c0400b150c586ff2f8893619c)\n\n[89\\. Fred D. Davis. “Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information Technology.” MIS Q.](https://doi.org/10.2307/249008)\n\n[90\\. A. Bandura. “Self-efficacy: toward a unifying theory of behavioral change..” Psychological review](https://doi.org/10.1037/0033-295X.84.2.191)\n\n[91\\. R. Kline. “Principles and Practice of Structural Equation Modeling.”](https://www.semanticscholar.org/paper/f12ea2b37862738605236add280919c762aeebcd)\n\n[92\\. I. Ajzen. “The theory of planned behavior.” Organizational Behavior and Human Decision Processes](https://doi.org/10.1016/0749-5978%2891%2990020-T)\n\n[93\\. Anol Bhattacherjee. “Understanding Information Systems Continuance: An Expectation-Confirmation Model.” MIS Q.](https://doi.org/10.2307/3250921)\n\n[94\\. Catherine Meininger. “Ensuring High-Quality Viewing Experiences and the Preservation of Creative Intent.” SMPTE Motion Imaging Journal](https://doi.org/10.5594/jmi.2024/vysn2972)\n\n[95\\. Impact of AI-Generated Visual Design Features on User Acceptance: A TAM-Based Analysis](http://apjcriweb.org/content/vol11no2/5.pdf)\n\n[96\\. EC SCHOLAR NET: AI-POWERED LEARNING AND COLLABORATION PLATFORM FOR ELECTRONICS STUDENTS](https://www.irjmets.com/uploadedfiles/paper/issue_1_january_2025/66192/final/fin_irjmets1736687365.pdf)\n\n[97\\. PEAKLIFE](https://peaklife.in/wp-content/uploads/2024/07/PEAKLIFE-Digital-Edition-June-August-Issue-2024.pdf)\n\n[98\\. AI Based Requirements Validation, QualityConsistency ...](https://www.v2solutions.com/whitepapers/ai-requirements-validation-quality-consistency-guide/)\n\n[99\\. Event Updated: Immersive Web Working Group - lists.w3.org](https://lists.w3.org/Archives/Public/public-immersive-web-wg/2022Sep/0003.html)\n\n[100\\. Accessible Platform Architectures Working Group Teleconference Meeting Minutes](https://www.w3.org/2022/03/30-apa-minutes)\n\n[101\\. Immersive Web Working Group - World Wide Web Consortium (W3C)](https://www.w3.org/immersive-web/)\n\n[102\\. Lexicon for W3C Workshop on Inclusive Design for Immer...](https://www.w3.org/2019/08/inclusive-xr-workshop/lexicon.html)\n\n[103\\. \\*PROPOSED\\* Immersive Web Working Group Charter - World Wide Web ...](https://www.w3.org/2020/03/proposed-immersive-web-wg-charter.html)\n\n[104\\. Immersive Web WG/CG TPAC 2023 Day3 – 15 September 2023](https://www.w3.org/2023/09/15-immersive-web-minutes.html)\n\n[105\\. W3C Immersive Web WG and the Future of Web-based AR](https://thearea.org/event/w3c-immersive-web-wg-and-the-future-of-web-based-ar/)\n\n[106\\. ウェブに統合されたXR技術 \"Immersive Web\" の現在と今後の発展・連携の展望](https://www.ttc.or.jp/application/files/7916/0644/0072/2_seminar20201127_Keio.pdf)\n\n[107\\. W3C 发布WebXR 深度感知、命中测试、DOM 覆盖模块的首 ...](https://www.chinaw3c.org/archives/2686/)\n\n[108\\. W3C Web 中文兴趣组 · 沉浸式 Web 线上研讨会](https://www.w3.org/2021/07/chinese-ig-xr/minutes.html)\n\n[109\\. An Open, Cross-Platform, Web-Based Metaverse Using WebXR and A-Frame](https://arxiv.org/pdf/2408.13520)\n\n[110\\. AI-enhanced web development](http://www.theseus.fi/bitstream/10024/866184/2/Rantanen_Jouni.pdf)\n\n[111\\. J. Behr, Peter Eschler et al. “X3DOM: a DOM-based HTML5/X3D integration model.” International Conference on 3D Technologies for the World Wide Web](https://doi.org/10.1145/1559764.1559784)\n\n[112\\. J. Behr, Yvonne Jung et al. “A scalable architecture for the HTML5/X3D integration model X3DOM.” International Conference on 3D Technologies for the World Wide Web](https://doi.org/10.1145/1836049.1836077)\n\n[113\\. Immersive Web CG/WG F2F - Day 1 – 21 April 2022](https://www.w3.org/2022/04/21-immersive-web-minutes.html)\n\n[114\\. M. Grosz. “World Wide Web Consortium.”](https://doi.org/10.1163/EJ.9789004163300.I-1081.558)\n\n[115\\. AGENDA](https://www.daybyday.press/IMG/pdf/agenda_-_awe_usa_2024_-june_18-20_2024_in_long_beach_california.pdf)\n\n[116\\. Web API Working Group](https://www.w3.org/2006/webapi/)\n\n[117\\. Agenda of W3C Workshop on Inclusive Design for Immersi...](https://www.w3.org/2019/08/inclusive-xr-workshop/agenda.html)\n\n[118\\. Context Information Management (CIM); VR and AR for Smart Learning: Guidelines for using NGSI-LD to train personnel in Smart Industries](https://www.etsi.org/deliver/etsi_gr/CIM/001_099/052/01.01.01_60/gr_CIM052v010101p.pdf)\n\n[119\\. Stéphane Sanchez, O. Roux et al. “Constraint-Based 3 D-Object Layout using a Genetic Algorithm.”](https://www.semanticscholar.org/paper/86b0e2d30b2e3e632becbc0cb5bfa8b4e3156724)\n\n[120\\. Learning Three.js – the JavaScript 3D Library for WebGL](https://aitskadapa.ac.in/e-books/CSE/JAVA/Learning%20Three.js_%20The%20JavaScript%203D%20Library%20for%20WebGL%20-%20Second%20Edition%20%28%20PDFDrive%20%29.pdf)\n\n[121\\. Three.js - Table of Contents](https://www.tutorialspoint.com/threejs/threejs_tutorial.pdf)\n\n[122\\. Building a Web-Based User Interface for Mobile Robots](https://www.theseus.fi/bitstream/10024/820621/2/Panuma_Aki.pdf)\n\n[123\\. WEB 3D Technologies in Digital Heritage](https://www.b-i-t-online.de/heft/2019-06-fachbeitrag-durovka.pdf)\n\n[124\\. Ken Xu, James Stewart et al. “Constraint-Based Automatic Placement for Scene Composition.” Graphics Interface](https://www.semanticscholar.org/paper/7ed98df610333740adfdacfc7f536f439b787eef)\n\n[125\\. Three.js Scenes](https://www.cs.utexas.edu/~theshark/courses/cs324e/lectures/cs324e-29.pdf)\n\n[126\\. Programming 3D Applications with HTML5 and WebGL](https://it.dru.ac.th/o-bookcs/pdfs/70.pdf)\n\n[127\\. Computer Graphics](https://www.josehu.com/assets/file/computer-graphics.pdf)\n\n[128\\. Three.js Essentials](http://bzz.wallizard.com:8081/share/books/ebook.2019/threejs/a.pdf)\n\n[129\\. S. Clay, J. Wilhelms. “Put: language-based interactive manipulation of objects.” IEEE Computer Graphics and Applications](https://doi.org/10.1109/38.486678)\n\n[130\\. Learning Three.js: The JavaScript 3D Library for WebGL](https://www.doc-developpement-durable.org/file/Projets-informatiques/cours-&-manuels-informatiques/java/Learning%20Three.js-%20The%20JavaScript%203D%20Library%20for%20WebGL.pdf)\n\n[131\\. Stéphane Sanchez, O. Roux et al. “Constraint-Based 3D-Object Layout using a Genetic Algorithm.”](https://www.semanticscholar.org/paper/648d555691e781291ae7b7bc542c5ae192caab22)\n\n[132\\. Jorge Andrés Zaccaro Valverde. “Arquitectura de computación distribuida para la web 3D.”](https://doi.org/10.11144/javeriana.10554.12715)\n\n[133\\. F. Ponchio, M. Dellepiane. “Fast decompression for web-based view-dependent 3D rendering.” Proceedings of the 20th International Conference on 3D Web Technology](https://doi.org/10.1145/2775292.2775308)\n\n[139\\. Shresth Singh. “Designing AR Interfaces for Enhanced User Experience.” Journal of Advances and Scholarly Researches in Allied Education](https://doi.org/10.29070/v8dpf082)\n\n[140\\. Evaluating the User Experience of UI/UX Design in Current Web Development.](https://ijrpr.com/uploads/V5ISSUE5/IJRPR27503.pdf)\n\n[141\\. Validating VR/AR Experiences: A Guide](https://www.numberanalytics.com/blog/validating-vr-ar-experiences)\n\n[142\\. AI Tool Simplifies Interfaces for AR & VR Experiences](https://www.digitalexperience.live/ai-based-tools-simplified-arvr-design)\n\n[143\\. Advanced AI Methodologies for Enhancing User Experience in Human-Computer Interaction](https://jisem-journal.com/index.php/journal/article/download/2706/1079)\n\n[144\\. 环境设计师视角下的AR技术用户满意度研究](https://pdf.hanspub.org/Design20230300000_50898172.pdf)\n\n[145\\. Unleashing the Power of User Experience (UX) Design Trends](https://www.blackdeers.com/insights/unleashing-the-power-of-user-experience-ux-design-trends)\n\n[146\\. P. Milgram, H. Takemura et al. “Augmented reality: a class of displays on the reality-virtuality continuum.” Other Conferences](https://doi.org/10.1117/12.197321)\n\n[147\\. D. Bowman, E. Kruijff et al. “3D User Interfaces: Theory and Practice.”](https://www.semanticscholar.org/paper/a6f0b7a7fbadc9402cf327aa2fb0d9c66a316c91)\n\n[148\\. Mixed-methods User Experience Evaluation in AR/VR](https://www.web3d.org/sites/default/files/attachment/node/2326/edit/44_GerryKim_ISO-IEC-JTC1-SC24-WG9-201901-Multi-Method%20UX%20Evaluation_MAR_2_5.pdf)\n\n[149\\. User-Centred evaluation methods for Augmented and Mixed Reality](https://www.politesi.polimi.it/retrieve/c684c885-deae-4fa6-a589-6345eeff2efa/2021_07_Picardi.pdf)\n\n[150\\. Artificial Intelligence for Graphical User Interface Design](https://lnu.diva-portal.org/smash/get/diva2:1825853/FULLTEXT01.pdf)\n\n[151\\. Shuwen Zhou, Wenxuan Zheng et al. “Enhancing User Experience in VR Environments through AI-Driven Adaptive UI Design.” Journal of Artificial Intelligence General science (JAIGS) ISSN:3006-4023](https://doi.org/10.60087/jaigs.v6i1.230)\n\n[152\\. A Methodological Approach to Inclusive Digital Urban Design](https://www.mdpi.com/2413-8851/9/6/196)\n\n[153\\. How AI is changing the way we design](https://makepixelperfect.com/how-ai-is-changing-the-way-we-design/)\n\n[154\\. Artificial Intelligence (AI) for User Experience (UX) design: A systematic literature review and future research agenda](https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/3110180/AI+UX+revision+08062023.pdf?sequence=1)\n\n[155\\. Multi-View Web Interfaces in Augmented Reality](https://openresearch-repository.anu.edu.au/bitstream/1885/271412/1/Multi_View_Web_Interfaces_in_Augmented_Reality.pdf)\n\n[156\\. The Future of Web Design: How AI and User-Centric approaches are transforming UX](https://www.stoneshark.se/2023/03/03/the-future-of-web-design-how-ai-and-user-centric-approaches-are-transforming-ux/)\n\n[157\\. Ai-powered dynamic web design and layout optimization: A paradigm shift in web experiences](https://www.computersciencejournals.com/ijcai/article/116/5-2-33-564.pdf)\n\n[159\\. immersive-web/dom-overlays - GitHub](https://github.com/immersive-web/dom-overlays)\n\n[160\\. W3C 发布WebXR 深度感知、命中测试、DOM 覆盖模块的首 ...](https://www.chinaw3c.org/archives/2686/)\n\n[161\\. 沉浸式 web 社区组” - Immersive Web at W3C · GitHub](https://github.com/immersive-web)\n\n[162\\. GitHub - immersive-web/homepage: Homepage of the Immersive Web WG](https://github.com/immersive-web/homepage)\n\n[163\\. A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments](https://arxiv.org/pdf/2504.16562)\n\n[164\\. \\*PROPOSED\\* Immersive Web Working Group Charter - World Wide Web ...](https://www.w3.org/2020/03/proposed-immersive-web-wg-charter.html)\n\n[165\\. WEB XR TECHNICAL WHITEPAPER](https://f.hubspotusercontent-eu1.net/hubfs/26083943/web%20xr%20whitepaper.pdf)\n\n[166\\. WEBXR DEVICE API EN LA IMPLEMENTACIÓN DE OBJETOS DE APRENDIZAJE CON REALIDAD VIRTUAL O AUMENTADA](http://ri.uaemex.mx/bitstream/handle/20.500.11799/142303/Tesis-2.pdf?sequence=1&isAllowed=y)\n\n[167\\. HTML library for webxr - Demos and projects - Babylon.js](https://forum.babylonjs.com/t/html-library-for-webxr/49852)\n\n[168\\. J. Behr, Yvonne Jung et al. “A scalable architecture for the HTML5/X3D integration model X3DOM.” International Conference on 3D Technologies for the World Wide Web](https://doi.org/10.1145/1836049.1836077)\n\n[169\\. Environment-Aware Rendering and Interaction in Web-Based Augmented Reality](https://3dvar.com/Ferr%C3%A3o2023Environment.pdf)\n\n[170\\. W3C Web 中文兴趣组 · 沉浸式 Web 线上研讨会](https://www.w3.org/2021/07/chinese-ig-xr/minutes.html)\n\n[171\\. Accessible Platform Architectures Working Group Teleconference Meeting Minutes](https://www.w3.org/2022/03/30-apa-minutes)\n\n[172\\. Experiential Landscape Design Using the Integration of Three-Dimensional Animation Elements and Overlay Methods](https://thesai.org/Downloads/Volume16No4/Paper_48-Experiential_Landscape_Design.pdf)\n\n[173\\. This Week in Accessibility: First lawsuit filed over lack of captions in VR](https://www.sheribyrnehaber.com/this-week-in-accessibility-first-lawsuit-filed-over-lack-of-captions-in-vr/)\n\n[174\\. Toolkit for Evaluation of Head Mounted Display Image Quality](https://cdrh-rst.fda.gov/toolkit-evaluation-head-mounted-display-image-quality)\n\n[175\\. Immersive Multimedia Intelligence: AI-Driven Generation and Interaction in 360° Video, Spatial Audio, and XR Environments](https://journalstar.in/wp-content/uploads/2025/06/202561075.pdf)\n\n[176\\. EXPERIMENTAL AND EMERGING WEB APIs](https://dspace.lib.uom.gr/bitstream/2159/29071/1/KavvadiasSpyridonMsc2023.pdf)\n\n[179\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[180\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[181\\. pmndrs/react-three-flex: Flexbox for react-three-fiber - GitHub](https://github.com/pmndrs/react-three-flex)\n\n[182\\. Constraint-Based Spring-Model Algorithm for Graph Layout](https://link.springer.com/content/pdf/10.1007/bfb0021818.pdf)\n\n[183\\. Graph-to-3D: End-to-End Generation and Manipulation of 3D Scenes Using Scene Graphs](https://openaccess.thecvf.com/content/ICCV2021/papers/Dhamo_Graph-to-3D_End-to-End_Generation_and_Manipulation_of_3D_Scenes_Using_Scene_ICCV_2021_paper.pdf)\n\n[184\\. Geometry-Based Layout Generation with Hyper-Relations AMONG Objects](https://cg.cs.tsinghua.edu.cn/papers/GMOD-2021-layout.pdf)\n\n[185\\. Ben Poole, Ajay Jain et al. “DreamFusion: Text-to-3D using 2D Diffusion.” ArXiv](https://doi.org/10.48550/arXiv.2209.14988)\n\n[186\\. Constraint Based 3D Scene Construction](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=c76333caa6efec8397b01b0192740d59787e40e4)\n\n[187\\. Chuan Fang, Xiaotao Hu et al. “Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints.” ArXiv](https://doi.org/10.48550/arXiv.2310.03602)\n\n[188\\. Chen-Hsuan Lin, Jun Gao et al. “Magic3D: High-Resolution Text-to-3D Content Creation.” 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52729.2023.00037)\n\n[189\\. Stéphane Sanchez, O. Roux et al. “Constraint-Based 3 D-Object Layout using a Genetic Algorithm.”](https://www.semanticscholar.org/paper/86b0e2d30b2e3e632becbc0cb5bfa8b4e3156724)\n\n[190\\. Topology Preserving Constrained Graph Layout](https://bridges.monash.edu/articles/report/Topology_Preserving_Constrained_Graph_Layout/20365320)\n\n[191\\. Sherwin Bahmani, Jeong Joon Park et al. “CC3D: Layout-Conditioned Generation of Compositional 3D Scenes.” 2023 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV51070.2023.00659)\n\n[192\\. Ziniu Hu, Ahmet Iscen et al. “SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code.” ArXiv](https://doi.org/10.48550/arXiv.2403.01248)\n\n[193\\. Stéphane Sanchez, O. Roux et al. “Constraint-Based 3D-Object Layout using a Genetic Algorithm.”](https://www.semanticscholar.org/paper/648d555691e781291ae7b7bc542c5ae192caab22)\n\n[194\\. Présentation de three.js ENSIIE2 : Option RVIG](http://lsc.univ-evry.fr/~didier/home/lib/exe/fetch.php?media=cours:ig:threejs.pdf)\n\n[199\\. AI-Driven Design Systems: The Future of Scalable UI Frameworks](https://eajournals.org/ejcsit/wp-content/uploads/sites/21/2025/05/AI-Driven-Design-Systems.pdf)\n\n[200\\. Enhancing Customer Experience with Virtual Reality (VR) and Augmented Reality (AR) in Business](https://ijrpr.com/uploads/V6ISSUE3/IJRPR41048.pdf)\n\n[201\\. 环境设计师视角下的AR技术用户满意度研究](https://pdf.hanspub.org/Design20230300000_50898172.pdf)\n\n[202\\. How AI Is Revolutionising UI/UX Design in 2025 and Beyond](https://digipixel.sg/how-ai-is-revolutionising-ui-ux-design-in-2025-and-beyond/)\n\n[203\\. Design Trends 2025: AI Transforms the Creative Industry](https://www.letterhend.com/blog/knowledge/design-trends-2025-ai-transforms-the-creative-industry/)\n\n[204\\. State of the Designer 2025: Designer and developer workplace, collaboration, and AI trends](https://static.figma.com/uploads/1b5e45f9a06ce51faceaae0a8669ced7b3e465dc)\n\n[205\\. AI Tool Simplifies Interfaces for AR & VR Experiences](https://www.digitalexperience.live/ai-based-tools-simplified-arvr-design)\n\n[206\\. Web Development in 2025 - What to Expect](https://www-ssrl.slac.stanford.edu/shipping/sites/default/files/webform/splab-uaa/sop/web-development-in-2025.pdf)\n\n[207\\. 202X WAY AHEAD FOR UX & UI BLENDING EXPERIENCES & INTERFACES](https://zensciences.com/wp-content/uploads/2023/01/Zensciences-UXUI-202X-Trend-Report.pdf)\n\n[208\\. Immersive Marketing & Measurement: How VR, AR, And AI Are Transforming Customer Engagement Tracking](https://journal.esrgroups.org/jes/article/download/4406/3244/8000)\n\n[209\\. Exploration of AI and AR Technologies in the Character Design of \"Dream of the Red Chamber\"](http://eprints.intimal.edu.my/2003/1/ij2024_35.pdf)\n\n[210\\. Bridging Real and Virtual Worlds: Exploring the Combined Potential of AR and VR in Metaverse Evolution](https://www.jetir.org/papers/JETIR2503197.pdf)\n\n[211\\. The digital customer experience in industrial manufact...](https://www2.deloitte.com/us/en/insights/industry/manufacturing/digital-customer-experience-in-industrial-manufacturing-and-construction.html)\n\n[212\\. AO Apparel Online](https://kishcogroup.com/img/AOI%20March%202024%20-%20Final__Low.pdf)\n\n[213\\. Artificial Intelligence-Driven Digital Communication : Evaluating the Mobile-Applications and Fostering Policies for Cultural Heritage Preservation](https://ejurnal.stie-trianandra.ac.id/index.php/jupumi/article/download/3748/2958/13395)\n\n[214\\. Web Design Best Practices in 2023 (Checklist Included) - 10Web](https://10web.io/blog/web-design-best-practices-and-principles/)\n\n[215\\. Shresth Singh. “Designing AR Interfaces for Enhanced User Experience.” Journal of Advances and Scholarly Researches in Allied Education](https://doi.org/10.29070/v8dpf082)\n\n[216\\. 5 industry-leading AI use cases in customer experience](https://cdn.twimbit.com/uploads/2025/04/02202834/5-industry-leading-AI-use-cases-in-customer-experience-.pdf)\n\n[217\\. The future of the digital customer experience in industrial manufacturing and construction](https://www2.deloitte.com/content/dam/insights/articles/us187397_e-i_future-of-the-digital-customer-experience/DI_E-I-Future-of-the-digital-customer.pdf.coredownload.pdf)\n\n[219\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[220\\. Thomas Kipf, M. Welling. “Semi-Supervised Classification with Graph Convolutional Networks.” ArXiv](https://arxiv.org/abs/1609.02907)\n\n[221\\. Ranjay Krishna, Yuke Zhu et al. “Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations.” International Journal of Computer Vision](https://doi.org/10.1007/s11263-016-0981-7)\n\n[222\\. Justin Johnson, Ranjay Krishna et al. “Image retrieval using scene graphs.” 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2015.7298990)\n\n[223\\. J. Behr, Peter Eschler et al. “X3DOM: a DOM-based HTML5/X3D integration model.” International Conference on 3D Technologies for the World Wide Web](https://doi.org/10.1145/1559764.1559784)\n\n[224\\. White Paper on Rich Media Environment Technology Landscape Report](https://www.openmobilealliance.org/release/RME/V1_0-20110329-A/OMA-WP-Rich_Media_Environment-20110329-A.pdf)\n\n[225\\. W3C standards and drafts](https://www.w3.org/TR/?tags%5B0%5D=webapi)\n\n[226\\. Rendering and the WebXR frame animation callback - Web APIs | MDN](https://developer.mozilla.org/en-US/docs/Web/API/WebXR_Device_API/Rendering)\n\n[227\\. Yongjae Lee, Byounghyun Yoo. “XR collaboration beyond virtual reality: work in the real world.” J. Comput. Des. Eng.](https://doi.org/10.1093/JCDE/QWAB012)\n\n[228\\. W3C standards and drafts | W3C - World Wide Web Consortium (W3C)](https://www.w3.org/TR/?tag=webapi)\n\n[229\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[230\\. M. Heusel, Hubert Ramsauer et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41)\n\n[231\\. Building an Interactive Image Grid with Three.js](https://tympanus.net/codrops/2025/03/18/building-an-interactive-image-grid-with-three-js/)\n\n[232\\. Ben Poole, Ajay Jain et al. “DreamFusion: Text-to-3D using 2D Diffusion.” ArXiv](https://doi.org/10.48550/arXiv.2209.14988)\n\n[233\\. Chen-Hsuan Lin, Jun Gao et al. “Magic3D: High-Resolution Text-to-3D Content Creation.” 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52729.2023.00037)\n\n[234\\. Transform your Website: Create 3D Magic with CSS Grid](https://blog.openreplay.com/transform-your-website--create-3d-magic-with-css-grid/)\n\n[235\\. Ajay Jain, B. Mildenhall et al. “Zero-Shot Text-Guided Object Generation with Dream Fields.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.00094)\n\n[236\\. Three.js Essentials](https://services.math.duke.edu/courses/math_everywhere/assets/techRefs/Threejs%20Essentials.pdf)\n\n[237\\. THREE.js代码备份——线框cube、按键、鼠标控制 - 琐碎之人 - 博客园](https://www.cnblogs.com/ForRickHuan/p/5734334.html)\n\n[238\\. Learning Three.js – the JavaScript 3D Library for WebGL](https://aitskadapa.ac.in/e-books/CSE/JAVA/Learning%20Three.js_%20The%20JavaScript%203D%20Library%20for%20WebGL%20-%20Second%20Edition%20%28%20PDFDrive%20%29.pdf)\n\n[239\\. Three.js - Table of Contents](https://www.tutorialspoint.com/threejs/threejs_tutorial.pdf)\n\n[240\\. Sherwin Bahmani, Jeong Joon Park et al. “CC3D: Layout-Conditioned Generation of Compositional 3D Scenes.” 2023 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV51070.2023.00659)\n\n[241\\. ...sghall/d3-threejs: CSS 3D Transforms with D3 and TH...](https://github.com/sghall/d3-threejs)\n\n[242\\. JavaScript Creativity](https://pepa.holla.cz/wp-content/uploads/2015/11/JavaScript-Creativity.pdf)\n\n[243\\. Programming 3D Applications with HTML5 and WebGL](https://it.dru.ac.th/o-bookcs/pdfs/70.pdf)\n\n[244\\. Optimizing 3D Object Visualization on the Web | Spring...](https://link.springer.com/chapter/10.1007/978-3-319-39907-2_27)\n\n[245\\. WEB 3D Technologies in Digital Heritage](https://www.b-i-t-online.de/heft/2019-06-fachbeitrag-durovka.pdf)\n\n[246\\. Threejs 3D场景浏览器完整代码实现-CSDN博客](https://blog.csdn.net/weixin_36364707/article/details/148201401)\n\n[247\\. three.js中使用CSS3D(CSS3DRenderer)方式展现效果修改展示...](https://blog.csdn.net/fzbucuo/article/details/106175815/)\n\n[248\\. Customizable and Performant 3D Portfolio](https://diposit.ub.edu/dspace/bitstream/2445/215524/1/tfg_badia_andreu_joan.pdf)\n\n[249\\. AI-Driven Design Systems: The Future of Scalable UI Frameworks](https://eajournals.org/ejcsit/wp-content/uploads/sites/21/2025/05/AI-Driven-Design-Systems.pdf)\n\n\\[250. AI in Design Statistics for 2025: [Impact, Trends, and ...](https://seosandwitch.com/ai-design-stats/)\n\n[251\\. Exploring the Role of Artificial Intelligence in the Design Industry: Client Satisfaction through Enhancing Quality while Preserving Human Creativity](https://www.hilarispublisher.com/open-access/exploring-the-role-of-artificial-intelligence-in-the-design-industry-client-satisfaction-through-enhancing-quality-while.pdf)\n\n[252\\. 2025 Web Design Trends](https://www.etra.agency/blog/2025-web-design-trends)\n\n[253\\. Enhancing User Experience Design through Artificial Intelligence Technologies](https://www.theseus.fi/bitstream/handle/10024/893272/Tsarikova_Marina.pdf?sequence=2&isAllowed=y)\n\n[254\\. State of the Designer 2025: Designer and developer workplace, collaboration, and AI trends](https://static.figma.com/uploads/1b5e45f9a06ce51faceaae0a8669ced7b3e465dc)\n\n[255\\. 디자이너의 생성형 인공지능(AIGC) 이용 만족도와 지속사용 의도에 미치는 영향 The Impact of Designers' Satisfaction with Generative Artificial Intelligence (AIGC) Use on Their Intention to Continue Usage](https://d-research.or.kr/bbs/download2.php?bo_table=paper&wr_id=2490&no=3)\n\n[256\\. Exploring the Futuristic Landscape: Website Design Trends of 2024](https://elvirainfotech.com/exploring-the-futuristic-landscape-website-design-trends-of-2024/)\n\n[257\\. A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments](https://arxiv.org/pdf/2504.16562)\n\n[258\\. Successful Implementation of AI in the Software Development Life Cycle](https://easychair.org/publications/preprint/mMZR/open)\n\n[259\\. How AI Is Revolutionising UI/UX Design in 2025 and Beyond](https://digipixel.sg/how-ai-is-revolutionising-ui-ux-design-in-2025-and-beyond/)\n\n[260\\. Integrated AI in web development: The future of smart web applications](https://ijrpr.com/uploads/V6ISSUE5/IJRPR45220.pdf)\n\n[261\\. The Future of Web Design and AI: A Glimpse into 2025](https://www.daltoncraighead.com/blog/the-future-of-web-design-and-ai-a-glimpse-into-2024)\n\n[262\\. Web Design Trends 2024 – From AI Personalization to Immersive 3D Interfaces](https://elements.envato.com/learn/web-design-trends)\n\n[263\\. 2024年移动端AI应用场景研究报告](http://www.csiajpw.com/UserFiles/Article/file/6387296526964092975924258.pdf)\n\n[264\\. Ai-powered dynamic web design and layout optimization: A paradigm shift in web experiences](https://www.computersciencejournals.com/ijcai/article/116/5-2-33-564.pdf)\n\n[265\\. How will AI websites change the user experience in 2024?](https://www.sitesgpt.com/blog/how-will-ai-websites-change-the-user-experience-in-2024)\n\n[269\\. webxr - A-Frame](https://aframe.io/docs/1.5.0/components/webxr.html)\n\n[270\\. Ranjay Krishna, Yuke Zhu et al. “Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations.” International Journal of Computer Vision](https://doi.org/10.1007/s11263-016-0981-7)\n\n[271\\. immersive-web/dom-overlays - GitHub](https://github.com/immersive-web/dom-overlays)\n\n[272\\. J. Behr, Peter Eschler et al. “X3DOM: a DOM-based HTML5/X3D integration model.” International Conference on 3D Technologies for the World Wide Web](https://doi.org/10.1145/1559764.1559784)\n\n[273\\. W3C standards and drafts](https://www.w3.org/TR/?tags%5B0%5D=webapi)\n\n[274\\. J. Behr, Yvonne Jung et al. “A scalable architecture for the HTML5/X3D integration model X3DOM.” International Conference on 3D Technologies for the World Wide Web](https://doi.org/10.1145/1836049.1836077)\n\n[275\\. J. Behr, Yvonne Jung et al. “Dynamic and interactive aspects of X3DOM.” International Conference on 3D Technologies for the World Wide Web](https://doi.org/10.1145/2010425.2010440)\n\n[276\\. Kristian Sons, P. Slusallek. “XML3D - Interactive 3D Graphics for the Web.” Dec3D](https://www.semanticscholar.org/paper/091a2c6854368291a751aac56cd0358bc81419e9)\n\n[277\\. Webová virtuální realita: nový způsob prezentace geoprostorových dat](https://is.muni.cz/th/lp8ls/Webova_virtualni_realita_novy_zpusob_prezentace_geoprostorovych_dat_fin.pdf?studium=271501;vysl%3D54919;lang%3Dcs)\n\n[278\\. W3C standards and drafts | W3C - World Wide Web Consortium (W3C)](https://www.w3.org/TR/?tag=webapi)\n\n[279\\. X3D: Extensible 3D Graphics for Web Authors](https://x3dgraphics.com/examples/X3dForWebAuthors/Chapter01-TechnicalOverview/Chapter01Technical_Overview.pdf)\n\n[280\\. W3C完成Web脚本规范](http://www.infoworld.com/article/2665726/operating-systems/w3c-signs-off-on-web-scripting-specs.html)\n\n[281\\. Exploring Mixed Reality Level Design Workflows](https://chesterrep.openrepository.com/bitstream/handle/10034/628071/exploring%20mixed%20reality%20level%20design%20workflows%20-%20j21850%20-%20final%20submission.pdf?sequence=1&isAllowed=y)\n\n[282\\. HTML Living Standard — Last Updated 12 May 2025](https://html.spec.whatwg.org/print.pdf?ref=hackernoon.com)\n\n[283\\. TPAC/2015 - W3C Wiki](https://www.w3.org/wiki/TPAC/2015)\n\n[284\\. WebXR 在 3D 引擎的实践](https://www.w3.org/2024/01/webevolve-series-events/media/slides/xu-qianwei.pdf)\n\n[285\\. Thomas M. J. Fruchterman, E. Reingold. “Graph drawing by force‐directed placement.” Software: Practice and Experience](https://doi.org/10.1002/spe.4380211102)\n\n[286\\. M. Kazhdan, Matthew Bolitho et al. “Poisson surface reconstruction.” Eurographics Symposium on Geometry Processing](https://doi.org/10.2312/SGP/SGP06/061-070)\n\n[287\\. I. Sutherland. “Sketchpad a Man-Machine Graphical Communication System.” Simulation](https://doi.org/10.1177/003754976400200514)\n\n[288\\. Kazuo Misue, P. Eades et al. “Layout Adjustment and the Mental Map.” J. Vis. Lang. Comput.](https://doi.org/10.1006/jvlc.1995.1010)\n\n[289\\. WEB 3D Technologies in Digital Heritage](https://www.b-i-t-online.de/heft/2019-06-fachbeitrag-durovka.pdf)\n\n[290\\. WEBGL AND THREE.JS 2D/ 3D graphic rendering Javascript API](https://informaticsweb.nic.in/sites/default/files/36_37_tup_webgl.pdf)\n\n[291\\. Greg J. Badros, Jojada J. Tirtowidjojo et al. “A constraint extension to scalable vector graphics.” The Web Conference](https://doi.org/10.1145/371920.372146)\n\n[292\\. Thomas Triebsees. “Constraint-based Model Transformation: Tracing the Preservation of Semantic Properties.” J. Softw.](https://doi.org/10.4304/JSW.2.3.19-29)\n\n[293\\. Learning Three.js – the JavaScript 3D Library for WebGL](https://aitskadapa.ac.in/e-books/CSE/JAVA/Learning%20Three.js_%20The%20JavaScript%203D%20Library%20for%20WebGL%20-%20Second%20Edition%20%28%20PDFDrive%20%29.pdf)\n\n[294\\. three · GitHub Topics · GitHub](https://github.com/topics/three?l=javascript)\n\n[295\\. Using 3D functionality available in current web-browsers to create and visualize geological models.](https://bora.uib.no/bora-xmlui/bitstream/handle/1956/16264/Master-Thesis-Final.pdf?sequence=2&isAllowed=y)\n\n[296\\. Three.js FRAMEWORK 3D](https://perso.univ-lyon1.fr/lionel.medini/enseignement/M1IF13/Exposes/2013/ThreeJS.pdf)\n\n[297\\. 3D Web Graphics with Web.GL and Three.js](https://www.damon4.com/2016/01/hello.html)\n\n[298\\. Introduction to Computer Graphics](https://math.hws.edu/eck/cs424/downloads/graphicsbook-linked.pdf)\n\n[299\\. Transform your Website: Create 3D Magic with CSS Grid](https://blog.openreplay.com/transform-your-website--create-3d-magic-with-css-grid/)\n\n[300\\. ...sghall/d3-threejs: CSS 3D Transforms with D3 and TH...](https://github.com/sghall/d3-threejs)\n\n[301\\. The Three.JS Primer - Free Access - Course.is](https://course.is/course/the-three-js-primer/)\n\n[302\\. Learning Three.js: The JavaScript 3D Library for WebGL](https://www.doc-developpement-durable.org/file/Projets-informatiques/cours-&-manuels-informatiques/java/Learning%20Three.js-%20The%20JavaScript%203D%20Library%20for%20WebGL.pdf)\n\n[305\\. Fred D. Davis. “Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information Technology.” MIS Q.](https://doi.org/10.2307/249008)\n\n\\[306. AI in Design Statistics for 2025: [Impact, Trends, and ...](https://seosandwitch.com/ai-design-stats/)\n\n[307\\. AI-Driven Design Systems: The Future of Scalable UI Frameworks](https://eajournals.org/ejcsit/wp-content/uploads/sites/21/2025/05/AI-Driven-Design-Systems.pdf)\n\n[308\\. The role of AI as an unconventional salesperson in consumer buying decisions, satisfaction and happiness](https://www.elsevier.es/index.php?p=revista&pRevista=pdf-simple&pii=S2444883425000105&r=489)\n\n[309\\. 2024年移动端AI应用场景研究报告](http://www.csiajpw.com/UserFiles/Article/file/6387296526964092975924258.pdf)\n\n[310\\. 2025 Web Design Trends](https://www.etra.agency/blog/2025-web-design-trends)\n\n[311\\. How AI Is Revolutionising UI/UX Design in 2025 and Beyond](https://digipixel.sg/how-ai-is-revolutionising-ui-ux-design-in-2025-and-beyond/)\n\n[312\\. A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments](https://arxiv.org/pdf/2504.16562)\n\n[313\\. How AI is Revolutionizing Web Design in 2025: Key Trends and Innovations](https://www.techleagues.com/ai-revolutionizing-web-design-2025/)\n\n[314\\. The Future of Web Design and AI: A Glimpse into 2025](https://www.daltoncraighead.com/blog/the-future-of-web-design-and-ai-a-glimpse-into-2024)\n\n[315\\. ARTIFICIAL INTELLIGENCE (AI) ADOPTION: AN EXTENDED COMPENSATORY LEVEL OF ACCEPTANCE](http://www.jecr.org/sites/default/files/2023vol24no1_Paper6.pdf)\n\n[316\\. The Impact of Artificial Intelligence on Commercial Interior Design: Enhancing Functionality, Aesthetics, and User Experience](https://researchtrendsjournal.com/counter/d/Sp-2-6-1/Sp-2-6-1.1.pdf)\n\n[317\\. Web Design Trends 2024 – From AI Personalization to Immersive 3D Interfaces](https://elements.envato.com/learn/web-design-trends)\n\n[318\\. 디자이너의 생성형 인공지능(AIGC) 이용 만족도와 지속사용 의도에 미치는 영향 The Impact of Designers' Satisfaction with Generative Artificial Intelligence (AIGC) Use on Their Intention to Continue Usage](https://d-research.or.kr/bbs/download2.php?bo_table=paper&wr_id=2490&no=3)\n\n[319\\. Exploring the Role of Artificial Intelligence in the Design Industry: Client Satisfaction through Enhancing Quality while Preserving Human Creativity](https://www.hilarispublisher.com/open-access/exploring-the-role-of-artificial-intelligence-in-the-design-industry-client-satisfaction-through-enhancing-quality-while.pdf)\n\n[320\\. 2024年用户界面设计趋势的重大转变](https://www.sitesgpt.com/blog/will-there-be-a-major-shift-in-user-interface-design-trends-in-2024)\n\n[321\\. Enhancing User Engagement through Adaptive UI/UX Design: A Study on Personalized Mobile App Interfaces](https://pdfs.semanticscholar.org/0d32/6b2a0acbff89ca48298767503f95433ceecd.pdf)\n\n[325\\. immersive-web/dom-overlays - GitHub](https://github.com/immersive-web/dom-overlays)\n\n[326\\. GitHub - immersive-web/homepage: Homepage of the Immersive Web WG](https://github.com/immersive-web/homepage)\n\n[327\\. \\*PROPOSED\\* Immersive Web Working Group Charter - World Wide Web ...](https://www.w3.org/2020/03/proposed-immersive-web-wg-charter.html)\n\n[328\\. WebXR Device API Working Draft Published](https://www.uploadvr.com/webxr-device-api-working-draft-published/)\n\n[329\\. Beyond Compliance 2023: Forum on Digital Ethics in Research](https://ercim-news.ercim.eu/images/stories/EN137/EN137-web.pdf)\n\n[330\\. Webová virtuální realita: nový způsob prezentace geoprostorových dat](https://is.muni.cz/th/lp8ls/Webova_virtualni_realita_novy_zpusob_prezentace_geoprostorovych_dat_fin.pdf?studium=271501;vysl%3D54919;lang%3Dcs)\n\n[331\\. An Open, Cross-Platform, Web-Based Metaverse Using WebXR and A-Frame](https://arxiv.org/pdf/2408.13520)\n\n[332\\. 3D-Visualization of Utility Lines in the Browser Using Augmented Reality on Tablets](https://eprints.ost.ch/1188/1/HS%202023%202024-SA-EP-Domeisen-Habegger-Visualisierung%20von%20Werkleitungen%20mit%20AR%20auf%20Web-Tablets.pdf)\n\n[333\\. W3C standards and drafts](https://www.w3.org/TR/?tags%5B0%5D=webapi)\n\n[334\\. Lexicon for W3C Workshop on Inclusive Design for Immer...](https://www.w3.org/2019/08/inclusive-xr-workshop/lexicon.html)\n\n[335\\. 标准组万维网联盟W3C发布首个WebXR API草案 - 映维网资讯](https://news.nweon.com/56479)\n\n[336\\. W3C Immersive Web WG and the Future of Web-based AR](https://thearea.org/event/w3c-immersive-web-wg-and-the-future-of-web-based-ar/)\n\n[337\\. Immersive Web WG/CG TPAC 2023 Day3 – 15 September 2023](https://www.w3.org/2023/09/15-immersive-web-minutes.html)\n\n[338\\. New public working draft of the W3C Web Authentication Specification](https://www.w3.org/blog/2016/new-public-working-draft-of-the-w3c-web-authentication-specification/)\n\n[339\\. J. Behr, Peter Eschler et al. “X3DOM: a DOM-based HTML5/X3D integration model.” International Conference on 3D Technologies for the World Wide Web](https://doi.org/10.1145/1559764.1559784)\n\n[340\\. WebXR Standards and Accessibility Architecture Issues](https://www.w3.org/WAI/APA/wiki/WebXR_Standards_and_Accessibility_Architecture_Issues)\n\n[341\\. Draft Specification of Streams API](http://www.i-programmer.info/news/87-web-development/5483-draft-specification-of-streams-api.html)\n\n[342\\. EXPERIMENTAL AND EMERGING WEB APIs](https://dspace.lib.uom.gr/bitstream/2159/29071/1/KavvadiasSpyridonMsc2023.pdf)\n\n[343\\. Web Real-Time Communications Working Group Charter - World Wide Web ...](https://www.w3.org/2024/08/webrtc-charter.html)\n\n[344\\. J. Behr, Yvonne Jung et al. “Dynamic and interactive aspects of X3DOM.” International Conference on 3D Technologies for the World Wide Web](https://doi.org/10.1145/2010425.2010440)\n\n[345\\. ...sghall/d3-threejs: CSS 3D Transforms with D3 and TH...](https://github.com/sghall/d3-threejs)\n\n[346\\. GitHub | Tresjs/tres: Declarative ThreeJS using Vue Components](https://github.com/Tresjs/tres)\n\n[347\\. GitHub - Tresjs/tres: Declarative ThreeJS using Vue Co...](https://github.com/tresjs/tres)\n\n[348\\. Github Repository 可视化(D3.js & Three.js) - 阿里云开发者社区](https://developer.aliyun.com/article/653652)\n\n[349\\. Georgios Gousios. “The GHTorent dataset and tool suite.” 2013 10th Working Conference on Mining Software Repositories (MSR)](https://doi.org/10.1109/MSR.2013.6624034)\n\n[350\\. J. Behr, Peter Eschler et al. “X3DOM: a DOM-based HTML5/X3D integration model.” International Conference on 3D Technologies for the World Wide Web](https://doi.org/10.1145/1559764.1559784)\n\n[351\\. Tégawendé F. Bissyandé, D. Lo et al. “Got issues? Who cares about it? A large scale investigation of issue trackers from GitHub.” 2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE)](https://doi.org/10.1109/ISSRE.2013.6698918)\n\n[352\\. Github Repository 可视化 (D3.js & Three.js)-CSDN博客](https://blog.csdn.net/weixin_33721427/article/details/88005799)\n\n[353\\. GitHub - sarme/three.js: JavaScript 3D library.](https://github.com/sarme/three.js/)\n\n[354\\. 3DHOP: 3D Heritage Online Presenter](https://iris.unimore.it/retrieve/handle/11380/1170215/337950/3DHOP%203D%20Heritage%20Online%20Presenter.pdf)\n\n[355\\. GitHub - bell-one/GaussianSplats3D: Three.js-based...](https://github.com/bell-one/GaussianSplats3D)\n\n[356\\. Kristian Sons, P. Slusallek. “XML3D - Interactive 3D Graphics for the Web.” Dec3D](https://www.semanticscholar.org/paper/091a2c6854368291a751aac56cd0358bc81419e9)\n\n[357\\. 基于 threejs 的 3D 展厅 - GitHub](https://github.com/winjune/vr-hall/blob/main/README.md)\n\n[358\\. Getting started with the CSS 3D renderer - Three.js Cookbook \\[Book\\]](https://www.oreilly.com/library/view/threejs-cookbook/9781783981182/ch01s04.html)\n\n[359\\. lathoub/three.cs: C# 3D library, based on Javascript three.js - GitHub](https://github.com/lathoub/three.cs)\n\n[360\\. J. Behr, Yvonne Jung et al. “Dynamic and interactive aspects of X3DOM.” International Conference on 3D Technologies for the World Wide Web](https://doi.org/10.1145/2010425.2010440)\n\n[361\\. Declarative 3D for the Web Architecture](https://web3d2012.web3d.org/presentations/workshops/Web3D2012-Declarative_3D_Workshop_2012.ppt)\n\n[362\\. GitHub - mtsee/vr-hall: three.js 3D vr hall](https://github.com/mtsee/vr-hall)\n\n[363\\. Three.js - Table of Contents](https://www.tutorialspoint.com/threejs/threejs_tutorial.pdf)\n\n[364\\. CGLIB—a constraint-based graphics library](https://www.sci.brooklyn.cuny.edu/~zhou/papers/cglib_spe.pdf)\n\n\\[365. AI in Design Statistics for 2025: [Impact, Trends, and ...](https://seosandwitch.com/ai-design-stats/)\n\n[366\\. The role of AI as an unconventional salesperson in consumer buying decisions, satisfaction and happiness](https://www.elsevier.es/index.php?p=revista&pRevista=pdf-simple&pii=S2444883425000105&r=489)\n\n[367\\. State of the Designer 2025: Designer and developer workplace, collaboration, and AI trends](https://static.figma.com/uploads/1b5e45f9a06ce51faceaae0a8669ced7b3e465dc)\n\n[368\\. AI-Driven Design Systems: The Future of Scalable UI Frameworks](https://eajournals.org/ejcsit/wp-content/uploads/sites/21/2025/05/AI-Driven-Design-Systems.pdf)\n\n[369\\. The Impact of Artificial Intelligence on Commercial Interior Design: Enhancing Functionality, Aesthetics, and User Experience](https://researchtrendsjournal.com/counter/d/Sp-2-6-1/Sp-2-6-1.1.pdf)\n\n[370\\. 2024年移动端AI应用场景研究报告](http://www.csiajpw.com/UserFiles/Article/file/6387296526964092975924258.pdf)\n\n[371\\. Impact of AI-Generated Visual Design Features on User Acceptance: A TAM-Based Analysis](http://apjcriweb.org/content/vol11no2/5.pdf)\n\n[372\\. AI-powered personalization: How machine learning is shaping the future of user experience](https://ijsra.net/sites/default/files/IJSRA-2024-0961.pdf)\n\n[373\\. 2024年网站构建用户体验的预期增强](https://www.sitesgpt.com/blog/what-enhancements-to-the-user-experience-should-i-expect-in-website-building-by-2024)\n\n[374\\. 2024年用户界面设计趋势的重大转变](https://www.sitesgpt.com/blog/will-there-be-a-major-shift-in-user-interface-design-trends-in-2024)\n\n[375\\. 디자이너의 생성형 인공지능(AIGC) 이용 만족도와 지속사용 의도에 미치는 영향 The Impact of Designers' Satisfaction with Generative Artificial Intelligence (AIGC) Use on Their Intention to Continue Usage](https://d-research.or.kr/bbs/download2.php?bo_table=paper&wr_id=2490&no=3)\n\n[376\\. Тренды 2024-2025: Российская реальность vs зарубежная](https://adindex.ru/publication/analitics/search/330427/img/E-Promo_Group_Trends_2024%E2%80%932025_Report.pdf)\n\n[377\\. Artificial Intelligence Index Report 2024](https://40006059.fs1.hubspotusercontent-na1.net/hubfs/40006059/Stanford_HAI_2024_AI-Index-Report.pdf)\n\n[378\\. Adaptive AI: Explore the Use Cases, Examples, and Others - Webisoft](https://webisoft.com/articles/adaptive-ai/#:~:text=fresh%20investment%20advice.-,Autonomous%20Vehicles,constantly%20improving%20how%20they%20drive.)\n\n[379\\. 2025 Web Design Trends](https://www.etra.agency/blog/2025-web-design-trends)\n\n[380\\. ARTIFICIAL INTELLIGENCE (AI) ADOPTION: AN EXTENDED COMPENSATORY LEVEL OF ACCEPTANCE](http://www.jecr.org/sites/default/files/2023vol24no1_Paper6.pdf)\n\n[381\\. Enhancing User Engagement through Adaptive UI/UX Design: A Study on Personalized Mobile App Interfaces](https://pdfs.semanticscholar.org/0d32/6b2a0acbff89ca48298767503f95433ceecd.pdf)"
  },
  {
    "id": 25,
    "question": "Could reinforcement learning techniques developed for large models be effectively applied to smaller models, or does distillation from larger systems remain superior?",
    "response": "**Research Report**\n\n**To:** Interested Parties\n\n**From:** Expert Researcher\n\n**Date:** July 26, 2025\n\n**Subject:** An Analysis of Reinforcement Learning versus Knowledge Distillation for Small Language Model Optimization\n\n**1.0 Executive Summary**\n\nThis report provides a comprehensive analysis of the comparative efficacy of two prominent techniques for optimizing small language models (SLMs) with fewer than 1 billion parameters: the direct application of reinforcement learning (RL) and knowledge distillation (KD) from larger teacher models. The central research question is whether RL techniques, which have proven highly effective for state-of-the-art large models, can be successfully downscaled to smaller models, or if distillation remains the superior method for imbuing SLMs with advanced capabilities.\n\nOur findings, based on an extensive review of recent benchmarks and studies, indicate that as of mid-2025, **knowledge distillation is generally the more effective and reliable pathway for developing high-performing, efficient SLMs, particularly for complex reasoning tasks.** While direct RL fine-tuning can improve performance, it often struggles to impart the nuanced, deep reasoning abilities that smaller models can inherit from larger counterparts via distillation.\n\nHowever, the field is rapidly evolving beyond this binary choice. The most promising advancements lie in **hybrid methodologies** that synergistically combine RL and KD. These approaches use RL to create a powerful teacher policy or generate high-quality training data, which is then efficiently compressed into a small student model through distillation. This suggests the future of SLM development is not a contest between RL and KD, but a sophisticated integration of both.\n\n**2.0 Introduction: The Challenge of Efficient AI**\n\nThe proliferation of large language models (LLMs) has revolutionized artificial intelligence. However, their immense computational and memory requirements present significant barriers to widespread deployment, especially in resource-constrained environments like edge devices. This has fueled intensive research into creating smaller, more efficient models that retain a substantial degree of their larger predecessors' capabilities. This report investigates the two leading strategies for achieving this goal: applying reinforcement learning directly to small models versus distilling knowledge from larger systems. We will evaluate these methods based on performance benchmarks, architectural viability, and deployment efficiency.\n\n**3.0 Comparative Performance on Reasoning Benchmarks**\n\nA key measure of a language model's utility is its performance on complex reasoning tasks, which are traditionally the domain of larger models. Benchmarks like MATH, GPQA, and AIME serve as critical proving grounds. Analysis of recent results reveals a clear trend where distillation-based approaches confer a significant advantage to SLMs.\n\n**3.1 The Superiority of Distillation for Reasoning Capability**\n\nMultiple studies demonstrate that SLMs created through knowledge distillation consistently outperform baseline models trained solely with reinforcement learning on challenging reasoning tasks \\[13\\]\\[66\\]. The underlying reason is that distillation allows a small student model to inherit not just the correct answers, but the high-quality reasoning _patterns_ and strategies developed by a much larger and more capable teacher model \\[51\\]\\[107\\]. Direct RL training on an SLM, starting from its more limited knowledge base, struggles to independently develop this depth of reasoning \\[51\\]\\[111\\].\n\nFor instance, experiments with DeepSeek-R1 distilled models show substantial performance gains over baseline models trained with RL alone \\[13\\]\\[58\\]. A notable study involving Qwen-family models (1.5B and 3B parameters) on the MATH 500 dataset found that a distilled model achieved 68% accuracy, a significant jump from the base model's 60% \\[64\\]\\[125\\]\\[182\\]. This demonstrates KD's power to introduce new knowledge and capabilities not originally present in the student \\[64\\]\\[64\\].\n\n**3.2 Distinguishing Accuracy from Capability**\n\nA crucial nuance in the RL vs. KD debate is the distinction between improving \"accuracy\" and enhancing \"capability.\" Research indicates that some RL methods, such as Reinforcement Learning with Value Networks (RLVR), tend to improve accuracy by optimizing the model to solve easier problems it already has some proficiency with, sometimes at the expense of performance on harder problems \\[64\\]\\[84\\]\\[84\\]. In contrast, teacher distillation has been shown to improve both accuracy and overall capability, enabling the student model to tackle problems across all difficulty levels, including those it previously could not solve \\[64\\]\\[64\\].\n\nThis has led some researchers to classify a direct comparison as \"apples-to-oranges\"; RLVR refines a model using its existing knowledge, while teacher distillation augments the model with new, external knowledge from a superior source \\[64\\]\\[64\\]. While one study showed an RL-trained model achieving 74.8% test accuracy on MATH, significantly higher than a 62.6% baseline \\[182\\], this appears to be an improvement in single-attempt accuracy rather than a fundamental expansion of reasoning ability, which is the hallmark of successful distillation.\n\n**4.0 The Viability Threshold: Minimum Model Size for Effective RL**\n\nA critical question is whether there is a minimum model size below which reinforcement learning ceases to be effective. The evidence suggests that such a threshold exists, further cementing distillation's role for creating ultra-compact models.\n\nStudies indicate that while RL can produce exceptional results on models in the 7B parameter range, its effectiveness diminishes sharply for \"super-tiny\" models (e.g., 0.5B parameters) \\[28\\]. On these very small architectures, direct RL application can lead to performance degradation, resulting in shorter, less coherent, or meaningless responses \\[28\\]. This degradation is likely due to the smaller models lacking the parameter capacity and emergent properties necessary for RL algorithms to effectively explore the policy space without catastrophic forgetting or instability.\n\nIn contrast, knowledge distillation has proven highly effective across a wide spectrum of small model sizes. The popular DistilBERT, for example, successfully compresses BERT from 110M to 66M parameters while retaining 97% of its performance \\[8\\]\\[12\\]. The TinyLLM project demonstrated significant performance boosts for models as small as 80M parameters \\[2\\]. Ablation studies have even used student models in the 30M parameter range successfully \\[115\\]\\[174\\].\n\nDespite this clear trend, the search for a precise, quantitative threshold—a specific model size where pure RL definitively matches KD performance under controlled conditions (equal data and compute)—remains an open research question. Peer-reviewed ablation studies providing this direct comparison are not yet available (Query result 6, Query result 9, Query result 11). Current knowledge suggests that for the smallest models, distillation is not just superior, but necessary.\n\n**5.0 Architectural and Hybrid Innovations**\n\nThe challenges of applying LLM techniques to SLMs have spurred innovations in both model architecture and training methodology. The most impactful developments often involve hybridizing RL and KD.\n\n**5.1 Adapting RL for Small Models**\n\nTo mitigate the inefficiency of full-parameter fine-tuning, parameter-efficient techniques like Low-Rank Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA) have been shown to be highly effective for SLMs \\[24\\]. Research indicates that for smaller models, gradient updates are inherently low-rank, making these methods a natural and computationally efficient fit \\[24\\]. This allows for RL fine-tuning on SLMs with significantly reduced memory and time costs.\n\n**5.2 The Rise of Hybrid RL-KD Approaches**\n\nRecognizing the complementary strengths of RL (policy improvement) and KD (knowledge compression), researchers are developing powerful hybrid techniques.\n\n**Re-distillation:** This promising approach involves first training a policy with RL and then using that optimized policy to generate training data. This data is then used to fine-tune a model via supervised fine-tuning (SFT), a process sometimes called \"re-distillation.\" This method has been shown to match the performance of more complex RL training with fewer samples and less computation, achieving accelerated convergence \\[1\\]\\[1\\]. For instance, a re-distilled model matched the performance of a much larger model on the MATH dataset using only 496 samples \\[1\\].\n\n**RL-Informed Data Generation:** Another hybrid strategy reframes a task as an RL problem but uses a powerful teacher LLM to generate high-quality training examples, complete with explanations \\[7\\]. A compact 3B parameter model trained with this method achieved state-of-the-art performance on the BRIGHT benchmark, outperforming larger models and demonstrating the power of using a teacher model to guide the learning process \\[7\\]\\[7\\].\n\n**Actor-Learner Distillation (ALD):** This paradigm explicitly separates the learning and inference processes. A large, computationally expensive \"Learner\" model is trained using RL to achieve maximum capability. Then, its knowledge is distilled into a small, fast \"Actor\" model designed specifically for efficient inference \\[86\\]. The deployed model is the efficient actor, benefiting from the low latency of a distilled model while possessing capabilities derived from intensive RL training.\n\n**6.0 Efficiency Analysis: Cost and Latency for Edge Deployment**\n\nFor practical applications, especially on edge devices like a Raspberry Pi, inference latency and computational footprint are paramount. In this domain, models produced via distillation have a decisive, built-in advantage.\n\n**6.1 The Inherent Efficiency of Distilled Models**\n\nKnowledge distillation is fundamentally a model compression technique. The resulting student model is, by design, smaller and has fewer parameters than its teacher \\[44\\]\\[55\\]\\[90\\]. This directly translates to lower memory requirements, faster loading times, and reduced computational cost per inference, making distilled models ideal for real-time applications \\[47\\]\\[54\\]\\[57\\]. Quantitatively, DistilBERT is 60% faster than its teacher \\[8\\]\\[12\\]and other distillation methods have demonstrated inference time reductions from 800ms to 275ms on a Raspberry Pi 4 \\[208\\]\\[277\\].\n\n**6.2 Latency on Resource-Constrained Devices**\n\nWhile specific, head-to-head latency benchmarks for hybrid RL+KD models versus pure KD models on a Raspberry Pi 4 using a specific quantization like Q4_K_M are not available in public research, the existing data allows for strong inferences. The final deployed artifact of a hybrid process like ALD or re-distillation is a distilled student model. Therefore, its inference latency characteristics would be governed by the distillation process and should be comparable to a purely distilled model of the same size and architecture.\n\nHybrid architectures themselves show great promise for efficiency. A \"MambaInLlama\" model, which can be seen as a form of architectural distillation, demonstrated a 5.1x inference speedup over a standard Llama-3B model on high-end hardware \\[147\\]\\[207\\]\\[270\\]. While these results are from a GPU and not a Raspberry Pi, they indicate the potential of hybrid designs to be significantly more efficient than their baselines. Even with advanced quantization via llama.cpp on a Raspberry Pi 4, performance for larger models remains a challenge, with speeds reported around 0.11 tokens/second (or ~9090 milliseconds per token) for a 7B model, underscoring the critical need for the compression that distillation provides \\[154\\]\\[213\\].\n\n**7.0 Conclusion and Future Outlook**\n\nAs of July 2025, the evidence strongly suggests that for creating small, capable, and efficient language models, **knowledge distillation remains a more robust and effective strategy than the direct application of reinforcement learning.** Distillation excels at transferring the complex reasoning capabilities of large models into a compact form, a feat that direct RL on small models struggles to replicate, especially for models below the ~1B parameter scale.\n\nHowever, the most significant progress is being made not by choosing one method over the other, but by integrating them. The future of SLM development points towards a pipeline where:\n\n1.  **Reinforcement Learning** is used on larger models to discover and refine novel, high-performance policies and reasoning strategies.\n2.  **Knowledge Distillation** is then employed as a powerful compression technique to transfer this sophisticated, RL-honed knowledge into a small, efficient student model suitable for real-world deployment.\n\nApproaches like re-distillation and actor-learner frameworks are the vanguards of this new paradigm. They leverage RL for its strength in capability generation and KD for its strength in efficiency creation. Therefore, the question is shifting from \"RL _or_ KD?\" to \"How can we best use RL _to power_ KD?\" for the next generation of small language models.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning](https://arxiv.org/pdf/2505.17988)\n\n[2\\. Beyond Answers: Transferring Reasoning Capabilities to Smaller LLMs Using Multi-Teacher Knowledge Distillation](https://arxiv.org/pdf/2402.04616)\n\n[3\\. MINIPLM: KNOWLEDGE DISTILLATION FOR PRE-TRAINING LANGUAGE MODELS](https://openreview.net/pdf?id=tJHDw8XfeC)\n\n[4\\. 100 DAYS AFTER DEEPSEEK-R1: A SURVEY ON REPLICATION STUDIES AND MORE DIRECTIONS FOR REASONING LANGUAGE MODELS](https://arxiv.org/pdf/2505.00551)\n\n[5\\. How to Fine-Tune Small Language Models to Think with ...](https://towardsdatascience.com/how-to-finetune-small-language-models-to-think-with-reinforcement-learning/)\n\n[6\\. Reinforcement Learning Finetunes Small Subnetworks in Large Language Models](https://arxiv.org/pdf/2505.11711)\n\n[7\\. Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking](https://arxiv.org/pdf/2504.03947)\n\n[8\\. RECENT ADVANCES IN NATURAL LANGUAGE PROCESSING SYSTEMS: A TECHNICAL OVERVIEW](https://www.irjmets.com/uploadedfiles/paper/issue_3_march_2025/68747/final/fin_irjmets1741604220.pdf)\n\n[9\\. Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks](https://openreview.net/pdf?id=xJLEQQrFia)\n\n[10\\. DeepSeek-R1: How Reinforcement Learning Unleashes ...](https://myedgetech.com/deepseek-r1-tr/)\n\n[11\\. RAG-Enhanced small Large Language Models: Enhancing Battlefield Analysis through Knowledge Distillation of Large Language Models](https://journal.kci.go.kr/jksci/archive/articlePdf?artiId=ART003187502)\n\n[12\\. Top 13 Small Language Models (SLMs) for 2025](https://www.analyticsvidhya.com/blog/2024/12/top-small-language-models/)\n\n[13\\. Understanding DeepSeek R1—A Reinforcement Learning ...](https://kili-technology.com/large-language-models-llms/understanding-deepseek-r1#:~:text=R1%20was%20trained-,Step%201:%20Reward-Based%20Training,model%20toward%20improved%20logical%20consistency.)\n\n[14\\. Shakti: A 2.5 Billion Parameter Small Language Model ...](https://arxiv.org/html/2410.11331v1)\n\n[21\\. R. S. Sutton, A. Barto. “Reinforcement Learning: An Introduction.” IEEE Trans. Neural Networks](https://doi.org/10.1109/TNN.1998.712192)\n\n[22\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[23\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[24\\. Exploring Efficient Learning of Small BERT Networks with LoRA and DoRA](http://web.stanford.edu/class/cs224n/final-reports/256989355.pdf)\n\n[25\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Playing Atari with Deep Reinforcement Learning.” ArXiv](https://arxiv.org/abs/1312.5602)\n\n[26\\. Efficient adaptation of reinforcement learning agents: from model-free exploration to symbolic world models](https://theses.hal.science/tel-04391194v1/file/KAMIENNY_Pierre_Alexandre_these_2023.pdf)\n\n[27\\. A Course in Reinforcement Learning](https://web.mit.edu/dimitrib/www/RLCOURSECOMPLETE.pdf)\n\n[28\\. From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs](https://arxiv.org/pdf/2504.13471)\n\n[29\\. Benjamin Spector, Serge J. Belongie. “Sample-Efficient Reinforcement Learning through Transfer and Architectural Priors.” ArXiv](https://arxiv.org/abs/1801.02268)\n\n[30\\. Greg Brockman, Vicki Cheung et al. “OpenAI Gym.” ArXiv](https://arxiv.org/abs/1606.01540)\n\n[31\\. Efficient Transformers in Reinforcement Learning using Actor-Learner Distillation](https://openreview.net/pdf?id=uR9LaO_QxF)\n\n[32\\. Demonstration of the Dyna Reinforcement Learning Framework for Reactive Close Proximity Operations](https://deepblue.lib.umich.edu/bitstream/handle/2027.42/195994/10.2514%3A6.2025-1002.pdf?sequence=1&isAllowed=y)\n\n[33\\. Meta Reinforcement Learning through Memory](https://www.ml.cmu.edu/research/phd-dissertation-pdfs/eparisot_phd_mld_2021.pdf)\n\n[34\\. Intel's New AI System Can Optimise Reinforcement Learning Training On A Single System](https://analyticsindiamag.com/intels-new-ai-system-can-optimise-reinforcement-learning-training-on-a-single-system/)\n\n[35\\. Micro-Armed Bandit: Lightweight & Reusable Reinforcement Learning for Microarchitecture Decision-Making](https://iacoma.cs.uiuc.edu/iacoma-papers/micro23.pdf)\n\n[36\\. Optimization of Trusses and Frames using Reinforcement Learning](https://repository.kulib.kyoto-u.ac.jp/dspace/bitstream/2433/288675/1/dkogk05200.pdf)\n\n[37\\. Smaller World Models for Reinforcement Learning | Neur...](https://dl.acm.org/doi/abs/10.1007/s11063-023-11381-3)\n\n[38\\. Cramming: Training a Language Model on a Single GPU in One Day](https://openreview.net/pdf?id=gUL6zYN4Uaf)\n\n[39\\. Efficient Expansion and Gradient Based Task Inference for Replay Free Incremental Learning](https://openaccess.thecvf.com/content/WACV2024/papers/Roy_Efficient_Expansion_and_Gradient_Based_Task_Inference_for_Replay_Free_WACV_2024_paper.pdf)\n\n[41\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[42\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[43\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[44\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[45\\. Deploy DeepSeek-R1 distilled Llama models with Amazon Bedrock Custom ...](https://aws.amazon.com/blogs/machine-learning/deploy-deepseek-r1-distilled-llama-models-with-amazon-bedrock-custom-model-import/)\n\n[46\\. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://semking.com/PDFs/DeepSeek-Chinese-AI-model-breakthrough-security-risk.pdf)\n\n[47\\. LLM Inference Optimization: Speed, Scale, and Savings - Ghost](https://latitude-blog.ghost.io/blog/llm-inference-optimization-speed-scale-and-savings/)\n\n[48\\. Scaling AI: Cost and Performance of AI at the Leading Edge](https://cset.georgetown.edu/wp-content/uploads/Scaling-AI-Cost-and-Performance-of-AI-at-the-Leading-Edge.pdf)\n\n[49\\. Song Han, Huizi Mao et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition](https://arxiv.org/abs/1510.00149)\n\n[50\\. Demystifying Small Language Models for Edge Deployment](https://openreview.net/attachment?id=qRhhT1BoUq&name=pdf)\n\n[51\\. Understanding DeepSeek R1—A Reinforcement Learning ...](https://kili-technology.com/large-language-models-llms/understanding-deepseek-r1#:~:text=R1%20was%20trained-,Step%201:%20Reward-Based%20Training,model%20toward%20improved%20logical%20consistency.)\n\n[52\\. Meta Reinforcement Learning through Memory](https://www.ml.cmu.edu/research/phd-dissertation-pdfs/eparisot_phd_mld_2021.pdf)\n\n[53\\. Machine Learning at Network Edges](https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-cn/mediares/magazine/publication/com_en/pdf/en202002.pdf)\n\n[54\\. Understanding Knowledge Distillation, its Process & Trends](https://botpenguin.com/glossary/knowledge-distillation)\n\n[55\\. Efficient Neural Network Inference for Resource Constrained Devices](https://ddd.uab.cat/pub/tesis/2023/hdl_10803_688291/jbc1de1.pdf)\n\n[56\\. A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs](https://www.arc.ics.keio.ac.jp/~matutani/papers/kawakami_ieicej2023.pdf)\n\n[57\\. Risks, Causes, and Mitigations of Widespread ...](https://arxiv.org/html/2408.04643v1)\n\n[58\\. Understanding DeepSeek R1—A Reinforcement Learning-Driven ...](https://kili-technology.com/large-language-models-llms/understanding-deepseek-r1#:~:text=improving%20scalability%E2%80%8B.-,Application%20of%20GRPO%20in%20DeepSeek-R1,without%20requiring%20a%20critic%20network.)\n\n[59\\. Reducing LLM Inference Costs While Preserving Performance](https://www.rohan-paul.com/p/reducing-llm-inference-costs-while)\n\n[60\\. Edge Computing and Large Language Models (LLMs)](https://dev.to/hakeem/edge-computing-and-large-language-models-llms-whats-the-connection-34id)\n\n[61\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[62\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[63\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[64\\. Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://www.arxiv.org/pdf/2505.14216)\n\n[65\\. Victor Sanh, Lysandre Debut et al. “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.” ArXiv](https://arxiv.org/abs/1910.01108)\n\n[66\\. Understanding DeepSeek R1—A Reinforcement Learning-Driven ...](https://kili-technology.com/large-language-models-llms/understanding-deepseek-r1#:~:text=improving%20scalability%E2%80%8B.-,Application%20of%20GRPO%20in%20DeepSeek-R1,without%20requiring%20a%20critic%20network.)\n\n[67\\. Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking](https://arxiv.org/pdf/2504.03947)\n\n[68\\. Xiaoqi Jiao, Yichun Yin et al. “TinyBERT: Distilling BERT for Natural Language Understanding.” ArXiv](https://doi.org/10.18653/v1/2020.findings-emnlp.372)\n\n[69\\. Knowledge Distillation with Training Wheels](http://arxiv.org/html/2502.17717v1)\n\n[81\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[82\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[83\\. Adriana Romero, Nicolas Ballas et al. “FitNets: Hints for Thin Deep Nets.” CoRR](https://arxiv.org/abs/1412.6550)\n\n[84\\. Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://www.arxiv.org/pdf/2505.14216)\n\n[85\\. Model compression using knowledge distillation with integrated gradients](https://arxiv.org/pdf/2506.14440)\n\n[86\\. Meta Reinforcement Learning through Memory](https://www.ml.cmu.edu/research/phd-dissertation-pdfs/eparisot_phd_mld_2021.pdf)\n\n[87\\. What differences in inference speed and memory usage might ...](https://milvus.io/ai-quick-reference/what-differences-in-inference-speed-and-memory-usage-might-you-observe-between-different-sentence-transformer-architectures-for-example-bertbase-vs-distilbert-vs-robertabased-models)\n\n[88\\. Jimmy Ba, R. Caruana. “Do Deep Nets Really Need to be Deep?.” Neural Information Processing Systems](https://arxiv.org/abs/1312.6184)\n\n[89\\. LLM-Based Edge Intelligence: A Comprehensive Survey on Architectures, Applications, Security and Trustworthiness](https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?arnumber=10669603)\n\n[90\\. Empowering Edge Intelligence: A Comprehensive Survey on On-Device AI Models](https://www.wangxubin.site/paper/On_Device_AI_CSUR_2025.pdf)\n\n[91\\. CodeGPT on XTC: Compressing a CodeGPT Model Using Hybrid Layer Reduction and Extreme Quantisation through Knowledge Distillation](https://repository.tudelft.nl/file/File_47facba4-c909-47e7-bf8a-52cafa7f74ac?preview=1)\n\n[92\\. Resource-Constrained Learning and Inference for Visual Perception](https://www.ri.cmu.edu/app/uploads/2022/05/Martin___Ph_D__Thesis.pdf)\n\n[93\\. Jianping Gou, B. Yu et al. “Knowledge Distillation: A Survey.” International Journal of Computer Vision](https://doi.org/10.1007/s11263-021-01453-z)\n\n[94\\. COMBINING QLORA AND KNOWLEDGE DISTILLATION: AN APPROACH TO ENHANCE PERFORMANCE AND EFFICIENCY](https://epub.jku.at/download/pdf/11767041.pdf)\n\n[101\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[102\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[103\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[104\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[105\\. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://www.thewirechina.com/wp-content/uploads/2025/01/DeepSeek-R1-Document.pdf)\n\n[106\\. Magistral](https://www.52nlp.cn/wp-content/uploads/2025/06/Magistral%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E8%8B%B1%E4%B8%AD%E5%AF%B9%E7%85%A7%E7%89%88.pdf)\n\n[107\\. Understanding DeepSeek R1—A Reinforcement Learning ...](https://kili-technology.com/large-language-models-llms/understanding-deepseek-r1#:~:text=R1%20was%20trained-,Step%201:%20Reward-Based%20Training,model%20toward%20improved%20logical%20consistency.)\n\n[108\\. Victor Sanh, Lysandre Debut et al. “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.” ArXiv](https://arxiv.org/abs/1910.01108)\n\n[109\\. TOWARDS UNDISTILLABLE MODELS BY MINIMIZING CONDITIONAL MUTUAL INFORMATION](https://openreview.net/pdf?id=0tMcsHsHgQ)\n\n[110\\. Yuxiang Sun, Pooyan Fazli. “Real-time Policy Distillation in Deep Reinforcement Learning.” ArXiv](https://arxiv.org/abs/1912.12630)\n\n[111\\. Llama-Nemotron: Efficient Reasoning Models](http://arxiv.org/pdf/2505.00949)\n\n[112\\. Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning](https://arxiv.org/pdf/2505.17988)\n\n[113\\. Efficient Transformers in Reinforcement Learning using Actor-Learner Distillation](https://openreview.net/pdf?id=uR9LaO_QxF)\n\n[114\\. Cristian Bucila, R. Caruana et al. “Model compression.” Knowledge Discovery and Data Mining](https://doi.org/10.1145/1150402.1150464)\n\n[115\\. Distilling Knowledge from Large-Scale Image Models for Object Detection](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11200.pdf)\n\n[116\\. Capture the Key in Reasoning to Enhance CoT Distillation Generalization](https://openreview.net/pdf?id=Ncy6pxvk8O)\n\n[117\\. RECENT ADVANCES IN NATURAL LANGUAGE PROCESSING SYSTEMS: A TECHNICAL OVERVIEW](https://www.irjmets.com/uploadedfiles/paper/issue_3_march_2025/68747/final/fin_irjmets1741604220.pdf)\n\n[118\\. Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://www.arxiv.org/pdf/2505.14216)\n\n[119\\. Adversarial Diffusion Distillation](https://stability.ai/s/adversarial_diffusion_distillation.pdf)\n\n[121\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[122\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[123\\. A. Krizhevsky. “Learning Multiple Layers of Features from Tiny Images.”](https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086)\n\n[124\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[125\\. Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://www.arxiv.org/pdf/2505.14216)\n\n[126\\. Adriana Romero, Nicolas Ballas et al. “FitNets: Hints for Thin Deep Nets.” CoRR](https://arxiv.org/abs/1412.6550)\n\n[127\\. 𝐾⁢𝐷⁢𝐴⁢𝑆⁢3: Knowledge distillation via Attention ...](https://arxiv.org/html/2312.08555v1)\n\n[128\\. MINIPLM: Knowledge Distillation for Pre-Training Language Models](https://arxiv.org/pdf/2410.17215)\n\n[129\\. Dmytro Kuzmenko, Nadiya Shvai. “Knowledge Transfer in Model-Based Reinforcement Learning Agents for Efficient Multi-Task Learning.”](https://arxiv.org/abs/2501.05329)\n\n[130\\. Understanding DeepSeek R1—A Reinforcement Learning ...](https://kili-technology.com/large-language-models-llms/understanding-deepseek-r1#:~:text=R1%20was%20trained-,Step%201:%20Reward-Based%20Training,model%20toward%20improved%20logical%20consistency.)\n\n[131\\. Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning](https://arxiv.org/pdf/2505.24850)\n\n[141\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[142\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[143\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[144\\. Song Han, Huizi Mao et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition](https://arxiv.org/abs/1510.00149)\n\n[145\\. Quantization](https://raw.githubusercontent.com/amodm/quantization-intro/main/2023-04-genai-meetup-quantization.pptx)\n\n[146\\. Benoit Jacob, S. Kligys et al. “Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.” 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2018.00286)\n\n[147\\. Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners](https://arxiv.org/pdf/2502.20339)\n\n[148\\. GitHub - sunkx109/llama.cpp: llama 2 Inference](https://github.com/sunkx109/llama.cpp/tree/master)\n\n[149\\. Raspberry Pi 4 4GB · Issue #58 · ggerganov/llama.cpp...](https://github.com/ggerganov/llama.cpp/issues/58)\n\n[150\\. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://dai.sjtu.edu.cn/my_file/pdf/56ebcd39-ce4f-4800-9f74-7aa108345782.pdf)\n\n[151\\. Distributed Llama - Distributed Inference of Large Language Models with Slow Synchronization over Ethernet](https://raw.githubusercontent.com/b4rtaz/distributed-llama/main/report/report.pdf)\n\n[152\\. Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for Efficient LLM Decoding on Embedded FPGA](https://arxiv.org/pdf/2502.10659)\n\n[153\\. Benchmarking Post-Training Quantization for Optimizing Machine Learning Inference on compute-limited Edge Devices](http://ots.fh-brandenburg.de/downloads/abschlussarbeiten/2021-03-02%20ba_mahmoud_abdelrahman.pdf)\n\n[154\\. TinyAgent: Quantization-aware Model Compression and Adaptation for On-device LLM Agent Deployment](https://openreview.net/pdf?id=ntI0eq0urB)\n\n[155\\. GitHub - krish-adi/llama.cpp: LLM inference in C/C++](https://github.com/krish-adi/llama.cpp/tree/master)\n\n[156\\. Running LLMs Locally on Consumer Devices](https://www.ijraset.com/best-journal/running-llms-locally-on-consumer-devices)\n\n[157\\. Quantize and run the original llama3 8B with llama.cpp](https://voorloopnul.com/blog/quantize-and-run-the-original-llama3-8b-with-llama-cpp/)\n\n[158\\. Llamba: Scaling Distilled Recurrent Models for Efficient Language Processing](https://openreview.net/pdf?id=uciWntM6iv)\n\n[161\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[162\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[163\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[164\\. Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://www.arxiv.org/pdf/2505.14216)\n\n[165\\. LLMs Can Easily Learn to Reason from Demonstrations ...](https://arxiv.org/html/2502.07374v2)\n\n[166\\. Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning](https://arxiv.org/pdf/2505.17988)\n\n[167\\. Victor Sanh, Lysandre Debut et al. “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.” ArXiv](https://arxiv.org/abs/1910.01108)\n\n[168\\. \\\\ours: Learning a Small Student from Multiple Large La...](http://arxiv.org/html/2402.04616v2)\n\n[169\\. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948)\n\n[170\\. Scaling AI: Cost and Performance of AI at the Leading Edge](https://cset.georgetown.edu/wp-content/uploads/Scaling-AI-Cost-and-Performance-of-AI-at-the-Leading-Edge.pdf)\n\n[171\\. Capture the Key in Reasoning to Enhance CoT Distillation Generalization](https://openreview.net/pdf?id=Ncy6pxvk8O)\n\n[172\\. Model Distillation for Large Language Models](https://heidloff.net/article/model-distillation-large-language-models/)\n\n[173\\. Gemma 2: Improving Open Language Models at a Practical Size](https://arxiv.org/pdf/2408.00118)\n\n[174\\. Distilling Knowledge from Large-Scale Image Models for Object Detection](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11200.pdf)\n\n[175\\. Self-Data Distillation for Recovering Quality in Pruned Large Language Models](https://openreview.net/pdf?id=aUaltIxtol)\n\n[176\\. Few-Shot Learning of Compact Models via Task-Specific Meta Distillation](https://openaccess.thecvf.com/content/WACV2023/papers/Wu_Few-Shot_Learning_of_Compact_Models_via_Task-Specific_Meta_Distillation_WACV_2023_paper.pdf)\n\n[177\\. Explainer: What's R1 & Everything Else?](https://timkellogg.me/blog/2025/01/25/r1)\n\n[178\\. Adriana Romero, Nicolas Ballas et al. “FitNets: Hints for Thin Deep Nets.” CoRR](https://arxiv.org/abs/1412.6550)\n\n[179\\. A Novel Deep Learning Model for Accurate Pest Detection ...](https://www.mdpi.com/2075-4450/14/7/660)\n\n[180\\. WELL-READ STUDENTS LEARN BETTER: ON THE IMPORTANCE OF PRE-TRAINING COMPACT MODELS](https://openreview.net/pdf?id=BJg7x1HFvB)\n\n[181\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[182\\. Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://www.arxiv.org/pdf/2505.14216)\n\n[183\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[184\\. Victor Sanh, Lysandre Debut et al. “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.” ArXiv](https://arxiv.org/abs/1910.01108)\n\n[185\\. Adriana Romero, Nicolas Ballas et al. “FitNets: Hints for Thin Deep Nets.” CoRR](https://arxiv.org/abs/1412.6550)\n\n[186\\. From 128K to 4M: Efficient Training of Ultra-Long Context ...](https://arxiv.org/html/2504.06214v1)\n\n[187\\. Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning](https://arxiv.org/pdf/2505.24850)\n\n[188\\. Tommaso Furlanello, Zachary Chase Lipton et al. “Born Again Neural Networks.” International Conference on Machine Learning](https://arxiv.org/abs/1805.04770)\n\n[189\\. Reinforcement Learning is all you need](https://arxiv.org/html/2503.09512v1)\n\n[190\\. MINILLM: KNOWLEDGE DISTILLATION OF LARGE LANGUAGE MODELS](https://openreview.net/pdf?id=5h0qf7IBZZ)\n\n[191\\. Robustness-Reinforced Knowledge Distillation with Correlation Distance and Network Pruning](http://arxiv.org/pdf/2311.13934)\n\n[201\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[202\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[203\\. GitHub - sunkx109/llama.cpp: llama 2 Inference](https://github.com/sunkx109/llama.cpp/tree/master)\n\n[204\\. Song Han, Huizi Mao et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition](https://arxiv.org/abs/1510.00149)\n\n[205\\. Efficient Audio Captioning with Encoder-Level Knowledge Distillation](https://www.isca-archive.org/interspeech_2024/xu24e_interspeech.pdf)\n\n[206\\. LLM Landscape Report](https://cdn.prod.website-files.com/668d66434307b08c724f8a81/67dc26e1400679d594e43df8_LLM%20Landscape%20Report.pdf)\n\n[207\\. Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners](https://arxiv.org/pdf/2502.20339)\n\n[208\\. Ranya Aloufi, H. Haddadi et al. “Paralinguistic Privacy Protection at the Edge.” ACM Transactions on Privacy and Security](https://doi.org/10.1145/3570161)\n\n[209\\. Benoit Jacob, S. Kligys et al. “Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.” 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2018.00286)\n\n[210\\. Quantization](https://raw.githubusercontent.com/amodm/quantization-intro/main/2023-04-genai-meetup-quantization.pptx)\n\n[211\\. Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://www.arxiv.org/pdf/2505.14216)\n\n[212\\. Xiaoqi Jiao, Yichun Yin et al. “TinyBERT: Distilling BERT for Natural Language Understanding.” ArXiv](https://doi.org/10.18653/v1/2020.findings-emnlp.372)\n\n[213\\. TinyAgent: Quantization-aware Model Compression and Adaptation for On-device LLM Agent Deployment](https://openreview.net/pdf?id=ntI0eq0urB)\n\n[214\\. THINKING SLOW, FAST: SCALING INFERENCE COMPUTE WITH DISTILLED REASONERS](https://openreview.net/pdf?id=0Vwmx0MIRG)\n\n[215\\. Accelerating LLM Inference: A 10× Latency Reduction Roadmap](https://www.rohan-paul.com/p/how-to-reduce-the-average-response)\n\n[216\\. Deep Learning Model Compression for Resource Efficient Activity Recognition on Edge Devices: A Case Study](https://www.scitepress.org/Papers/2024/124233/124233.pdf)\n\n[221\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[222\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[223\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[224\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[225\\. Knowledge Distillation知识蒸馏 - wioponsen - 博客园](https://www.cnblogs.com/wioponsen/p/13876871.html)\n\n[226\\. Victor Sanh, Lysandre Debut et al. “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.” ArXiv](https://arxiv.org/abs/1910.01108)\n\n[227\\. Distilling Knowledge from Large-Scale Image Models for Object Detection](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11200.pdf)\n\n[228\\. Smaller World Models for Reinforcement Learning | Neur...](https://dl.acm.org/doi/abs/10.1007/s11063-023-11381-3)\n\n[229\\. Compressing Visual-linguistic Model via Knowledge Distillation](https://openaccess.thecvf.com/content/ICCV2021/papers/Fang_Compressing_Visual-Linguistic_Model_via_Knowledge_Distillation_ICCV_2021_paper.pdf)\n\n[230\\. Learning Mixture Models with Simultaneous Data Partitioning and Parameter Estimation](https://openreview.net/pdf?id=36g8Ept_CCj)\n\n[231\\. Knowledge Distillation from A Stronger Teacher | Papers With Code](https://paperswithcode.com/paper/knowledge-distillation-from-a-stronger)\n\n[232\\. Model Compression Algorithm via Reinforcement Learning and ... - MDPI](https://www.mdpi.com/2227-7390/11/22/4589)\n\n[233\\. Zhuoyuan Mao, Tetsuji Nakagawa. “LEALLA: Learning Lightweight Language-agnostic Sentence Embeddings with Knowledge Distillation.” Conference of the European Chapter of the Association for Computational Linguistics](https://doi.org/10.48550/arXiv.2302.08387)\n\n[234\\. \\\\ours: Learning a Small Student from Multiple Large La...](http://arxiv.org/html/2402.04616v2)\n\n[235\\. Xumei Xi, YuKe Zhao et al. “Integrating Offline Reinforcement Learning with Transformers for Sequential Recommendation.” Proceedings of the 17th ACM Conference on Recommender Systems](https://doi.org/10.1145/3604915.3610641)\n\n[236\\. Supervised Masked Knowledge Distillation for Few-Shot Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Supervised_Masked_Knowledge_Distillation_for_Few-Shot_Transformers_CVPR_2023_paper.pdf)\n\n[237\\. Compact Language Models via Pruning and Knowledge Distillation](http://arxiv.org/pdf/2407.14679)\n\n[238\\. Lucie Charlotte Magister, Jonathan Mallinson et al. “Teaching Small Language Models to Reason.” ArXiv](https://doi.org/10.48550/arXiv.2212.08410)\n\n[239\\. Self-Knowledge Distillation with Progressive Refinement of Targets](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Self-Knowledge_Distillation_With_Progressive_Refinement_of_Targets_ICCV_2021_paper.pdf)\n\n[240\\. Pre-trained models: Past, present and future](https://nlp.csai.tsinghua.edu.cn/~ly/papers/aiopen21_hx.pdf)\n\n[241\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[242\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[243\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[244\\. Improving Multi-step Reasoning for LLMs with Deliberative ...](https://arxiv.org/html/2406.14283v4)\n\n[245\\. S²R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning](https://www.arxiv.org/pdf/2502.12853)\n\n[246\\. Adriana Romero, Nicolas Ballas et al. “FitNets: Hints for Thin Deep Nets.” CoRR](https://arxiv.org/abs/1412.6550)\n\n[247\\. Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://www.arxiv.org/pdf/2505.14216)\n\n[248\\. Cristian Bucila, R. Caruana et al. “Model compression.” Knowledge Discovery and Data Mining](https://doi.org/10.1145/1150402.1150464)\n\n[249\\. LLM-NEO: Parameter Efficient Knowledge Distillation for Large Language Models](http://arxiv.org/pdf/2411.06839)\n\n[250\\. PAT: Pruning-Aware Tuning for Large Language Models](https://www.arxiv.org/pdf/2408.14721)\n\n[251\\. Revisiting Knowledge Distillation for Autoregressive Language Models](https://openreview.net/pdf/73503af2a5797fb9046f0fa702c3a4d5ea5ceaf8.pdf)\n\n[252\\. 微软&清华联合研究发现：7B级LLaMA-2模型数学准确率高达97.7%！](https://zhuanlan.zhihu.com/p/687021921)\n\n[253\\. A Comparative Study between Full-Parameter and LoRA ...](https://ar5iv.labs.arxiv.org/html/2304.08109)\n\n[254\\. Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning](https://openreview.net/pdf?id=EirFORi6v8)\n\n[255\\. Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning](https://arxiv.org/pdf/2502.06781)\n\n[256\\. Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning](https://arxiv.org/pdf/2505.24850)\n\n[257\\. Knowledge Distillation](https://www.cs.ubc.ca/~lsigal/532S_2018W2/4b.pdf)\n\n[261\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[262\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[263\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[264\\. Victor Sanh, Lysandre Debut et al. “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.” ArXiv](https://arxiv.org/abs/1910.01108)\n\n[265\\. LLM inference benchmarks with llamacpp and AMD EPYC ...](https://ahelpme.com/ai/llm-inference-benchmarks-with-llamacpp-and-amd-epyc-7282-cpu/)\n\n[266\\. Efficient Audio Captioning with Encoder-Level Knowledge Distillation](https://www.isca-archive.org/interspeech_2024/xu24e_interspeech.pdf)\n\n[267\\. Xiaoqi Jiao, Yichun Yin et al. “TinyBERT: Distilling BERT for Natural Language Understanding.” ArXiv](https://doi.org/10.18653/v1/2020.findings-emnlp.372)\n\n[268\\. optimizing-llm-inference-with-quantization 联合开发网](https://www.pudn.com/Download/item/id/1710659745879301.html)\n\n[269\\. Best Practices for LLM Inference Performance Monitoring](https://blog.spheron.network/best-practices-for-llm-inference-performance-monitoring)\n\n[270\\. Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners](https://arxiv.org/pdf/2502.20339)\n\n[271\\. A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone](https://arxiv.org/pdf/2505.12781)\n\n[272\\. Llama2.pi: Running LLMs on the Bleeding Edge](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/final-projects/MatthewDing.pdf)\n\n[273\\. Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://www.arxiv.org/pdf/2505.14216)\n\n[274\\. Phi-4 quantization and inference speedup](https://techcommunity.microsoft.com/blog/machinelearningblog/phi-4-quantization-and-inference-speedup/4360047)\n\n[275\\. TinyAgent: Quantization-aware Model Compression and Adaptation for On-device LLM Agent Deployment](https://openreview.net/pdf?id=ntI0eq0urB)\n\n[276\\. Distributed Llama - Distributed Inference of Large Language Models with Slow Synchronization over Ethernet](https://raw.githubusercontent.com/b4rtaz/distributed-llama/main/report/report.pdf)\n\n[277\\. Ranya Aloufi, H. Haddadi et al. “Paralinguistic Privacy Protection at the Edge.” ACM Transactions on Privacy and Security](https://doi.org/10.1145/3570161)\n\n[278\\. HLSTransform: Energy-Efficient Llama 2 Inference on FPGAs Via High Level Synthesis](https://openreview.net/pdf?id=tvmCsGnyLq)\n\n[279\\. Co-Designing Efficient Systems and Algorithms for Sparse and Quantized Deep Learning Computing](https://dspace.mit.edu/bitstream/handle/1721.1/158928/tang-kentang-phd-eecs-2025-thesis.pdf?sequence=1&isAllowed=y)\n\n[280\\. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://dai.sjtu.edu.cn/my_file/pdf/56ebcd39-ce4f-4800-9f74-7aa108345782.pdf)\n\n[281\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[282\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[283\\. Song Han, Huizi Mao et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition](https://arxiv.org/abs/1510.00149)\n\n[284\\. Distilling Rule-based Knowledge into Large Language Models](https://arxiv.org/pdf/2311.08883.pdf)\n\n[285\\. Adriana Romero, Nicolas Ballas et al. “FitNets: Hints for Thin Deep Nets.” CoRR](https://arxiv.org/abs/1412.6550)\n\n[286\\. Cristian Bucila, R. Caruana et al. “Model compression.” Knowledge Discovery and Data Mining](https://doi.org/10.1145/1150402.1150464)\n\n[287\\. F. Iandola, Matthew W. Moskewicz et al. “SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size.” ArXiv](https://arxiv.org/abs/1602.07360)\n\n[288\\. Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling](https://arxiv.org/pdf/2408.16737)\n\n[289\\. Smaller World Models for Reinforcement Learning | Neur...](https://dl.acm.org/doi/abs/10.1007/s11063-023-11381-3)\n\n[290\\. Knowledge Distillation](https://www.cse.iitd.ac.in/~mausam/courses/cov884/spring2022/slides/06-kd.pdf)\n\n[291\\. Self-Knowledge Distillation with Progressive Refinement of Targets](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Self-Knowledge_Distillation_With_Progressive_Refinement_of_Targets_ICCV_2021_paper.pdf)\n\n[292\\. Multi-matrix Factorization Attention](https://openreview.net/pdf/5daf7ba5798e240262bdf26cb181f232f55bbf56.pdf)\n\n[293\\. PANDALM: AN AUTOMATIC EVALUATION BENCHMARK FOR LLM INSTRUCTION TUNING OPTIMIZATION](https://openreview.net/pdf/abded65724c71911185dbb4ab72a213967689bc0.pdf)\n\n[294\\. Compact Language Models via Pruning and Knowledge Distillation](http://arxiv.org/pdf/2407.14679)\n\n[295\\. Foundation Models for Reinforcement Learning](https://ml.cs.tsinghua.edu.cn/~chengyang/reading_meeting/Reading_Meeting_20230331.pdf)\n\n[296\\. Knowledge Distillation for Model-Agnostic Meta-Learning](https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA200239)\n\n[297\\. New Sound Detection Approach Improves on State of the Art](https://www.amazon.science/blog/new-sound-detection-approach-improves-on-state-of-the-art)\n\n[298\\. NVIDIA Minitron: Pruning & Distillation for Efficient AI Models](https://ajithp.com/2024/08/25/minitron-nvidias-breakthrough-in-llm-efficiency-pruning-and-distillation-for-smaller-faster-ai-models/)"
  },
  {
    "id": 26,
    "question": "Do we expect a different set of benchmarks for evaluating AI models as we shift from scale-up to scale-out paradigms, or should we focus entirely on the app layer?",
    "response": "**Research Report**\n\n**To:** Interested Parties\n\n**From:** Expert Researcher\n\n**Date:** July 26, 2025\n\n**Subject:** The Future of AI Evaluation: Benchmarking in the Era of Scale-Up vs. Scale-Out and the Rise of Application-Layer Metrics\n\n**Executive Summary**\n\nThis report addresses a critical question facing the artificial intelligence industry in 2025: as AI systems increasingly diverge into \"scale-up\" and \"scale-out\" architectural paradigms, should our evaluation methods evolve to create new, distinct benchmarks for each, or should we shift our focus entirely to the application layer?\n\nThe analysis, based on extensive research, concludes that this is not an \"either/or\" proposition. A comprehensive and effective evaluation strategy for next-generation AI requires a dual-track approach.\n\n1.  **Specialized Infrastructure Benchmarks are Necessary:** The fundamental technical differences between scale-up (vertical scaling) and scale-out (horizontal scaling) are profound, particularly regarding network latency requirements. Scale-up systems demand extreme, sub-microsecond latency for tightly-coupled, memory-semantic operations, while scale-out systems are designed for millisecond-level latency across distributed nodes. Existing benchmarks like MLPerf, while valuable, show significant limitations in addressing this chasm, often featuring fixed problem sizes and a centralized evaluation focus that fails to capture the full complexity of large-scale distributed systems. Therefore, we do expect and require a different, more specialized set of benchmarks that are sensitive to the unique performance characteristics and architectural goals of each paradigm.\n2.  **An Application-Layer Focus is Non-Negotiable:** Simultaneously, as AI becomes more integrated into mission-critical and user-facing products, evaluating performance solely on infrastructure metrics (like FLOPs or throughput under constraints) is insufficient. The ultimate measure of an AI system's success is its real-world impact. This necessitates a robust and standardized framework for measuring application-layer performance, encompassing metrics such as task success rate for AI agents, business value generation, operational KPIs, reliability, safety, and ethical considerations like fairness and explainability.\n\nThis report argues that focusing _entirely_ on the app layer would be a mistake, as it would obscure the underlying hardware and network bottlenecks that are critical for performance and cost-efficiency. Conversely, relying only on infrastructure benchmarks provides an incomplete picture of a model's true utility. The future of AI evaluation lies in the synthesis of both: specialized benchmarks that accurately model the performance of scale-up and scale-out infrastructures, and a mature, context-aware set of metrics to evaluate the success of the applications built upon them.\n\n**1\\. The Foundational Shift: Differentiating Scale-Up and Scale-Out Paradigms**\n\nTo understand the need for new evaluation methodologies, one must first grasp the technical divergence between the scale-up and scale-out paradigms. These are not merely different ways to add more computing power; they represent fundamentally different philosophies in system design, with profound implications for AI model training and deployment.\n\n**Scale-Up (Vertical Scaling)** involves enhancing the resources of a _single_ system or node \\[5\\]\\[7\\]\\[14\\]. This means adding more powerful CPUs, more GPUs, or more memory to one machine. From the software's perspective, the system simply becomes a single, more powerful computer \\[14\\]. The primary technical goal of scale-up is to maximize intra-cluster communication bandwidth and minimize latency among these tightly packed components \\[7\\]. This architecture is essential for workloads that require high-frequency data interaction between GPUs, such as tensor parallelism and expert parallelism, where communication overhead must be virtually nonexistent \\[15\\]\\[133\\].\n\n**Scale-Out (Horizontal Scaling)**, in contrast, involves adding _more systems_ or nodes to an infrastructure and distributing the workload across them \\[5\\]\\[14\\]. It is the foundation of modern distributed computing \\[14\\]. This paradigm relies on breaking tasks down and processing them in parallel across multiple machines connected by a network \\[4\\]. It is generally more flexible and cost-effective for expansion, as one can simply add more commodity or near-commodity nodes \\[5\\]\\[10\\]. Scale-out architectures are well-suited for tasks that can be independently parallelized, such as data parallelism and pipeline parallelism \\[15\\].\n\nThe most critical distinction for the purpose of benchmarking is **network latency**. Scale-up networks are designed for extreme low latency, targeting sub-microsecond or even nanosecond levels to mimic direct memory access between components \\[123\\]\\[127\\]\\[135\\]. Scale-out networks, often built on Ethernet and RDMA, prioritize flexibility and are designed with a higher tolerance for latency, typically operating in the millisecond range \\[5\\]\\[127\\]\\[133\\].\n\n|     |     |     |\n| --- | --- | --- |\n| **Feature** | **Scale-Up (Vertical Scaling)** | **Scale-Out (Horizontal Scaling)** |\n| **Resource Expansion** | Increases resources within a single system (e.g., more powerful GPUs) \\[82\\] | Adds more independent systems/nodes to the infrastructure \\[82\\] |\n| **System Perception** | A single, more powerful computer \\[14\\] | A distributed network of multiple computers \\[14\\] |\n| **Primary Goal** | Minimize latency for tightly-coupled computation \\[7\\]\\[88\\] | Maximize flexibility, resilience, and aggregate throughput \\[5\\]\\[10\\] |\n| **Target Latency** | Extreme low latency (sub-microsecond/nanosecond) \\[123\\]\\[135\\] | Higher latency tolerance (millisecond-level) \\[127\\]\\[133\\] |\n| **Typical Parallelism** | Tensor Parallelism, Expert Parallelism \\[15\\]\\[133\\] | Data Parallelism, Pipeline Parallelism \\[15\\]\\[133\\] |\n| **Cost & Flexibility** | Higher cost, less flexible for rapid expansion \\[5\\]\\[10\\]\\[87\\] | More cost-effective, highly flexible and scalable \\[5\\]\\[10\\]\\[87\\] |\n| **Use Cases** | Single large, complex calculations; memory-intensive tasks \\[4\\]\\[8\\] | Distributed data processing; parallelizable web services \\[4\\]\\[12\\] |\n\nThis fundamental dichotomy in latency and communication patterns suggests that a single benchmark cannot adequately stress-test and compare both architectures.\n\n**2\\. Assessing the Current State of AI Benchmarking (as of 2025)**\n\n**2.1. Established Frameworks: The Case of MLPerf and its Limitations**\n\nMLCommons' MLPerf has become an industry standard for evaluating the performance of AI systems for both training and inference \\[25\\]. It measures how systems perform on a range of tasks, including the training of large language models like Llama 2 and Llama 3.1 on massive clusters of up to 512 GPUs \\[21\\]\\[30\\]\\[33\\]. MLPerf specifically assesses horizontal scalability by evaluating how well a system parallelizes tasks as more GPUs are added, making interconnect performance a key factor \\[25\\]. The benchmarks are designed to support distributed training, which is now a standard industry practice \\[24\\]\\[34\\].\n\nDespite its widespread adoption, MLPerf exhibits several limitations, particularly when evaluating the next generation of large-scale, distributed AI systems:\n\n**Fixed Problem Size:** A significant limitation is the use of a fixed problem size for many benchmarks. This means that as computational power increases, the benchmark measures how fast a system can solve the _same_ problem, not how well it can tackle a _larger_ or more complex problem. This fails to provide a complete functional assessment of systems designed for massive scale \\[67\\]\\[71\\]\\[77\\].\n\n**Small Dataset Sizes:** Some benchmarks use datasets, like MovieLens 20-million, that are now considered small. This can artificially limit the batch size and thus cap the scalability that can be demonstrated, particularly for powerful scale-out systems \\[69\\].\n\n**Centralized Evaluation Paradigm:** MLPerf is largely designed for the centralized evaluation of hardware and software stacks. It does not adequately measure the complex, communication-intensive capabilities of truly distributed AI services, such as those envisioned for 6G networks or federated learning environments \\[78\\].\n\n**Slow Model Updates:** While MLPerf does update its suite, concerns remain about the frequency of model updates. The rapid pace of algorithmic improvement means benchmarks can quickly become outdated, failing to capture the performance of newer, more efficient models \\[76\\].\n\nThese limitations indicate a growing gap between what current standard benchmarks measure and what cutting-edge AI systems are being designed to do.\n\n**2.2. The Latency Chasm: A Core Challenge for Benchmark Design**\n\nThe stark difference in latency requirements between scale-up and scale-out systems presents a major challenge for benchmark design. A benchmark created to test a scale-out system might have a latency target in the milliseconds for its server scenario (e.g., 15ms for ResNet50 in MLPerf Inference) \\[305\\]\\[309\\]. Applying this benchmark to a scale-up system designed for sub-microsecond latency would be meaningless; the system would pass easily without its core capability—ultra-low latency communication—ever being tested.\n\nConversely, designing a benchmark around the sub-microsecond requirements of a scale-up system would be impossible for a scale-out architecture to pass. This is not because the scale-out system is \"worse,\" but because it is optimized for a different set of trade-offs (cost, flexibility, resilience over vast numbers of nodes).\n\nTherefore, we expect the emergence of benchmarks with specifically modified parameters to address this chasm. For scale-up systems, benchmarks must be designed to measure performance on tasks involving massive, high-frequency data exchange between tightly coupled processors, where even nanoseconds of latency impact overall performance \\[124\\]\\[127\\]. For scale-out systems, benchmarks must focus on how efficiently the system performs with network communication as a primary variable, evaluating performance as the number of nodes increases and measuring resilience to node failure \\[152\\].\n\n**3\\. The Ascendancy of the Application Layer: Measuring What Matters**\n\nWhile infrastructure performance is a critical enabler, the industry is rightly shifting its gaze towards the end goal: the value and reliability of the AI application itself. An AI model that trains quickly on a massive cluster but provides inaccurate, biased, or unsafe outputs in production is a failure. This has led to the emergence of a rich and varied set of application-layer metrics.\n\n**3.1. A New Lexicon for Evaluation**\n\nBeyond traditional metrics like accuracy, precision, and recall, a new set of evaluation criteria is becoming critical for production AI systems:\n\n**Observability and Reliability:** In production, AI systems generate vast quantities of logs, metrics, and traces. Evaluating a system's observability—the ability to monitor its internal state and performance—is crucial for ensuring reliability and trustworthiness \\[42\\]\\[59\\].\n\n**Agentic System Success:** For advanced AI agents, evaluation must move beyond linguistic correctness. Key metrics include task success rate, adaptability to new information, context awareness, and overall human satisfaction with the agent's performance \\[44\\]. Frameworks are now including evaluators for relevance, coherence, and safety in complex agent workflows \\[46\\].\n\n**Business and Operational KPIs:** The ultimate test of an AI system in an enterprise context is its impact on the bottom line. This includes operational KPIs like inference latency, throughput, and resource consumption \\[52\\], as well as business value metrics like increased customer acquisition, higher developer satisfaction, and feature adoption rates \\[53\\]\\[55\\].\n\n**Multimodality and Ethics:** For multimodal systems that process text, images, and audio, new metrics are needed to score cross-modal retrieval accuracy and modality alignment \\[56\\]. Critically, ethical metrics measuring fairness, bias, trustworthiness, and explainability are becoming mandatory components of any comprehensive evaluation \\[50\\]\\[51\\].\n\n**3.2. How Infrastructure Choices Cascade to the Application Layer**\n\nThe choice between a scale-up and scale-out architecture directly influences these application-layer metrics, though the provided research frustratingly lacks peer-reviewed empirical studies showing statistically significant differences in production environments (Query results for this were negative). However, we can infer the likely impacts based on the architectural properties:\n\n**Latency and Throughput:** A scale-up deployment, with its lower internal latency, would likely excel on application-layer latency metrics for real-time, single-user tasks. In contrast, a scale-out deployment, with its higher number of cores and aggregate memory bandwidth, is often better for CPU-intensive tasks and would likely show superior application-level throughput for massively parallel workloads \\[146\\]. The network communication overhead in scale-out systems is a key performance factor that directly impacts application latency as the system scales \\[152\\].\n\n**Reliability and Fault Tolerance:** Scale-out systems are inherently designed with fault tolerance in mind; the failure of one node does not bring down the entire system \\[148\\]. This would translate to higher application-layer reliability and availability metrics. Scale-up systems represent a single point of failure and require different, potentially more costly mechanisms like checkpointing to achieve similar levels of reliability \\[148\\].\n\n**Task-Specific Performance:** The choice of infrastructure can determine success for specific types of applications. For example, a study found that shuffle-intensive data processing tasks perform better in scale-up configurations due to fast intermediate storage and no network bottleneck, while CPU-intensive tasks perform better in scale-out \\[146\\]\\[208\\]. This would directly affect the performance and cost-efficiency metrics of applications with these characteristics.\n\nThe evidence suggests that while we can infer these differences, the industry lacks rigorous, comparative studies. This is a significant gap in our collective understanding and an area ripe for future research.\n\n**4\\. A Dual-Track Future for AI Evaluation**\n\nThe evidence strongly supports a future where AI evaluation proceeds along two parallel, interconnected tracks: one for specialized infrastructure benchmarking and another for holistic application-layer assessment.\n\n**4.1. Evolving Infrastructure Benchmarks**\n\nWe are already seeing the emergence of new benchmarks that move beyond the scope of traditional frameworks. As of 2025, benchmarks like **FrontierMath** and **CyberGym** are challenging the AI community to evaluate models not just on speed, but on sophisticated cognitive capabilities like advanced mathematical reasoning and cybersecurity proficiency \\[113\\]\\[117\\]. FrontierMath, developed by Epoch AI and a consortium of mathematicians, specifically targets problems that require deep reasoning rather than pattern matching, and top models in 2025 are still struggling to achieve even single-digit success rates \\[113\\]\\[400\\]\\[400\\].\n\nWhile the provided research lacks documented case studies of Fortune 500 companies implementing these specific new benchmarks for _distributed training validation_ (Query results for this were negative), their existence points to a clear trend: benchmarking is becoming more specialized and capability-focused. Future infrastructure benchmarks will need to be explicitly designed for either scale-up or scale-out paradigms, with latency targets, communication patterns, and failure modes that reflect the realities of each architecture.\n\n**4.2. Prioritizing the Application Layer**\n\nUltimately, infrastructure is a means to an end. The most important questions about an AI system are answered at the application layer. Does the AI-powered legal research tool find the correct precedent \\[230\\]? Does the generative AI model in manufacturing produce a viable new design \\[52\\]? Is the customer service chatbot successfully resolving user issues?\n\nAnswering these questions requires moving beyond standardized tests to embrace a more holistic evaluation framework that includes:\n\n**Human-in-the-loop evaluation:** For generative and agentic AI, qualitative human assessment is indispensable \\[47\\].\n\n**Real-world task simulation:** Benchmarks must increasingly mirror the complexity and unpredictability of the production environment.\n\n**Business-specific KPIs:** The value of an AI system must be tied directly to the core metrics of the enterprise it serves \\[53\\].\n\n**5\\. Conclusion**\n\nThe debate over whether to focus on new infrastructure benchmarks or shift entirely to the application layer presents a false dichotomy. The maturation of the AI industry and the architectural divergence of its underlying hardware demand that we do both.\n\nThe profound technical differences between scale-up and scale-out systems, especially concerning network latency, render a one-size-fits-all approach to infrastructure benchmarking obsolete. We expect and need a new generation of specialized benchmarks capable of stress-testing the unique capabilities and bottlenecks of each paradigm.\n\nHowever, these infrastructure metrics are only part of the story. As AI models become autonomous agents and critical components of our economic and social fabric, we must prioritize evaluating their real-world performance, safety, and value. The focus on the application layer is not an alternative to infrastructure benchmarking but its necessary complement. A truly comprehensive evaluation framework will provide a clear line of sight from the nanoseconds of latency in a scale-up interconnect to the dollars of revenue generated by the application it powers. This dual-track approach is the only way to ensure we are building AI systems that are not only powerful but also effective, reliable, and trustworthy.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. P. Cochat, L. Vaucoret et al. “Et al.” Evidence Based Mental Health](https://doi.org/10.1136/ebmh.11.4.102)\n\n[2\\. Muthu Dayalan. “MapReduce: simplified data processing on large clusters.” Commun. ACM](https://doi.org/10.1145/1327452.1327492)\n\n[3\\. Position: AI Scaling: From Up to Down and Out](https://arxiv.org/pdf/2502.01677)\n\n[4\\. 通信行业深度报告:光通信:AI算力中心的神经网络-研究报告正文 _ 数据...](https://data.eastmoney.com/report/zw_industry.jshtml?infocode=AP202506271698551853)\n\n[5\\. AIGC为什么要区分Scale-out和Scale-up两张网络?](https://www.360doc.cn/article/3066843_1133808433.html)\n\n[6\\. Scaling AI to Generate Better and Different Outcomes](https://cisr.mit.edu/sites/default/files/2021-12/2021_1201_ScalingAI_WixomSomehGregory_AudioTranscript.docx)\n\n[7\\. Demystifying AI: Eight Key Terms You Need to Know - Ayar Labs](https://ayarlabs.com/blog/demystifying-ai-eight-key-terms-you-need-to-know/#:~:text=While%20scale-up%20networks%20are,manage%20longer-distance%20data%20transfers.)\n\n[8\\. 行业年度策略报告 增程车 2.0，AI 智驾 1.0](https://pdf.dfcfw.com/pdf/H3_AP202412121641273719_1.pdf?1734013634000.pdf)\n\n[9\\. 面向GPU算力纵向扩展的Scale-up网络技术研究](http://m.10jqka.com.cn/20250710/c669534852.shtml)\n\n[10\\. AIGC为什么要区分Scale-out和Scale-up两张网络？](http://www.360doc.com/content/24/0912/13/3066843_1133808433.shtml)\n\n[11\\. AI Model Deployment Explained: Tools & Best Practices](https://orq.ai/blog/ai-model-deployment)\n\n[12\\. \\[论文审查\\] Genetic Bottleneck and the Emergence of High ...](https://www.themoonlight.io/zh/review/genetic-bottleneck-and-the-emergence-of-high-intelligence-by-scaling-out-and-high-throughput)\n\n[13\\. AI Data Centers: Scaling Up and Scaling Out](https://www.aflhyperscale.com/wp-content/uploads/2024/12/AI-Data-Centers-Scaling-Up-and-Scaling-Out-White-Paper.pdf)\n\n[14\\. Prospecting for Performance: Data Center Networking in 2025](https://arrcus-admin.prod.unomena.io/media/documents/AvidThnik_NGI_2025_DataCenter_Networking_AI_Cloud.pdf)\n\n[15\\. AIGC为什么要区分Scale-out和Scale-up两张网络？ - 360Doc](http://www.360doc.com/content/24/0912/13/3066843_1133808433.shtml#:~:text=Scale-up%EF%BC%88%E5%90%91%E4%B8%8A/%E5%9E%82%E7%9B%B4%E6%89%A9%E5%B1%95%EF%BC%89%EF%BC%9A%E9%80%9A%E8%BF%87%E5%A2%9E%E5%8A%A0,%E7%B3%BB%E7%BB%9F%E6%9D%A5%E5%85%B1%E5%90%8C%E5%AE%8C%E6%88%90%E4%BB%BB%E5%8A%A1%E3%80%82)\n\n[16\\. SCALING UP VS. SCALING OUT IN A QLIKVIEW ENVIRONMENT](https://community.qlik.com/cyjdu72974/attachments/cyjdu72974/qlikview-management/13394/1/DS-Technical-Brief-Scaling-Up-vs-Scaling-Out-EN.pdf)\n\n[17\\. AI大模型浪潮下的GPU互连革命：UALink与ALS引领技术 ...](https://news.mydrivers.com/1/1012/1012354.htm)\n\n[18\\. 大模型扩展新维度:Scaling Down、Scaling Out|AI|悉尼大学...](https://k.sina.com.cn/article_3996876140_ee3b7d6c0270149g6.html)\n\n[21\\. New Benchmarks Measure AI Performance Across ...](https://www.thesoftwarereport.com/new-benchmarks-measure-ai-performance-across-hardware-systems/)\n\n[22\\. Top 10 MCP Server Tools and Software for Enhanced AI ...](https://superagi.com/top-10-mcp-server-tools-and-software-for-enhanced-ai-performance-in-2025/)\n\n[23\\. MLPerf Client Benchmark - MLCommons](https://mlcommons.org/benchmarks/client/)\n\n[24\\. New MLPerf Storage v1.0 Benchmark Results ... - MLCommons](https://mlcommons.org/2024/09/mlperf-storage-v1-0-benchmark-results/)\n\n[25\\. Understanding GPU Performance Using MLCommons MLPerf Benchmarks](https://hps.vi4io.org/_media/teaching/summer_term_2024/stud/nthpda/ossama-bin-raza-report.pdf)\n\n[26\\. MLPerf Inference on Cisco UCS C885A M8 HGX platform ...](https://www.cisco.com/c/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/mlperf-ucs-c885a-h100-h200-wp.html)\n\n[27\\. AI Inferencing at the Edge - NetApp with Lenovo ThinkSystem - Solution Design](https://lenovonetapp.com/pdf/ai-inferencing-at-the-edge-netapp-with-lenovo-thinksystem-solution-design.pdf)\n\n[28\\. Q1 FY26 Earnings Summary](https://s201.q4cdn.com/141608511/files/doc_financials/2026/q1/NVDA-F1Q26-Quarterly-Presentation-FINAL.pdf)\n\n[29\\. 영상인식 및 분류용 인공지능 가속기의 최신 성능평가: MLPerf를 중심으로](https://www.kibme.org/resources/journal/20200206100621546.pdf)\n\n[30\\. Leading AI Scalability Benchmarks with Microsoft Azure](https://signal65.com/wp-content/uploads/2024/11/Signal65-Leading-AI-Scalability-Benchmarks-Microsoft-Azure-v1.0.pdf)\n\n[31\\. MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning Systems from μWatts to MWatts for Sustainable AI](https://www.catalyzex.com/author/David%20Kanter)\n\n[32\\. How MLPerf Proves AI Client and Storage Performance](https://techfieldday.com/2025/how-mlperf-proves-ai-client-and-storage-performance/)\n\n[33\\. MLPerf Reveals Blackwell Gains, Untether's Power Efficiency](https://xpu.pub/2024/09/20/mlperf-4-1/)\n\n[34\\. What are the different MLPerf benchmarks ...](https://www.microcontrollertips.com/what-are-the-different-mlperf-benchmarks-from-mlcommons/)\n\n[35\\. Benchmarking Energy and Latency in TinyML: A Novel Method for Resource-Constrained AI](https://arxiv.org/pdf/2505.15622v1)\n\n[41\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[42\\. Comprehensive Evaluation Metrics for AI Observability](https://coralogix.com/ai-blog/evaluation-metrics-for-ai-observability/)\n\n[43\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[44\\. LLM-Powered AI Agent Systems and Their Applications in Industry](https://arxiv.org/pdf/2505.16120)\n\n[45\\. Mingyu Liang, Wenyin Fu et al. “Mystique: Enabling Accurate and Scalable Generation of Production AI Benchmarks.” Proceedings of the 50th Annual International Symposium on Computer Architecture](https://doi.org/10.1145/3579371.3589072)\n\n[46\\. Evaluating Agentic AI Systems: A Deep Dive into ...](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/evaluating-agentic-ai-systems-a-deep-dive-into-agentic-metrics/4403923)\n\n[47\\. Developing scalable quality assurance pipelines for AI systems: Leveraging LLMs in enterprise applications](https://journalwjarr.com/sites/default/files/fulltext_pdf/WJARR-2025-1268.pdf)\n\n[48\\. Rudolf Hoffmann, Christoph Reich. “A Systematic Literature Review on Artificial Intelligence and Explainable Artificial Intelligence for Visual Quality Assurance in Manufacturing.” Electronics](https://doi.org/10.3390/electronics12224572)\n\n[49\\. Mingyu Liang, Wenyin Fu et al. “Mystique: Accurate and Scalable Production AI Benchmarks Generation.” ArXiv](https://doi.org/10.48550/arXiv.2301.04122)\n\n[50\\. AI and the Future of Skills, Volume 2: Methods for Evaluating AI Capabilities](https://www.oecd.org/content/dam/oecd/en/publications/reports/2023/11/ai-and-the-future-of-skills-volume-2_a3986583/a9fe53cb-en.pdf)\n\n[51\\. AI Performance Metrics: The Science & Art of Measuring AI](https://www.version1.com/en-us/en-ushttps:/www.version1.com/blog/ai-performance-metrics-the-science-and-art-of-measuring-ai/)\n\n[52\\. AI Models for Manufacturing KPI Benchmarking](https://querio.ai/articles/ai-models-for-manufacturing-kpi-benchmarking)\n\n[53\\. Through the Looking Glass of AI - Project Management in Wonderland](https://pmworldlibrary.net/wp-content/uploads/2024/07/pmwj143-Jul2024-Veera-through-the-looking-glass-of-AI.pdf)\n\n[54\\. What Metrics Should We Use To Measure Commercial AI?](https://sigai.acm.org/static/aimatters/5-2/AIMatters-5-2-09-Hughes.pdf)\n\n[55\\. Engineering in the Age of AI: Leveraging Copilot for Enhanced Software Development](https://eajournals.org/ijeats/wp-content/uploads/sites/55/2025/04/Engineering-in-the-Age-of-AI.pdf)\n\n[56\\. What are some common evaluation metrics for multimodal AI?](https://milvus.io/ai-quick-reference/what-are-some-common-evaluation-metrics-for-multimodal-ai#:~:text=Three%20key%20categories%20of%20metrics,performs%20in%20real-world%20scenarios.)\n\n[57\\. Monitoring Machine Learning Systems from the Point of View of AI Ethics](https://ceur-ws.org/Vol-3901/paper_2.pdf)\n\n[58\\. Global Approaches to Auditing Artificial Intelligence: A Literature Review](https://cdn.prod.website-files.com/643ecb10be528d2c1da863cb/66d90fcff02e88fae7266ef5_SR2024.1%20-%20FINAL.pdf)\n\n[59\\. How Siemens Approaches AI Lifecycle Management in Production](https://assets.new.siemens.com/siemens/assets/api/uuid:12ddaadc-f20b-4dfa-9146-bfac61d955e4/ARC-View-Siemens-AI-final-draft_original.pdf)\n\n[61\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[62\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[63\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[64\\. MLPerf Inference v1.0: 2000 Suite Results, New Power Measurements](https://www.anandtech.com/show/16632/mlperf-inference-v10-2000-suite-results-new-power-measurements)\n\n[65\\. ECS Strategic Research and Innovation Agenda 2024](https://ecssria.eu/ECS-SRIA-2024.pdf)\n\n[66\\. 关于衡量AI算力的“FLOPS”](https://www.17aiot.com/ai/45516.html)\n\n[67\\. Zhixiang Ren, Yongheng Liu et al. “AIPerf: Automated machine learning as an AI-HPC benchmark.” ArXiv](https://doi.org/10.26599/BDMA.2021.9020004)\n\n[68\\. MLPerf Training Benchmark](http://www.arxiv.org/pdf/1910.01500v2)\n\n[69\\. Snehil Verma, Qinzhe Wu et al. “Demystifying the MLPerf Benchmark Suite.” ArXiv](https://arxiv.org/abs/1908.09207)\n\n[70\\. MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems](https://arxiv.org/pdf/2412.07067)\n\n[71\\. AI性能基准测试：中国标准的崛起](https://www.airbq.com/2020/11/ai-performance-benchmarks-have-since.html)\n\n[72\\. MLPerf Tiny Benchmark](https://openreview.net/pdf?id=8RxxwAut1BI)\n\n[73\\. MLPerf Inference Benchmark](http://www.cs.utoronto.ca/~pekhimenko/Papers/MLPerf_Inference-Arxiv.pdf)\n\n[74\\. 新的MLPerf Storage v1.0 基准测试结果表明 - 中国存储网](https://www.chinastor.cn/hpc-top500/172733709131581.html)\n\n[75\\. Sustainable AI: Experiences, Challenges & Recommendations](https://conferences.computer.org/sc-wpub/pdfs/SC-W2024-6oZmigAQfgJ1GhPL0yE3pS/555400b805/555400b805.pdf)\n\n[76\\. AI基准测试MLPerf模型少、更新慢,地平线提出的MAPS会更好吗?](https://www.leiphone.com/category/chips/OZU1heTuCKm87GW7.html)\n\n[77\\. Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence](https://arxiv.org/pdf/2402.09880)\n\n[78\\. Communications of HUAWEI RESEARCH](https://www-file.huawei.com/-/media/corp2020/pdf/publications/huawei-research/2024/huawei-research-issue7-en.pdf)\n\n[79\\. Llama 2 70B: An MLPerf Inference Benchmark for Large Language Models](https://mlcommons.org/2024/03/mlperf-llama2-70b/)\n\n[80\\. Benchmark MLPerf Training | MLCommons Version 2.0 Results](https://mlcommons.org/benchmarks/training/)\n\n[81\\. Muthu Dayalan. “MapReduce: simplified data processing on large clusters.” Commun. ACM](https://doi.org/10.1145/1327452.1327492)\n\n[82\\. Prospecting for Performance: Data Center Networking in 2025](https://arrcus-admin.prod.unomena.io/media/documents/AvidThnik_NGI_2025_DataCenter_Networking_AI_Cloud.pdf)\n\n[83\\. Scale-up vs Scale-out for Hadoop: Time to rethink?](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/a20-appuswamy.pdf)\n\n[84\\. Benjamin Hindman, A. Konwinski et al. “Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center.” Symposium on Networked Systems Design and Implementation](https://www.semanticscholar.org/paper/3e375c8d996b8c46c83b9d3f72732d7477e5fd86)\n\n[85\\. Scaling Up AI/ML with Kubernetes](https://pages.run.ai/hubfs/PDFs/Scaling-Up-AI-ML-with-Kubernetes.pdf)\n\n[86\\. Yanpei Chen, S. Alspaugh et al. “Interactive Analytical Processing in Big Data Systems: A Cross-Industry Study of MapReduce Workloads.” ArXiv](https://doi.org/10.14778/2367502.2367519)\n\n[87\\. Differences in scale-up vs. scale-out storage](https://www.techtarget.com/searchstorage/tip/Differences-in-scale-up-vs-scale-out-storage)\n\n[88\\. Demystifying AI: Eight Key Terms You Need to Know - Ayar Labs](https://ayarlabs.com/blog/demystifying-ai-eight-key-terms-you-need-to-know/#:~:text=While%20scale-up%20networks%20are,manage%20longer-distance%20data%20transfers.)\n\n[89\\. 光通信：AI算力中心的神经网络 - A股-研报详情- 新浪](https://stock.finance.sina.com.cn/stock/view/paper.php?symbol=sh000001&reportid=804433965423)\n\n[90\\. Yanpei Chen, A. Ganapathi et al. “The Case for Evaluating MapReduce Performance Using Workload Suites.” 2011 IEEE 19th Annual International Symposium on Modelling, Analysis, and Simulation of Computer and Telecommunication Systems](https://doi.org/10.1109/MASCOTS.2011.12)\n\n[91\\. 2023 Global Trends in AI Report](https://www.weka.io/wp-content/uploads/files/resources/2023/08/2023-Global-Trends-AI-Report.pdf)\n\n[92\\. AI and Analytics at Scale: Lessons from Real-World Production Systems](https://www.brighttalk.com/webcast/12641/513919?utm_source=brighttalk-portal&utm_medium=web&utm_campaign=channel-page&utm_content=recorded)\n\n[93\\. Scale-Up vs. Scale-Out Storage: Tips to Consider](https://stackifydev.showmeproject.com/scale-up-vs-scale-out-storage-tips-to-consider/)\n\n[94\\. Scaling AI with Graphcore and Pure Storage](https://www.purestorage.com/content/dam/pdf/en/reference-architectures/ra-artificial-intelligence-graphcore-flashblade.pdf)\n\n[95\\. Khaled Elmeleegy. “Piranha: Optimizing Short Jobs in Hadoop.” Proc. VLDB Endow.](https://doi.org/10.14778/2536222.2536225)\n\n[96\\. Scaling AI Infrastructure: The Role of Optical I/O Technology](https://ayarlabs.com/blog/ai-and-optical-io/)\n\n[97\\. Developing scalable quality assurance pipelines for AI systems: Leveraging LLMs in enterprise applications](https://journalwjarr.com/sites/default/files/fulltext_pdf/WJARR-2025-1268.pdf)\n\n[98\\. AI and the Need for Purpose-Built Cloud Infrastructure](https://azure.microsoft.com/fr-fr/blog/ai-and-the-need-for-purposebuilt-cloud-infrastructure/)\n\n[99\\. Hao Yu, J. Moreira et al. “Performance Studies of a WebSphere Application, Trade, in Scale-out and Scale-up Environments.” 2007 IEEE International Parallel and Distributed Processing Symposium](https://doi.org/10.1109/IPDPS.2007.370632)\n\n[101\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[102\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[103\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[104\\. The 2025 Edge AI Technology Report](https://www.ceva-ip.com/wp-content/uploads/2025-Edge-AI-Technology-Report.pdf)\n\n[105\\. AI-Compass LLM训练框架生态：整合ms-swift、Unsloth](https://www.cnblogs.com/ting1/p/18994652)\n\n[106\\. Deep Learning Benchmarks 2025](https://www.byteplus.com/en/topic/466080)\n\n[107\\. 信息技术产业行业研究](https://pdf.dfcfw.com/pdf/H3_AP202504171657949059_1.pdf?1744915239000.pdf)\n\n[108\\. HiPEAC Vision 2025: High Performance, Edge and Cloud Computing](https://vision.hipeac.net/pdf/hipeac-vision-2025--articles.pdf)\n\n[109\\. 2024年中国人工智能产业研究报告](http://www.csiajpw.com/UserFiles/Article/file/6387919622965470286850981.pdf)\n\n[110\\. PAI3: Artificial Intelligence that Belongs to the People](https://files.gitbook.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FAcwxwOlsxqPn3KqGT1FU%2Fuploads%2F8oCEc9WCYlAXBpfV2s9K%2FPAI3-%20%20Artificial%20Intelligence%20that%20Belongs%20to%20the%20People%20-%20White%20paper.pdf?alt=media&token=816a7093-15a0-4d)\n\n[111\\. Yanping Huang, Yonglong Cheng et al. “GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/d79a26226393f687ddbc375e32055b40b8ad8d38)\n\n[112\\. Alexander Sergeev, Mike Del Balso. “Horovod: fast and easy distributed deep learning in TensorFlow.” ArXiv](https://arxiv.org/abs/1802.05799)\n\n[113\\. 2025年人工智能指数报告](https://pdf.dfcfw.com/pdf/H3_AP202506201694220072_1.pdf?1750413587000.pdf)\n\n[114\\. Foteini Strati, Zhendong Zhang et al. “Sailor: Automating Distributed Training over Dynamic, Heterogeneous, and Geo-distributed Clusters.”](https://arxiv.org/abs/2504.17096)\n\n[115\\. Integrated AI: The sky is delivering (mid-2025 AI ...](https://lifearchitect.ai/the-sky-is-delivering/)\n\n[116\\. 2025 THEMATIC OUTLOOK: AI and geopolitics forge new paths](https://img.cbinews.com/attachment/2025/01/14/LcfpnDXt8FbLoWztVVc74Z4Tyr4j2v8r7r7BusNO.pdf)\n\n[117\\. CyberGym: Evaluating AI Agents’ Cybersecurity Capabilities with Real-World Vulnerabilities at Scale](https://arxiv.org/pdf/2506.02548)\n\n[118\\. Benchmarking the Performance and Energy Efficiency of AI Accelerators for AI Training](https://arxiv.org/pdf/1909.06842v6)\n\n[119\\. A STANDARD FRAMEWORK FOR AI-DRIVEN OPTIMIZATIONS IN VARIOUS COMPLEX DOMAINS](https://informs-sim.org/wsc24papers/inv168.pdf)\n\n[120\\. Bo Wu, Sid Wang et al. “LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin.”](https://arxiv.org/abs/2505.24034)\n\n[121\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[122\\. Muthu Dayalan. “MapReduce: simplified data processing on large clusters.” Commun. ACM](https://doi.org/10.1145/1327452.1327492)\n\n[123\\. 最近很火的“超节点”，到底是干啥的？](https://www.methsoft.ac.cn/content/software/2025/%E6%9C%80%E8%BF%91%E5%BE%88%E7%81%AB%E7%9A%84%E2%80%99%E8%B6%85%E8%8A%82%E7%82%B9%E2%80%98%EF%BC%8C%E5%88%B0%E5%BA%95%E6%98%AF%E5%B9%B2%E5%95%A5%E7%9A%84%EF%BC%9F.pdf)\n\n[124\\. Rack is the Unit of AI](https://auradine.com/rack-is-the-unit-of-ai/)\n\n[125\\. AI算力跟踪深度：辨析Scale Out与Scale Up——AEC在光铜互联夹缝中挤出市场的What、Why、How](https://file.iyanbao.com/pdf/82229-201c07f9-6f13-498b-9668-51acba99f913.pdf)\n\n[126\\. AI Data Centers: Scaling Up and Scaling Out](https://www.aflhyperscale.com/wp-content/uploads/2024/12/AI-Data-Centers-Scaling-Up-and-Scaling-Out-White-Paper.pdf)\n\n[127\\. AIGC为什么要区分Scale-out和Scale-up两张网络?](https://www.360doc.cn/article/3066843_1133808433.html)\n\n[128\\. 通信行业深度报告:光通信:AI算力中心的神经网络-研究报告正文 _ 数据...](https://data.eastmoney.com/report/zw_industry.jshtml?infocode=AP202506271698551853)\n\n[129\\. 智算中心组网技术及应用 Intelligent Computing Center Networking Technology and Applications](https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-cn/mediares/magazine/publication/com_cn/article/202502/9.pdf)\n\n[130\\. Networking Design for HPC and AI on IBM Power Systems](https://www.redbooks.ibm.com/redpapers/epubs/redp5478.epub)\n\n[131\\. Demystifying AI: Eight Key Terms You Need to Know - Ayar Labs](https://ayarlabs.com/blog/demystifying-ai-eight-key-terms-you-need-to-know/#:~:text=While%20scale-up%20networks%20are,manage%20longer-distance%20data%20transfers.)\n\n[132\\. Update on Recent Developments in Industry](https://mentor.ieee.org/802.1/dcn/25/1-25-0001-01-ICne-update-on-recent-developments-in-industry.pdf)\n\n[133\\. AIGC为什么要区分Scale-out和Scale-up两张网络？](http://www.360doc.com/content/24/0912/13/3066843_1133808433.shtml)\n\n[134\\. 从GTC到OFC(22):面向机器学习的光网络](http://www.c-fol.net/news/7_202505/20250509185345.html)\n\n[135\\. 最近很火的“超节点”,到底是干啥的?\\_Scale_Out_时延](http://healthnews.sohu.com/a/891297022_121948416)\n\n[136\\. Prospecting for Performance: Data Center Networking in 2025](https://arrcus-admin.prod.unomena.io/media/documents/AvidThnik_NGI_2025_DataCenter_Networking_AI_Cloud.pdf)\n\n[137\\. L. Barroso, U. Hölzle. “The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines.” Synthesis Lectures on Computer Architecture](https://doi.org/10.2200/S00193ED1V01Y200905CAC006)\n\n[138\\. Navigating Power Challenges in the Data Centre Era](https://cdn.digitalisationworld.com/uploads/pdfs/5cdf512e3e874ca92326ec64c273a1d572925dd3dbc072ab.pdf)\n\n[139\\. Scale-Out Networking in the Data Center](https://cseweb.ucsd.edu/~vahdat/papers/scale-out-micro10.pdf)\n\n[141\\. Muthu Dayalan. “MapReduce: simplified data processing on large clusters.” Commun. ACM](https://doi.org/10.1145/1327452.1327492)\n\n[142\\. Brian F. Cooper, Adam Silberstein et al. “Benchmarking cloud serving systems with YCSB.” ACM Symposium on Cloud Computing](https://doi.org/10.1145/1807128.1807152)\n\n[143\\. 有关AI,你应该区别的观点与事实,那事实又是什么呢? - 哔哩哔哩](https://www.bilibili.com/opus/1085593876493762585)\n\n[144\\. Lei Wang, Jianfeng Zhan et al. “BigDataBench: A big data benchmark suite from internet services.” 2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA)](https://doi.org/10.1109/HPCA.2014.6835958)\n\n[145\\. Yanpei Chen, S. Alspaugh et al. “Interactive Analytical Processing in Big Data Systems: A Cross-Industry Study of MapReduce Workloads.” ArXiv](https://doi.org/10.14778/2367502.2367519)\n\n[146\\. Scale-up vs Scale-out for Hadoop: Time to rethink?](http://rowstron.azurewebsites.net/Publications/mem_socc13.pdf)\n\n[147\\. Yanpei Chen, A. Ganapathi et al. “The Case for Evaluating MapReduce Performance Using Workload Suites.” 2011 IEEE 19th Annual International Symposium on Modelling, Analysis, and Simulation of Computer and Telecommunication Systems](https://doi.org/10.1109/MASCOTS.2011.12)\n\n[148\\. A Framework for an In-depth Comparison of Scale-up and Scale-out](http://alumni.soe.ucsc.edu/~msevilla/papers/sevilla-discs13.pdf)\n\n[149\\. Scale-up and Scale-out - spork](https://www.cnblogs.com/spork/archive/2009/12/29/1634766.html)\n\n[150\\. Scale-up vs Scale-out for Hadoop Time to think?](https://courses.grainger.illinois.edu/cs525/sp2014/Scale-up%20vs%20Scale-out%20for%20Hadoop.pptx)\n\n[151\\. 光通信：AI算力中心的神经网络 - A股-研报详情- 新浪](https://stock.finance.sina.com.cn/stock/view/paper.php?symbol=sh000001&reportid=804433965423)\n\n[152\\. Mystique: Accurate and Scalable Production AI Benchmarks Generation](https://arxiv.org/pdf/2301.04122v2)\n\n[153\\. New Study Reveals Generative AI Has Eclipsed Other AI Applications In ...](https://www.weka.io/company/weka-newsroom/press-releases/global-trends-in-ai-2024/)\n\n[154\\. Scaling AI to Generate Better and Different Outcomes](https://cisr.mit.edu/sites/default/files/2021-12/2021_1201_ScalingAI_WixomSomehGregory_AudioTranscript.docx)\n\n[155\\. Demystifying AI: Eight Key Terms You Need to Know - Ayar Labs](https://ayarlabs.com/blog/demystifying-ai-eight-key-terms-you-need-to-know/#:~:text=While%20scale-up%20networks%20are,manage%20longer-distance%20data%20transfers.)\n\n[156\\. Five Key Insights from Strategy Building to Scaled Operations](https://www.panewslab.com/en/articles/8t0909zj)\n\n[157\\. POWER9 Scale-Out & Scale-Up Performance Review v18b](https://public.dhe.ibm.com/systems/power/community/aix/PowerVM_webinars/79_POWER9_Performance_Review.pdf)\n\n[161\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[162\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[163\\. 信息技术产业行业研究](https://pdf.dfcfw.com/pdf/H3_AP202504171657949059_1.pdf?1744915239000.pdf)\n\n[164\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[165\\. The 2025 AI-Driven Demand Generation Benchmark Report](https://lead-spot.net/research/the-2025-ai-driven-demand-generation-benchmark-report/)\n\n[166\\. HiPEAC Vision 2025: High Performance, Edge and Cloud Computing](https://vision.hipeac.net/pdf/hipeac-vision-2025.pdf)\n\n[167\\. Top 15 AI DLP Best Practices with Case Studies in 2025](https://research.aimultiple.com/ai-dlp/)\n\n[168\\. 李飞飞团队2025AI指数报告:AI和计算机科学教育普及程度仍不够](https://xueqiu.com/7423950559/330924320)\n\n[169\\. Deepseek to Qwen: Top AI models released in 2025 - Digit](https://www.digit.in/features/general/deepseek-to-qwen-top-ai-models-released-in-2025.html)\n\n[170\\. 重组凝血因子 VIII 行业 DeepSeek 企业智能化转型全链路指南：技术驱动与业务创新实践研究报告(2025 版)](https://doc.51baogao.cn/20250315/2857351/%E9%87%8D%E7%BB%84%E5%87%9D%E8%A1%80%E5%9B%A0%E5%AD%90VIII%E8%A1%8C%E4%B8%9ADeepSeek%E4%BC%81%E4%B8%9A%E6%99%BA%E8%83%BD%E5%8C%96%E8%BD%AC%E5%9E%8B%E5%85%A8%E9%93%BE%E8%B7%AF%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8A%80%E6%9C%AF%E9%A9%B1%E5%8A%A8%E4%B8%8E%E4%B8%9A%E5%8A%A1%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5%E7%A0%94%E7%A9%B6%E6%8A%A5%E5%91%8A%282025%E7%89%88%29.docx)\n\n[171\\. J. Dean, G. Corrado et al. “Large Scale Distributed Deep Networks.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/3127190433230b3dc1abd0680bb58dced4bcd90e)\n\n[172\\. Driving AI innovation: The power of collaboration](https://www.ai.se/sites/default/files/2025-04/ai-labs-driving-ai-innovation-april-2205.pdf)\n\n[173\\. 2025 AI 赋能教育行业发展趋势报告](https://pdf.dfcfw.com/pdf/H3_AP202506201694183506_1.pdf?1750407945000.pdf)\n\n[174\\. Leveraging Kubernetes for AI/ML Workloads: Case studies in autonomous driving and large language model infrastructure](https://journalwjaets.com/sites/default/files/fulltext_pdf/WJAETS-2025-0320.pdf)\n\n[175\\. Best AI Tools for Soft Skills Training in 2025 - Disco.co](https://www.disco.co/blog/ai-tools-for-soft-skills-training)\n\n[176\\. 2025 \"人工智能+\" 行业发展蓝皮书](http://www.sccio.cn/uploads/20250522/8696496143e2c9e662e2e45890c9c1b4.pdf)\n\n[177\\. 【人工智能】HAI发布2025年AI指数报告简评](https://meta-quantum.today/?p=7701)\n\n[178\\. Alexander Sergeev, Mike Del Balso. “Horovod: fast and easy distributed deep learning in TensorFlow.” ArXiv](https://arxiv.org/abs/1802.05799)\n\n[179\\. 抗氧剂 636 行业 DeepSeek 企业智能化转型全链路指南：技术驱动与业务创新实践研究报告(2025 版)](https://doc.51baogao.cn/20250314/2710368/%E6%8A%97%E6%B0%A7%E5%89%82636%E8%A1%8C%E4%B8%9ADeepSeek%E4%BC%81%E4%B8%9A%E6%99%BA%E8%83%BD%E5%8C%96%E8%BD%AC%E5%9E%8B%E5%85%A8%E9%93%BE%E8%B7%AF%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8A%80%E6%9C%AF%E9%A9%B1%E5%8A%A8%E4%B8%8E%E4%B8%9A%E5%8A%A1%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5%E7%A0%94%E7%A9%B6%E6%8A%A5%E5%91%8A%282025%E7%89%88%29.docx)\n\n[180\\. Yicheng Feng, Yuetao Chen et al. “Echo: Simulating Distributed Training At Scale.”](https://arxiv.org/abs/2412.12487)\n\n[181\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[182\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[183\\. Brian F. Cooper, Adam Silberstein et al. “Benchmarking cloud serving systems with YCSB.” ACM Symposium on Cloud Computing](https://doi.org/10.1145/1807128.1807152)\n\n[184\\. AI-Driven Design Systems: The Future of Scalable UI Frameworks](https://eajournals.org/ejcsit/wp-content/uploads/sites/21/2025/05/AI-Driven-Design-Systems.pdf)\n\n[185\\. Robert Adolf, Saketh Rama et al. “Fathom: reference workloads for modern deep learning methods.” 2016 IEEE International Symposium on Workload Characterization (IISWC)](https://doi.org/10.1109/IISWC.2016.7581275)\n\n[186\\. Cody A. Coleman, D. Narayanan et al. “DAWNBench : An End-to-End Deep Learning Benchmark and Competition.”](https://www.semanticscholar.org/paper/b245959da6bdaa0b711341844aeaa473b7706453)\n\n[187\\. AI Data Centers: Scaling Up and Scaling Out](https://www.aflhyperscale.com/wp-content/uploads/2024/12/AI-Data-Centers-Scaling-Up-and-Scaling-Out-White-Paper.pdf)\n\n[188\\. Conquer AI performance, cost, and scale with AWS AI chips](https://reinvent.awsevents.com/content/dam/reinvent/2024/slides/cmp/CMP209_Conquer-AI-performance-cost-and-scale-with-AWS-AI-chips.pdf)\n\n[189\\. Mystique: Accurate and Scalable Production AI Benchmarks Generation](https://arxiv.org/pdf/2301.04122v2)\n\n[190\\. AI Scale-Up and Memory Disaggregation: Two Use Cases ...](https://ayarlabs.com/blog/ai-scale-up-and-memory-disaggregation-two-use-cases-enabled-by-ucie-and-optical-io/)\n\n[191\\. Scale-up vs scale-out for Hadoop: time to rethink?](https://dl.acm.org/doi/epdf/10.1145/2523616.2523629)\n\n[192\\. Intel® Hyperflex™ Architecture High-Performance Design Handbook](https://cdrdv2-public.intel.com/795075/ug-683353-795075.pdf)\n\n[193\\. Wanling Gao, Jianfeng Zhan et al. “BigDataBench: A Scalable and Unified Big Data and AI Benchmark Suite.” arXiv: Distributed, Parallel, and Cluster Computing](https://arxiv.org/abs/1802.08254)\n\n[194\\. Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI '20)](https://www.usenix.org/sites/default/files/osdi20-full_proceedings_interior.pdf)\n\n[195\\. Scale up Vs. Scale out in Cloud Storage and Graph Processing Systems](https://lexu1.web.engr.illinois.edu/scaleOutUp.pdf)\n\n[196\\. SUPERMICRO WITH GAUDI 3 AI DELIVERS SCALABLE PERFORMANCE FOR AI REQUIREMENTS](https://www.supermicro.com/white_paper/white_paper_Gaudi3_Reference_Archtecture.pdf)\n\n[197\\. Latency, Cost & Accuracy: Picking AI Models for Leads](https://www.jeeva.ai/blog/latency-cost-accuracy-pick-ai-model-real-time-lead-enrichment)\n\n[198\\. M. Naumov, John Kim et al. “Deep Learning Training in Facebook Data Centers: Design of Scale-up and Scale-out Systems.” ArXiv](https://arxiv.org/abs/2003.09518)\n\n[199\\. Navigating the Challenges of AI Infrastructure Design: Balancing Power, Latency, Reliability, and Data Requirements](https://www.f5.com/es_es/resources/white-papers/overcoming-ai-infrastructure-challenges-balancing-power-latency-reliability-and-data-requirements)\n\n[201\\. Muthu Dayalan. “MapReduce: simplified data processing on large clusters.” Commun. ACM](https://doi.org/10.1145/1327452.1327492)\n\n[202\\. Joy Buolamwini, Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” FAT](https://www.semanticscholar.org/paper/18858cc936947fc96b5c06bbe3c6c2faa5614540)\n\n[203\\. Aapo Kyrola, G. Blelloch et al. “GraphChi: Large-Scale Graph Computation on Just a PC.” USENIX Symposium on Operating Systems Design and Implementation](https://www.semanticscholar.org/paper/b6d3df4881f8c211ac5a6449b1fd9bb26f93f76b)\n\n[204\\. M. Ferdman, Almutaz Adileh et al. “Clearing the clouds: a study of emerging scale-out workloads on modern hardware.” ASPLOS XVII](https://doi.org/10.1145/2150976.2150982)\n\n[205\\. 光通信：AI算力中心的神经网络 - A股-研报详情- 新浪](https://stock.finance.sina.com.cn/stock/view/paper.php?symbol=sh000001&reportid=804433965423)\n\n[206\\. Shengsheng Huang, Jie Huang et al. “The HiBench benchmark suite: Characterization of the MapReduce-based data analysis.” 2010 IEEE 26th International Conference on Data Engineering Workshops (ICDEW 2010)](https://doi.org/10.1109/ICDEW.2010.5452747)\n\n[207\\. MLOps to scale AI | McKinsey](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/mlops-so-ai-can-scale)\n\n[208\\. Scale-up vs Scale-out for Hadoop: Time to rethink?](http://rowstron.azurewebsites.net/Publications/mem_socc13.pdf)\n\n[209\\. 不过AI运算的特点,scaleup域的重要性显著超过scaleout,这...](https://xueqiu.com/7246475498/318018529)\n\n[210\\. A Framework for an In-depth Comparison of Scale-up and Scale-out](http://alumni.soe.ucsc.edu/~msevilla/papers/sevilla-discs13.pdf)\n\n[211\\. Mystique: Accurate and Scalable Production AI Benchmarks Generation](https://arxiv.org/pdf/2301.04122v2)\n\n[212\\. Hao Yu, J. Moreira et al. “Performance Studies of a WebSphere Application, Trade, in Scale-out and Scale-up Environments.” 2007 IEEE International Parallel and Distributed Processing Symposium](https://doi.org/10.1109/IPDPS.2007.370632)\n\n[213\\. Scaling AI to Generate Better and Different Outcomes](https://cisr.mit.edu/sites/default/files/2021-12/2021_1201_ScalingAI_WixomSomehGregory_AudioTranscript.docx)\n\n[214\\. The AI-powered enterprise: Unlocking the potential of AI at scale](https://www.capgemini.com/wp-content/uploads/2021/02/State-of-AI_Report_Web-2.pdf)\n\n[215\\. Scaling AI in Manufacturing Operations: A Practitioners' Perspective](https://www.capgemini.com/gb-en/wp-content/uploads/sites/3/2019/12/Report-%E2%80%93-AI-in-MfG-Ops.pdf)\n\n[216\\. Research AI vs Applied AI vs Generative AI — EITC](http://dev.eitc.org/research-opportunities/new-media-and-new-digital-economy/ai-machine-learning-deep-learning-and-neural-networks/ai-research-and-applications/foundations-of-ai/research-ai-vs-applied-ai-vs-generative-ai)\n\n[217\\. Parijat Dube, Hao Yu et al. “Performance Evaluation of a Commercial Application, Trade, in Scale-out Environments.” 2007 15th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems](https://doi.org/10.1109/MASCOTS.2007.51)\n\n[221\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[222\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[223\\. Martín Abadi, P. Barham et al. “This Paper Is Included in the Proceedings of the 12th Usenix Symposium on Operating Systems Design and Implementation (osdi '16). Tensorflow: a System for Large-scale Machine Learning Tensorflow: a System for Large-scale Machine Learning.”](https://www.semanticscholar.org/paper/4954fa180728932959997a4768411ff9136aac81)\n\n[224\\. TPCx-AI - An Industry Standard Benchmark for Artificial Intelligence and Machine Learning Systems](https://www.vldb.org/pvldb/vol16/p3649-rabl.pdf)\n\n[225\\. HiPEAC Vision 2025: High Performance, Edge and Cloud Computing](https://vision.hipeac.net/pdf/hipeac-vision-2025.pdf)\n\n[226\\. 信息技术产业行业研究](https://pdf.dfcfw.com/pdf/H3_AP202504171657949059_1.pdf?1744915239000.pdf)\n\n[227\\. Deloitte：2025年生成式AI应用案例集报告](https://finance.sina.com.cn/tech/roll/2025-06-30/doc-infcuwqw7987540.shtml?froms=ggmp)\n\n[228\\. J. Dean, G. Corrado et al. “Large Scale Distributed Deep Networks.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/3127190433230b3dc1abd0680bb58dced4bcd90e)\n\n[229\\. DeepSeek 有望推动 AI 应用加速落地，AI 端侧与国产算力需求或迎提速](https://pdf.dfcfw.com/pdf/H3_AP202502101642932622_1.pdf?1739193955000.pdf)\n\n[230\\. AI-Driven Cybersecurity: A Cornerstone of National Security Amidst Emerging Threats and Innovative Solutions](https://www.ijraset.com/best-journal/aidriven-cybersecurity-a-cornerstone-of-national-security-amidst-emerging-threats-and-innovative-solution-843)\n\n[231\\. Safe at the Margins: A General Approach to Safety Alignment in Low-Resource English Languages – A Singlish Case Study](http://www.arxiv.org/pdf/2502.12485)\n\n[232\\. 李飞飞团队2025AI指数报告:AI和计算机科学教育普及程度仍不够](https://xueqiu.com/7423950559/330924320)\n\n[233\\. Latest AI training benchmarks show Nvidia has no competition](https://www.zdnet.com/article/latest-ai-training-benchmarks-show-nvidia-has-no-competition/)\n\n[234\\. The Adoption of Artificial Intelligence in Firms: New Evidence for Policymaking](https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/05/the-adoption-of-artificial-intelligence-in-firms_8fab986b/f9ef33c3-en.pdf)\n\n[235\\. 把握AI应用落地、算力技术迭代新趋势——通信行业2025年投资策略](https://aigc.idigital.com.cn/djyanbao/%E3%80%90%E5%9B%BD%E8%81%94%E8%AF%81%E5%88%B8%E3%80%91%E9%80%9A%E4%BF%A1%E8%A1%8C%E4%B8%9A2025%E5%B9%B4%E6%8A%95%E8%B5%84%E7%AD%96%E7%95%A5%EF%BC%9A%E6%8A%8A%E6%8F%A1AI%E5%BA%94%E7%94%A8%E8%90%BD%E5%9C%B0%E3%80%81%E7%AE%97%E5%8A%9B%E6%8A%80%E6%9C%AF%E8%BF%AD%E4%BB%A3%E6%96%B0%E8%B6%8B%E5%8A%BF-2024-12-20.pdf)\n\n[236\\. Horovod versus Ray: A Comprehensive Comparison](https://www.byteplus.com/en/topic/499136)\n\n[237\\. AI 与 DeepSeek 赋能高效的班组管理](http://www.shchance.com.cn/files/20250320/9ef2c5aefe014dd68faf0365843f901e.pdf)\n\n[238\\. Alexander Sergeev, Mike Del Balso. “Horovod: fast and easy distributed deep learning in TensorFlow.” ArXiv](https://arxiv.org/abs/1802.05799)\n\n[239\\. Foteini Strati, Zhendong Zhang et al. “Sailor: Automating Distributed Training over Dynamic, Heterogeneous, and Geo-distributed Clusters.”](https://arxiv.org/abs/2504.17096)\n\n[240\\. Artificial Intelligence Index Report 2025](https://hai-production.s3.amazonaws.com/files/hai_ai-index-report-2025_chapter1_final.pdf)\n\n[241\\. Muthu Dayalan. “MapReduce: simplified data processing on large clusters.” Commun. ACM](https://doi.org/10.1145/1327452.1327492)\n\n[242\\. N. Jouppi, C. Young et al. “In-datacenter performance analysis of a tensor processing unit.” 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)](https://doi.org/10.1145/3079856.3080246)\n\n[243\\. Brian F. Cooper, Adam Silberstein et al. “Benchmarking cloud serving systems with YCSB.” ACM Symposium on Cloud Computing](https://doi.org/10.1145/1807128.1807152)\n\n[244\\. M. Naumov, Dheevatsa Mudigere et al. “Deep Learning Recommendation Model for Personalization and Recommendation Systems.” ArXiv](https://arxiv.org/abs/1906.00091)\n\n[245\\. Shengsheng Huang, Jie Huang et al. “The HiBench benchmark suite: Characterization of the MapReduce-based data analysis.” 2010 IEEE 26th International Conference on Data Engineering Workshops (ICDEW 2010)](https://doi.org/10.1109/ICDEW.2010.5452747)\n\n[246\\. Attack of the Killer Microseconds](https://cacm.acm.org/magazines/2017/4/215032-attack-of-the-killer-microseconds/fulltext)\n\n[247\\. Latency Monitoring: Why Every Millisecond Counts in AI](https://www.sandgarden.com/learn/latency-monitoring)\n\n[248\\. AI Data Centers: Scaling Up and Scaling Out](https://www.aflhyperscale.com/wp-content/uploads/2024/12/AI-Data-Centers-Scaling-Up-and-Scaling-Out-White-Paper.pdf)\n\n[249\\. AI Scale-Up and Memory Disaggregation: Two Use Cases ...](https://ayarlabs.com/blog/ai-scale-up-and-memory-disaggregation-two-use-cases-enabled-by-ucie-and-optical-io/)\n\n[250\\. SUPERMICRO WITH GAUDI 3 AI DELIVERS SCALABLE PERFORMANCE FOR AI REQUIREMENTS](https://www.supermicro.com/white_paper/white_paper_Gaudi3_Reference_Archtecture.pdf)\n\n[251\\. Scalable Backend Architecture for New Age AI-Powered SaaS Applications: Supporting Billions of Users](https://www.jetir.org/papers/JETIR2502131.pdf)\n\n[252\\. Kubernetes Scheduling for AI - Challenges of Using K8s for AI](https://pages.run.ai/hubfs/PDFs/White%20Papers/Kubernetes%20Scheduling%20for%20AI.pdf)\n\n[253\\. M. Naumov, John Kim et al. “Deep Learning Training in Facebook Data Centers: Design of Scale-up and Scale-out Systems.” ArXiv](https://arxiv.org/abs/2003.09518)\n\n[254\\. AI Application Planning: Choosing Between Traditional ML ...](https://www.aimpointdigital.com/blog/ai-application-planning-choosing-between-traditional-ml-and-generative-ai)\n\n[256\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[257\\. Muthu Dayalan. “MapReduce: simplified data processing on large clusters.” Commun. ACM](https://doi.org/10.1145/1327452.1327492)\n\n[258\\. Scale up Vs. Scale out in Cloud Storage and Graph Processing Systems](https://lexu1.web.engr.illinois.edu/scaleOutUp.pdf)\n\n[259\\. Scalable and Modular AI Deployment Powered by NVIDIA Clara Deploy](https://developer.download.nvidia.com/whitepapers/2020/clara-deploy-whitepaper.pdf)\n\n[260\\. DEPLOYING AI/ML IN MICROSERVICES: A COMPARATIVE STUDY OF TOOLS AND METHODOLOGIES WITH SCALABILITY ANALYSIS AND CASE STUDIES OF LEADING COMPANIES](https://tianjindaxuexuebao.com/dashboard/uploads/6.14997638.pdf)\n\n[261\\. Scale-up vs Scale-out for Hadoop: Time to rethink?](http://rowstron.azurewebsites.net/Publications/mem_socc13.pdf)\n\n[262\\. 通信行业深度报告:光通信:AI算力中心的神经网络- 策略研报 _ 数据中心...](https://data.eastmoney.com/report/zw_strategy.jshtml?encodeUrl=Igfp5TXPWswTruE6JxsZabCiBz8ZAjgPqQOqiZP0xWA%3D)\n\n[263\\. Yanpei Chen, S. Alspaugh et al. “Interactive Analytical Processing in Big Data Systems: A Cross-Industry Study of MapReduce Workloads.” ArXiv](https://doi.org/10.14778/2367502.2367519)\n\n[264\\. Shengsheng Huang, Jie Huang et al. “The HiBench benchmark suite: Characterization of the MapReduce-based data analysis.” 2010 IEEE 26th International Conference on Data Engineering Workshops (ICDEW 2010)](https://doi.org/10.1109/ICDEW.2010.5452747)\n\n[265\\. Yanpei Chen, A. Ganapathi et al. “The Case for Evaluating MapReduce Performance Using Workload Suites.” 2011 IEEE 19th Annual International Symposium on Modelling, Analysis, and Simulation of Computer and Telecommunication Systems](https://doi.org/10.1109/MASCOTS.2011.12)\n\n[266\\. 智算中心组网技术及应用 Intelligent Computing Center Networking Technology and Applications](https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-cn/mediares/magazine/publication/com_cn/article/202502/9.pdf)\n\n[267\\. NVIDIA AI Inference Performance Milestones: Delivering Leading Throughput, Latency and Efficiency](https://developer.nvidia.com/blog/nvidia-ai-inference-performance-milestones-delivering-leading-throughput-latency-and-efficiency/)\n\n[268\\. AI Data Centers: Scaling Up and Scaling Out](https://www.aflhyperscale.com/wp-content/uploads/2024/12/AI-Data-Centers-Scaling-Up-and-Scaling-Out-White-Paper.pdf)\n\n[269\\. Scaling to 1000+ AI-solutions in deployment by building trust through transparency](https://www.kisacoresearch.com/sites/default/files/presentations/jonathan_berte_-_robovision.pdf)\n\n[270\\. 面向GPU算力纵向扩展的Scale-up网络技术研究](http://m.10jqka.com.cn/20250710/c669534852.shtml)\n\n[271\\. Inference Latency: Definition, Importance - Ultralytics](https://www.ultralytics.com/glossary/inference-latency#:~:text=Inference%20latency%20is%20a%20critical,produces%20a%20prediction%20or%20output.)\n\n[272\\. Demystifying AI: Eight Key Terms You Need to Know - Ayar Labs](https://ayarlabs.com/blog/demystifying-ai-eight-key-terms-you-need-to-know/#:~:text=While%20scale-up%20networks%20are,manage%20longer-distance%20data%20transfers.)\n\n[273\\. Scalable Cosmic AI Inference using Cloud Serverless Computing with FMI](https://arxiv.org/pdf/2501.06249)\n\n[274\\. A Framework for an In-depth Comparison of Scale-up and Scale-out](http://alumni.soe.ucsc.edu/~msevilla/papers/sevilla-discs13.pdf)\n\n[275\\. The Race to Efficiency: A New Perspective on AI Scalin...](http://arxiv.org/html/2501.02156v1)\n\n[276\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[277\\. R. S. Sutton, A. Barto. “Reinforcement Learning: An Introduction.” IEEE Trans. Neural Networks](https://doi.org/10.1109/TNN.1998.712192)\n\n[278\\. AI Industry Landscape Report 2025](https://repository.ceibs.edu/files/59116885/AI_Industry_landscape_report_2025.pdf)\n\n[279\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[280\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[281\\. DeepSeek AI: A comprehensive guide for enterprise implementation](https://aigc.idigital.com.cn/djyanbao/%E3%80%90DeepSeek%20AI%E3%80%91%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%9E%E6%96%BD%E5%85%A8%E9%9D%A2%E6%8C%87%E5%8D%97-2025-06-02.pdf)\n\n[282\\. Frontiers of AI Research in 2025](https://www.fticonsulting.com/canada/-/media/files/insights/articles/2025/feb/frontiers-ai-research-2025.pdf?rev=a1255fca983742558f24faf3699907f6&hash=4CABB8337A45113CEDB01889A9FE7026)\n\n[283\\. FRONTIERMATH: A BENCHMARK FOR EVALUATING ADVANCED MATHEMATICAL REASONING IN AI](https://arxiv.org/pdf/2411.04872)\n\n[284\\. 2025 AI Deployment and Governance Survey Report](https://www.governanceinstitute.com.au/app/uploads/2025/04/AI-deployment-and-governance.pdf)\n\n[285\\. Revolutionary Success in Real-World AI Case Studies 2025](https://fixusglobal.com/case-studies/revolutionary-success-in-real-world-ai-case-studies-2025/)\n\n[286\\. Frontier AI's Impact on the Cybersecurity Landscape](http://arxiv.org/pdf/2504.05408)\n\n[287\\. 2025 AI 赋能教育行业发展趋势报告](https://pdf.dfcfw.com/pdf/H3_AP202506201694183506_1.pdf?1750407945000.pdf)\n\n[288\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[289\\. International AI Safety Report](https://italianelfuturo.com/wp-content/uploads/2025/01/International_AI_Safety_Report_2025_accessible_f_250130_073143.pdf)\n\n[290\\. Artificial Intelligence Index Report 2025 Policy Highlights](https://hai-production.s3.amazonaws.com/files/hai-ai-index-2025-policy-highlights.pdf)\n\n[291\\. Top FrontierMath score in 2025? - Manifold Markets](https://manifold.markets/SG/top-frontiermath-score-in-2025)\n\n[292\\. 2025 \"人工智能+\" 行业发展蓝皮书](http://www.sccio.cn/uploads/20250522/8696496143e2c9e662e2e45890c9c1b4.pdf)\n\n[296\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[297\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[298\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[299\\. MLPerf Training Benchmark](https://arxiv.org/pdf/1910.01500v1.pdf)\n\n[300\\. Robert Adolf, Saketh Rama et al. “Fathom: reference workloads for modern deep learning methods.” 2016 IEEE International Symposium on Workload Characterization (IISWC)](https://doi.org/10.1109/IISWC.2016.7581275)\n\n[301\\. Cody A. Coleman, D. Narayanan et al. “DAWNBench : An End-to-End Deep Learning Benchmark and Competition.”](https://www.semanticscholar.org/paper/b245959da6bdaa0b711341844aeaa473b7706453)\n\n[302\\. 영상인식 및 분류용 인공지능 가속기의 최신 성능평가: MLPerf를 중심으로](https://www.kibme.org/resources/journal/20200206100621546.pdf)\n\n[303\\. Scale MLPerf-0.6 models on Google TPU-v3 Pods](http://learningsys.org/neurips19/assets/papers/14_CameraReadySubmission_main.pdf)\n\n[304\\. Vijayarāghava Reḍḍī, C. Cheng et al. “MLPerf Inference Benchmark.” 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)](https://doi.org/10.1109/ISCA45697.2020.00045)\n\n[305\\. Benchmark MLPerf Inference: Datacenter | MLCommons V3.1](https://mlcommons.org/benchmarks/inference-datacenter/)\n\n[306\\. MLPerf Reveals Blackwell Gains, Untether's Power Efficiency](https://xpu.pub/2024/09/20/mlperf-4-1/)\n\n[307\\. Leading AI Scalability Benchmarks with Microsoft Azure](https://signal65.com/wp-content/uploads/2024/11/Signal65-Leading-AI-Scalability-Benchmarks-Microsoft-Azure-v1.0.pdf)\n\n[308\\. MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance](https://hds.harvard.edu/sites/hwpi.harvard.edu/files/vlsiarch/files/mlperf_an_industry_standard_benchmark_suite_for_machine_learning_performance.pdf)\n\n[309\\. What are the different MLPerf benchmarks ...](https://www.microcontrollertips.com/what-are-the-different-mlperf-benchmarks-from-mlcommons/)\n\n[310\\. Meeting the challenges of AI workloads with the Dell AI portfolio: A comparison of the Dell AI portfolio vs. similar offerings from HPE](https://www.principledtechnologies.com/Dell/AI-portfolio-vs-HPE-0124.pdf)\n\n[311\\. Hyperparameter Optimization with SigOpt for MLPerf Training](https://www.intel.cn/content/www/cn/zh/developer/articles/technical/hyperparameter-optimization-mlperf-training-sigopt.html)\n\n[312\\. Colby R. Banbury, V. Reddi et al. “MLPerf Tiny Benchmark.” ArXiv](https://arxiv.org/abs/2106.07597)\n\n[313\\. Llama 2 70B: An MLPerf Inference Benchmark for Large Language Models](https://mlcommons.org/2024/03/mlperf-llama2-70b/)\n\n[314\\. New MLPerf Training v5.0 Benchmark Results Reflect Rapid](https://www.globenewswire.com/news-release/2025/06/04/3093769/0/en/New-MLPerf-Training-v5-0-Benchmark-Results-Reflect-Rapid-Growth-and-Evolution-of-the-Field-of-AI.html)\n\n[316\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[317\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[318\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[319\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[320\\. LLMOps: A Comprehensive Guide to Deploying Large Language Models in Production](https://www.ijsat.org/papers/2025/1/2412.pdf)\n\n[321\\. Bigger is not Always Better: Scaling Properties of Latent Diffusion Models](https://openreview.net/pdf/8c7c846f73382de823f068af2edb85b066871480.pdf)\n\n[322\\. LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators](https://arxiv.org/pdf/2411.00136)\n\n[323\\. Optimizing Transformer Models for Low-Latency Inference: Techniques, Architectures, and Code Implementations](https://www.ijsr.net/archive/v14i4/SR25409073105.pdf)\n\n[324\\. AI Readiness Report 2024](https://go.scale.com/hubfs/Content/Scale%20Zeitgeist%20AI%20Readiness%20Report%202024%204-29%20final.pdf)\n\n[325\\. N. Jouppi, C. Young et al. “In-datacenter performance analysis of a tensor processing unit.” 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)](https://doi.org/10.1145/3079856.3080246)\n\n[326\\. Scalable and Modular AI Deployment Powered by NVIDIA Clara Deploy](https://developer.download.nvidia.com/whitepapers/2020/clara-deploy-whitepaper.pdf)\n\n[327\\. Mystique: Accurate and Scalable Production AI Benchmarks Generation](https://arxiv.org/pdf/2301.04122v2)\n\n[328\\. Generative AI Services: A research report comparing provider strengths, challenges and competitive differentiators](https://www.unisys.com/siteassets/collateral/analyst-report/ar-11142024-generative-ai-services.pdf)\n\n[329\\. Optimizing Resource Allocation for Deep Learning Workloads in Heterogeneous Cloud Environments](https://www.ijfmr.com/papers/2024/6/31895.pdf)\n\n[330\\. AI Data Centers: Scaling Up and Scaling Out](https://www.aflhyperscale.com/wp-content/uploads/2024/12/AI-Data-Centers-Scaling-Up-and-Scaling-Out-White-Paper.pdf)\n\n[331\\. AI Model Deployment Explained: Tools & Best Practices](https://orq.ai/blog/ai-model-deployment)\n\n[332\\. Deploy AI models in production](https://chatllama.baseten.co/)\n\n[333\\. Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts](https://hal.science/hal-04908002v1/document)\n\n[336\\. Top FrontierMath score in 2025? - Manifold Markets](https://manifold.markets/SG/top-frontiermath-score-in-2025)\n\n[337\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[338\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[339\\. 2025: The Year the Frontier Firm Is Born](https://www.microsoft.com/en-us/worklab/work-trend-index/2025-the-year-the-frontier-firm-is-born)\n\n[340\\. Import AI 420: Prisoner Dilemma AI; FrontierMath Tier 4](https://jack-clark.net/2025/07/14/import-ai-420-prisoner-dilemma-ai-frontiermath-tier-4-and-how-to-regulate-ai-companies/)\n\n[341\\. Jakub Konecný, H. B. McMahan et al. “Federated Learning: Strategies for Improving Communication Efficiency.” ArXiv](https://arxiv.org/abs/1610.05492)\n\n[342\\. FRONTIERMATH: A BENCHMARK FOR EVALUATING ADVANCED MATHEMATICAL REASONING IN AI](https://arxiv.org/pdf/2411.04872)\n\n[343\\. Frontiers of AI Research in 2025](https://www.fticonsulting.com/canada/-/media/files/insights/articles/2025/feb/frontiers-ai-research-2025.pdf?rev=a1255fca983742558f24faf3699907f6&hash=4CABB8337A45113CEDB01889A9FE7026)\n\n[344\\. How 2025 AI Forecasts Fared So Far](https://www.lesswrong.com/posts/FwS8THsPGi36M2tj6/how-2025-ai-forecasts-fared-so-far)\n\n[345\\. FrontierMath: The Benchmark that Highlights AI's Limits in ...](https://www.marktechpost.com/2024/11/08/frontiermath-the-benchmark-that-highlights-ais-limits-in-mathematics/)\n\n[346\\. Frontier AI Issue 1 2025](https://www.frontier-enterprise.com/wp-content/uploads/2025/01/Frontier-AI-2025-FA.pdf)\n\n[347\\. \\[2411.04872v5\\] FrontierMath: A Benchmark for Evaluatin...](http://arxiv.org/abs/2411.04872v5)\n\n[348\\. DRAFT REPORT of the Joint California Policy Working Group on AI Frontier Models](https://www.courthousenews.com/wp-content/uploads/2025/03/draft-report-california-frontier-ai-policy-framework.pdf)\n\n[349\\. Frontier AI 2025: A Special Issue](https://www.frontier-enterprise.com/frontier-ai-2025-a-special-issue/)\n\n[350\\. Deep Learning Benchmarks 2025](https://www.byteplus.com/en/topic/466080)\n\n[356\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[357\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[358\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[359\\. Benchmark MLPerf Inference: Datacenter | MLCommons V3.1](https://mlcommons.org/benchmarks/inference-datacenter/)\n\n[360\\. Benchmark MLPerf Training | MLCommons Version 2.0 Results](https://mlcommons.org/benchmarks/training/)\n\n[361\\. What are the different MLPerf benchmarks ...](https://www.microcontrollertips.com/what-are-the-different-mlperf-benchmarks-from-mlcommons/)\n\n[362\\. Robert Adolf, Saketh Rama et al. “Fathom: reference workloads for modern deep learning methods.” 2016 IEEE International Symposium on Workload Characterization (IISWC)](https://doi.org/10.1109/IISWC.2016.7581275)\n\n[363\\. Cody A. Coleman, D. Narayanan et al. “DAWNBench : An End-to-End Deep Learning Benchmark and Competition.”](https://www.semanticscholar.org/paper/b245959da6bdaa0b711341844aeaa473b7706453)\n\n[364\\. MLPerf Mobile Inference Benchmark: An industry-standard open-source machine learning benchmark for on-device AI](https://github.com/mlcommons/mobile_open)\n\n[365\\. Meet AI Demands at Any Scale](https://www.braintree.co.za/wp-content/uploads/2024/10/Meet-AI-Demands-at-Any-Scale-Whitepaper.pdf)\n\n[366\\. Benchmark MLPerf Inference: Mobile | MLCommons V3.1 Results](https://mlcommons.org/benchmarks/inference-mobile/)\n\n[367\\. International Roadmap for Devices and Systems: 2023 Update - Applications Benchmarking](https://irds.ieee.org/images/files/pdf/2023/2023IRDS_AB.pdf)\n\n[368\\. Artificial Intelligence Index Report 2023](https://milanoluisshub.it/wp-content/uploads/2023/04/HAI_AI-Index_Report_2023_compressed.pdf)\n\n[369\\. Vijayarāghava Reḍḍī, C. Cheng et al. “MLPerf Inference Benchmark.” 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)](https://doi.org/10.1109/ISCA45697.2020.00045)\n\n[370\\. Benchmark MLPerf Inference: Tiny | MLCommons V1.1 Results](http://bithollow.org/benchmarks/inference-tiny/index.html)\n\n[371\\. 영상인식 및 분류용 인공지능 가속기의 최신 성능평가: MLPerf를 중심으로](https://www.kibme.org/resources/journal/20200206100621546.pdf)\n\n[372\\. MLPerf Reveals Blackwell Gains, Untether's Power Efficiency](https://xpu.pub/2024/09/20/mlperf-4-1/)\n\n[373\\. New MLPerf Training v5.0 Benchmark Results Reflect Rapid](https://www.globenewswire.com/news-release/2025/06/04/3093769/0/en/New-MLPerf-Training-v5-0-Benchmark-Results-Reflect-Rapid-Growth-and-Evolution-of-the-Field-of-AI.html)\n\n[374\\. Advanced server memory is foundational for AI](https://www.microncpg.com/content/dam/micron-cpg-corporate/campaigns/everyday-ai/ebook/micron-server-ddr5-ai-use-cases-ebook/micron-server-DDR5-AI-use-cases-ebook-v5.pdf)\n\n[376\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[377\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[378\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[379\\. Muthu Dayalan. “MapReduce: simplified data processing on large clusters.” Commun. ACM](https://doi.org/10.1145/1327452.1327492)\n\n[380\\. Pre-RMSNorm and Pre-CRMSNorm transformers | Proceeding...](https://dl.acm.org/doi/10.5555/3666122.3668105)\n\n[381\\. .../llm-analysis: Latency and Memory Analysis of Trans...](https://github.com/K-Wu/llm-analysis)\n\n[382\\. SHOW-o: ONE SINGLE TRANSFORMER TO UNIFY MULTIMODAL UNDERSTANDING AND GENERATION](https://openreview.net/pdf?id=o6Ynz6OIQ6)\n\n[383\\. An Autonomous Parallelization of Transformer Model Inference on ...](https://dl.acm.org/doi/10.1145/3650200.3656628)\n\n[384\\. Response length perception and sequence scheduling:](https://dl.acm.org/doi/10.5555/3666122.3668981)\n\n[385\\. When the Edge Meets Transformers: Distributed Inference with Transformer Models](https://iqua.ece.toronto.edu/papers/chenghao-icdcs24.pdf)\n\n[386\\. Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training](https://arxiv.org/pdf/2411.13055)\n\n[387\\. Hierarchical Protein Backbone Generation with Latent and Structure Diffusion](https://openreview.net/pdf/29bcff732d1441419a26a360ba32d2b387308d8a.pdf)\n\n[388\\. ICCV 2023 Open Access Repository](https://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html)\n\n[389\\. Yanpei Chen, A. Ganapathi et al. “The Case for Evaluating MapReduce Performance Using Workload Suites.” 2011 IEEE 19th Annual International Symposium on Modelling, Analysis, and Simulation of Computer and Telecommunication Systems](https://doi.org/10.1109/MASCOTS.2011.12)\n\n[390\\. Diffusion, Masked-Token, and Next-Token Prediction](https://arxiv.org/html/2405.13218v2)\n\n[391\\. Juhyeon Lee, Insung Bahk et al. “An Autonomous Parallelization of Transformer Model Inference on Heterogeneous Edge Devices.” Proceedings of the 38th ACM International Conference on Supercomputing](https://doi.org/10.1145/3650200.3656628)\n\n[392\\. Papers with Code - Scalable Diffusion Models with Transformers](https://paperswithcode.com/paper/scalable-diffusion-models-with-transformers)\n\n[393\\. Optimizing Systems for Deep Learning Applications](https://vtechworks.lib.vt.edu/bitstreams/cff9c2b6-cb4f-44b1-ac48-f20084ed746b/download)\n\n[394\\. Understanding the Performance of Transformer Inference](https://dspace.mit.edu/bitstream/handle/1721.1/151543/ouyang-aouyang-meng-eecs-2023-thesis.pdf?sequence=1&isAllowed=y)\n\n[395\\. EnerVerse: Envisioning Embodied Future Space for Robot...](http://arxiv.org/html/2501.01895v2)\n\n[396\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[397\\. P. Kairouz, H. B. McMahan et al. “Advances and Open Problems in Federated Learning.” Found. Trends Mach. Learn.](https://doi.org/10.1561/2200000083)\n\n[398\\. Martín Abadi, Andy Chu et al. “Deep Learning with Differential Privacy.” Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security](https://doi.org/10.1145/2976749.2978318)\n\n[399\\. Keith Bonawitz, Hubert Eichner et al. “Towards Federated Learning at Scale: System Design.” ArXiv](https://arxiv.org/abs/1902.01046)\n\n[400\\. FRONTIERMATH: A BENCHMARK FOR EVALUATING ADVANCED MATHEMATICAL REASONING IN AI](https://arxiv.org/pdf/2411.04872)\n\n[401\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[402\\. FrontierMath: The Benchmark that Highlights AI's Limits in ...](https://www.marktechpost.com/2024/11/08/frontiermath-the-benchmark-that-highlights-ais-limits-in-mathematics/)\n\n[403\\. Chaoyang He, Songze Li et al. “FedML: A Research Library and Benchmark for Federated Machine Learning.” ArXiv](https://arxiv.org/abs/2007.13518)\n\n[404\\. Top FrontierMath score in 2025? - Manifold Markets](https://manifold.markets/SG/top-frontiermath-score-in-2025)\n\n[405\\. 电子行业 2025 年度策略报告：AI 云侧与端侧共振，自主可控砥砺前行](https://pdf.dfcfw.com/pdf/H3_AP202412301641466830_1.pdf?1735570500000.pdf)"
  },
  {
    "id": 27,
    "question": "If the lesson of DeepSeek isn’t a 'Sputnik moment' but rather an 'internet moment,' how should policymakers radically rethink AI governance to avoid repeating historical regulatory failures?",
    "response": "**Research Report**\n\n**To:** Interested Policymakers and Regulatory Bodies\n\n**From:** Expert Researcher\n\n**Date:** July 26, 2025\n\n**Subject:** A New Governance Paradigm for the AI Era: Why the DeepSeek Event Demands an 'Internet Moment' Response, Not a 'Sputnik Moment' Reaction\n\n**Executive Summary**\n\nThe recent emergence of DeepSeek, a highly capable and cost-effective open-source AI model from a private Chinese startup, has sent shockwaves through the global technology and policy landscape. The predominant narrative has framed this as a \"Sputnik moment\" for the United States, suggesting a new state-level technological race for AI supremacy. This report argues that such a framing is not only inaccurate but dangerously misleading. The characteristics of DeepSeek’s development and proliferation—its open-source nature, low cost, and decentralized potential—signal an event more akin to the commercialization of the internet.\n\nViewing DeepSeek through this \"internet moment\" lens reveals a different set of challenges, demanding a radical rethinking of AI governance. A reactive, state-centric, \"Sputnik-style\" response focused on national competition will fail to address the diffuse, global, and rapidly scaling risks posed by decentralized AI. Instead, policymakers must learn from the catastrophic regulatory failures of the early internet era—specifically the unchecked rise of data monopolies and the systemic erosion of privacy—to architect a new, more resilient governance paradigm. This report analyzes the nature of the DeepSeek challenge, revisits the key regulatory failures of the internet's commercialization, and outlines a forward-looking governance framework that embraces decentralization, hybrid models, and proactive architectural design to foster innovation while safeguarding against systemic risks.\n\n**1\\. The DeepSeek Catalyst: Deconstructing the 'Sputnik' vs. 'Internet' Analogy**\n\nThe initial reaction to DeepSeek, which included a $1 trillion correction in U.S. tech stocks and urgent calls for increased national R&D, was classic \"Sputnik\" panic \\[2\\]\\[9\\]\\[10\\]. The \"Sputnik moment\" analogy implies a singular, state-driven technological leap by a geopolitical rival, necessitating a centralized, government-led response to \"catch up\" and regain dominance. However, a deeper analysis of DeepSeek reveals characteristics that fundamentally break this analogy and align far more closely with the dawn of the public internet.\n\n**1.1. Why the 'Sputnik Moment' Narrative is Flawed**\n\nWhile DeepSeek’s performance is impressive and has rightfully challenged the U.S. AI industry's assumptions about the necessity of massive scale \\[5\\]\\[11\\]it is not a singular bolt from the blue. Several factors undermine the \"Sputnik\" framing:\n\n**Private, Not Purely State-Driven:** DeepSeek was developed by a private startup, showcasing the dynamism of China's commercial tech sector, not a monolithic state project \\[2\\]\\[11\\].\n\n**Built on Existing Foundations:** Its success builds upon established methods pioneered largely in the U.S., such as transformer models, mixture-of-experts, and reinforcement learning \\[11\\]. It represents a significant and efficient implementation, but not a fundamental scientific break from the existing global research paradigm.\n\n**Nuanced Performance:** While DeepSeek excelled on public benchmarks, some private, real-world tests have indicated underperformance compared to leading U.S. models, suggesting a more complex capability landscape than headlines imply \\[2\\].\n\n**Constrained by National Policy:** The model operates under China's strict political censorship and data collection regimes, which limit its global applicability and raise transparency concerns \\[2\\]\\[1\\]\\[1\\]. This is a qualitative difference from a universally applicable technology like a satellite.\n\n**1.2. The Compelling Case for an 'Internet Moment'**\n\nThe true lesson of DeepSeek lies not in a race between nations, but in the paradigm shift it represents for AI development and deployment. Its defining characteristics mirror the forces that shaped the early internet:\n\n**Democratization and Radical Cost Reduction:** DeepSeek-R1 achieved comparable performance to top-tier models at a fraction of the training and inference cost \\[2\\]\\[1\\]\\[12\\]. This dramatic reduction in cost is a powerful democratizing force, much like the advent of personal computers and dial-up access opened up the internet.\n\n**Decentralized Proliferation via Open Source:** The model's open-source nature is its most critical feature \\[4\\]\\[6\\]. Unlike closed, API-gated models, open-source AI can be freely downloaded, studied, modified, and redeployed by anyone, anywhere \\[58\\]\\[55\\]. The creator loses control over its ultimate use, making centralized enforcement of safeguards nearly impossible \\[55\\]. This mirrors the permissionless innovation of the World Wide Web, where anyone could create a website or service.\n\n**Governance Evasion by Design:** The very architecture of open-source models enables governance evasion. Users can strip away safety filters, fine-tune the model for malicious purposes, and exploit vulnerabilities that are more easily discovered due to the code's accessibility \\[122\\]\\[125\\]\\[188\\]. Documented incidents already show DeepSeek being \"jailbroken\" to generate ransomware code and instructions for creating toxins, showcasing how easily creator-imposed controls can be bypassed \\[180\\]\\[184\\]\\[190\\].\n\nThis combination of low cost, open access, and decentralized proliferation means that, like the internet, the technology will spread far beyond the control of any single company or nation-state. The primary challenge is not about one country getting ahead, but about managing the unpredictable risks of a powerful, globally accessible, and endlessly modifiable technology.\n\n**2\\. Historical Echoes: The Defining Regulatory Failures of the Early Internet**\n\nTo understand how to govern this new AI era, we must first honestly confront the profound regulatory failures of the last \"internet moment.\" The light-touch, reactive approach of the 1990s and 2000s directly enabled the systemic problems that plague our digital society today.\n\n**2.1. The Original Sin: Delayed Action and Regulatory Restraint**\n\nDuring the internet's public launch and early commercialization, the U.S. Congress exercised \"notable restraint,\" with the first significant regulation only arriving with the Telecommunications Act of 1996 \\[24\\]. This hands-off approach was predicated on the belief that markets would self-correct and that regulation would stifle innovation. The delay allowed foundational patterns of power and control to solidify before any meaningful oversight was in place. The FCC’s long-standing struggle to even define the internet—wavering between a \"telecom service\" and an \"information service\"—created regulatory loopholes that powerful players exploited to avoid oversight of both market power and data practices \\[27\\].\n\n**2.2. Architecting Monopolies: The Failure of Competition Law**\n\nPolicymakers and regulators failed to adapt competition law to the realities of digital markets \\[23\\]\\[25\\]. They were overly optimistic that new entrants would naturally disrupt incumbents, failing to appreciate the powerful network effects and data advantages that create digital moats. This led to two key failures:\n\n1.  **Permissive Merger Approvals:** The most cited example is Facebook's acquisition of a nascent Instagram, a move now seen as a \"biggest recent failure of regulation\" that eliminated a future competitor before it could mature \\[25\\].\n2.  **Ignoring Data as a Barrier to Entry:** Regulators failed to recognize that the vast accumulation of user data by platforms like Google and Amazon was not just a byproduct of their service but their primary competitive advantage. This data created insurmountable barriers for new entrants, stifling innovation and market diversity \\[26\\]. The deregulation of the telecommunications industry further allowed a few ISPs to achieve monopolistic control over internet access itself \\[32\\].\n\n**2.3. Normalizing Surveillance: The Systemic Erosion of Privacy**\n\nThe commercialization of the internet transformed it from a non-commercial, academic space into a marketplace fueled by user data \\[30\\]. While the Federal Trade Commission (FTC) acknowledged significant market failures affecting consumer privacy as early as the mid-1990s, it lacked the legal authority and political will to act decisively \\[31\\]. The business model of collecting vast troves of personal data for targeted advertising became the economic engine of the web, normalizing a level of surveillance that would have been unthinkable just a decade prior. This failure to establish strong baseline privacy protections from the outset created a deep-rooted deficit that subsequent regulations like GDPR and CCPA have struggled to correct.\n\n**3\\. A New Governance Paradigm: Radically Rethinking Regulation for the AI 'Internet Moment'**\n\nAvoiding a repeat of these historical failures requires a fundamental shift in regulatory philosophy. A \"Sputnik\" response—pouring national funds into centralized, state-aligned AI labs—is precisely the wrong approach. It doubles down on centralization and fails to address the core problem of decentralized proliferation. The goal should not be to build a bigger, better, closed-source model, but to architect a global ecosystem where both open and closed AI can develop safely and equitably.\n\n**3.1. Beyond Centralized Control: The Imperative for Hybrid and Decentralized Models**\n\nThe diffuse nature of open-source AI risk necessitates a move away from purely top-down, command-and-control regulation. The documented vulnerabilities of DeepSeek to jailbreaking, adversarial fine-tuning, and data poisoning show that risks emerge at the point of use, not just at the point of creation \\[177\\]\\[183\\]\\[188\\].\n\nA more resilient approach lies in **hybrid governance frameworks** that strategically balance centralization and decentralization. In this model, a central body (or a collaboration of national regulators) could be responsible for setting foundational standards, conducting high-level risk assessments, and maintaining a centralized inventory of high-risk AI applications \\[311\\]\\[313\\]. However, the execution, innovation, and adaptation would be decentralized, allowing domain-specific experts and business units to innovate rapidly within those established guardrails \\[255\\]\\[310\\]. The \"Dual Governance\" model, which combines centralized rules from agencies like the FTC with community-driven, crowdsourced safety mechanisms, provides a practical template for this approach \\[257\\]\\[305\\].\n\n**3.2. Proactively Architecting Against Monopolies and Data Concentration**\n\nTo avoid repeating the internet's most costly error, policymakers must proactively design against the concentration of data and power. This requires moving beyond reactive antitrust enforcement and toward building systems with decentralization as a core architectural principle. This is where emerging \\[3\\] technologies offer a compelling, if still nascent, alternative.\n\nFrameworks like **ETHOS (Ethical Technology and Holistic Oversight System)** propose leveraging technologies like blockchains, smart contracts, and Decentralized Autonomous Organizations (DAOs) for AI governance \\[253\\]\\[253\\]. The core mechanisms directly counter the dynamics that led to internet monopolies:\n\n**Immutable and Transparent Ledgers:** Using blockchain for an AI agent registry creates a public, unalterable record of models and their behavior, preventing the opacity that allows large platforms to hide their data practices \\[133\\]\\[139\\].\n\n**Automated Compliance via Smart Contracts:** Smart contracts can automate and enforce rules—such as data usage permissions or compliance checks—without a central intermediary, reducing the potential for a single entity to manipulate the system \\[135\\]\\[140\\]. This directly addresses the failure of central authorities to effectively police platform behavior.\n\n**User-Centric Data Sovereignty:** Technologies like Self-Sovereign Identity (SSI) and federated learning, which can be integrated into these frameworks, allow data to remain under user control or decentralized across devices, preventing the mass data aggregation that fueled internet monopolies \\[74\\]\\[79\\]\\[142\\].\n\nWhile promising, it is crucial to note that empirical, quantitative evidence demonstrating the reduction of data concentration in frameworks like ETHOS is still emerging . The focus for policymakers should be on supporting pilot programs and research into quantifiable decentralization metrics (like the Gini coefficient or Nakamoto coefficient used for blockchains) to validate these architectural benefits \\[276\\].\n\n**3.3. Embedding Accountability in a Decentralized World**\n\nIn a world of modifiable, open-source AI, assigning liability is a profound challenge. When a fine-tuned version of DeepSeek is used to cause harm, who is responsible? A purely centralized approach fails here. Decentralized governance offers a path toward shared accountability.\n\n**Decentralized Autonomous Organizations (DAOs)**, while still maturing, present a model for community-driven oversight \\[33\\]\\[77\\]. A DAO could govern an AI model's development, manage a bug bounty program for identifying safety flaws, or vote on acceptable use cases. AI tools are already being used to make DAO governance itself more efficient and transparent \\[93\\]\\[101\\].\n\nHowever, significant research gaps remain. The provided literature contains no peer-reviewed, empirical studies documenting specific reduction rates for algorithmic bias incidents in DAO-governed AI systems . This highlights a critical frontier for policy-supported research. The goal should be to foster frameworks where accountability is a distributed responsibility, baked into the governance protocol of the AI system itself, rather than a punishment to be meted out after a catastrophe.\n\n**4\\. Conclusion and Recommendations**\n\nThe emergence of DeepSeek is not a call to arms in a new Cold War-style technology race. It is a fire bell in the night, warning us that the AI era is unfolding like the early internet: decentralized, unpredictable, and with the potential for both unprecedented progress and systemic failure. A \"Sputnik\" response—centralized, nationalistic, and reactive—is destined to fail. It is a strategy for a world that no longer exists.\n\nPolicymakers must embrace the \"internet moment\" analogy and its stark lessons. This requires a radical paradigm shift in governance thinking, guided by the following principles:\n\n1.  **Prioritize Architectural Solutions over Reactive Regulation:** Instead of waiting for monopolies to form and then trying to break them up, foster and fund the development of inherently decentralized systems (like those proposed in the ETHOS framework) that are architecturally resistant to data concentration.\n2.  **Champion Hybrid Governance Models:** Abandon the false choice between total centralization and a lawless free-for-all. Implement flexible, hybrid frameworks that combine centralized standard-setting with decentralized innovation and execution, such as the \"Dual Governance\" model.\n3.  **Invest in the Science of Decentralized Governance:** Acknowledge the current lack of empirical data and launch targeted initiatives to measure the real-world effectiveness of DAO-based governance and other decentralized models in mitigating AI risks like bias and misuse. Fund pilot programs and the development of quantitative metrics for accountability and data concentration.\n4.  **Shift from Control to Resilience:** Accept that perfect control over a globally accessible, open-source technology is impossible. The policy goal must shift from preventing proliferation to building a resilient, adaptive, and transparent ecosystem that can rapidly identify and mitigate harm wherever it emerges.\n\nThe challenge posed by DeepSeek is not to build a bigger American model. It is to collectively build a better global digital infrastructure. The regulatory failures of the internet were not inevitable; they were choices. We are now at a similar inflection point with artificial intelligence, and we must choose more wisely.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. DeepSeek: The Sputnik Moment of the AI Era?](https://www.inss.re.kr/upload/bbs/BBSA05/202504/F20250401150142304.pdf)\n\n[2\\. China's AI Sputnik moment is not what it seems](https://engelsbergideas.com/notebook/chinas-ai-sputnik-moment-is-not-what-it-seems/)\n\n[3\\. DeepSeek – AI's Sputnik moment?](https://rescap.com/wp-content/uploads/Resolution-Capital-DeepSeek-AIs-Sputnik-moment.pdf)\n\n[4\\. Sputnik Moment, Democratization of AI, or Media Hype?](https://caiml.org/dighum/dighum-lectures/johannes-oster-deep-seek-sputnik-moment-democratization-of-ai-or-media-hype-2025-02-25/oster-slides.pdf)\n\n[5\\. DeepSeek: A Sputnik Moment?](https://www.oddo-bhf.com/2025/02/11/deepseek-a-sputnik-moment/)\n\n[6\\. AI 大模型之战](https://img1.xinmin.cn/xmzk/2025-02-24/100224.pdf)\n\n[7\\. Is DeepSeek a Metacognition AI?](https://www.qeios.com/read/PJ3POM/pdf)\n\n[8\\. DeepSeek's 'Sputnik moment' exposes holes in US chip curbs](https://techxplore.com/news/2025-01-deepseek-sputnik-moment-exposes-holes.pdf)\n\n[9\\. DeepSeek – lessons from AI's 'Sputnik moment'](https://diginomica.com/deepseek-lessons-ais-sputnik-moment-0)\n\n[10\\. Equity Focus: New Kid in Town](https://wealthmanagement.bnpparibas/content/dam/wmpublicsite/global/pdfs/en/february-2025/Equity%20Focus%20Feb25%20EN%20v2%20ES.pdf)\n\n[11\\. Commentary: Is DeepSeek triggering a Sputnik moment for ...](https://www.channelnewsasia.com/commentary/deepseek-artificial-intelligence-united-states-china-ai-tech-race-4918491)\n\n[12\\. DeepSeek – AI’s Sputnik moment? - Andrew Parsons | L...](https://www.livewiremarkets.com/wires/deepseek-ai-s-sputnik-moment)\n\n[21\\. The horror of the internet has three causes](http://ericposner.com/the-horror-of-the-internet-has-three-causes/)\n\n[22\\. Internet Monopoly Platform Crisis](https://hightechforum.org/internet-monopoly-platform-crisis/)\n\n[23\\. Legal Regulation of Monopoly Behaviour of Internet Platforms](https://www.clausiuspress.com/assets/default/article/2024/11/04/article_1730735697.pdf)\n\n[24\\. GenAI and Antitrust: Tread Lightly in Times of Uncertainty](https://www.clearygottlieb.com/-/media/files/genai-and-antitrust-tread-lightly-in-times-of-uncertainty.pdf)\n\n[25\\. Regulating in a digital world](https://www.regulation.org.uk/library/2019-HoL-Regulating_in_a_Digital_World.pdf)\n\n[26\\. 数字经济时代下的反垄断法律规制研究](https://www.hanspub.org/journal/paperinformation?paperid=98300)\n\n[27\\. 3. Humanity is at a precipice; its future is at stake](https://computing.smu.edu.sg/sites/scis.smu.edu.sg/files/news/20191028-PewResearchCenter-News-Humanity.pdf)\n\n[28\\. Preventing the Balkanization of the Internet](https://ethiopianbusinessreview.net/preventing-the-balkanization-of-the-internet/)\n\n[29\\. America’s Antitrust Enforcement Credibility Crisis](https://scottcleland.com/wp-content/uploads/2023/11/Americas-Antitrust-Enforcement-Credibility-Crisis-White-Paper.pdf)\n\n[30\\. Problems with the Internet](https://www.rachel.ie/problems-with-the-internet/)\n\n[31\\. A communications oligopoly on steroids: Why antitrust enforcement and regulatory oversight in digital communications matter](https://equitablegrowth.org/wp-content/uploads/2017/07/071817-kimmelman-cooper2.pdf)\n\n[32\\. Deregulation Has Created Monopolies - And That’s Why Your Internet Sucks](https://trofire.com/2014/03/07/deregulation-created-monopolies-thats-internet-sucks/)\n\n[33\\. Decentralized Governance of AI Agents](https://arxiv.org/pdf/2412.17114)\n\n[34\\. Rishi Bommasani, Drew A. Hudson et al. “On the Opportunities and Risks of Foundation Models.” ArXiv](https://arxiv.org/abs/2108.07258)\n\n[35\\. Araz Taeihagh. “Governance of artificial intelligence.” Policy and Society](https://doi.org/10.1080/14494035.2021.1928377)\n\n[36\\. Flexible, Pro-Innovation Governance Strategies for Artificial Intelligence](https://www.rstreet.org/wp-content/uploads/2023/04/Final_Study283.pdf)\n\n[37\\. The Convergence of Artificial Intelligence and the Life Sciences: Safeguarding Technology, Rethinking Governance, and Preventing Catastrophe](https://www.nti.org/wp-content/uploads/2023/10/NTIBIO_AI_Executive-Summary_FINAL.pdf)\n\n[38\\. Laura Weidinger, J. Uesato et al. “Taxonomy of Risks posed by Language Models.” Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency](https://doi.org/10.1145/3531146.3533088)\n\n[39\\. AI Innovation Concentration and the Governance Challenge](https://www.cigionline.org/documents/2584/no.292.pdf)\n\n[40\\. R. Vinuesa, Hossein Azizpour et al. “The role of artificial intelligence in achieving the Sustainable Development Goals.” Nature Communications](https://doi.org/10.1038/s41467-019-14108-y)\n\n[41\\. A Roadmap for Governing AI: Technology Governance and Power Sharing Liberalism](https://ash.harvard.edu/wp-content/uploads/2024/01/340040_hks_ashgovroadmap_v2.pdf)\n\n[42\\. Urs Gasser, Virgílio A. F. Almeida. “A Layered Model for AI Governance.” IEEE Internet Computing](https://doi.org/10.1109/MIC.2017.4180835)\n\n[43\\. A Perspective on Decentralizing AI](https://nanda.media.mit.edu/decentralized_AI_perspective.pdf)\n\n[44\\. Alan Chan, Rebecca Salganik et al. “Harms from Increasingly Agentic Algorithmic Systems.” Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency](https://doi.org/10.1145/3593013.3594033)\n\n[45\\. 人工智能风险治理：模式、工具与策略](https://www.rdfybk.com/qw/DownPdf?id=854439)\n\n[53\\. DeepSeek Redefining AI Excellence Beyond OpenAI](https://ijrpr.com/uploads/V6ISSUE2/IJRPR38836.pdf)\n\n[54\\. DeepSeek solidified open-source AI as a serious conten...](https://cointelegraph.com/news/deep-seek-solidified-open-source-ai-serious-contender-ai-founder)\n\n[55\\. AI Disruption at Scale: DeepSeek’s Open-Source Model and Its Macroeconomic Impact on Markets, Labor, and Global Growth](https://academiainsight.com/index.php/ijbmfr/article/download/398/209/457)\n\n[56\\. DeepSeek: A Game Changer in the Global AI Race?](https://research.nus.edu.sg/eai/wp-content/uploads/sites/2/2025/03/EAIC-86-20250306-English_Chinese-2.pdf)\n\n[57\\. Top AI Models of 2025: A Comparative Analysis ...](https://flexxited.com/blog/top-ai-models-of-2025-a-comparative-analysis-of-deepseek-and-openai)\n\n[58\\. HACKING WITH AI: The Use of Generative AI in Malicious Cyber Activity](https://dfrlab.org/wp-content/uploads/sites/3/2024/02/csi-report-hacking-with-ai.pdf)\n\n[59\\. 글로벌 AI 패러다임 변화와 대응 전략 - 트럼프 정부의 AI 정책 전환과 중국 딥시크의 부상을 중심으로 -](https://www.kistep.re.kr/boardDownload.es?bid=0031&list_no=94030&seq=1)\n\n[60\\. DeepSeek (深度求索) AI and its Implications for Innovation in Financial Services](https://www.kcmi.re.kr/kcmifile/webzine_content/OPINION/6527/webzinepdf_6527.pdf)\n\n[61\\. Open source and under control: The DeepSeek paradox](https://www.businesstimes.com.sg/opinion-features/open-source-and-under-control-deepseek-paradox)\n\n[62\\. 观点：DeepSeek巩固了开源AI对于集中管理项目的有力竞争地位](https://news.marsbit.co/flash/20250209111426560832.html)\n\n[63\\. A Timely Quick Literature Review on the Deepseek in Chinese Publications](https://press.jandoo.ac/journal/csm/article/download/148/113/632)\n\n[64\\. Does DeepSeek disrupt your AI strategy? It should.](https://www.protiviti.com/sites/default/files/2025-02/vision-infocus-does-deepseek-disrupt-your-ai-strategy.pdf)\n\n[65\\. How DeepSeek is Reshaping the AI Landscape](https://www.fxmweb.com/insights/how-deepseek-is-reshaping-the-ai-landscape.html)\n\n[66\\. DeepSeek: Transforming the Foundations of Education](https://www.preprints.org/manuscript/202503.1776/v1/download)\n\n[67\\. 熊节、塞尔吉奥·阿马德乌：DeepSeek为什么要开源？这可能与人工智能的领导权息息相关](https://k.sina.com.cn/article_1887344341_707e96d502001l1fa.html)\n\n[68\\. Deepseek将面临更大挑战](https://www.bilibili.com/video/av113928453554557)\n\n[73\\. Anthony Dewayne Hunt. “Bitcoin: A Peer-to-Peer Electronic Cash System.”](https://doi.org/10.2139/ssrn.3440802)\n\n[74\\. P. Kairouz, H. B. McMahan et al. “Advances and Open Problems in Federated Learning.” Found. Trends Mach. Learn.](https://doi.org/10.1561/2200000083)\n\n[75\\. Jakub Konecný, H. B. McMahan et al. “Federated Learning: Strategies for Improving Communication Efficiency.” ArXiv](https://arxiv.org/abs/1610.05492)\n\n[76\\. Tian Li, Anit Kumar Sahu et al. “Federated Learning: Challenges, Methods, and Future Directions.” IEEE Signal Processing Magazine](https://doi.org/10.1109/MSP.2020.2975749)\n\n[77\\. Decentralized Governance of AI Agents](https://arxiv.org/pdf/2412.17114)\n\n[78\\. Flexible, Pro-Innovation Governance Strategies for Artificial Intelligence](https://www.rstreet.org/wp-content/uploads/2023/04/Final_Study283.pdf)\n\n[79\\. Decentralized Intelligence Network (DIN)](https://arxiv.org/pdf/2407.02461)\n\n[80\\. A Perspective on Decentralizing AI](https://nanda.media.mit.edu/decentralized_AI_perspective.pdf)\n\n[81\\. The challenges of the General Data Protection Regulation to protect data subjects against the adverse effects of artificial intelligence](https://iris.unibocconi.it/retrieve/4a3e6974-68bd-4ca7-8ece-d6b9f97f1cf4/Thesis_Marengo_Federico.pdf)\n\n[82\\. AI Revolution in Web3: The Decentralized Future](https://ideausher.com/blog/ai-in-web3/)\n\n[83\\. AI-Driven Data Governance Frameworks for Enhanced Privacy and Compliance](https://ijsrm.net/index.php/ijsrm/article/download/4380/3768/17836)\n\n[84\\. ICSA Bulletin](https://www.icsa.org/wp-content/uploads/2024/07/ICSA-Bulletin-2024-July-Issue.pdf)\n\n[85\\. Decentralized AI - The Future of Artificial Intelligence.](https://swarmzero.ai/blog/what-is-decentralized-ai)\n\n[86\\. Decentralized Artificial Intelligence Could Be Tech Giants’ Biggest Nightmare](https://insidetelecom.com/the-rise-of-decentralized-artificial-intelligence/)\n\n[87\\. Dual Governance: The intersection of centralized regulation and ...](https://paperswithcode.com/paper/dual-governance-the-intersection-of)\n\n[93\\. AI Agents: The Next Big Thing in Decentralized AI](https://devlabs.angelhack.com/blog/ai-agents/)\n\n[94\\. DAO Governance and AI](https://stablelab.xyz/blog/dao-governance-and-ai)\n\n[95\\. DAO Governance](https://www.aidf.nus.edu.sg/wp-content/uploads/2023/02/DAO_Governance-Han-Lee-Li-WP23-022723.pdf)\n\n[96\\. AI DAO White Paper](http://aidao.finance/assets/files/aidao_wp.pdf)\n\n[97\\. Samer Hassan, Primavera De Filippi. “Decentralized Autonomous Organization.” Internet Policy Rev.](https://doi.org/10.14763/2021.2.1556)\n\n[98\\. Robin Fritsch, Marino Müller et al. “Analyzing Voting Power in Decentralized Governance: Who controls DAOs?.” ArXiv](https://doi.org/10.48550/arXiv.2204.01176)\n\n[99\\. Exploring governance frameworks and decision processes in blockchain-based decentralized autonomous organizations](https://www.sciencepubco.com/index.php/IJBAS/article/download/33319/18076/68994)\n\n[100\\. Usman W. Chohan. “The Decentralized Autonomous Organization and Governance Issues.” Regulation of Financial Institutions eJournal](https://doi.org/10.2139/SSRN.3082055)\n\n[101\\. AI Powered Governance Models For DAOs - Codearies](https://codearies.com/development-of-ai-powered-governance-models-for-daos/)\n\n[102\\. Bridging Communities: AI and Metaverse Projects Whitepaper](https://www.aimeta.club/assets/images/AIMETA-Whitepaper-ENG.pdf)\n\n[103\\. DAO Governance: An Empirical Investigation on the Heterogeneity amongst DAO Governance Systems](https://dawo24.org/wp-content/uploads/2024/06/DAWO24_Full_Paper_31.pdf)\n\n[104\\. Decentralized Governance of AI Agents](https://arxiv.org/pdf/2412.17114)\n\n[105\\. Youssef El Faqir, J. Arroyo et al. “An overview of decentralized autonomous organizations on the blockchain.” Proceedings of the 16th International Symposium on Open Collaboration](https://doi.org/10.1145/3412569.3412579)\n\n[106\\. Governance of a DAO for Facilitating Dialogue on Human-Algorithm Interaction and the Impact of Emerging Technologies on Society](https://braga.net.br/img/mscf.pdf)\n\n[107\\. Examples of Early DAO Projects and Learnings From Them](https://btcpeers.com/examples-of-early-dao-projects-and-learnings-from-them/)\n\n[113\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[114\\. OpenAI vs. DeepSeek (2025): AI Security, Data Governance ...](https://guptadeepak.com/openai-vs-deepseek-navigating-the-ai-trust-paradox-in-an-era-of-geopolitical-tensions/)\n\n[115\\. DeepSeek Security, Privacy, and Governance: Hidden ...](https://theori.io/blog/deepseek-security-privacy-and-governance-hidden-risks-in-open-source-ai)\n\n[116\\. mp.weixin.qq.com/s?\\__biz=MzAwMTM4OTMxNw==&mid=2650911510&idx=...](https://mp.weixin.qq.com/s?__biz=MzAwMTM4OTMxNw%3D%3D&mid=2650911510&idx=1&sn=d7b9e70fa8b7964fb132e881b2dc76ad&chksm=801cc7f6a4b1f2af00862235102bf4e374d6a9cbc70d3cf25c6c13c4407bf966194817f78e93&scene=27)\n\n[117\\. Evaluating AI Security: Insights from DeepSeek-R1](https://www.aujas.com/hubfs/Evaluating-AI-Security-Insights-from-DeepSeek-R1.pdf)\n\n[118\\. Cisco study shows DeepSeek is very susceptible to attacks](https://www.tomsguide.com/ai/cisco-study-shows-deepseek-is-very-susceptible-to-attacks-heres-why)\n\n[119\\. Deepseek's AI model proves easy to jailbreak - and wor...](https://www.zdnet.com/article/deepseeks-ai-model-proves-easy-to-jailbreak-and-worse/undefined)\n\n[120\\. IMPACT OF THE DEEP SEEK DATA BREACH ON THE DATABASE INFRASTRUCTURE](https://www.irjmets.com/uploadedfiles/paper/issue_2_february_2025/67703/final/fin_irjmets1739612164.pdf)\n\n[121\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[122\\. Does DeepSeek disrupt your AI strategy? It should.](https://www.protiviti.com/sites/default/files/2025-02/vision-infocus-does-deepseek-disrupt-your-ai-strategy-global.pdf)\n\n[123\\. Security Now! #1011 - 02-04-25 Jailbreaking AI](https://www.grc.com/sn/sn-1011-notes.pdf)\n\n[124\\. AI Disruption at Scale: DeepSeek’s Open-Source Model and Its Macroeconomic Impact on Markets, Labor, and Global Growth](https://academiainsight.com/index.php/ijbmfr/article/download/398/209/457)\n\n[125\\. DeepSeek's Breakthrough: A New Era for AI with Less Compute Power](https://opentools.ai/news/deepseeks-breakthrough-a-new-era-for-ai-with-less-compute-power)\n\n[126\\. Open Source Marketing Strategy of AI Companies from the Perspective of the Knowledge Gap Hypothesis Research--On the Case of DeepSeek](https://www.ewadirect.com/proceedings/aemps/article/view/22109/pdf)\n\n[127\\. Efficiency and safety of the DeepSeek R1 model compared to OpenAI models](https://e-postulat.ru/index.php/Postulat/article/download/6089/6185)\n\n[128\\. DeepSeek breakthrough emboldens open-source AI ...](https://www.cnbc.com/2025/02/04/deepseek-breakthrough-emboldens-open-source-ai-models-like-meta-llama.html)\n\n[129\\. DeepSeek's open-source model challenges proprietary AI ...](https://www.glideapps.com/news/deepseek-open-source-challenges-proprietary-ai-yann-lecun-meta)\n\n[130\\. Jiancheng Ye, Sophie Bronstein et al. “DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models.”](https://arxiv.org/abs/2506.01257)\n\n[133\\. Anthony Dewayne Hunt. “Bitcoin: A Peer-to-Peer Electronic Cash System.”](https://doi.org/10.2139/ssrn.3440802)\n\n[134\\. Daniel Davis Wood. “ETHEREUM: A SECURE DECENTRALISED GENERALISED TRANSACTION LEDGER.”](https://www.semanticscholar.org/paper/3c50bb6cc3f5417c3325a36ee190e24f0dc87257)\n\n[135\\. K. Christidis, M. Devetsikiotis. “Blockchains and Smart Contracts for the Internet of Things.” IEEE Access](https://doi.org/10.1109/ACCESS.2016.2566339)\n\n[136\\. Loi Luu, D. Chu et al. “Making Smart Contracts Smarter.” Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security](https://doi.org/10.1145/2976749.2978309)\n\n[137\\. Solutions for the IoT](https://www.baslerweb.com/en/learning/blockchain/)\n\n[138\\. Nicola Atzei, Massimo Bartoletti et al. “A Survey of Attacks on Ethereum Smart Contracts (SoK).” IACR Cryptol. ePrint Arch.](https://doi.org/10.1007/978-3-662-54455-6_8)\n\n[139\\. Yi-Hong Yeh, Sheng-Chun Hsueh et al. “A Double-Blind Trial Platform Based on Distributed Ledger Technology.” Electronics](https://doi.org/10.3390/electronics13010132)\n\n[140\\. A Brief Introduction to Deepfakes](https://www.devteam.space/blog/how-to-build-a-deepfake-protection-software/)\n\n[141\\. A Blockchain-Based Architecture for Collaborative DDoS Mitigation with Smart Contracts](https://link.springer.com/content/pdf/10.1007/978-3-319-60774-0_2.pdf)\n\n[142\\. Decentralized Governance of AI Agents](https://arxiv.org/pdf/2412.17114)\n\n[143\\. SMART CONTRACTS](https://www.barnaschone.co.za/smart-contracts/)\n\n[144\\. Video: “Blockchain and monopolization” - Network Law...](https://www.networklawreview.org/video-blockchain-monopolization/)\n\n[145\\. Thibault Schrepel. “Blockchain and monopolization.” Blockchain + Antitrust](https://doi.org/10.4337/9781800885530.00018)\n\n[146\\. A Blockchain-Based Architecture for Collaborative DDoS ... - Springer](https://link.springer.com/chapter/10.1007/978-3-319-60774-0_2)\n\n[147\\. Internet Economics XIII](https://files.ifi.uzh.ch/CSG/teaching/IFI-2020.01.pdf)\n\n[148\\. The Price of Smart Contract Privacy](https://minesparis-psl.hal.science/hal-04702045v1/document)\n\n[149\\. Hybrid Smart Contracts in Ethereum](https://www.dpss.inesc-id.pt/~jpbarreto/data/uploads/thesis/leonor-loureiro-midterm.pdf)\n\n[150\\. Z. Shah, Imdad Ullah et al. “Blockchain Based Solutions to Mitigate Distributed Denial of Service (DDoS) Attacks in the Internet of Things (IoT): A Survey.” Sensors (Basel, Switzerland)](https://doi.org/10.3390/s22031094)\n\n[151\\. Harnessing Blockchain for Collective Defense: A Strategy for Detecting and Combating DDoS Attacks](https://www.internationaljournalssrg.org/IJEEE/2024/Volume11-Issue3/IJEEE-V11I3P125.pdf)\n\n[152\\. Труды Института Системного Программирования РАН](https://www.ispras.ru/upload/uf/a56/a56d0d9fa3fa84ea3ac144e01619db30.pdf)\n\n[153\\. Scott M. Lundberg, Su-In Lee. “A Unified Approach to Interpreting Model Predictions.” Neural Information Processing Systems](https://arxiv.org/abs/1705.07874)\n\n[154\\. Ninareh Mehrabi, Fred Morstatter et al. “A Survey on Bias and Fairness in Machine Learning.” ACM Computing Surveys (CSUR)](https://doi.org/10.1145/3457607)\n\n[155\\. Solon Barocas, Andrew D. Selbst. “Big Data's Disparate Impact.” California Law Review](https://doi.org/10.2139/SSRN.2477899)\n\n[156\\. Michael Feldman, Sorelle A. Friedler et al. “Certifying and Removing Disparate Impact.” Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining](https://doi.org/10.1145/2783258.2783311)\n\n[157\\. AI Data Security: Best Practices for Securing Data Used to Train & Operate AI Systems](https://www.ic3.gov/CSA/2025/250522.pdf)\n\n[158\\. PROACTIVE DATA GOVERNANCE: USING AI AND ML TO ANTICIPATE AND MITIGATE RISKS](https://iaeme.com/MasterAdmin/Journal_uploads/IJAIML/VOLUME_3_ISSUE_2/IJAIML_03_02_013.pdf)\n\n[159\\. Choose and utilize metrics - Training | Microsoft Learn](https://learn.microsoft.com/zh-tw/training/modules/evaluate-generative-ai-apps/4-choose-utilize-metrics/)\n\n[160\\. F. Kamiran, T. Calders. “Data preprocessing techniques for classification without discrimination.” Knowledge and Information Systems](https://doi.org/10.1007/s10115-011-0463-8)\n\n[161\\. AI Governance: A controls playbook with mappings to the European Union AI Act and the NIST AI Risk Management Framework](https://yourdataconnect.com/wp-content/uploads/2024/05/AI_Governance_May_2024.pdf)\n\n[162\\. Risk Sources and Risk Management Measures in Support of Standards for General-Purpose AI Systems](https://arxiv.org/pdf/2410.23472)\n\n[163\\. Quantitative AI Risk Assessments: Opportunities and Challenges](https://arxiv.org/pdf/2209.06317)\n\n[164\\. Bias in Business: Minimising Bias with Responsible Machine Learning](https://www.gbgplc.com/apac/blog/bias-in-business-minimising-bias-with-responsible-machine-learning/)\n\n[165\\. Detecting Risk of Biased Output with Balance Measures](https://dl.acm.org/doi/10.1145/3530787)\n\n[166\\. Enhancing Data Security in Artificial Intelligence Systems: A Cybersecurity and Information Governance Approach](https://files.sdiarticle5.com/wp-content/uploads/2025/05/Ms_JERR_135480.docx)\n\n[167\\. The Future of DAOs is Powered by AI - Aragon's Blog](https://blog.aragon.org/ai-daos-the-future-of-daos-powered-by-artificial-intelligence/)\n\n[168\\. AI-Powered DevSecOps: Navigating Automation, Risk and ...](https://devops.com/ai-powered-devsecops-navigating-automation-risk-and-compliance-in-a-zero-trust-world/)\n\n[169\\. AI影響アセスメント](http://maruyama-mitsuhiko.cocolog-nifty.com/security/files/2024iwmaiimpactassessment2.0en20ja.docx)\n\n[170\\. DAO Governance and AI](https://stablelab.xyz/blog/dao-governance-and-ai)\n\n[173\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[174\\. I. Goodfellow, Jonathon Shlens et al. “Explaining and Harnessing Adversarial Examples.” CoRR](https://arxiv.org/abs/1412.6572)\n\n[175\\. Christian Szegedy, Wojciech Zaremba et al. “Intriguing properties of neural networks.” CoRR](https://arxiv.org/abs/1312.6199)\n\n[176\\. Nicholas Carlini, D. Wagner. “Towards Evaluating the Robustness of Neural Networks.” 2017 IEEE Symposium on Security and Privacy (SP)](https://doi.org/10.1109/SP.2017.49)\n\n[177\\. IMPACT OF THE DEEP SEEK DATA BREACH ON THE DATABASE INFRASTRUCTURE](https://www.irjmets.com/uploadedfiles/paper/issue_2_february_2025/67703/final/fin_irjmets1739612164.pdf)\n\n[178\\. Nicolas Papernot, P. Mcdaniel et al. “Practical Black-Box Attacks against Machine Learning.” Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security](https://doi.org/10.1145/3052973.3053009)\n\n[179\\. Does using DeepSeek create security risks? | TechTarget](https://www.techtarget.com/searchenterpriseai/tip/Does-using-DeepSeek-create-security-risks)\n\n[180\\. DeepSeek AI Offline After Cybersecurity Attacks and Data ...](https://natlawreview.com/article/deepseek-ais-security-woes-impersonations-what-you-need-know)\n\n[181\\. Global AI Regulation: DeepSeek Ban Impact & Cross-Border ...](https://www.compliancehub.wiki/global-ai-regulation-wave-how-italys-deepseek-ban-triggered-a-worldwide-scrutiny-of-chinese-ai-models-germany-netherlands-taiwan/)\n\n[182\\. Safety Evaluation of DeepSeek Models in Chinese Contexts](https://papers-pdfs.assets.alphaxiv.org/2502.11137v2.pdf)\n\n[183\\. THE DARK DEEP SIDE OF DEEPSEEK: FINE-TUNING ATTACKS AGAINST THE SAFETY ALIGNMENT OF CoT-ENABLED MODELS](https://www.arxiv.org/pdf/2502.01225)\n\n[184\\. Security Flaws Found In DeepSeek Leads To Jailbreak - Techworm](https://www.techworm.net/2025/01/security-flaws-found-in-deepseek-leads-to-jailbreak.html)\n\n[185\\. DeepSeek被指不当使用OpenAI模型](https://stateofmankind.net/PDF/0129mx.pdf)\n\n[186\\. DeepSeek全球爆火：敲响AI狂潮下的安全警钟](https://www.cwasp.cn/articles/1123.html)\n\n[187\\. CNME | ISSUE 389 | JANUARY 2025](https://www.tahawultech.com/wp-content/uploads/2015/09/cnme_20250205.pdf)\n\n[188\\. Jiancheng Ye, Sophie Bronstein et al. “DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models.”](https://arxiv.org/abs/2506.01257)\n\n[189\\. 是否有任何可用的补丁可以修复DeepSeek R1中的漏洞](https://codingmall.com/knowledge-base/25-global/246230-deepseek-r1)\n\n[190\\. Security Now! #1011 - 02-04-25 Jailbreaking AI](https://www.grc.com/sn/sn-1011-notes.pdf)\n\n[191\\. 邓建鹏、赵治松:DeepSeek 的破局与变局:论生成式人工智能...](https://mp.weixin.qq.com/s?__biz=MzI5MzA2NjUwOA%3D%3D&mid=2650241676&idx=1&sn=c08e56db79cd78ad68200a886a87b800&chksm=f47418efc30391f9886e457c97e84db32bf2eb856a4e3bb4f8979936fb24d26fe3d029a5211c&scene=27)\n\n[192\\. Red Teaming Report LLM Featured: DeepSeek-R1](https://cdn.prod.website-files.com/6690a78074d86ca0ad978007/679bc2e71b48e423c0ff7e60_1%20RedTeaming_DeepSeek_Jan29_2025%20%281%29.pdf)\n\n[193\\. J. Rochet, Jean Triole. “Platform competition in two sided markets.” LSE Research Online Documents on Economics](https://doi.org/10.1162/154247603322493212)\n\n[194\\. J. Rochet, J. Tirole. “Two-sided markets: a progress report.” The RAND Journal of Economics](https://doi.org/10.1111/J.1756-2171.2006.TB00036.X)\n\n[195\\. S. Choudary, Marshall W. van Alstyne et al. “Platform Revolution: How Networked Markets Are Transforming the Economy--and How to Make Them Work for You.”](https://www.semanticscholar.org/paper/108041751271defd1ec505ffb93b2ec598d434f8)\n\n[196\\. David S. Evans. “The Antitrust Economics of Multi-Sided Platform Markets.” Yale Journal on Regulation](https://www.semanticscholar.org/paper/aef4ba3a170b9863f305c8562ad0ce1726ae2f98)\n\n[197\\. Decentralized Governance of AI Agents](https://arxiv.org/pdf/2412.17114)\n\n[198\\. Ethos' Deeply Integrated Distributed Types](https://flyn.org/publications/2014-Ethos-Types.pdf)\n\n[199\\. Ioannis Mollas, Zoe Chrysopoulou et al. “ETHOS: a multi-label hate speech detection dataset.” Complex & Intelligent Systems](https://doi.org/10.1007/s40747-021-00608-2)\n\n[200\\. Ethos Integration: Understanding APIs & the Ethos framework](https://www.ellucian.com/assets/emea-ap/meuc-2019-ethos-integration-understanding-apis-and-ethos-framework.pdf)\n\n[201\\. Ethos in Action Decision-Making Framework](https://www.ifpma.org/wp-content/uploads/2024/03/ifpma-ethos-in-action-facilitation-guide-mar-2024.pdf)\n\n[203\\. Z. Obermeyer, Brian W. Powers et al. “Dissecting racial bias in an algorithm used to manage the health of populations.” Science](https://doi.org/10.1126/science.aax2342)\n\n[204\\. Ninareh Mehrabi, Fred Morstatter et al. “A Survey on Bias and Fairness in Machine Learning.” ACM Computing Surveys (CSUR)](https://doi.org/10.1145/3457607)\n\n[205\\. Moritz Hardt, Eric Price et al. “Equality of Opportunity in Supervised Learning.” ArXiv](https://arxiv.org/abs/1610.02413)\n\n[206\\. Joy Buolamwini, Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” FAT](https://www.semanticscholar.org/paper/18858cc936947fc96b5c06bbe3c6c2faa5614540)\n\n[207\\. B. Zhang, Blake Lemoine et al. “Mitigating Unwanted Biases with Adversarial Learning.” Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society](https://doi.org/10.1145/3278721.3278779)\n\n[208\\. Risks of AI Algorithmic Bias in Higher Education](https://www.schiller.edu/blog/risks-of-ai-algorithmic-bias-in-higher-education/)\n\n[209\\. Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms](https://siliconflatirons.org/wp-content/uploads/2021/02/Algorithmic-bias-detection-and-mitigation_-Best-practices-and-policies-to-reduce-consumer-harms.pdf)\n\n[210\\. Algorithmic Bias in Education](https://www.pcla.wiki/index.php/Algorithmic_Bias_in_Education)\n\n[211\\. Effect of AI intervention programs for older adults on the ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC11930482/)\n\n[212\\. Investigating Sources and Effects of Bias in AI-Based Systems – Results from an MLR](https://doras.dcu.ie/29092/1/BiasInAISystemsCameraReady.pdf)\n\n[213\\. AI技术在DDoS缓解中的机遇与挑战](https://www.vnetwork.vn/en-US/news/ung-dung-ai-trong-chong-tan-cong-ddos/)\n\n[214\\. ENHANCED DATA UTILIZATION FOR EFFICIENT AND TRUSTWORTHY DEEP LEARNING](https://knowledge.uchicago.edu/record/12434/files/Zhuokai_PhD_Dissertation_final_draft.pdf)\n\n[215\\. AI Revolution in Higher Education. What you need to know](https://documents1.worldbank.org/curated/en/099757104152527995/pdf/IDU-b1e5ef00-75ff-4ba4-a4b6-84899c3ea968.pdf?fbclid=IwY2xjawJ37HhleHRuA2FlbQIxMABicmlkETAxZ09Cc3RBNGY3TXNwdjhhAR5EdIkifp2_fGYL632mfNC-0CabAfRwGwDKcAjfg5vqeNG48Z7REIiJ-0nH4Q_aem_EcOjnpVxGYwTyV1k6R9Skw)\n\n[216\\. DAO Governance](https://www.aidf.nus.edu.sg/wp-content/uploads/2023/02/DAO_Governance-Han-Lee-Li-WP23-022723.pdf)\n\n[217\\. Journal of Applied Economics and Policy Studies](https://www.ewadirect.com/journal/jaeps/volumes/vol/13/508.pdf)\n\n[218\\. Governance of a DAO for Facilitating Dialogue on Human-Algorithm Interaction and the Impact of Emerging Technologies on Society](https://braga.net.br/img/mscf.pdf)\n\n[219\\. Towards Algorithm Auditing: A Survey on Managing Legal, Ethical and Technological Risks of AI, ML and Associated Algorithms](https://uploads-ssl.webflow.com/6305e5d52c28356b4fe71bac/6333094010329a5eaa8a5969_SSRN-id3778998_compressed.pdf)\n\n[220\\. Auditing the quality of datasets used in algorithmic decision-making systems](https://www.europarl.europa.eu/RegData/etudes/STUD/2022/729541/EPRS_STU%282022%29729541_EN.pdf)\n\n[221\\. Mitigating the impact of biased artificial intelligenc...](https://link.springer.com/article/10.1038/s43856-022-00214-4)\n\n[223\\. Anthony Dewayne Hunt. “Bitcoin: A Peer-to-Peer Electronic Cash System.”](https://doi.org/10.2139/ssrn.3440802)\n\n[224\\. Building a centralized data lake: A game-changer for enterprise data management](https://gjeta.com/sites/default/files/GJETA-2025-0134.pdf)\n\n[225\\. F. Breugel, J. Worrell. “An Algorithm for Quantitative Verification of Probabilistic Transition Systems.” International Conference on Concurrency Theory](https://doi.org/10.1007/3-540-44685-0_23)\n\n[226\\. Enabling efficient collection and usage of network performance metrics at the edge](https://research.chalmers.se/publication/545462/file/545462_Fulltext.pdf)\n\n[227\\. Multi-Modal Trust Architecture for AI-HR Systems: Analyzing Technical Determinants of User Acceptance in Enterprise-Scale People Analytics Platforms](https://www.ijfmr.com/papers/2024/1/31724.pdf)\n\n[228\\. L. Valerio, A. Passarella et al. “A communication efficient distributed learning framework for smart environments.” ArXiv](https://doi.org/10.1016/j.pmcj.2017.07.014)\n\n[229\\. IOTA调研报告](https://zhuanlan.zhihu.com/p/83085933)\n\n[230\\. Decentralized Governance of AI Agents](https://arxiv.org/pdf/2412.17114)\n\n[231\\. Technology platforms and approaches for building ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC10729736/)\n\n[233\\. Z. Obermeyer, Brian W. Powers et al. “Dissecting racial bias in an algorithm used to manage the health of populations.” Science](https://doi.org/10.1126/science.aax2342)\n\n[234\\. Ninareh Mehrabi, Fred Morstatter et al. “A Survey on Bias and Fairness in Machine Learning.” ACM Computing Surveys (CSUR)](https://doi.org/10.1145/3457607)\n\n[235\\. Moritz Hardt, Eric Price et al. “Equality of Opportunity in Supervised Learning.” ArXiv](https://arxiv.org/abs/1610.02413)\n\n[236\\. Joy Buolamwini, Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” FAT](https://www.semanticscholar.org/paper/18858cc936947fc96b5c06bbe3c6c2faa5614540)\n\n[237\\. B. Zhang, Blake Lemoine et al. “Mitigating Unwanted Biases with Adversarial Learning.” Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society](https://doi.org/10.1145/3278721.3278779)\n\n[238\\. Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms](https://siliconflatirons.org/wp-content/uploads/2021/02/Algorithmic-bias-detection-and-mitigation_-Best-practices-and-policies-to-reduce-consumer-harms.pdf)\n\n[239\\. Algorithmic Bias in Education](https://www.pcla.wiki/index.php/Algorithmic_Bias_in_Education)\n\n[240\\. Effect of AI intervention programs for older adults on the ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC11930482/)\n\n[241\\. AI技术在DDoS缓解中的机遇与挑战](https://www.vnetwork.vn/en-US/news/ung-dung-ai-trong-chong-tan-cong-ddos/)\n\n[242\\. Algorithmic Bias in Education | International Journal of Artificial Intelligence in Education](https://link.springer.com/article/10.1007/s40593-021-00285-9)\n\n[243\\. Investigating Sources and Effects of Bias in AI-Based Systems – Results from an MLR](https://doras.dcu.ie/29092/1/BiasInAISystemsCameraReady.pdf)\n\n[244\\. Auditing the quality of datasets used in algorithmic decision-making systems](https://www.europarl.europa.eu/RegData/etudes/STUD/2022/729541/EPRS_STU%282022%29729541_EN.pdf)\n\n[245\\. International Scientific Report on the Safety of Advanced AI: Interim Report](https://www.developmentaid.org/api/frontend/cms/file/2024/05/international_scientific_report_on_the_safety_of_advanced_ai_interim_report.pdf)\n\n[246\\. Towards Algorithm Auditing: A Survey on Managing Legal, Ethical and Technological Risks of AI, ML and Associated Algorithms](https://uploads-ssl.webflow.com/6305e5d52c28356b4fe71bac/6333094010329a5eaa8a5969_SSRN-id3778998_compressed.pdf)\n\n[247\\. Journal of Applied Economics and Policy Studies](https://www.ewadirect.com/journal/jaeps/volumes/vol/13/508.pdf)\n\n[248\\. Governance of a DAO for Facilitating Dialogue on Human-Algorithm Interaction and the Impact of Emerging Technologies on Society](https://braga.net.br/img/mscf.pdf)\n\n[249\\. Bias in AI: tackling the issues through regulations and standards](https://publicpolicy.ie/downloads/papers/2024/Bias_in_AI.pdf)\n\n[250\\. Technical Perspective: The Impact of Auditing for Algorithmic Bias](https://acmwebvm01.acm.org/magazines/2023/1/267956-technical-perspective-the-impact-of-auditing-for-algorithmic-bias/abstract)\n\n[251\\. ENHANCED DATA UTILIZATION FOR EFFICIENT AND TRUSTWORTHY DEEP LEARNING](https://knowledge.uchicago.edu/record/12434/files/Zhuokai_PhD_Dissertation_final_draft.pdf)\n\n[252\\. Mitigating the impact of biased artificial intelligenc...](https://link.springer.com/article/10.1038/s43856-022-00214-4)\n\n[253\\. Decentralized Governance of AI Agents](https://arxiv.org/pdf/2412.17114)\n\n[254\\. Review of Gen AI Models for Financial Risk Management: Architectural Frameworks and Implementation Strategies](https://www.preprints.org/frontend/manuscript/270cde8b1e9cf8f395e60b54b26af2fc/download_pub)\n\n[255\\. Centralizing or Decentralizing Generative AI? The Answer](https://aws.amazon.com/blogs/enterprise-strategy/centralizing-or-decentralizing-generative-ai-the-answer-both/)\n\n[256\\. Building the Next Generation of UK Decacorns](https://www.cbi.org.uk/media/ukmng1qh/occ-fact-pack.pdf)\n\n[257\\. Dual Governance: The intersection of centralized regulation and ...](https://montrealethics.ai/dual-governance-the-intersection-of-centralized-regulation-and-crowdsourced-safety-mechanisms-for-generative-ai/)\n\n[258\\. Two Frameworks for Balancing AI Innovation and Risk](https://faisalhoque.com/two-frameworks-for-balancing-ai-innovation-and-risk/)\n\n[259\\. Democratizing AI Governance: Balancing Expertise and P...](http://arxiv.org/html/2502.08651v1)\n\n[260\\. Frontier Governance Framework](https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Frontier-Governance-Framework.pdf)\n\n[261\\. Foundation Models: A New Paradigm for Artificial Intelligence](https://d-nb.info/132563624X/34)\n\n[262\\. Governance of Artificial Intelligence: A Risk and Guideline-Based Integrative Framework](https://www.sciencedirect.com/science/article/pii/S0740624X22000181)\n\n[263\\. Dual Governance: The intersection of centralized ... - Paper Reading](https://paperreading.club/page?id=177792)\n\n[264\\. Beware the gap: Governance arrangements in the face of AI innovation](https://download.asic.gov.au/media/mtllqjo0/rep-798-published-29-october-2024.pdf)\n\n[265\\. AI Governance: How to Mitigate Risks & Maximize Benefits](https://atlan.com/know/ai-readiness/ai-governance/)\n\n[266\\. Avijit Ghosh, D. Lakshmi. “Dual Governance: The intersection of centralized regulation and crowdsourced safety mechanisms for Generative AI.” ArXiv](https://doi.org/10.48550/arXiv.2308.04448)\n\n[267\\. A Formal Framework for Assessing and Mitigating Emergent Security Risks in Generative AI Models: Bridging Theory and Dynamic Risk Mitigation](https://openreview.net/pdf?id=2wZJJOItPz)\n\n[268\\. 包容适应与风险防范：生成式人工智能助推政府治理现代化的耦合框架](https://www.social.uestc.edu.cn/cn/article/pdf/preview/10.14071/j.1008-8105%282024%29-5013.pdf)\n\n[269\\. Master The Art Of AI+ Enterprise Transformation](https://govindhtech.com/master-the-art-of-ai-enterprise-transformation/)\n\n[273\\. Anthony Dewayne Hunt. “Bitcoin: A Peer-to-Peer Electronic Cash System.”](https://doi.org/10.2139/ssrn.3440802)\n\n[274\\. K. Christidis, M. Devetsikiotis. “Blockchains and Smart Contracts for the Internet of Things.” IEEE Access](https://doi.org/10.1109/ACCESS.2016.2566339)\n\n[275\\. J. Benet. “IPFS - Content Addressed, Versioned, P2P File System.” ArXiv](https://arxiv.org/abs/1407.3561)\n\n[276\\. A Data Engineering Framework for Ethereum Beacon Chain Rewards: From Data Collection to Decentralization Metrics](https://www.nature.com/articles/s41597-025-04623-7.pdf)\n\n[277\\. Enabling efficient collection and usage of network performance metrics at the edge](https://research.chalmers.se/publication/545462/file/545462_Fulltext.pdf)\n\n[278\\. A Deep Dive into Ethereum’s PoS Transition: Protocol Design Choices and Their Empirical Unexpected Limitations](https://upcommons.upc.edu/bitstream/handle/2117/424719/TMCG1de1.pdf?sequence=1)\n\n[279\\. Developing a Data Strategy to Reduce Manual Processes and Leverage Actionable Insight](https://www.cefpro.com/developing-a-data-strategy-to-reduce-manual-processes-and-leverage-actionable-insight-2/)\n\n[280\\. IAEE ENERGY FORUM](https://www.iaee.org/documents/EF242.pdf)\n\n[283\\. Z. Obermeyer, Brian W. Powers et al. “Dissecting racial bias in an algorithm used to manage the health of populations.” Science](https://doi.org/10.1126/science.aax2342)\n\n[284\\. Ninareh Mehrabi, Fred Morstatter et al. “A Survey on Bias and Fairness in Machine Learning.” ACM Computing Surveys (CSUR)](https://doi.org/10.1145/3457607)\n\n[285\\. DAO Governance: An Empirical Investigation on the Heterogeneity amongst DAO Governance Systems](https://dawo24.org/wp-content/uploads/2024/06/DAWO24_Full_Paper_31.pdf)\n\n[286\\. Joy Buolamwini, Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” FAT](https://www.semanticscholar.org/paper/18858cc936947fc96b5c06bbe3c6c2faa5614540)\n\n[287\\. International AI Safety Report](https://italianelfuturo.com/wp-content/uploads/2025/01/International_AI_Safety_Report_2025_accessible_f_250130_073143.pdf)\n\n[288\\. AI-assisted peer review](https://www.nature.com/articles/s41599-020-00703-8.pdf)\n\n[289\\. Leveraging Secured AI-Driven Data Analytics for Cybersecurity: Safeguarding Information and Enhancing Threat Detection](https://ijrpr.com/uploads/V5ISSUE10/IJRPR34195.pdf)\n\n[290\\. Hybrid AI Agents: The New Architecture for Responsible AI and Brand-Safe AI-RAG Systems](https://kama.ai/mecuhir/uploads/2025/05/Hybrid-AI-Agents-Final-2.pdf)\n\n[291\\. Governance of a DAO for Facilitating Dialogue on Human-Algorithm Interaction and the Impact of Emerging Technologies on Society](https://braga.net.br/img/mscf.pdf)\n\n[292\\. R. Bellamy, A. Mojsilovic et al. “AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias.” IBM Journal of Research and Development](https://doi.org/10.1147/jrd.2019.2942287)\n\n[293\\. Anna Jobin, M. Ienca et al. “Artificial Intelligence: the global landscape of ethics guidelines.” ArXiv](https://doi.org/10.1038/s42256-019-0088-2)\n\n[294\\. Investigating Sources and Effects of Bias in AI-Based Systems – Results from an MLR](https://doras.dcu.ie/29092/1/BiasInAISystemsCameraReady.pdf)\n\n[295\\. The AI DAO WHITE PAPER](https://cdn.prod.website-files.com/66a75c91714b945c303ed032/68115ff59dfba9e442375661_AI%20DAO%20White%20Paper%20VF.pdf)\n\n[303\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[304\\. Decentralized Governance of AI Agents](https://arxiv.org/pdf/2412.17114)\n\n[305\\. Dual Governance: The intersection of centralized regulation and ...](https://montrealethics.ai/dual-governance-the-intersection-of-centralized-regulation-and-crowdsourced-safety-mechanisms-for-generative-ai/)\n\n[306\\. AI AND QUANTUM: Catalyst For Transformative Growth In Financial Services](https://22287007.fs1.hubspotusercontent-na1.net/hubfs/22287007/Oliver-Wyman_AI-and-Quantum_2025%20%282%29.pdf)\n\n[307\\. Artificial Intelligence in the Public Sector](https://documents1.worldbank.org/curated/en/746721616045333426/pdf/Artificial-Intelligence-in-the-Public-Sector-Summary-Note.pdf)\n\n[308\\. Turning AI governance risk into reward with the AI Control Tower](https://static.rainfocus.com/servicenow/k24/sess/1702672535066001Yc05/FinalPresentation/SES1417_AI-Governance-Control-Tower_GirishBrian_1717012278669001jHWZ.pdf)\n\n[309\\. Governance of AI adoption in central banks](https://www.bis.org/publ/othp90.pdf)\n\n[310\\. Centralizing or Decentralizing Generative AI? The Answer](https://aws.amazon.com/blogs/enterprise-strategy/centralizing-or-decentralizing-generative-ai-the-answer-both/)\n\n[311\\. Balancing Innovation and Risk: AI Governance in Financial ...](https://hmstrategy.com/balancing-innovation-and-risk-ai-governance-in-financial-services/)\n\n[312\\. AI Inventories: Practical Challenges for Organizational Risk Management](https://20965052.fs1.hubspotusercontent-na1.net/hubfs/20965052/AI%20Inventories%20Practical%20Challenges%20for%20Organizational%20Risk%20Management%20%283%29.pdf)\n\n[313\\. Integrating AI Strategy in Corporate Governance](https://primathon.in/blog/integrating-ai-strategy-in-corporate-governance/?amp=1)\n\n[314\\. Democratizing AI Governance: Balancing Expertise and P...](http://arxiv.org/html/2502.08651v1)\n\n[315\\. Foundation Models: A New Paradigm for Artificial Intelligence](https://d-nb.info/132563624X/34)\n\n[316\\. AI governance in practice: developing secure and ...](https://academy.itu.int/training-courses/full-catalogue/ai-governance-practice-developing-secure-and-innovative-frameworks)\n\n[317\\. Safety and Global Governance of Generative AI Report](https://www.wfeo.org/wp-content/uploads/2024/CEIT_Safety-and-Global-Governance-of-Generative-AI.pdf)\n\n[318\\. Driving responsible innovation: Reflections on a year of AI governance](https://kpmg.com/kpmg-us/content/dam/kpmg/pdf/2024/ai-governance-whitepaper.pdf)\n\n[323\\. Anthony Dewayne Hunt. “Bitcoin: A Peer-to-Peer Electronic Cash System.”](https://doi.org/10.2139/ssrn.3440802)\n\n[324\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[325\\. Song Han, Jeff Pool et al. “Learning both Weights and Connections for Efficient Neural Network.” Neural Information Processing Systems](https://arxiv.org/abs/1506.02626)\n\n[326\\. Yue Zhao, Meng Li et al. “Federated Learning with Non-IID Data.” ArXiv](https://doi.org/10.48550/arXiv.1806.00582)\n\n[327\\. Virginia Smith, Chao-Kai Chiang et al. “Federated Multi-Task Learning.” Neural Information Processing Systems](https://arxiv.org/abs/1705.10467)\n\n[328\\. A Deep Dive into Ethereum’s PoS Transition: Protocol Design Choices and Their Empirical Unexpected Limitations](https://upcommons.upc.edu/bitstream/handle/2117/424719/TMCG1de1.pdf?sequence=1)\n\n[329\\. Concentration versus excellence: lessons learned of European R&D &I framework programs](https://link.springer.com/content/pdf/10.1007/s11192-024-05220-y.pdf)\n\n[330\\. Decentralized Governance of AI Agents](https://arxiv.org/pdf/2412.17114)\n\n[331\\. Edge Inference Solutions](https://www.luxonis.com/edge-inference)\n\n[333\\. Z. Obermeyer, Brian W. Powers et al. “Dissecting racial bias in an algorithm used to manage the health of populations.” Science](https://doi.org/10.1126/science.aax2342)\n\n[334\\. Journal of Applied Economics and Policy Studies](https://www.ewadirect.com/journal/jaeps/volumes/vol/13/508.pdf)\n\n[335\\. Moritz Hardt, Eric Price et al. “Equality of Opportunity in Supervised Learning.” ArXiv](https://arxiv.org/abs/1610.02413)\n\n[336\\. Ninareh Mehrabi, Fred Morstatter et al. “A Survey on Bias and Fairness in Machine Learning.” ACM Computing Surveys (CSUR)](https://doi.org/10.1145/3457607)\n\n[337\\. Joy Buolamwini, Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” FAT](https://www.semanticscholar.org/paper/18858cc936947fc96b5c06bbe3c6c2faa5614540)\n\n[338\\. Solon Barocas, Andrew D. Selbst. “Big Data's Disparate Impact.” California Law Review](https://doi.org/10.2139/SSRN.2477899)\n\n[339\\. AI-assisted peer review](https://www.nature.com/articles/s41599-020-00703-8.pdf)\n\n[340\\. Exploring governance frameworks and decision processes in blockchain-based decentralized autonomous organizations](https://www.sciencepubco.com/index.php/IJBAS/article/download/33319/18076/68994)\n\n[341\\. U. Peters. “Algorithmic Political Bias in Artificial Intelligence Systems.” Philosophy & Technology](https://doi.org/10.1007/s13347-022-00512-8)\n\n[342\\. Leveraging Secured AI-Driven Data Analytics for Cybersecurity: Safeguarding Information and Enhancing Threat Detection](https://ijrpr.com/uploads/V5ISSUE10/IJRPR34195.pdf)\n\n[343\\. Diversity by Design and AI: A legal and operational framework for bias mitigation](https://www.privacylaws.com/media/4775/wed-0930-diversity-by-design-and-ai.pdf)\n\n[344\\. Investigating Sources and Effects of Bias in AI-Based Systems – Results from an MLR](https://doras.dcu.ie/29092/1/BiasInAISystemsCameraReady.pdf)\n\n[345\\. Algorithmic bias, data ethics, and governance: Ensuring fairness, transparency and compliance in AI-powered business analytics applications](https://journalwjarr.com/sites/default/files/fulltext_pdf/WJARR-2025-0571.pdf)\n\n[346\\. TESTIMONY OF NICOL TURNER LEE](https://www.congress.gov/118/meeting/house/116588/witnesses/HHRG-118-IF16-Wstate-TurnerLeeN-20231114.pdf)\n\n[347\\. Artificial Intelligence, Bias, and Decision Making: A Review of Major Findings](https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1032&context=mwais2025)\n\n[353\\. Centralizing or Decentralizing Generative AI? The Answer](https://aws.amazon.com/blogs/enterprise-strategy/centralizing-or-decentralizing-generative-ai-the-answer-both/)\n\n[354\\. Governing the Agent-to-Agent Economy of Trust via Prog...](http://arxiv.org/html/2501.16606v1)\n\n[355\\. Decentralized Governance of AI Agents](https://arxiv.org/pdf/2412.17114)\n\n[356\\. Hybrid Governance Models → Term](https://fashion.sustainability-directory.com/term/hybrid-governance-models/)\n\n[357\\. Driving responsible innovation: Reflections on a year of AI governance](https://kpmg.com/kpmg-us/content/dam/kpmg/pdf/2024/ai-governance-whitepaper.pdf)\n\n[358\\. Exploring governance frameworks and decision processes in blockchain-based decentralized autonomous organizations](https://www.sciencepubco.com/index.php/IJBAS/article/download/33319/18076/68994)\n\n[359\\. Democratizing AI Governance: Balancing Expertise and P...](http://arxiv.org/html/2502.08651v1)\n\n[360\\. Integrating Enterprise AI: How to Align with Your Existing IT](https://coworker.ai/blog/enterprise-ai-integration-guide)\n\n[361\\. HARNESSING AI RESPONSIBLY: CRAFTING AN EFFECTIVE DATA & AI GOVERNANCE FRAMEWORK](https://www.infosys.com/services/data-ai-topaz/insights/harnessing-ai-responsibly.pdf)\n\n[362\\. Accelerating AI with Data Management](https://www.capitalone.com/tech/ai/data-management/)\n\n[363\\. Building the Next Generation of UK Decacorns](https://www.cbi.org.uk/media/ukmng1qh/occ-fact-pack.pdf)\n\n[364\\. AI-powered CRM and ERP systems: Transforming business operations through smart technology](https://journalwjarr.com/sites/default/files/fulltext_pdf/WJARR-2025-1499.pdf)\n\n[365\\. What is Enterprise AI? Benefits, Best Practices, and Examples](https://www.automationanywhere.com/rpa/enterprise-ai#:~:text=Integration%20with%20existing%20systems:%20AI,work%20seamlessly%20within%20current%20workflows.)\n\n[366\\. AIREA: An AI-Driven Optimization Framework for Intelligent Automation in Large-Scale Enterprise Systems](https://urfjournals.org/open-access/airea-an-ai-driven-optimization-framework-for-intelligent-automation-in-large-scale-enterprise-systems.pdf)\n\n[367\\. Scale your enterprise AI capabilities | IBM](https://www.ibm.com/ca-fr/resources/the-data-differentiator/scale-ai)\n\n[368\\. Integrating AI Strategy in Corporate Governance](https://primathon.in/blog/integrating-ai-strategy-in-corporate-governance/?amp=1)"
  },
  {
    "id": 28,
    "question": "How might the proliferation of permissively licensed, reasoning-step-revealing models like DeepSeek R1 fundamentally alter the economics of AI application development?",
    "response": "**Research Report: The Economic Transformation of AI Application Development in the Age of Permissively Licensed, Reasoning-Step-Revealing Models**\n\n**Date:** July 26, 2025\n\n**Author:** Expert AI Researcher\n\n**Executive Summary**\n\nThe artificial intelligence landscape is undergoing a seismic shift, driven by the emergence of powerful, open, and transparent large language models (LLMs). The release of models like DeepSeek R1 in January 2025 marks a pivotal moment, introducing a new paradigm that fundamentally alters the economics of AI application development. This report analyzes how the proliferation of permissively licensed, reasoning-step-revealing models is reshaping the industry. The core finding is that this trend is democratizing access to advanced AI, significantly lowering costs across development and compliance, and accelerating innovation cycles. This shift transfers economic value from proprietary model licensing to the application layer, favoring enterprises that can leverage transparency for customization, trust, and efficiency. However, it also introduces new economic considerations related to infrastructure, specialized talent, and security, creating a new set of strategic trade-offs for businesses navigating the AI frontier.\n\n**1\\. A New Paradigm in AI: The DeepSeek R1 Case Study**\n\nThe AI market, long dominated by closed-source, proprietary models from vendors like OpenAI, is being disrupted by a new class of open models. DeepSeek R1, released in January 2025, serves as a prime exemplar of this transformation, combining two critical features that have profound economic implications: a highly permissive license and inherent reasoning transparency.\n\n**1.1 The Freedom of Permissive Licensing**\n\nDeepSeek R1 is governed by the MIT License, one of the most permissive open-source licenses available \\[6\\]\\[14\\]. This is a stark contrast to the more restrictive licenses accompanying other popular models like Llama 3 or Gemma \\[6\\]\\[14\\]. The MIT License grants users sweeping freedoms that directly impact development economics:\n\n**Zero-Cost Commercial Use:** The model can be integrated into commercial products and services without licensing fees, eliminating a significant upfront and ongoing cost barrier \\[1\\]\\[2\\]\\[3\\].\n\n**Unhindered Modification and Distribution:** Developers can freely modify, customize, and distribute the model and its derivatives, fostering a vibrant ecosystem of innovation and tailored solutions \\[2\\]\\[3\\]\\[4\\].\n\n**Right to Distill:** The license explicitly allows for model distillation, enabling businesses to use the powerful R1 model to train smaller, more efficient, and specialized models for specific tasks, further reducing deployment costs \\[2\\]\\[7\\]\\[18\\].\n\n**Full Intellectual Property Control:** Businesses retain full IP rights over their custom implementations, preventing the vendor lock-in that characterizes proprietary ecosystems \\[1\\]\\[3\\].\n\nThis open approach is a strategic move to democratize AI technology, breaking down resource barriers that previously favored large, well-funded corporations \\[9\\]\\[16\\]\\[27\\].\n\n**1.2 The Economic Value of Reasoning Transparency**\n\nBeyond its license, DeepSeek R1’s architecture is designed to reveal its reasoning process. It can \"think out loud,\" exposing the step-by-step logic it uses to arrive at a conclusion \\[34\\]\\[103\\]. This is achieved through a combination of a Mixture-of-Experts (MoE) architecture and a training process heavily reliant on Reinforcement Learning (RL), which encourages emergent behaviors like self-reflection and verification \\[30\\]\\[33\\]\\[39\\]. This transparency is not merely a technical curiosity; it is a critical economic driver for several reasons:\n\n**Enhanced Trust and Reliability:** By making the AI's \"thought process\" visible, developers and end-users can better understand and trust its outputs. This is paramount in high-stakes, regulated industries like finance, law, and healthcare \\[22\\]\\[34\\]\\[96\\].\n\n**Accelerated Debugging and Refinement:** When a model produces an error, transparent reasoning allows developers to pinpoint the exact step where the logic failed. This dramatically reduces the time and resources spent on debugging and fine-tuning compared to troubleshooting a \"black box\" model \\[103\\]\\[112\\]\\[222\\].\n\n**Improved Auditability and Compliance:** The ability to produce an audit trail of its reasoning process makes it easier for organizations to meet regulatory requirements that demand explainability in automated decision-making \\[100\\]\\[163\\]\\[327\\].\n\n**2\\. Recalibrating the Economics of AI Development**\n\nThe combination of permissive licensing and reasoning transparency fundamentally alters the cost-benefit analysis of building and deploying AI applications.\n\n**2.1 Lowering Barriers to Entry and Igniting Competition**\n\nThe most immediate economic impact is the drastic reduction in the cost of entry into the AI market.\n\n**From High Licensing Fees to Zero-Cost Foundations:** Previously, building sophisticated AI applications often required paying substantial and recurring licensing or API fees to proprietary vendors \\[43\\]\\[46\\]. Open-source models like DeepSeek R1 eliminate this entire cost category, making advanced AI accessible to startups, small businesses, and researchers who were previously priced out \\[53\\]\\[55\\]\\[346\\].\n\n**Shifting Market Power:** This democratization is creating a more competitive market. The dominance of proprietary vendors is being challenged, evidenced by market share shifts away from leaders like OpenAI towards both other proprietary competitors and the open-source community \\[148\\]\\[150\\]\\[151\\]. The release of DeepSeek R1 was so disruptive that it reportedly caused a significant drop in NVIDIA's stock price, as the model demonstrated the ability to achieve competitive results with less advanced hardware, threatening established value chains \\[84\\]\\[93\\]. By February 2025, just a month after its release, DeepSeek R1 had already captured 17.7% of AI development activity, signaling rapid adoption by developers \\[341\\].\n\n**2.2 Accelerating Innovation and Shortening Development Cycles**\n\nWhile direct, third-party validated studies quantifying reductions in \"AI feature development cycle time\" remain nascent, a wealth of qualitative and anecdotal evidence points toward a significant acceleration of innovation.\n\n**Increased Developer Productivity:** Enterprise teams adopting DeepSeek R1 have reported significant long-term productivity gains, with 78% of teams noting improvements after three months of use \\[116\\]\\[335\\]. This is driven by the automation of repetitive tasks and faster decision-making \\[176\\]\\[336\\].\n\n**Rapid Deployment and Prototyping:** The availability of powerful, pre-trained open models eliminates the immense cost and time of training a foundational model from scratch. Furthermore, cloud platforms like Tencent Cloud's HAI now enable one-click deployment of DeepSeek R1, reducing model setup and configuration from weeks of effort to just minutes \\[179\\]\\[236\\].\n\n**Faster Iteration Through Transparency:** As noted, the ability to see the model's reasoning steps allows developers to debug and refine applications far more quickly. This translates into shorter iteration loops and a faster path from concept to production-ready feature. This is a clear efficiency gain over opaque proprietary models where debugging can be a process of trial and error.\n\n**2.3 Drastic Reductions in Operational and Compliance Costs**\n\nBeyond initial development, the economic benefits extend throughout the application lifecycle.\n\n**Lowering Operational Costs:** DeepSeek R1's efficient design has led to dramatic operational cost savings. Its MoE architecture activates only a fraction of its total parameters for any given request, significantly reducing computational requirements \\[24\\]\\[30\\]. Partners like Cykel AI reported reducing their AI-related costs by up to 96% by integrating DeepSeek R1 \\[21\\]\\[337\\]. Other reports indicate it can be up to 30 times cheaper to run than OpenAI's o1, operating at just 4% of the cost for some tasks \\[98\\]\\[111\\]\\[280\\]. Furthermore, the availability of smaller, distilled versions of the model (from 1.5B to 70B parameters) allows for deployment on more moderate and less expensive hardware \\[23\\]\\[29\\].\n\n**The \"Transparency Dividend\" in Compliance:** In regulated industries, the cost of compliance is a major operational expense. Reasoning-step transparency directly addresses this. Reports on the implementation of explainable AI (XAI) in financial services—a direct proxy for the capabilities of models like R1—show staggering cost reductions. These include a **47.6% annual reduction in compliance-related costs** \\[156\\]\\[375\\]a **52.6% decrease in compliance-related operational costs** \\[156\\]\\[375\\]and a **76.9% reduction in regulatory violations** \\[375\\]. One study noted that 48% of enterprises avoided regulatory fines, saving a collective $2.1 million in 2025 \\[161\\]. DeepSeek R1's \"audit-friendly output\" is specifically designed to meet these regulatory demands for rationale documentation, turning a compliance burden into a competitive advantage \\[222\\]\\[327\\].\n\n**3\\. Market Dynamics and Enterprise Adoption Trends**\n\nThe availability of models like DeepSeek R1 is not just a technical development; it is reshaping enterprise IT strategy and market composition.\n\n**The Shift to Open Source:** There is a clear trend of enterprises moving away from a sole reliance on proprietary AI. In 2023, 80% of companies relied on third-party generative AI software. By 2024, that number had shifted dramatically, with 47% of companies choosing internal development, often leveraging open-source models \\[146\\]\\[149\\]. The preference among top enterprise leaders for open-source AI models rose to 46% in 2024 \\[137\\].\n\n**Adoption in Regulated Industries:** Financial services, an industry often cautious with new technology, has seen notable adoption. By February 2025, dozens of securities firms, fund companies, and insurance firms in China had announced or completed integration of DeepSeek models \\[262\\]. One report indicates DeepSeek has already served over 200 such enterprises \\[267\\]. Use cases are demonstrating tangible ROI, such as an automated valuation system that reduced contract review time by 93.75% and saved over 20 million yuan in annual operational costs \\[223\\], and a 40% reduction in fraud detection time \\[287\\].\n\n**Experimentation vs. Production:** Despite the enthusiasm, a gap remains between exploration and full-scale deployment. One survey indicated that while 57% of respondents had experimented with DeepSeek R1, only 3% had integrated it into production environments \\[117\\]. This highlights that while the economic potential is clear, practical implementation challenges remain.\n\n**4\\. New Challenges and the Evolving Economic Equation**\n\nThe shift towards open, transparent models is not a simple cost reduction but a rebalancing of the economic equation. Costs are moving from predictable licensing fees to other, more variable categories.\n\n**New Cost Centers: Infrastructure and Talent:** While the software may be free, running it is not. Enterprises must invest in their own infrastructure, including powerful GPUs and servers, to host, fine-tune, and run these models at scale \\[43\\]\\[50\\]\\[55\\]. This requires a highly skilled technical team for deployment, maintenance, and ongoing optimization—a significant investment in human capital \\[43\\]. Transitioning existing AI pipelines to new architectures like MoE can also present significant technical and financial hurdles \\[129\\]\\[182\\].\n\n**The Cost of Security and Risk Mitigation:** With greater freedom comes greater responsibility. A critical analysis of DeepSeek R1 highlighted significant security risks, including a high compliance risk score (9.0), an 81% failure rate in certain hallucination tests, and the potential to generate toxic language \\[328\\]. Enterprises must invest in robust security vetting, governance, and guardrail systems to mitigate these risks, which represents a new and crucial cost center. For global firms, the model's Chinese origin and dataset provenance can also pose geopolitical and data privacy challenges, potentially limiting adoption \\[121\\]\\[128\\].\n\n**Performance Variability and Customization Costs:** Open-source models are not a one-size-fits-all solution. Their out-of-the-box performance on highly specialized tasks can be variable, necessitating costly fine-tuning and adaptation to meet specific business needs \\[117\\]\\[193\\].\n\n**5\\. Conclusion: A Fundamental Reshaping of AI Value**\n\nThe proliferation of permissively licensed, reasoning-step-revealing models like DeepSeek R1 represents a fundamental inflection point in the economics of AI. The era of prohibitive licensing costs and opaque, black-box models is giving way to a more open, transparent, and competitive ecosystem.\n\nThis paradigm shift redefines the economic calculus for AI development. It drastically lowers the barrier to entry, accelerates innovation through enhanced productivity and easier debugging, and delivers substantial operational and compliance cost savings, particularly through the \"transparency dividend.\"\n\nThe core economic trade-off is a shift in spending from third-party vendor licensing fees (OPEX) toward internal infrastructure and specialized talent (a mix of CAPEX and OPEX). This gives enterprises unprecedented control over their AI destiny but also saddles them with greater responsibility for security, maintenance, and ethical implementation.\n\nLooking forward, this trend will continue to intensify competition, forcing proprietary vendors to compete on price, performance, and transparency. The economic value in the AI stack is migrating upwards—away from the foundational model itself and towards the application layer. In this new landscape, the ability to effectively customize, integrate, debug, and, most importantly, _trust_ an AI's output is becoming the ultimate economic differentiator. The reasoning process is no longer a hidden liability but a transparent, auditable, and highly valuable asset.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. DeepSeek-R1 - Advanced Reasoning AI Model](https://deepseekv3.org/deepseek-r1)\n\n[2\\. DeepSeek 团队分享了针对DeepSeek-R1 部署的最佳设置](https://www.p2hp.com/show/81.html)\n\n[3\\. DeepSeek-R1 - 進階推理人工智慧模型](https://deepseekv3.org/zh-Hant/deepseek-r1)\n\n[4\\. Why building big AIs costs billions, and how Chinese startup DeepSeek dramatically changed the calculus](https://techxplore.com/news/2025-01-big-ais-billions-chinese-startup.pdf)\n\n[5\\. DeepSeekのR1推論AIモデルがアップデート - GPT Master](https://chatgpt-enterprise.jp/blog/deepseek-r1-update/)\n\n[6\\. Deepseek's R1 model closes the gap with OpenAI and Google ...](https://the-decoder.com/deepseeks-r1-model-closes-the-gap-with-openai-and-google-after-major-update/)\n\n[7\\. 灵椿不借青云力，自有开源照夜明：DeepSeek的欧盟合规之 ...](https://chancebridge.com/article/?id=2676)\n\n[8\\. deepseek-r1/license](https://ollama.com/library/deepseek-r1/blobs/6e4c38e1172f)\n\n[9\\. DeepSeek-R1重磅开源！性能对标OpenAI，模型蒸馏、MIT协议](https://www.yizz.cn/8964.html)\n\n[10\\. DeepSeek模型解读](https://www.lnvut.edu.cn/ai/zd_mxjd20250203.pdf)\n\n[11\\. DeepSeek: 行业应用与实践](https://zy.9611.xyz/A-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/E06-%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A66%EF%BC%9ADeepSeek%E8%A1%8C%E4%B8%9A%E5%BA%94%E7%94%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5%20V2.pdf)\n\n[12\\. 2025年B站最新AI大模型零基础教程！全部基于Deepseek（GPT-4+RAG+Langchain+Agent智能体+LangGraph+提示词）](https://b23.tv/BV14hQLYbEZk?t=1110)\n\n[13\\. 开源软件/组件声明 open source software/components NOTICE](https://mkp-res.hc-cdn.com/marketplace/public/appv2/attachment/F1E/361/32B/0000000000F1E36132B.20250416090902.f446a5a27d184487bc16d80b62583512.docx)\n\n[14\\. 天空访谈: AI, DeepSeek R1, 国产算力卡的适配进展, 算力...](https://view.inews.qq.com/k/20250218A08IBF00)\n\n[15\\. DeepSeek-R1/LICENSE at main · mapbased/DeepSeek-R1 · ...](https://github.com/mapbased/DeepSeek-R1/blob/main/LICENSE)\n\n[16\\. Jiancheng Ye, Sophie Bronstein et al. “DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models.”](https://arxiv.org/abs/2506.01257)\n\n[17\\. DeepSeek+DeepResearch: 让科研像聊天一样简单](https://its.hainanu.edu.cn/__local/F/C1/2A/4B7455C80F7EEA27E3593335CDE_BC452C0E_12D4458.pdf)\n\n[18\\. DeepSeek-R1 发布，性能对标OpenAI o1 正式版 - MKEAI](https://www.mkeai.com/info/detail/50.html)\n\n[19\\. Margaret Mitchell, Simone Wu et al. “Model Cards for Model Reporting.” Proceedings of the Conference on Fairness, Accountability, and Transparency](https://doi.org/10.1145/3287560.3287596)\n\n[20\\. DeepSeek+DeepResearch 让科研像聊天一样简单](https://library.qiangtu.com/download/1011/pdf/1011.pdf)\n\n[21\\. PIONEER AI PARTNER COMPANY, CYKEL AI, ANNOUNCES INTEGRATION OF DEEPSEEK R1, REDUCING AI COSTS UP TO 96%](https://cdn.prod.website-files.com/633c6a023878f61ab5c1c1e5/67b887f8db6ac5b2e54fa807_JPEG%20PR_0220_Cykel%20DeepSeek%20R1%20integration.pdf)\n\n[22\\. Enterprise IT World MEA](https://enterpriseitworldmea.com/wp-content/uploads/2025/03/new-mea.pdf)\n\n[23\\. Top 5 Chinese Open-Source LLM Models - Index.dev](https://www.index.dev/blog/chinese-open-source-llms#:~:text=Discover%20the%20top%205%20Chinese,key%20contributor%20in%20this%20space.)\n\n[24\\. RAG System for AI Reasoning with DeepSeek R1 Distilled ...](https://www.analyticsvidhya.com/blog/2025/02/distilled-deepseek-r1-model/)\n\n[25\\. DeepSeek explained: Everything you need to know - TechTarget](https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know#:~:text=DeepSeek,%20a%20Chinese%20AI%20firm,models,%20challenging%20U.S.%20tech%20giants.)\n\n[26\\. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948)\n\n[27\\. Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models](https://www.arxiv.org/pdf/2503.08275)\n\n[28\\. Alibaba touts open-source AI model as surpassing DeepSeek R1 performance](https://hkshippers.org.hk/assets/uploads/media-uploader/071741318953.pdf)\n\n[29\\. Running DeepSeek R1 0528 Qwen 8B Locally](https://apidog.com/blog/deepseek-r1-0528-qwen-8b-local-ollama-lm-studio/)\n\n[30\\. Content Analysis of DeepSeek’s Disruption in the Technology Industry](https://carijournals.org/journals/index.php/JTS/article/download/2573/2999/7409?srsltid=AfmBOopT6u0wyk9FWDr6VnL8go6RINMf-fV3A5-gqESKY7NooDTAv8yH)\n\n[31\\. DeepSeek R1: Open-Source AI Reasoning Model That Beats OpenAI's o1](https://techwiser.com/deepseek-r1-open-source-ai-reasoning-model-that-beats-openais-o1/)\n\n[32\\. Deploy DeepSeek R1 on AWS Bedrock](https://community.aws/content/2sIJqPaPMtmNxlRIQT5CzpTtziA/deploy-deepseek-r1-on-aws-bedrock)\n\n[33\\. How DeepSeek Narrowed the Gap to OpenAI's o1 Model](https://dev.to/mahmoudayoub/how-deepseek-narrowed-the-gap-to-openais-o1-model-a-revolutionary-step-in-reasoning-ai-43ph)\n\n[34\\. DeepSeek vs OpenAI: Which is Better in 2025?](https://interviewkickstart.com/blogs/articles/deepseek-vs-openai)\n\n[35\\. Deploying DeepSeek R1 on Sangfor HCI](https://www.sangfor.com/blog/cloud-and-infrastructure/deploying-deepseek-r1-on-sangfor-hci)\n\n[36\\. 「至善」普通大学生AI逆袭文档](https://zhishanzhe.com/pdfs/%E3%80%8C%E8%87%B3%E5%96%84%E3%80%8D%E6%99%AE%E9%80%9A%E5%A4%A7%E5%AD%A6%E7%94%9FAI%E9%80%86%E8%A2%AD%E6%96%87%E6%A1%A3%20.pdf)\n\n[37\\. 通信行业周报 2025年第6周 Deepseek-R1开源推动AI应用发展，头部AI厂支持Deepseek](https://www.faxianai.com/wp-content/uploads/2025/05/1747035038-3%E3%80%81%E9%80%9A%E4%BF%A1%E8%A1%8C%E4%B8%9A%E5%91%A8%E6%8A%A52025%E5%B9%B4%E7%AC%AC6%E5%91%A8%EF%BC%9ADeepseek-R1%E5%BC%80%E6%BA%90%E6%8E%A8%E5%8A%A8AI%E5%BA%94%E7%94%A8%E5%8F%91%E5%B1%95%EF%BC%8C%E5%A4%B4%E9%83%A8AI%E5%8E%82%E6%94%AF%E6%8C%81Deepseek.pdf)\n\n[38\\. Ultimate Comparison of DeepSeek Models: V3, R1, and R1-Zero](https://blog.spheron.network/ultimate-comparison-of-deepseek-models-v3-r1-and-r1-zero)\n\n[39\\. DeepSeek-R1: A New Reasoning Model](https://bytejournal.blog/assets/reports/DeepSeek%20R1%20Model%20Analysis.pdf)\n\n[40\\. Building DeepSeek AI Models: Architecture, Implementation, and Optimization](https://walzone.com/books/wp-content/uploads/2025/03/document.pdf)\n\n[41\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[42\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[43\\. AI Development Cost Estimation: Pricing Structure, ROI](https://www.coherentsolutions.com/insights/ai-development-cost-estimation-pricing-structure-roi)\n\n[44\\. Choosing Between Open-Source LLM & Proprietary AI Model](https://inclusioncloud.com/insights/blog/open-source-llm-vs-proprietary-models/)\n\n[45\\. Emily M. Bender, Timnit Gebru et al. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.” Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency](https://doi.org/10.1145/3442188.3445922)\n\n[46\\. Compare proprietary vs. open source for enterprise AI](https://www.techtarget.com/searchenterpriseai/tip/Compare-proprietary-vs-open-source-for-enterprise-AI)\n\n[47\\. FOUNDATIONAL MODELS: BUILDING BLOCKS FOR GENERATIVE AI APPLICATIONS](https://www.pymnts.com/wp-content/uploads/2023/09/2-FOUNDATIONAL-MODELS-BUILDING-BLOCKS-FOR-GENERATIVE-AI-APPLICATIONS-Stefan-Geirhofer-Scott-McKinney.pdf)\n\n[48\\. Open source vs proprietary models of generative AI](https://par-tec.com/tech-know-how-to-go-open-source-vs-proprietary-models-of-generative-ai/)\n\n[49\\. AI-Agent-Engineering](https://zenodo.org/records/14999965/files/AI_agent_engineering.pdf?download=1)\n\n[50\\. How Much Does It Cost to Build an AI Agent?](https://www.esferasoft.com/blog/how-much-does-it-cost-to-build-an-ai-agent)\n\n[51\\. 企业在部署生成式AI时选择开源与专有模型的考量](https://towardsai.net/p/artificial-intelligence/the-choice-for-businesses-between-open-source-and-proprietary-models-to-deploy-generative-ai)\n\n[52\\. Irene Solaiman, Miles Brundage et al. “Release Strategies and the Social Impacts of Language Models.” ArXiv](https://arxiv.org/abs/1908.09203)\n\n[53\\. Proprietary vs Open-Source AI Models in Generative AI](https://adasci.org/proprietary-vs-open-source-ai-models-in-generative-ai/)\n\n[54\\. Open Source vs Proprietary Enterprise AI Platforms and LMS](https://botscrew.com/blog/open-source-proprietary-enterprise-ai-comparison/)\n\n[55\\. GenAI-driven transformation: Preparing your company for success with GenAI on every level](https://content.rolandberger.com/hubfs/07_presse/Roland%20Berger%20GenAI-driven%20transformation%20final.pdf)\n\n[56\\. Irene Solaiman. “The Gradient of Generative AI Release: Methods and Considerations.” Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency](https://doi.org/10.1145/3593013.3593981)\n\n[57\\. How to Calculate Business Value and Cost for Generative AI Use Cases](https://www.bainsight.com/wp-content/uploads/Gartner-How-to-calculate-Business-Value-and-Cost-for-GenAI.pdf)\n\n[58\\. Weighing the Open-Source, Hybrid Option for Adopting Generative AI](https://fr.cloudera.com/content/dam/www/marketing/resources/analyst-reports/weighing-the-open-source-hybrid-option-for-adopting-generative-ai.pdf?daqp=true)\n\n[59\\. Proprietary AI Models Are Dead. Long Live Proprietary AI Models  - The New Stack](https://thenewstack.io/proprietary-ai-models-are-dead-long-live-proprietary-ai-models/)\n\n[60\\. SUI GENERIS RIGHT FOR TRAINED AI MODELS](https://ipo.org/wp-content/uploads/2020/11/SG-model-rights-committee-paper-pub.pdf)\n\n[61\\. Wesley M. Cohen, Daniel A. Levinthal. “ABSORPTIVE CAPACITY: A NEW PERSPECTIVE ON LEARNING AND INNOVATION.” Administrative Science Quarterly](https://doi.org/10.2307/2393553)\n\n[62\\. H. Chesbrough. “Open Innovation: The New Imperative for Creating and Profiting from Technology.”](https://www.semanticscholar.org/paper/715273da7a7ea032b81af409c948179965d6eb2f)\n\n[63\\. J. Tirole, J. Lerner. “Some Simple Economics of Open Source.” IO: Firm Structure](https://doi.org/10.1111/1467-6451.00174)\n\n[64\\. D. Harhoff, J. Henkel et al. “Mit Sloan School of Management Mit Sloan School Working Paper 4749-09 Profiting from Voluntary Information Spillovers: How Users Benefit by Freely Revealing Their Innovations Profiting from Voluntary Information Spillovers: How Users Benefit by Freely Revealing Their Innovations \\*.”](https://www.semanticscholar.org/paper/1c8fe7749643409dcb09b75e2fe7590540262625)\n\n[65\\. 16 Changes to AI in the Enterprise: 2025 Edition](https://a16z.com/ai-enterprise-2025/)\n\n[66\\. A. Nuvolari. “Collective Invention during the British Industrial Revolution The Case of the Cornish Pumping Engine.” Or Spektrum](https://doi.org/10.1093/CJE/BEH011)\n\n[67\\. Open Source vs Proprietary Enterprise AI Platforms and LMS](https://botscrew.com/blog/open-source-proprietary-enterprise-ai-comparison/)\n\n[68\\. Why Entrepreneurs Need to Beware of Misleading \"Open\" AI Models](https://www.entrepreneur.com/science-technology/why-entrepreneurs-need-to-beware-of-misleading-open-ai/472948)\n\n[69\\. The Future of Enterprise AI Agents: Unlocking Autonomous Transformation in 2025](https://pl.cloudera.com/content/dam/www/marketing/resources/analyst-reports/the-future-of-enterprise-ai-agents.pdf?daqp=true)\n\n[70\\. Mid-2025 AI Update: What's Actually Working in Enterprise](https://gradientflow.com/mid-2025-ai-update-whats-actually-working-in-enterprise/)\n\n[71\\. FIRST AMENDED COMPLAINT](https://fingfx.thomsonreuters.com/gfx/legaldocs/znpnzrgyzpl/AI%20COPYRIGHT%20LAWSUIT%20amended.pdf)\n\n[72\\. Fast-track AI workflows with Lenovo & NVIDIA® Accelerated Solutions](https://pages.lenovo.com/rs/183-WCT-620/images/Lenovo_NVIDIA_GenAI_eBook_FINALpdf.pdf?version=0)\n\n[73\\. Open sourcing AI: intellectual property at the service of platform leadership](https://www.jipitec.eu/jipitec/article/download/356/349/1838)\n\n[74\\. Trends – Artificial Intelligence (AI)](https://www.mrbaogao.com/storage/attachments/2025/06/03/TJX0rgflGCJtvJM5o6ziHtrMrE6Yg9BpRiZSu2I0.pdf)\n\n[75\\. S. Jee, S. Sohn. “A firm’s creation of proprietary knowledge linked to the knowledge spilled over from its research publications: the case of artificial intelligence.” Industrial and Corporate Change](https://doi.org/10.1093/icc/dtad002)\n\n[76\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[77\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[78\\. DeepSeek Review: The Open Source Alternative to OpenAI - Greptile](https://www.greptile.com/blog/deepseek-review)\n\n[79\\. Xing Hao, Guigang Zhang et al. “Deep Learning.” Int. J. Semantic Comput.](https://doi.org/10.1142/S1793351X16500045)\n\n[80\\. DeepSeek R1 Vs Open AI O1: Who's the Winner?](https://www.ankursnewsletter.com/p/deepseek-r1-vs-open-ai-o1-whos-the)\n\n[81\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[82\\. DeepSeek R1 vs. OpenAI o1: A Cost-Effecti... | WhiteCube.ai](https://whitecube.ai/ai-daily-news/2025-01/2025-01-21-deepseek-r1-vs-openai-o1-a-cost-effective-ai-revolution/)\n\n[83\\. 东兴晨报](https://file.iyanbao.com/pdf/54a1f-c9f67e72-6eef-48a2-8a29-d9213f28d4f6.pdf)\n\n[84\\. DeepSeek R1: Why Open Source Is the Future of Enterprise AI ...](https://blog.premai.io/deepseek-r1-why-open-source-is-the-future-of-enterprise-ai-development/)\n\n[85\\. DeepSeek Disrupts AI Market](https://futurumgroup.com/insights/deepseek-disrupts-ai-market-with-low-cost-training-and-open-source-yet-many-questions-loom/)\n\n[86\\. DeepSeek R1 vs OpenAI O1: AI Model Comparison (2025)](https://www.zignuts.com/blog/deepseek-r1-vs-openai-o1-comparison)\n\n[87\\. How Deepseek R1 Outperformed OpenAI 01: A Comprehensive Analysis](https://media.licdn.com/dms/document/media/v2/D561FAQFqo2s9ArPsNQ/feedshare-document-pdf-analyzed/B56ZSc73gvGsAc-/0/1737799729227?e=1749686400&v=beta&t=QRPMaCsx8JLAxmGruIaqMFwhlpMpXSvRVlEvAmjoPdE)\n\n[88\\. DeepSeek's open-source AI roils markets, sending tech ...](https://kr-asia.com/deepseeks-open-source-ai-roils-markets-sending-tech-stocks-into-a-tailspin)\n\n[89\\. DeepSeek R1 vs Top AI Models: 5 Key Differences Reshaping ...](https://www.cognitivetoday.com/2025/02/deepseek-r1-vs-top-ai-models-5-key-differences-reshaping-the-industry-in-2025/)\n\n[90\\. DeepSeek vs OpenAI (2025): A Comparative Analysis](https://aitechtonic.com/deepseek-vs-openai/)\n\n[91\\. Enterprise IT World MEA](https://enterpriseitworldmea.com/wp-content/uploads/2025/03/new-mea.pdf)\n\n[92\\. DeepSeek-R1 vs. OpenAI's o1: A New Step in Open ...](https://www.marktechpost.com/2025/01/25/deepseek-r1-vs-openais-o1-a-new-step-in-open-source-and-proprietary-models/)\n\n[93\\. Concorrenza nel mercato dei Foundation Models: Dinamiche competitive & Policy recommendations](https://webthesis.biblio.polito.it/35633/1/tesi.pdf)\n\n[94\\. Open-source revolution: How DeepSeek-R1 challenges OpenAI's o1 with ...](https://venturebeat.com/ai/open-source-revolution-how-deepseek-r1-challenges-openais-o1-with-superior-processing-cost-efficiency/)\n\n[95\\. DeepSeek vs. OpenAI: The Battle of Open Reasoning Models](https://www.unite.ai/deepseek-vs-openai-the-battle-of-open-reasoning-models/)\n\n[96\\. Enterprise IT World MEA](https://enterpriseitworldmea.com/wp-content/uploads/2025/03/For-Online_compressed.pdf)\n\n[97\\. Private, secure DeepSeek-R1 in production in US & EU ...](https://www.baseten.co/blog/private-secure-deepseek-r1-in-production-in-us-eu-data-centers/)\n\n[98\\. DeepSeek's R1 and OpenAI's Deep Research just ...](https://venturebeat.com/ai/deepseeks-r1-and-openais-deep-research-just-redefined-ai-rag-distillation-and-custom-models-will-never-be-the-same/)\n\n[99\\. DeepSeek R1: DeepSeek API](https://deepseeksr1.com/api/)\n\n[100\\. DeepSeek-R1 self-improves and unseats o1 with ...](https://predibase.com/blog/deepseek-r1-self-improves-and-unseats-o1-with-reinforcement-learning)\n\n[101\\. PIONEER AI PARTNER COMPANY, CYKEL AI, ANNOUNCES INTEGRATION OF DEEPSEEK R1, REDUCING AI COSTS UP TO 96%](https://cdn.prod.website-files.com/633c6a023878f61ab5c1c1e5/67b887f8db6ac5b2e54fa807_JPEG%20PR_0220_Cykel%20DeepSeek%20R1%20integration.pdf)\n\n[102\\. DeepSeek R1: Technical Insights (Part 3)](https://www.wwt.com/blog/deepseek-r1-technical-insights-part-3)\n\n[103\\. Comparative Deep Dive: Grok 4 vs DeepSeek R1 - Topmost Ads](https://topmostads.com/grok-4-vs-deepseek-r1/)\n\n[104\\. DeepSeek and the rise of AI reasoning](https://www.tricentis.com/blog/deepseek-and-the-rise-of-ai-reasoning)\n\n[105\\. Fine-Grained Reasoning Evaluation: How Well Does DeepSeek-R1 Handle Causal Inference?](https://tijer.org/tijer/papers/TIJER2505178.pdf)\n\n[106\\. DeepSeek’s Game-Changing Launch: Why “Good-Enough” Intelligence Could Transform Financial Services—Again](https://www.kyndryl.com/content/dam/kyndrylprogram/doc/en/2025/the-kyndryl-institute-deepseek.pdf)\n\n[107\\. DeepSeek vs OpenAI: Which is Better in 2025?](https://interviewkickstart.com/blogs/articles/deepseek-vs-openai)\n\n[108\\. The economic impact of artificial intelligence: A case study of DeepSeek R1](https://www.allfinancejournal.com/article/view/430/8-1-9)\n\n[109\\. DeepSeek 核心十问十答](https://www.faxianai.com/wp-content/uploads/2025/05/1747041355-3%E3%80%81%E8%AE%A1%E7%AE%97%E6%9C%BA-DeepSeek%E6%A0%B8%E5%BF%83%E5%8D%81%E9%97%AE%E5%8D%81%E7%AD%94.pdf)\n\n[110\\. Stargate Project: Trump’s AI Moonshot](https://rrs-catering.eu/wp-content/uploads/2025/02/145423465104540981.pdf)\n\n[111\\. AI Weekly: 2025년의 AI: 중국의 선전포고와 미국의 대응](https://securities.miraeasset.com/bbs/download/2133957.pdf?attachmentId=2133957)\n\n[112\\. DeepSeek: A breakthrough moment for AI - KPMG International](https://kpmg.com/us/en/media/news/insights-on-deepseek.html#:~:text=DeepSeek,%20a%20Chinese%20AI%20startup,,%20cost-effectiveness%20and%20transparency.)\n\n[113\\. DeepSeek R1: A powerful and affordable AI breakthrough](https://rabiloo.com/blog/deepseek-r1-a-powerful-and-affordable-ai-breakthrough)\n\n[114\\. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://semking.com/PDFs/DeepSeek-Chinese-AI-model-breakthrough-security-risk.pdf)\n\n[115\\. Explainable Sentiment Analysis with DeepSeek-R1: Performance, Efficiency, and Few-Shot Learning](https://www.arxiv.org/pdf/2503.11655)\n\n[116\\. DeepSeek R1 vs V3: Which AI Rules Coding? (2025 ...](https://mpgone.com/deepseek-r1-vs-v3-which-ai-rules-coding-2025-breakdown/)\n\n[117\\. DeepSeek Survey Results: Insights from AI Leaders](https://predibase.com/blog/deepseek-adoption-survey-what-hundreds-of-ai-leaders-and-builders-told-us)\n\n[118\\. Takeaways from the DeepSeek-R1 model](https://dev.to/aws/takeaways-from-the-deepseek-r1-model-2dli)\n\n[119\\. SGLang adoption for DeepSeek V3 and R1 · sgl-project/...](https://github.com/sgl-project/sglang/discussions/3322)\n\n[120\\. INNOVATING WITH SPRING BOOT: AI-POWERED APPLICATIONS FOR MODERN BUSINESSES](https://iaeme.com/MasterAdmin/Journal_uploads/IJRCAIT/VOLUME_8_ISSUE_1/IJRCAIT_08_01_189.pdf)\n\n[121\\. AI Adoption and Risk Report](https://info.cyberhaven.com/hubfs/Content%20PDF/Cyberhaven%20Labs%20-%202025%20AI%20Adoption%20&%20Risk%20Report.pdf)\n\n[122\\. In Focus: AI after DeepSeek](https://www.invesco.com/content/dam/invesco/apac/en/pdf/insights/2025/march/invesco-in-focus-ai-after-deepseek-mar-2025.pdf)\n\n[123\\. DeepSeek-R1: Features, Use Cases, and Comparison with OpenAI](https://www.mygreatlearning.com/blog/deepseek-r1-features-use-cases/)\n\n[124\\. DeepSeek R1 & R1-Zero: A New Milestone in Language ...](https://mirrorsecurity.io/blog/deepseek-r1-r1-zero-a-new-milestone-in-language-model-reasoning-safe-ai-adoption)\n\n[125\\. Alibaba touts open-source AI model as surpassing DeepSeek R1 performance](https://hkshippers.org.hk/assets/uploads/media-uploader/071741318953.pdf)\n\n[126\\. Top AI Models of 2025: A Comparative Analysis ...](https://flexxited.com/blog/top-ai-models-of-2025-a-comparative-analysis-of-deepseek-and-openai)\n\n[127\\. Quarterly Economic Update](https://www.rsa-al.gov/uploads/files/Quarterly_Investment_Handout_-_March_5_2025_v2.pdf)\n\n[128\\. Using Protect AI's Products to Analyze DeepSeek-R1](https://protectai.com/blog/protect-ai-analyze-deepseek)\n\n[129\\. Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models](https://www.arxiv.org/pdf/2503.08275)\n\n[130\\. DeepSeek R1 Launch: Leading China's AI Breakthroughs ...](https://hkaift.com/deepseek-r1-launch-leading-chinas-ai-breakthroughs-and-advancing-fintech-development/)\n\n[131\\. DeepSeek R1: Quality, Performance & Price Analysis](https://artificialanalysis.ai/models/deepseek-r1)\n\n[132\\. Three Takeaways from the DeepSeek R1 Release](https://www.altmansolon.com/thought-leadership/deepseek-r1-impact-ai-industry)\n\n[133\\. ParkTown Residence sets tone as first mega-development of 2025](https://cdn.shaunsocial.com/rakan83/files/message_item/2025/02/10/27e83b5e-2e2a-4e98-9318-109fbdd58697.pdf)\n\n[136\\. The Evolving AI Value Chain and Monetization Landscape in 2024](https://www.ijirset.com/upload/2024/august/15_The.pdf)\n\n[137\\. In focus: AI statistics, insights and trends | Definition](https://www.thisisdefinition.com/resources/ai-statistics)\n\n[138\\. ROI of AI](https://newsroom.ibm.com/image/IBM_ROI_of_AI_Report-December_2024.pdf)\n\n[139\\. Global share of agentic AI in software applications 2028](https://www.statista.com/statistics/1552165/global-share-of-agentic-ai-software-applications/)\n\n[140\\. 100 AI statistics that define AI Trends 2025 for enterprises | Zams](https://www.zams.com/blog/ai-statistics-ai-trends-2025)\n\n[141\\. TMT Predictions 2025: Bridging the gaps](https://newsletter.radensa.ru/wp-content/uploads/2024/11/Deloitte_TMT-Predictions_2025-129-pgs.pdf)\n\n[142\\. AI Trends and use cases](https://www.cacio.cz/wp-content/uploads/2024/07/Spana_20240523-CACIO-AI-trends-v03.pdf)\n\n[143\\. AI Analytics Market Size, Share, Trends & Forecast by 2034 | FMI](https://www.futuremarketinsights.com/reports/ai-analytics-market)\n\n[144\\. Research Briefing: CES 2024 is set to showcase the next era of AI beauty technology](https://staging.glossy.co/beauty/research-briefing-ces-2024-is-set-to-showcase-the-next-era-of-ai-beauty-technology/)\n\n[145\\. Index Insider: Enterprises to Nearly Double AI Spend in 2024](https://isg-one.com/articles/index-insider-enterprises-to-nearly-double-ai-spend-in-2024?utm_source=marketo&utm_medium=isg_insider&utm_campaign=isginsider174)\n\n[146\\. 2024 企業 AI 市場雙重洗牌：OpenAI 市佔下滑與 RAG 技術崛起](https://blog.finsight.investments/portfolio/generative-ai-2024-market-shifts-analysis/)\n\n[147\\. GB200 rack the spotlight for AI server in 2H24-2025F](https://www.kgi.com.hk/en/-/media/files/kgishk/research-reports/tw-reports/2024/01/it-hardware_27052024.pdf)\n\n[148\\. 人工智能2024:模型端OpenAI走下神坛,应用端商业化渐清晰|...](http://finance.sina.com.cn/roll/2025-01-01/doc-inecmnky9781906.shtml)\n\n[149\\. 2024年AI投资千亿!OpenAI市场份额下降16%,Anthropic谷歌双赢](https://h5.ifeng.com/c/vivo/v0023sxNlWc5ACuGBU23HAiXVJ1zKcU978hck3L1pQc6HR8__)\n\n[150\\. AI Weekly | 2024.12.5](https://securities.miraeasset.com/bbs/download/2132866.pdf?attachmentId=2132866)\n\n[151\\. 人工智能 2024: 模型端 OpenAI 走下神坛，应用端商业化渐清晰](https://www.newworldtimes.us/newspaperfiles/20250103/440_NWTB_24_C.pdf)\n\n[152\\. AI & Automation Trends 2025](https://marketing.auxis.com/hubfs/2025%20PDFs/Co%20Branded%20AI%20and%20Automation%20Trends%202025.pdf)\n\n[153\\. Use of artificial intelligence in enterprises - Statistics Explained](https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Use_of_artificial_intelligence_in_enterprises)\n\n[154\\. 2025展望：终端需求复苏延续，端侧AI创新落地提速](https://pdf.dfcfw.com/pdf/H3_AP202412171641330124_1.pdf?1734454824000.pdf)\n\n[155\\. 2024年企业AI应用趋势洞察报告](https://www.hulianhutongshequ.cn/upload/tank/report/2024/202406/1/61ed1ad78ead4dc4b98e2ff06226db60.pdf)\n\n[156\\. EXPLAINABLE AI (XAI) FOR TRANSPARENT FINANCIAL DECISION-MAKING: A TECHNICAL FRAMEWORK](https://iaeme.com/MasterAdmin/Journal_uploads/IJRCAIT/VOLUME_7_ISSUE_2/IJRCAIT_07_02_131.pdf)\n\n[157\\. AI-powered financial anomaly detection: Intelligent systems identifying irregularities in enterprise financial data flows](https://journalwjarr.com/sites/default/files/fulltext_pdf/WJARR-2025-1461.pdf)\n\n[158\\. Scott M. Lundberg, Su-In Lee. “A Unified Approach to Interpreting Model Predictions.” Neural Information Processing Systems](https://arxiv.org/abs/1705.07874)\n\n[159\\. AUTOMATING SAP FICO: A CASE STUDY IN RECEIVABLES RECONCILIATION AND PROCESS ENHANCEMENT](https://iaeme.com/MasterAdmin/Journal_uploads/IJCET/VOLUME_16_ISSUE_1/IJCET_16_01_201.pdf)\n\n[160\\. How AI is Transforming Financial Services: From Risk to ...](https://whatfix.com/blog/ai-in-financial-services/)\n\n[161\\. AI and Machine Learning in Blockchain Compliance ...](https://coinlaw.io/ai-and-machine-learning-in-blockchain-compliance-statistics/)\n\n[162\\. ENHANCING COMPLIANCE AND KYC PROCESSES THROUGH AI, OCR, AND AUTOMATION: A COST-EFFECTIVE APPROACH](https://iaeme.com/MasterAdmin/Journal_uploads/IJCET/VOLUME_15_ISSUE_5/IJCET_15_05_094.pdf)\n\n[163\\. Explainable AI in Financial Institutions](https://jarvisinvest.com/jarvis-library/explainable-ai-in-financial-institutions/)\n\n[164\\. Key AI Milestones Leading to 2025](https://www.silenteight.com/explore-learn/major-events-in-ai-2018-2025)\n\n[165\\. Machine Learning and AI in Fraud Detection and AML Compliance](https://youverify.co/blog/machine-learning-ai-in-fraud-detection-aml-compliance)\n\n[166\\. Leverage Your Regulatory & Compliance Documents with ...](https://www.msg-compliance.de/en/blog/leverage-your-regulatory-compliance-documents)\n\n[167\\. The Role of AI in preventing financial fraud and enhancing compliance](https://gsconlinepress.com/journals/gscarr/sites/default/files/GSCARR-2025-0086.pdf)\n\n[168\\. AI in Financial Services: AI-First for Retail Banking](https://www.infosys.com/industries/financial-services/insights/documents/ai-first-retail-banking.pdf)\n\n[169\\. Explainable AI in financial technologies: Balancing innovation with regulatory compliance](https://ijsra.net/sites/default/files/IJSRA-2024-1870.pdf)\n\n[170\\. Pengjian Liang. “Leveraging artificial intelligence in Regulatory Technology (RegTech) for financial compliance.” Applied and Computational Engineering](https://doi.org/10.54254/2755-2721/93/20240964)\n\n[176\\. The economic impact of artificial intelligence: A case study of DeepSeek R1](https://www.allfinancejournal.com/article/view/430/8-1-9)\n\n[177\\. AI Enterprise Learning Case Studies: Success Stories](https://hyperspace.mv/case-studies-of-ai-implementation-in-enterprise-learning/)\n\n[178\\. DeepSeek等AI工具在企业应用的有关案例](https://www.cjtouzi.com/ddjs/ghjl/202506/P020250605571047305570.docx)\n\n[179\\. DeepSeek行业应用案例集：解锁智能变革密码](https://www.dboop.com/img/deepseek/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%EF%BC%9A2025%E5%B9%B4DeepSeek%E8%A1%8C%E4%B8%9A%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B%E9%9B%86%E8%A7%A3%E9%94%81%E6%99%BA%E8%83%BD%E5%8F%98%E9%9D%A9%E5%AF%86%E7%A0%81.pdf)\n\n[180\\. In Focus: AI after DeepSeek](https://www.invesco.com/content/dam/invesco/apac/en/pdf/insights/2025/march/invesco-in-focus-ai-after-deepseek-mar-2025.pdf)\n\n[181\\. Enterprise AI Case Studies](https://www.enterpriseaiworld.com/Articles/Enterprise-AI-Case-Studies)\n\n[182\\. Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models](https://www.arxiv.org/pdf/2503.08275)\n\n[183\\. ACCELERATING MODEL DEVELOPMENT IN FINANCE: A MODERN FRAMEWORK FOR ENHANCED EFFICIENCY AND PERFORMANCE](https://scholar9.com/publication/83b766b15cf344c9434d2dd1cd92e318.pdf)\n\n[184\\. Leveraging Artificial Intelligence for Automated Testing and Quality Assurance in Software Development Lifecycles](https://ijrpr.com/uploads/V5ISSUE12/IJRPR36667.pdf)\n\n[185\\. DeepSeek R1 & R1-Zero: A New Milestone in Language ...](https://mirrorsecurity.io/blog/deepseek-r1-r1-zero-a-new-milestone-in-language-model-reasoning-safe-ai-adoption)\n\n[186\\. Meenu Mary John, H. H. Olsson et al. “Towards an AI‐driven business development framework: A multi‐case study.” Journal of Software: Evolution and Process](https://doi.org/10.1002/smr.2432)\n\n[187\\. Alibaba touts open-source AI model as surpassing DeepSeek R1 performance](https://hkshippers.org.hk/assets/uploads/media-uploader/071741318953.pdf)\n\n[188\\. Three Takeaways from the DeepSeek R1 Release](https://www.altmansolon.com/thought-leadership/deepseek-r1-impact-ai-industry)\n\n[189\\. Quarterly Economic Update](https://www.rsa-al.gov/uploads/files/Quarterly_Investment_Handout_-_March_5_2025_v2.pdf)\n\n[190\\. 通信行业周报 2025年第6周 Deepseek-R1开源推动AI应用发展，头部AI厂支持Deepseek](https://www.faxianai.com/wp-content/uploads/2025/05/1747035038-3%E3%80%81%E9%80%9A%E4%BF%A1%E8%A1%8C%E4%B8%9A%E5%91%A8%E6%8A%A52025%E5%B9%B4%E7%AC%AC6%E5%91%A8%EF%BC%9ADeepseek-R1%E5%BC%80%E6%BA%90%E6%8E%A8%E5%8A%A8AI%E5%BA%94%E7%94%A8%E5%8F%91%E5%B1%95%EF%BC%8C%E5%A4%B4%E9%83%A8AI%E5%8E%82%E6%94%AF%E6%8C%81Deepseek.pdf)\n\n[191\\. DeepSeek 是否会改变 AI 投资范式？](https://www.faxianai.com/wp-content/uploads/2025/05/1747037599-4%E3%80%81%E7%94%B5%E5%AD%90-DeepSeek%E6%98%AF%E5%90%A6%E4%BC%9A%E6%94%B9%E5%8F%98AI%E6%8A%95%E8%B5%84%E8%8C%83%E5%BC%8F%EF%BC%9F.pdf)\n\n[192\\. DeepSeek正在改变企业级人工智能应用的规则?IT领导者需要...](http://www.fjcio.cn/Item/14306.aspx)\n\n[193\\. Chinese AI startup DeepSeek unveils open-source model to rival OpenAI ...](https://www.computerworld.com/article/3808579/chinese-ai-startup-deepseek-unveils-open-source-model-to-rival-openai-o1.html)\n\n[196\\. 电子行业 2025 年中期投资策略](https://pdf.dfcfw.com/pdf/H3_AP202506171692623676_1.pdf?1750180147000.pdf)\n\n[197\\. 人工智能创新百花齐放，半导体自主可控加速推进——电子行业2025年中期投资策略](https://pdf.dfcfw.com/pdf/H3_AP202506201694461230_1.pdf?1750444688000.pdf)\n\n[198\\. DeepSeek R1: A New Era in Open-Source AI Performance](https://consensuslabs.ch/blog/deepseek-r1-open-source-ai-benchmark-comparison)\n\n[199\\. DeepSeek推动AI产业变革，上游算力、下游终端迎发展机遇](https://piinm.com/uploads/20250520/97a096f9ceac88c94b29aefba6bff6ce.pdf)\n\n[200\\. DeepSeek行业应用实践报告100+份汇总解读|附PDF下载](https://www.cnblogs.com/tecdat/p/18758214)\n\n[201\\. Brief analysis of DeepSeek R1 and its implications for...](http://arxiv.org/html/2502.02523v3)\n\n[202\\. The Future of AI: DeepSeek R1 and the Path Ahead](https://www.opengrowth.com/blogs/the-future-of-ai-deepseek-and-the-path-ahead)\n\n[203\\. Deepseek横空出世，关注并行科技等AI相关优质标的——北交所策略专题报告](https://pdf.dfcfw.com/pdf/H3_AP202502091642915853_1.pdf?1739132601000.pdf)\n\n[204\\. Deepseek 撼动全球 AI 产业，助力 AI 应用、AI 端侧落地加速](https://pdf.dfcfw.com/pdf/H3_AP202502041642778628_1.pdf?1738741765000.pdf)\n\n[205\\. 证券研究报告-晨会聚焦](https://file.iyanbao.com/pdf/723ec-ca354408-7faf-4193-b5ee-a0947658a483.pdf)\n\n[206\\. AI产业迈入成长期，坚定中国AI产业发展信心](https://pdf.dfcfw.com/pdf/H3_AP202504091653771051_1.pdf?1744219458000.pdf)\n\n[207\\. 计算机行业事件点评：DEEPSEEK撼动全球AI产业助力AI应用](http://stock.finance.sina.com.cn/stock/go.php/vReport_Show/kind/lastest/rptid/792054828906/index.phtml)\n\n[208\\. DeepSeek-R1 颠覆性在于实现 AI 平权，重估资产价值——传媒互联网行业专题报告](https://hulianhutongshequ.cn/upload/tank/report/2025/202502/1/246368d1336d4330b7d3da7028aec703.pdf)\n\n[209\\. DeepSeek从入门到精通(12)——R1、V3版本比较](https://www.360doc.cn/article/12750417_1152505045.html)\n\n[210\\. DeepSeek-R1带来AI技术突破，持续关注GPT-5的推出进程——计算机行业月报](https://pdf.dfcfw.com/pdf/H3_AP202502141643071176_1.pdf?1739519347000.pdf)\n\n[211\\. “国民应用”带动中国资产重估——再论 DeepSeek 投资路径](https://aigc.idigital.com.cn/djyanbao/%E3%80%90%E6%B5%99%E5%95%86%E8%AF%81%E5%88%B8%E3%80%91%E5%86%8D%E8%AE%BADeepSeek%E6%8A%95%E8%B5%84%E8%B7%AF%E5%BE%84%EF%BC%9A%E2%80%9C%E5%9B%BD%E6%B0%91%E5%BA%94%E7%94%A8%E2%80%9D%E5%B8%A6%E5%8A%A8%E4%B8%AD%E5%9B%BD%E8%B5%84%E4%BA%A7%E9%87%8D%E4%BC%B0-2025-02-25.pdf)\n\n[212\\. 半导体行业策略：云巅千帆竞渡，端侧万物生辉，自主驭潮生](https://pdf.dfcfw.com/pdf/H3_AP202503211645511498_1.pdf?1742567556000.pdf)\n\n[213\\. Why DeepSeek's R1 Is Actually Good News For ...](https://www.verdantix.com/insights/blogs/why-deepseek-s-r1-is-actually-good-news-for-enterprises-everywhere)\n\n[214\\. DeepSeek Vakası: Yapay Zekâ Savaşlarında Stratejik Kontrol Noktası](https://storage.prod.researchhub.com/uploads/papers/users/51660/df012a2f-30b0-41d2-a068-ad539edf2493/DeepSeekR1.pdf)\n\n[216\\. Michael C. Jensen, W. Meckling. “THEORY OF THE FIRM: MANAGERIAL BEHAVIOR, AGENCY COSTS AND OWNERSHIP STRUCTURE.”](https://doi.org/10.1007/978-94-009-9257-3_8)\n\n[217\\. D. Simunic. “The Pricing Of Audit Services - Theory And Evidence.” Journal of Accounting Research](https://doi.org/10.2307/2490397)\n\n[218\\. Anwer S. Ahmed, Bruce K. Billings et al. “The Role of Accounting Conservatism in Mitigating Bondholder-Shareholder Conflicts over Dividend Policy and in Reducing Debt Costs.” The Accounting Review](https://doi.org/10.2308/ACCR.2002.77.4.867)\n\n[219\\. T. B. Bell, W. Landsman et al. “Auditors' Perceived Business Risk and Audit Fees: Analysis and Evidence.” Journal of Accounting Research](https://doi.org/10.1111/1475-679X.00002)\n\n[220\\. P. Iliev. “The Effect of SOX Section 404: Costs, Earnings Quality and Stock Prices.” LSN: Law & Finance: Empirical (Topic)](https://doi.org/10.2139/ssrn.983772)\n\n[221\\. The economic impact of artificial intelligence: A case study of DeepSeek R1](https://www.allfinancejournal.com/article/view/430/8-1-9)\n\n[222\\. Comparative Deep Dive: Grok 4 vs DeepSeek R1 - Topmost Ads](https://topmostads.com/grok-4-vs-deepseek-r1/)\n\n[223\\. Deepseek在金融领域36大应用场景 - 53AI](https://www.53ai.com/news/AIjinrong/2025051331674.html#:~:text=%E5%8F%8D%E6%AC%BA%E8%AF%88%E6%A3%80%E6%B5%8B%EF%BC%9A%E5%8C%97%E4%BA%AC%E9%93%B6%E8%A1%8C,%E5%AE%A2%E6%88%B7%E7%AD%89%E5%BE%85%E6%97%B6%E9%97%B4%E5%A4%A7%E5%B9%85%E5%87%8F%E5%B0%91%E3%80%82)\n\n[224\\. DeepSeek大语言模型证券行业落地应用研究](https://mp.weixin.qq.com/s?__biz=MzI1Njk3MTAwNg%3D%3D&mid=2247506586&idx=1&sn=0f9c69d2f331cfc2d0208c18b2cba166&chksm=eb0404b6867fcac25c34e35eb9d8d0330e1dd84b7163cd5d291cf63d236a5a96ff82e847128e&scene=27)\n\n[225\\. 基金量化观察 金融工程周报 证券研究报告 性能对标 OpenAI o1，Deepseek-R1 推理性能评测](https://pdf.dfcfw.com/pdf/H301_AP202501221642427254_1.pdf)\n\n[226\\. DeepSeek 背景综述及在金融领域应用场景初探](https://hulianhutongshequ.cn/upload/tank/report/2025/202503/1/ea1e905ce8234982aeca50d7ebbb68fa.pdf)\n\n[227\\. DeepSeek-R1: The Open-Source AI Challenger Rewriting ...](https://www.zartis.com/deepseek-r1-the-open-source-ai-challenger-rewriting-the-rules-of-enterprise-ai/)\n\n[228\\. The future of audit](https://www.cicpa.org.cn/ztzl1/zthf/ztjz/jiangyi/201211/P020201130250890102866.ppt)\n\n[229\\. The economic impact of artificial intelligence: A case study of DeepSeek R1](https://www.allfinancejournal.com/article/view/430/8-1-9)\n\n[230\\. AI Enterprise Learning Case Studies: Success Stories](https://hyperspace.mv/case-studies-of-ai-implementation-in-enterprise-learning/)\n\n[231\\. DeepSeek等AI工具在企业应用的有关案例](https://www.cjtouzi.com/ddjs/ghjl/202506/P020250605571047305570.docx)\n\n[232\\. In Focus: AI after DeepSeek](https://www.invesco.com/content/dam/invesco/apac/en/pdf/insights/2025/march/invesco-in-focus-ai-after-deepseek-mar-2025.pdf)\n\n[233\\. Alibaba touts open-source AI model as surpassing DeepSeek R1 performance](https://hkshippers.org.hk/assets/uploads/media-uploader/071741318953.pdf)\n\n[234\\. Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models](https://www.arxiv.org/pdf/2503.08275)\n\n[235\\. Is DeepSeek R1 Slow? Performance Insights & Tips - BytePlus](https://www.byteplus.com/en/topic/384352#:~:text=One%20of%20the%20primary%20factors,Inconsistent%20network%20latency)\n\n[236\\. DeepSeek行业应用案例集：解锁智能变革密码](https://pic.goubyte.com/2025-04-21-pftmhhijqi.pdf)\n\n[237\\. DeepSeek: The Sputnik Moment of the AI Era?](https://www.inss.re.kr/upload/bbs/BBSA05/202504/F20250401150142304.pdf)\n\n[238\\. Three Takeaways from the DeepSeek R1 Release](https://www.altmansolon.com/thought-leadership/deepseek-r1-impact-ai-industry)\n\n[239\\. DeepSeek 是否会改变 AI 投资范式？](https://www.faxianai.com/wp-content/uploads/2025/05/1747037599-4%E3%80%81%E7%94%B5%E5%AD%90-DeepSeek%E6%98%AF%E5%90%A6%E4%BC%9A%E6%94%B9%E5%8F%98AI%E6%8A%95%E8%B5%84%E8%8C%83%E5%BC%8F%EF%BC%9F.pdf)\n\n[240\\. Quarterly Economic Update](https://www.rsa-al.gov/uploads/files/Quarterly_Investment_Handout_-_March_5_2025_v2.pdf)\n\n[241\\. DeepSeek Debrief: >128 Days Later](https://semianalysis.com/2025/07/03/deepseek-debrief-128-days-later/)\n\n[242\\. 通信行业周报 2025年第6周 Deepseek-R1开源推动AI应用发展，头部AI厂支持Deepseek](https://www.faxianai.com/wp-content/uploads/2025/05/1747035038-3%E3%80%81%E9%80%9A%E4%BF%A1%E8%A1%8C%E4%B8%9A%E5%91%A8%E6%8A%A52025%E5%B9%B4%E7%AC%AC6%E5%91%A8%EF%BC%9ADeepseek-R1%E5%BC%80%E6%BA%90%E6%8E%A8%E5%8A%A8AI%E5%BA%94%E7%94%A8%E5%8F%91%E5%B1%95%EF%BC%8C%E5%A4%B4%E9%83%A8AI%E5%8E%82%E6%94%AF%E6%8C%81Deepseek.pdf)\n\n[243\\. Cross-border mechanism to fight crime](https://efile.fara.gov/docs/3457-Informational-Materials-20250224-661.pdf)\n\n[244\\. Chinese AI startup DeepSeek unveils open-source model to rival OpenAI ...](https://www.computerworld.com/article/3808579/chinese-ai-startup-deepseek-unveils-open-source-model-to-rival-openai-o1.html)\n\n[245\\. DeepSeek正在改变企业级人工智能应用的规则?IT领导者需要...](http://www.fjcio.cn/Item/14306.aspx)\n\n[249\\. Xing Hao, Guigang Zhang et al. “Deep Learning.” Int. J. Semantic Comput.](https://doi.org/10.1142/S1793351X16500045)\n\n[250\\. 中小企业AI应用浪潮：超六成企业已采纳，价值认知显著](https://www.199it.com/archives/1774931.html)\n\n[251\\. 证券研究报告-晨会聚焦](https://file.iyanbao.com/pdf/06679-9a63bdf5-f80e-4936-98ba-af41b0f47cc6.pdf)\n\n[252\\. 「企業IT利活用動向調査2025」結果分析（DX導入、生成AIの活用状況等）](https://jipdec.or.jp/archives/publications/cmchdt0000002pup-att/J0005194.pdf)\n\n[253\\. The Adoption of Artificial Intelligence in Firms: New Evidence for Policymaking](https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/05/the-adoption-of-artificial-intelligence-in-firms_8fab986b/f9ef33c3-en.pdf)\n\n[254\\. 证券行业 2025 年 3 月报 DeepSeek 激活券商朱格拉周期](https://file.iyanbao.com/pdf/c5ef8-e49952e5-5ef0-4ee0-9fce-e36845523c9c.pdf)\n\n[255\\. 信息技术产业行业研究 DeepSeek 推动 AI 平权，看好相关产业链投资机会](https://pdf.dfcfw.com/pdf/H3_AP202502101642944002_1.pdf?1739215684000.pdf)\n\n[256\\. Essential Business Stats & Trends in 2025](https://www.intuition.com/wp-content/uploads/2024/11/Essential-business-stats-trends-in-2025-compressed-1.pdf)\n\n[257\\. DeepSeek 模型冲击下,美企 AI 投资猛增:2025 年支出惊人](https://usstock.hexun.com/2025-03-04/217675311.html)\n\n[258\\. The economic impact of artificial intelligence: A case study of DeepSeek R1](https://www.allfinancejournal.com/article/view/430/8-1-9)\n\n[259\\. Zonghan Wu, Junlin Wang et al. “Towards Competent AI for Fundamental Analysis in Finance: A Benchmark Dataset and Evaluation.”](https://arxiv.org/abs/2506.07315)\n\n[260\\. DeepSeek 背景综述及在金融领域应用场景初探](https://aigc.idigital.com.cn/djyanbao/%E3%80%90%E4%B8%AD%E9%82%AE%E8%AF%81%E5%88%B8%E3%80%91Deepseek%E8%83%8C%E6%99%AF%E7%BB%BC%E8%BF%B0%E5%8F%8A%E5%9C%A8%E9%87%91%E8%9E%8D%E9%A2%86%E5%9F%9F%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E5%88%9D%E6%8E%A2-2025-02-26.pdf)\n\n[261\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2023.” ArXiv](https://doi.org/10.48550/arXiv.2310.03715)\n\n[262\\. Deepseek 赋能非银机构主业务效率提升](https://pdf.dfcfw.com/pdf/H3_AP202502161643142462_1.pdf?1739722745000.pdf)\n\n[263\\. 三大要素齐发力，AI应用步入全面加速期——计算机行业深度分析](https://pdf.dfcfw.com/pdf/H3_AP202503071644153388_1.pdf?1741373367000.pdf)\n\n[264\\. 三年亏超5亿，在手现金不足覆盖负债，海致科技市场份额受到 ...](https://t.cj.sina.com.cn/articles/view/5224789915/1376bf79b00101e2ls)\n\n[265\\. Top 33 Latest DeepSeek AI Statistics, Data and Trends](https://blog.9cv9.com/top-33-latest-deepseek-ai-statistics-data-and-trends/)\n\n[266\\. DeepSeek-R1-0528: Understanding the Latest AI ...](https://mpgone.com/deepseek-r1-0528-understanding-the-latest-ai-technology/)\n\n[267\\. DeepSeek在财务领域的应用:AI赋能金融决策与自动化 - 哔哩哔哩](https://www.bilibili.com/read/cv42103289)\n\n[268\\. Transforming Paradigms: A Global AI in Financial Services Survey](https://www3.weforum.org/docs/WEF_AI_in_Financial_Services_Survey.pdf)\n\n[269\\. DeepSeek (深度求索) AI and its Implications for Innovation in Financial Services](https://www.kcmi.re.kr/kcmifile/webzine_content/OPINION/6527/webzinepdf_6527.pdf)\n\n[270\\. What DeepSeek’s AI Disruption means for Financial Services](https://cdn.prod.website-files.com/66b2540840d76ee38a6eea40/67a1f7adec00fbe178378d8b_DeepSeek%E2%80%99s%20AI%20Disruption%20-%20%20A%20Bazara%20Publication.pdf)\n\n[271\\. DeepSeek’s Game-Changing Launch: Why “Good-Enough” Intelligence Could Transform Financial Services—Again](https://www.kyndryl.com/content/dam/kyndrylprogram/doc/en/2025/the-kyndryl-institute-deepseek.pdf)\n\n[272\\. DeepSeek 核心十问十答](https://www.faxianai.com/wp-content/uploads/2025/05/1747041355-3%E3%80%81%E8%AE%A1%E7%AE%97%E6%9C%BA-DeepSeek%E6%A0%B8%E5%BF%83%E5%8D%81%E9%97%AE%E5%8D%81%E7%AD%94.pdf)\n\n[273\\. 证券研究报告-晨会聚焦](https://file.iyanbao.com/pdf/40f93-9851d53a-54c7-422f-a33a-9ee83c9a1148.pdf)\n\n[274\\. Morning India](https://www.dsij.in/productAttachment/premarketreports/MORNING_INDIA-20250129-MOSL-MI-PG080.pdf)\n\n[275\\. 10 Cents on the Dollar: How DeepSeek's 90% Cost Cut ...](https://bytes.scl.org/10-cents-on-the-dollar-how-deepseeks-90-cost-cut-changes-everything/)\n\n[276\\. The economic impact of artificial intelligence: A case study of DeepSeek R1](https://www.allfinancejournal.com/article/view/430/8-1-9)\n\n[277\\. Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models](https://www.arxiv.org/pdf/2503.08275)\n\n[278\\. DeepSeek and the Basic Act on AI: Implications for the Korean Financial Sector](https://www.kif.re.kr/kif4/publication/viewer?mid=220&cno=346088&fcd=2025002634RU&ismail=1&email=%5B$email%5D&ft=0)\n\n[279\\. Stargate Project: Trump’s AI Moonshot](https://rrs-catering.eu/wp-content/uploads/2025/02/145423465104540981.pdf)\n\n[280\\. DeepSeek V3 and R1: An Overview of Technology Innovations and Implications for United States National Security](https://figshare.com/ndownloader/files/53282696)\n\n[281\\. Lower AI Costs Will Drive Innovation, Efficiency, and Adoption](https://tcwgroup.co.jp/-/media/Downloads/com/Insights/2025/250219-AILower-Costs.pdf?rev=7bb172cd986a4bdbbc7668e669442ff3&sc_lang=ja-JP)\n\n[282\\. DeepSeek 背景综述及在金融领域应用场景初探](https://cnpsec.com/plat_files/upload/png_upload/20250227/202502271740638923227.pdf)\n\n[283\\. DeepSeek+DeepResearch: 让科研像聊天一样简单（三）（第九期）](https://jspxzx.hebeu.edu.cn/__local/7/14/AC/DF8F2C34226C1168454D0DFADE8_3A3FC50C_3CD800.pdf)\n\n[285\\. What DeepSeek’s AI Disruption means for Financial Services](https://cdn.prod.website-files.com/66b2540840d76ee38a6eea40/67a12cf2c086219ab54f3c39_DeepSeek%E2%80%99s%20AI%20Disruption%20-%20%20A%20Bazara%20Publication2_compressed.pdf)\n\n[286\\. DeepSeek (深度求索) AI and its Implications for Innovation in Financial Services](https://www.kcmi.re.kr/kcmifile/webzine_content/OPINION/6527/webzinepdf_6527.pdf)\n\n[287\\. The economic impact of artificial intelligence: A case study of DeepSeek R1](https://www.allfinancejournal.com/article/view/430/8-1-9)\n\n[288\\. Shujie Feng. “Integrating artificial intelligence in financial services: Enhancements, applications, and future directions.” Applied and Computational Engineering](https://doi.org/10.54254/2755-2721/69/20241455)\n\n[289\\. DeepSeek R1: A powerful and affordable AI breakthrough](https://rabiloo.com/blog/deepseek-r1-a-powerful-and-affordable-ai-breakthrough)\n\n[290\\. DeepSeek热潮席卷证券业 十余家券商快速完成本地化部署](https://epaper.stcn.com/att/202502/10/4cb096d6-2fb6-4b2a-8e2d-08f0da577dda.pdf)\n\n[291\\. Morning India](https://www.dsij.in/productAttachment/premarketreports/MORNING_INDIA-20250129-MOSL-MI-PG080.pdf)\n\n[292\\. A Survey of Generative AI in Finance](https://hal.science/hal-05020829v1/document)\n\n[293\\. Haosen Xu, Kaiyi Niu et al. “Leveraging artificial intelligence for enhanced risk management in financial services: Current applications and future prospects.”](https://www.semanticscholar.org/search?q=Leveraging%20artificial%20intelligence%20for%20enhanced%20risk%20management%20in%20financial%20services%3A%20Current%20applications%20and%20future%20prospects&sort=relevance)\n\n[294\\. 接入DeepSeek！国金证券、兴业证券等官宣，金融IT人士](https://www.tfcaijing.com/article/page/30597148794a4b583542754e46386f6e4932426a34513d3d)\n\n[295\\. Memory Watch](https://stock.pstatic.net/stock-research/industry/63/20250131_industry_600843000.pdf)\n\n[296\\. DeepSeek and the Basic Act on AI: Implications for the Korean Financial Sector](https://www.kif.re.kr/kif4/publication/viewer?mid=220&cno=346088&fcd=2025002634RU&ismail=1&email=%5B$email%5D&ft=0)\n\n[297\\. 金证股份自研大模型及AI应用全面接入DeepSeek _ 东方财富网](http://finance.eastmoney.com/a/202502113316346079.html)\n\n[298\\. DeepSeek (深度求索) AI and its Implications for Innovation ...](https://www.kcmi.re.kr/en/publications/pub_detail_view?syear=2025&zcd=002001017&zno=1843&cno=6527)\n\n[299\\. Marco Hogewoning. “Emerging Technologies.” Women of Color in Tech](https://doi.org/10.32388/l59zjx)\n\n[300\\. Zonghan Wu, Junlin Wang et al. “Towards Competent AI for Fundamental Analysis in Finance: A Benchmark Dataset and Evaluation.”](https://arxiv.org/abs/2506.07315)\n\n[305\\. 通信行业周报 2025年第6周 Deepseek-R1开源推动AI应用发展，头部AI厂支持Deepseek](https://www.faxianai.com/wp-content/uploads/2025/05/1747035038-3%E3%80%81%E9%80%9A%E4%BF%A1%E8%A1%8C%E4%B8%9A%E5%91%A8%E6%8A%A52025%E5%B9%B4%E7%AC%AC6%E5%91%A8%EF%BC%9ADeepseek-R1%E5%BC%80%E6%BA%90%E6%8E%A8%E5%8A%A8AI%E5%BA%94%E7%94%A8%E5%8F%91%E5%B1%95%EF%BC%8C%E5%A4%B4%E9%83%A8AI%E5%8E%82%E6%94%AF%E6%8C%81Deepseek.pdf)\n\n[306\\. OEMs' Next-generation In-vehicle Infotainment (IVI) System ...](http://www.researchinchina.com/Htmls/Report/2025/77087.html)\n\n[307\\. The economic impact of artificial intelligence: A case study of DeepSeek R1](https://www.allfinancejournal.com/article/view/430/8-1-9)\n\n[308\\. Deepseek 撼动全球 AI 产业，助力 AI 应用、AI 端侧落地加速](https://pdf.dfcfw.com/pdf/H3_AP202502041642778628_1.pdf?1738741765000.pdf)\n\n[309\\. DeepSeek 推动 AI 大模型行业创新，中国科技龙头大幅提升 AI 战略地位](https://pdf.dfcfw.com/pdf/H3_AP202503041644029131_1.pdf?1741084223000.pdf)\n\n[310\\. 证券研究报告-晨会聚焦](https://file.iyanbao.com/pdf/06679-9a63bdf5-f80e-4936-98ba-af41b0f47cc6.pdf)\n\n[311\\. DeepSeek与AIGC应用](https://www.360doc.cn/article/79615920_1148052322.html)\n\n[312\\. Evaluating DeepSeek AI vs. Top Competitors in 2025 - Future AGI](https://futureagi.com/blogs/evaluating-deepseek-ai-vs-top-competitors#:~:text=adaptive%20problem-solving.-,DeepSeek%20R1%20vs.,O3,%20Anthropic's%20Claude%203.5%20Sonnet))\n\n[313\\. DeepSeek 背景综述及在金融领域应用场景初探](https://aigc.idigital.com.cn/djyanbao/%E3%80%90%E4%B8%AD%E9%82%AE%E8%AF%81%E5%88%B8%E3%80%91Deepseek%E8%83%8C%E6%99%AF%E7%BB%BC%E8%BF%B0%E5%8F%8A%E5%9C%A8%E9%87%91%E8%9E%8D%E9%A2%86%E5%9F%9F%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E5%88%9D%E6%8E%A2-2025-02-26.pdf)\n\n[314\\. Knowledge AI](https://www.feishu.cn/hc/en-US/category/7529386162243518465-knowledge-ai)\n\n[315\\. DeepSeek 应用上线 20 天日活超 2000 万，R1 模型强化学习技术突破](https://file.iyanbao.com/pdf/f1279-a005e3ad-b012-41d7-8f5f-bdfe703aa49b.pdf)\n\n[316\\. DeepSeek shatters beliefs about the cost of AI, leaving US tech giants reeling](https://techxplore.com/news/2025-01-deepseek-shatters-beliefs-ai-tech.pdf)\n\n[317\\. 15 Best Verified Alternative Ways to Access DeepSeek R1 ...](https://www.popai.pro/resources/ai-tools/best-verified-alternative-ways-to-access-deepseek-r1-during-server-busy/)\n\n[318\\. 研报精选](https://www.cicc.com/upload/file/2025/03/04/7b735364-2845-4fb8-92c5-c1802c5e1e06.pdf)\n\n[319\\. 每週研報 03/02/2025](https://sec.victorysec.com.hk/uploads/deepseek_R1_AI_20250203_3272c0d1bc.pdf)\n\n[320\\. 2025年Q1人工智能AI行业500+份报告汇总解读|附下载|算法|...](https://www.163.com/dy/article/JTEQC8OS0518G5DJ.html)\n\n[321\\. What DeepSeek’s AI Disruption means for Financial Services](https://cdn.prod.website-files.com/66b2540840d76ee38a6eea40/67a1f7adec00fbe178378d8b_DeepSeek%E2%80%99s%20AI%20Disruption%20-%20%20A%20Bazara%20Publication.pdf)\n\n[322\\. 文章归档 - 哎咿呀](https://www.aiyiya.com/aggregation)\n\n[323\\. A Survey of Generative AI in Finance](https://hal.science/hal-05020829v1/document)\n\n[324\\. DeepSeek R1：中国AI的革命性突破，低成本高性能挑战全球巨头](https://www.bilibili.com/video/av113893355621236)\n\n[325\\. What DeepSeek’s AI Disruption means for Financial Services](https://cdn.prod.website-files.com/66b2540840d76ee38a6eea40/67a1f7adec00fbe178378d8b_DeepSeek%E2%80%99s%20AI%20Disruption%20-%20%20A%20Bazara%20Publication.pdf)\n\n[326\\. TOWN OF CRESTON REGULAR COMMITTEE OF THE WHOLE MEETING AGENDA](https://www.creston.ca/sites/default/files/2023-11/November%2021%20COTW.pdf)\n\n[327\\. Comparative Deep Dive: Grok 4 vs DeepSeek R1 - Topmost Ads](https://topmostads.com/grok-4-vs-deepseek-r1/)\n\n[328\\. DeepSeek-R1: A Smorgasbord Of Security Risks](https://informationsecuritybuzz.com/deepseek-r1-a-smorgasbord-of-security-risks/)\n\n[329\\. Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models](https://www.arxiv.org/pdf/2503.08275)\n\n[330\\. DeepSeek-R1: The Open-Source AI Challenger Rewriting ...](https://www.zartis.com/deepseek-r1-the-open-source-ai-challenger-rewriting-the-rules-of-enterprise-ai/)\n\n[331\\. DeepSeek R1 Launch: Leading China's AI Breakthroughs ...](https://hkaift.com/deepseek-r1-launch-leading-chinas-ai-breakthroughs-and-advancing-fintech-development/)\n\n[332\\. The economic impact of artificial intelligence: A case study of DeepSeek R1](https://www.allfinancejournal.com/article/view/430/8-1-9)\n\n[333\\. 读懂Deepseek大模型 探索证券AI业务场景](http://event.d1net.com/uploadfile/2025/0304/20250304021152822.pdf)\n\n[335\\. DeepSeek R1 vs V3: Which AI Rules Coding? (2025 ...](https://mpgone.com/deepseek-r1-vs-v3-which-ai-rules-coding-2025-breakdown/)\n\n[336\\. The economic impact of artificial intelligence: A case study of DeepSeek R1](https://www.allfinancejournal.com/article/view/430/8-1-9)\n\n[337\\. PIONEER AI PARTNER COMPANY, CYKEL AI, ANNOUNCES INTEGRATION OF DEEPSEEK R1, REDUCING AI COSTS UP TO 96%](https://cdn.prod.website-files.com/633c6a023878f61ab5c1c1e5/67b887f8db6ac5b2e54fa807_JPEG%20PR_0220_Cykel%20DeepSeek%20R1%20integration.pdf)\n\n[338\\. Saleema Amershi, Andrew Begel et al. “Software Engineering for Machine Learning: A Case Study.” 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)](https://doi.org/10.1109/ICSE-SEIP.2019.00042)\n\n[339\\. AI Disruption at Scale: DeepSeek’s Open-Source Model and Its Macroeconomic Impact on Markets, Labor, and Global Growth](https://academiainsight.com/index.php/ijbmfr/article/download/398/209/457)\n\n[340\\. DeepSeek等AI工具在企业应用的有关案例](https://www.cjtouzi.com/ddjs/ghjl/202506/P020250605571047305570.docx)\n\n[341\\. AI Adoption and Risk Report](https://info.cyberhaven.com/hubfs/Content%20PDF/Cyberhaven%20Labs%20-%202025%20AI%20Adoption%20&%20Risk%20Report.pdf)\n\n[342\\. Efficiency and safety of the DeepSeek R1 model compared to OpenAI models](https://e-postulat.ru/index.php/Postulat/article/download/6089/6185)\n\n[343\\. Three Takeaways from the DeepSeek R1 Release](https://www.altmansolon.com/thought-leadership/deepseek-r1-impact-ai-industry)\n\n[344\\. Miryung Kim, Thomas Zimmermann et al. “\\[Journal First\\] Data Scientists in Software Teams: State of the Art and Challenges.” 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)](https://doi.org/10.1145/3180155.3182515)\n\n[345\\. Lower AI Costs Will Drive Innovation, Efficiency, and Adoption](https://tcwgroup.co.jp/-/media/Downloads/com/Insights/2025/250219-AILower-Costs.pdf?rev=7bb172cd986a4bdbbc7668e669442ff3&sc_lang=ja-JP)\n\n[346\\. DeepSeek-R1's bold bet on reinforcement learning: How it outpaced ...](https://venturebeat.com/ai/deepseek-r1s-bold-bet-on-reinforcement-learning-how-it-outpaced-openai-at-3-of-the-cost/)\n\n[347\\. Deepseek 撼动全球 AI 产业，助力 AI 应用、AI 端侧落地加速](https://pdf.dfcfw.com/pdf/H3_AP202502041642778628_1.pdf?1738741765000.pdf)\n\n[348\\. DeepSeek行业应用案例集：解锁智能变革密码](https://yb.1qh.cn/reports/2025-02-28/f887d70cc7d47aaf859f7fd9a66cbd18a5c83cc5.pdf)\n\n[349\\. DeepSeek正在改变企业级人工智能应用的规则?IT领导者需要...](http://www.fjcio.cn/Item/14306.aspx)\n\n[350\\. Impact of DeepSeek Across Diverse Scenarios: A Game ...](https://www.citictel-cpc.com/en-sg/blog/deepseek-game-changer-challenge)\n\n[351\\. Chinese AI startup DeepSeek unveils open-source model to rival OpenAI ...](https://www.computerworld.com/article/3808579/chinese-ai-startup-deepseek-unveils-open-source-model-to-rival-openai-o1.html)\n\n[352\\. J. Rushby. “Quality Measures and Assurance for AI Software1.”](https://www.semanticscholar.org/paper/4eec5a069b48d60aeaefb6e78abe997b32dcd147)\n\n[355\\. Scott M. Lundberg, Su-In Lee. “A Unified Approach to Interpreting Model Predictions.” Neural Information Processing Systems](https://arxiv.org/abs/1705.07874)\n\n[356\\. DeepSeek R1完成小版本升级，英伟达1Q FY26收入持续高速增长](https://pdf.dfcfw.com/pdf/H3_AP202506031683955519_1.pdf?1748947799000.pdf)\n\n[357\\. 证券研究报告-晨会聚焦](https://file.iyanbao.com/pdf/06679-9a63bdf5-f80e-4936-98ba-af41b0f47cc6.pdf)\n\n[358\\. 电子行业深度报告 DeepSeek 推动模型平权，关注 AI 终端及算力领域](https://pdf.dfcfw.com/pdf/H3_AP202502201643324447_1.pdf?1740069374000.pdf)\n\n[359\\. 中欧港股数字经济混合型发起式证券投资基金（QDII）2025年第1季度报告](https://www.zofund.com/tempdir/minisite/20250414/98dce7a0-8476-4b19-95ae-871893218b79_1744626884030.pdf)\n\n[360\\. 开源晨会 0205](https://file.iyanbao.com/pdf/33d46-e43a1fca-501c-4728-893c-cd0f1cd0bd1d.pdf)\n\n[361\\. A Survey of Generative AI in Finance](https://hal.science/hal-05020829v1/document)\n\n[362\\. DeepSeek R1来了，点亮数据中心行业新机遇](https://stage.ali.spgchinaratings.cn/research/pdf/20250217_commentary_deepseek-impact_cn.pdf)\n\n[363\\. 东兴晨报](https://file.iyanbao.com/pdf/54a1f-c9f67e72-6eef-48a2-8a29-d9213f28d4f6.pdf)\n\n[364\\. 电子行业 2025 年中期投资策略](https://pdf.dfcfw.com/pdf/H3_AP202506171692623676_1.pdf?1750180147000.pdf)\n\n[365\\. 港股重點關注名單 (2025 年 2 月 18 日)](https://www.easecurities.com.hk/tc/Research/EAS_HK_Equity_Key_Focuslist_20250218.pdf)\n\n[366\\. Macroeconomic and Equity Capital Markets Update](https://docs.londonstockexchange.com/sites/default/files/documents/macro-and-ecm-updated-november-final-version-2025.pdf)\n\n[367\\. 2025 DeepSeek技术全景解析 重塑全球AI生态的中国力量](https://pdf.dfcfw.com/pdf/H3_AP202503071644137559_1.pdf?1741340780000.pdf)\n\n[368\\. DeepSeek (深度求索) AI and its Implications for Innovation in Financial Services](https://www.kcmi.re.kr/kcmifile/webzine_content/OPINION/6527/webzinepdf_6527.pdf)\n\n[369\\. Evaluating DeepSeek AI vs. Top Competitors in 2025 - Future AGI](https://futureagi.com/blogs/evaluating-deepseek-ai-vs-top-competitors#:~:text=adaptive%20problem-solving.-,DeepSeek%20R1%20vs.,O3,%20Anthropic's%20Claude%203.5%20Sonnet))\n\n[370\\. DeepSeek 背景综述及在金融领域应用场景初探](https://cnpsec.com/plat_files/upload/png_upload/20250227/202502271740638923227.pdf)\n\n[371\\. Evaluating DeepSeek AI vs. Top Competitors in 2025](https://futureagi.com/blogs/evaluating-deepseek-ai-vs-top-competitors)\n\n[372\\. David Mhlanga. “Industry 4.0 in Finance: The Impact of Artificial Intelligence (AI) on Digital Financial Inclusion.” International Journal of Financial Studies](https://doi.org/10.3390/ijfs8030045)\n\n[373\\. Worldwide cloud service spending to grow by 19% in 2025 - Canalys](https://www.canalys.com/newsroom/worldwide-cloud-service-q4-2024#:~:text=In%20Q4%202024,%20global%20cloud,which%20significantly%20accelerated%20cloud%20adoption.)\n\n[375\\. EXPLAINABLE AI (XAI) FOR TRANSPARENT FINANCIAL DECISION-MAKING: A TECHNICAL FRAMEWORK](https://iaeme.com/MasterAdmin/Journal_uploads/IJRCAIT/VOLUME_7_ISSUE_2/IJRCAIT_07_02_131.pdf)\n\n[376\\. Scott M. Lundberg, Su-In Lee. “A Unified Approach to Interpreting Model Predictions.” Neural Information Processing Systems](https://arxiv.org/abs/1705.07874)\n\n[377\\. Marco Tulio Ribeiro, Sameer Singh et al. ““Why Should I Trust You?”: Explaining the Predictions of Any Classifier.” Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining](https://doi.org/10.1145/2939672.2939778)\n\n[378\\. Regulatory Compliance Automation in Banking Technology](https://ijemh.com/issue_dcp/Regulatory%20Compliance%20Automation%20in%20Banking%20Technology.pdf)\n\n[379\\. Enhancing Compliance Risk Assessment Frameworks in the Banking Sector: An AI Approaches for phases of Compliance Risk Assessment](https://ijrpr.com/uploads/V6ISSUE1/IJRPR37748.pdf)\n\n[380\\. Alejandro Barredo Arrieta, Natalia Díaz Rodríguez et al. “Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI.” Inf. Fusion](https://doi.org/10.1016/j.inffus.2019.12.012)\n\n[381\\. AI and Ethics: Our White Paper](https://www.datakeen.co/en/ai-and-ethics/)\n\n[382\\. Enhancing financial security: AI-driven anti-money laundering (AML) and compliance monitoring in the banking sector](https://journalwjarr.com/sites/default/files/fulltext_pdf/WJARR-2025-0365.pdf)\n\n[383\\. EXPLAINABLE AI FOR REGULATORY COMPLIANCE IN TREASURY OPERATIONS: BALANCING TRANSPARENCY AND PERFORMANCE](https://files.sdiarticle5.com/wp-content/uploads/2025/03/Ms_AJRCOS_132847.docx)\n\n[384\\. Tim Miller. “Explanation in Artificial Intelligence: Insights from the Social Sciences.” Artif. Intell.](https://doi.org/10.1016/J.ARTINT.2018.07.007)\n\n[385\\. Banking on AI: How U.S. Financial Institutions are Transforming with Artificial Intelligence](https://pyramidci.com/wp-content/uploads/2025/04/PCI_032025_Whitepaper_Banking-on-AI-1.pdf)\n\n[386\\. Unlocking the Potential: AI in Banking](https://www.minnesota.bank/common/Uploaded%20files/Events/LEAD24_Integris_AI%20in%20Banking.pdf)\n\n[387\\. EXPLAINABLE AI IN REGULATORY COMPLIANCE: BALANCING TRANSPARENCY AND PERFORMANCE IN AI DRIVEN TREASURY MANAGEMENT](https://files.sdiarticle5.com/wp-content/uploads/2025/03/Revised-ms_AJRCOS_132847_v1.docx)\n\n[388\\. The Transformative Power of AI: Real-World Banking Applications](https://eastgate-software.com/jp/the-transformative-power-of-ai-real-world-banking-applications/)"
  },
  {
    "id": 29,
    "question": "What unrecognized parallels exist between the architectural philosophy of TCP/IP (best-effort delivery enabling new applications) and emerging AI model paradigms that embrace imperfection?",
    "response": "**Research Report: The Unrecognized Architectural Parallel Between TCP/IP and \"Imperfect\" AI**\n\n**Date:** July 26, 2025\n\n**Author:** AI Research Assistant\n\n**Executive Summary**\n\nAt first glance, the architectural principles of the internet's foundational protocol suite, TCP/IP, and the design of cutting-edge Artificial Intelligence (AI) models appear to occupy entirely separate technological domains. However, a deeper analysis reveals a profound and largely unrecognized parallel. This report argues that the core philosophy of TCP/IP—specifically its \"best-effort\" delivery model and the \"end-to-end principle\" which intentionally embraces imperfection in the network core to enable permissionless innovation at the edges—provides a powerful historical and technical analogue for emerging AI paradigms. These new AI architectures are shifting away from the pursuit of monolithic perfection and are instead intentionally incorporating uncertainty, approximation, and probabilistic outputs to achieve greater robustness, safety, and application flexibility. This shared philosophy of leveraging a simple, \"imperfect\" core to foster complex intelligence at the periphery represents a powerful and recurring design pattern in transformational technologies.\n\n**1\\. The Foundational Philosophy of TCP/IP: How Imperfection Forged the Internet**\n\nThe internet's remarkable success and extensibility are not accidental; they are the direct result of a set of core design principles, chief among them the end-to-end argument and its implementation via a \"best-effort\" delivery model. This philosophy deliberately simplified the network's core responsibilities, creating a foundation of \"intelligent imperfection\" that proved essential for future innovation.\n\n**1.1. The \"Best-Effort\" Core: A Contract of No Guarantees**\n\nThe Internet Protocol (IP), which sits at the heart of the TCP/IP suite, operates on a \"best-effort\" delivery model. This model is defined by its explicit lack of guarantees \\[6\\]\\[11\\]\\[16\\]. When a device sends a data packet (or datagram) into the network, the IP layer makes its best effort to deliver it, but it does not promise success. The key technical characteristics of this model include:\n\n**Connectionless Transmission:** Each packet is treated as an independent entity, routed individually through the network based on current conditions without a pre-established connection \\[2\\]\\[4\\]\\[7\\].\n\n**No Guarantees of Quality or Delivery:** The network provides no intrinsic assurance that a packet will arrive at its destination, that packets will arrive in the correct order, that they will not be duplicated, or that they will arrive within a certain timeframe \\[8\\]\\[9\\]\\[11\\].\n\n**Simplicity in the Network:** The model prioritizes efficiency and simplicity by not expending resources to handle complex error correction or retransmission within the network's core routers \\[6\\]\\[10\\]. If a router encounters an issue, its simplest and most common response is to discard the problematic packet \\[6\\]\\[10\\].\n\nThis approach can be likened to a standard postal service, which does its best to deliver a letter but offers no inherent guarantee against loss or delay \\[11\\]. This seeming deficiency is, in fact, the system's greatest strength.\n\n**1.2. The End-to-End Principle: Pushing Intelligence to the Edge**\n\nThe best-effort model is a direct implementation of the \"end-to-end argument,\" a seminal design principle articulated in the 1980s that shaped the internet's architecture \\[156\\]\\[157\\]. The argument posits that functions like reliability, error correction, and sequencing should be implemented in the end-user applications (the \"ends\" of the network), not in the intermediary nodes (the network \"core\") \\[6\\]\\[9\\]\\[13\\].\n\nThe responsibility for ensuring that a stream of data arrives intact and in order falls to higher-level protocols like the Transmission Control Protocol (TCP), which operates at the endpoints \\[9\\]\\[12\\]. TCP checks for lost packets, requests retransmissions, and reorders packets correctly, effectively building a reliable service on top of an unreliable one.\n\n**1.3. A Platform for Permissionless Innovation**\n\nThis architectural choice—a simple, un-opinionated core with intelligence at the ends—was revolutionary. By refusing to optimize the network for any _known_ application, the designers created a platform that could support countless _unknown_ future applications \\[13\\]. Because the network itself was \"dumb\" and made no assumptions, it did not preclude any particular use case. This led to an explosion of permissionless innovation:\n\nThe **World Wide Web (HTTP)** was built on top of TCP/IP without requiring any changes to the underlying network.\n\n**Voice over IP (VoIP)** and **video streaming** could use protocols like UDP, which sits directly on top of IP and forgoes the reliability of TCP, because these applications can tolerate some packet loss and prefer lower latency.\n\n**Peer-to-peer file sharing**, **online gaming**, and countless other applications emerged, each implementing its own logic for handling the network's inherent imperfections.\n\nThe network's \"imperfection\" was the feature that guaranteed its long-term flexibility and relevance, a stark contrast to the highly specialized, state-controlled telecommunication networks that preceded it.\n\n**2\\. The Rise of \"Imperfect\" AI: Embracing Uncertainty as a Feature**\n\nA parallel philosophical shift is now occurring in the field of Artificial Intelligence. While early AI often pursued deterministic logic and perfect accuracy \\[21\\], contemporary and emerging AI paradigms are increasingly built on principles of probability, approximation, and uncertainty. This embrace of \"imperfection\" is not a sign of failure but a deliberate architectural choice to create models that are more robust, safe, and useful in the complex, ambiguous real world.\n\n**2.1. Intentional Uncertainty: From Black Box to Probabilistic Oracle**\n\nThe most direct parallel lies in the move from models that provide a single, deterministic prediction to those that provide a probabilistic output, quantifying their own confidence. This is a fundamental shift from a \"black box\" that gives an answer to a more nuanced system that expresses what it knows and, crucially, what it _doesn't_ know.\n\n**Bayesian Neural Networks (BNNs):** BNNs are a class of models that learn probability distributions over their parameters instead of single point estimates. This architectural choice allows them to inherently model their own uncertainty \\[102\\]\\[108\\]. They can even distinguish between _epistemic uncertainty_ (uncertainty in the model itself, which can be reduced with more data) and _aleatoric uncertainty_ (inherent randomness or noise in the data) \\[181\\].\n\n**Monte Carlo Dropout (MCD):** A widely used and computationally efficient technique that approximates Bayesian inference in deep neural networks \\[179\\]\\[181\\]. By keeping dropout (a regularization technique that randomly drops neurons during training) active during test time and running multiple forward passes, MCD generates a distribution of outputs. The variance in this distribution serves as a proxy for the model's uncertainty \\[183\\]. This is a literal \"injection of intentional uncertainty\" to gain a richer, more robust signal.\n\n**2.2. Approximation as a Strategic Trade-Off**\n\nBeyond uncertainty, many advanced AI systems intentionally use approximation as a core part of their strategy, trading a degree of precision for performance, scalability, or strategic advantage.\n\n**AlphaGo's Probabilistic Strategy:** DeepMind's AlphaGo, which famously defeated the world's best Go players, does not function by calculating every possible move. Instead, it uses a probabilistic approach combining Monte Carlo tree search with deep neural networks to evaluate board positions \\[91\\]. Critically, its architecture is optimized to maximize the _probability of winning_, not the margin of victory \\[93\\]\\[97\\]. This means it might make a move that a human sees as suboptimal or \"imperfect\" because it leads to a position with a 99% chance of a narrow win, rather than a riskier move with an 80% chance of a landslide victory \\[97\\]. This is a deliberate trade-off, embracing an \"imperfect\" path for greater overall robustness.\n\n**Approximate Computing in AI Hardware:** The field of approximate computing, which trades computational precision for gains in speed and power efficiency, has found a natural home in AI \\[125\\]. Because machine learning models are inherently error-tolerant and deal with noisy, real-world data, they do not always require perfect arithmetic precision. Companies like Google have explicitly used approximate computing principles in their Tensor Processing Units (TPUs) to accelerate machine learning workloads \\[124\\].\n\n**2.3. Designed Imperfection for AI Safety and Alignment**\n\nThe most advanced application of this philosophy is in AI safety, where \"imperfection\" is explicitly designed into a system to make it safer and more aligned with human values.\n\n**Uncertainty-Aware Safety Systems:** In safety-critical applications like autonomous driving or medical diagnosis, a model that fails silently is catastrophic. An \"imperfect\" model that knows when it is uncertain is far safer \\[134\\]\\[135\\]\\[299\\]. An autonomous vehicle's perception system that flags high uncertainty when encountering a novel object can trigger a fallback procedure, such as slowing down or handing control to a human driver \\[242\\]\\[242\\].\n\n**Metrics for Calibrated Imperfection:** The maturity of this field is demonstrated by the development of specific metrics to evaluate not just a model's accuracy, but the quality of its uncertainty estimates. Metrics like **Expected Calibration Error (ECE)**, which measures the consistency between a model's predicted confidence and its actual accuracy, and **Uncertainty Accuracy (UAcc)**, which rewards models for being uncertain about incorrect predictions, are now central to evaluating safety-critical AI \\[239\\]\\[299\\]\\[305\\]. The engineering goal is shifting from pure accuracy to well-calibrated, reliable uncertainty \\[239\\]\\[243\\].\n\n**Fairness Through Imperfection:** AI fairness research has even explored specifying \"desired imperfections\" in models to counteract systemic biases present in training data and ensure more equitable outcomes \\[28\\].\n\n**3\\. Synthesizing the Parallel: A Shared Design Pattern for Robust Systems**\n\nThe philosophies governing TCP/IP and \"imperfect\" AI, though developed decades apart in different fields, converge on an identical architectural pattern. This pattern prioritizes a simple, unspecialized core that pushes complexity and intelligence to the edges, thereby creating a flexible and robust platform for innovation.\n\n|     |     |     |\n| --- | --- | --- |\n| **Architectural Principle** | **TCP/IP Implementation** | **\"Imperfect\" AI Analogue** |\n| **Simple, \"Dumb\" Core** | The **IP layer** is connectionless and offers only best-effort delivery. It is kept as simple as possible \\[6\\]\\[10\\]. | A **base AI model** (e.g., a neural network) that produces a probabilistic output or a distribution, not a single, final decision. |\n| **Intelligent, Complex Edges** | **TCP and Applications** (HTTP, SMTP, etc.) run on the end systems, implementing reliability, sequencing, and application-specific logic \\[6\\]\\[9\\]\\[13\\]. | **Downstream applications or control systems** interpret the AI's probabilistic output. They implement the logic for handling uncertainty (e.g., requesting human review, triggering a safety protocol, exploring novel recommendations). |\n| **Best-Effort Delivery** | The network promises to **try its best** to deliver a packet but offers no guarantee of success, order, or integrity \\[6\\]\\[11\\]. | A model provides a **probabilistic prediction**—its \"best guess\" along with a measure of its confidence, but no guarantee of absolute truth. |\n| **Enabling Permissionless Innovation** | The un-opinionated network allowed an **explosion of unforeseen applications** (Web, VoIP, P2P) because developers could build whatever they needed on top \\[13\\]. | Probabilistic models provide a **richer output signal** (the full distribution) than a single label. This enables a wider range of downstream applications (e.g., a single perception model can be used for aggressive, standard, or cautious driving modes by interpreting its uncertainty differently). |\n| **Robustness Through Distributed Responsibility** | The network is resilient because packet failure is expected. **Responsibility for recovery is distributed** to the endpoints \\[10\\]. | Systems are more robust because model uncertainty is expected. The model **signals its potential failure**, distributing the responsibility for safe handling to the larger system \\[134\\]\\[299\\]. |\n\nWhile the search results did not provide a definitive case of approximation _errors_ in collaborative filtering leading to new commercial features \\[162\\]\\[229\\]\\[279\\]the underlying principle holds. The inherent _approximateness_ of these models is what allows for \"serendipitous\" recommendations that connect users to unexpected but relevant items \\[172\\]\\[230\\]. A \"perfect\" system might only suggest obviously similar items, whereas the statistical \"noise\" in an approximate model can surface the novel connections that drive engagement.\n\n**4\\. Conclusion and Future Outlook**\n\nThe architectural philosophy that made the internet a dynamic and enduring platform for innovation is re-emerging as a central design pattern for building the next generation of Artificial Intelligence. The parallel is not merely an academic curiosity; it is a prescriptive guide for the future of AI engineering.\n\nBy embracing the TCP/IP model of a simple, \"imperfect\" core with intelligence at the edges, AI developers can move beyond the creation of brittle, monolithic models that aim for an unattainable and often undesirable perfection. The future lies in building models that are honest about their own limitations—models that produce rich, probabilistic outputs that quantify their uncertainty.\n\nThis shift changes the very definition of a \"good\" model. The goal is no longer just maximizing a raw accuracy score. It is about optimizing for well-calibrated uncertainty, as measured by metrics like ECE and UAcc \\[299\\]\\[305\\]. This creates a clear pathway toward AI systems that are not only more powerful but also demonstrably safer, more robust, and more adaptable. Just as the end-to-end principle unleashed decades of permissionless innovation across the globe, this embrace of \"intelligent imperfection\" may be the key to building AI that can finally navigate the complexity and uncertainty of the real world.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. S. Blake, D. Black et al. “An Architecture for Differentiated Services.” RFC](https://doi.org/10.17487/rfc2475)\n\n[2\\. 网络协议的演进和创新](https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-cn/mediares/magazine/publication/com_cn/article/202406/12.pdf)\n\n[3\\. R. Braden, D. Clark et al. “Integrated Services in the Internet Architecture: an Overview.” RFC](https://doi.org/10.17487/RFC1633)\n\n[4\\. TCP/IP Model](http://www.eie.polyu.edu.hk/~em/it0607pdf/9%20TCPIP.ppt)\n\n[5\\. V. Jacobson. “Congestion avoidance and control.” Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication](https://doi.org/10.1145/52324.52356)\n\n[6\\. TCP/IP Illustrated, Volume 1: The Protocols](https://www.r-5.org/files/books/computers/internals/net/Richard_Stevens-TCP-IP_Illustrated-EN.pdf)\n\n[7\\. Illustrated TCP/IP](https://www.cs.dartmouth.edu/~sergey/netreads/ipv6/naugle-illustrated-tcpip.pdf)\n\n[8\\. ISCOM2600G (A) Series Configuration Guide (CLI) (Rel_11)](https://www.davantel.com/wp-content/uploads/2021/03/ISCOM2600G-A-Series-Configuration-Guide-CLI-Rel_11.pdf)\n\n[9\\. TCP/IP Tutorial and Technical Overview](https://www.redbooks.ibm.com/redbooks/pdfs/gg243376.pdf)\n\n[10\\. TCP/IP Illustrated, Volume 1: The Protocols 第一章学习笔记](https://www.cnblogs.com/Ronnie-97/p/14583055.html)\n\n[11\\. Communication Networks (20A04603T)](https://vemu.org/uploads/lecture_notes/09_01_2024_627881163.pdf)\n\n[12\\. S. Salmon, H. ElAarag. “Simulation based experiments using EDNAS: The Event-Driven Network Architecture Simulator.” Proceedings of the 2011 Winter Simulation Conference (WSC)](https://doi.org/10.1109/WSC.2011.6148023)\n\n[13\\. Open Access and Information Commons](http://benkler.org/Open%20Access%20Commons%20Oxford%20Handbook%20Prepub.pdf)\n\n[14\\. V. Cerf, R. Kahn. “A protocol for packet network intercommunication.” Comput. Commun. Rev.](https://doi.org/10.1109/9780470546543.ch54)\n\n[15\\. ABCs of OS/390 System Programming Volume 4](https://www.redbooks.ibm.com/redbooks/pdfs/sg245654.pdf)\n\n[16\\. TCP/IP协议(5): IP(Internet Protocol) 协议 —— 连接各...](https://www.cnblogs.com/yanglingwell/p/16983025.html)\n\n[17\\. Placement of Function in a Best Effort World](https://www.cs.cmu.edu/~dga/15-744/S07/lectures/03-besteffort.pdf)\n\n[18\\. Network Forensics](https://www.kneda.net/documentos/Network%20Forensics.pdf)\n\n[21\\. Artificial General Intelligence (AGI): Bridging Human Cognition ...](https://www.xugj520.cn/en/archives/agi-development-human-machine-cognition.html)\n\n[22\\. C. Rudin. “Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead.” Nature Machine Intelligence](https://doi.org/10.1038/s42256-019-0048-x)\n\n[23\\. Dario Amodei, C. Olah et al. “Concrete Problems in AI Safety.” ArXiv](https://arxiv.org/abs/1606.06565)\n\n[24\\. Tim Miller. “Explanation in Artificial Intelligence: Insights from the Social Sciences.” Artif. Intell.](https://doi.org/10.1016/J.ARTINT.2018.07.007)\n\n[25\\. Stuart Russell. “Human Compatible: Artificial Intelligence and the Problem of Control.”](https://www.semanticscholar.org/paper/6df2126301ab415aed034b0bcd9589b1897fe983)\n\n[26\\. Anna Jobin, M. Ienca et al. “The global landscape of AI ethics guidelines.” Nature Machine Intelligence](https://doi.org/10.1038/s42256-019-0088-2)\n\n[27\\. Emerging AI Models and the Problems We Can’t See](https://www.gallostech.io/blog/emerging-ai-models-and-the-problems-we-cant-see/)\n\n[28\\. K. Holtman. “Demanding and Designing Aligned Cognitive Architectures.” ArXiv](https://arxiv.org/abs/2112.10190)\n\n[29\\. Agent Architectures](https://smythos.com/ai-agents/agent-architectures/)\n\n[30\\. Artificial Intelligence in Architecture: Transforming Cities and Buildings](https://www.preprints.org/frontend/manuscript/37931de7866ed828b37b76d9455471f1/download_pub)\n\n[31\\. Recent Emerging Techniques in Explainable Artificial Intelligence to Enhance the Interpretable and Understanding of AI Models for Human](https://link.springer.com/content/pdf/10.1007/s11063-025-11732-2.pdf)\n\n[32\\. AI and the Beauty of Imperfection in Design](https://www.istitutomarangoni.com/en/maze35/industry/ai-design-imperfection?itm_campaign=Maze35_ALL_FDA_220700_other_EN_ALL)\n\n[33\\. \"An Ideal Human\": Expectations of AI Teammates in Human-AI Teaming ...](https://dl.acm.org/doi/abs/10.1145/3432945)\n\n[34\\. V. Jacobson. “Congestion avoidance and control.” Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication](https://doi.org/10.1145/52324.52356)\n\n[35\\. Noam Chomsky. “Lectures on Government and Binding.”](https://www.semanticscholar.org/paper/70859d32881e57fe4195f9b8b524f3f4abaa9a80)\n\n[36\\. Richard S. Kayne. “The Antisymmetry of Syntax.”](https://www.semanticscholar.org/paper/84fa8366777337bc5b011c184a1628e646955ff3)\n\n[37\\. G. Cinque. “Types of Ā-dependencies.”](https://www.semanticscholar.org/paper/cc9cf21a547a56cabb3411a574b5288952eaa150)\n\n[38\\. J. Platt. “Probabilistic Outputs for Support vector Machines and Comparisons to Regularized Likelihood Methods.”](https://www.semanticscholar.org/paper/42e5ed832d4310ce4378c44d05570439df28a393)\n\n[39\\. TCP/IP performance test 2](https://indico.cern.ch/event/641667/contributions/2603861/attachments/1465715/2266188/tcp_test_20170517.pdf)\n\n[40\\. All the brilliance of AI on minimalist platforms | InfoWorld](https://www.infoworld.com/article/3715633/all-the-brilliance-of-ai-on-minimalist-platforms.html)\n\n[41\\. Minimalism in Modern Design and UI UX](https://www.divami.com/blog/design-minimalism-the-2021-guide)\n\n[42\\. Noam Chomsky. “The Minimalist Program.” Journal of Linguistics](https://doi.org/10.1017/S0022226797006889)\n\n[43\\. E. Altman, Konstantin Avrachenkov et al. “A stochastic model of TCP/IP with stationary random losses.” Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication](https://doi.org/10.1145/347059.347549)\n\n[44\\. A. Madry, Aleksandar Makelov et al. “Towards Deep Learning Models Resistant to Adversarial Attacks.” ArXiv](https://arxiv.org/abs/1706.06083)\n\n[45\\. Alejandro Barredo Arrieta, Natalia Díaz Rodríguez et al. “Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI.” Inf. Fusion](https://doi.org/10.1016/j.inffus.2019.12.012)\n\n[46\\. Tim Miller. “Explanation in Artificial Intelligence: Insights from the Social Sciences.” Artif. Intell.](https://doi.org/10.1016/J.ARTINT.2018.07.007)\n\n[47\\. D. Sculley, Gary Holt et al. “Hidden Technical Debt in Machine Learning Systems.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/1eb131a34fbb508a9dd8b646950c65901d6f1a5b)\n\n[48\\. Jessica Fjeld, Nele Achten et al. “Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI.” SSRN Electronic Journal](https://doi.org/10.2139/ssrn.3518482)\n\n[49\\. Quantum Computing and Imperfection: Exploring the Role of Imperfection in AI and Computing](http://axelschultze.com/2018/10/20/)\n\n[50\\. AI in Architecture: 10 Use Cases, Examples & Technologies](https://www.itransition.com/ai/architecture)\n\n[51\\. Survey of Instruction Set Architectures](https://www.cs.umd.edu/users/meesh/411/5ed-Appendix-K.pdf)\n\n[52\\. AI Engineering: Progressing towards Robust and Trustworthy AI Systems](https://www.fortiss.org/fileadmin/user_upload/06_Ergebnisse/Studien_und_Roadmaps/fortiss_report_AI_Engineering_en_web.pdf)\n\n[53\\. Software Architecture in an AI World - O'Reilly](https://www.oreilly.com/radar/software-architecture-in-an-ai-world/)\n\n[54\\. ARTIFICIAL INTELLIGENCE](https://www.allassignmenthelp.com/samples/artificial-intelligence.html)\n\n[55\\. Survey of AI Technologies and AI R&D Trajectories](https://assets-global.website-files.com/62c4cf7322be8ea59c904399/65e83959fd414a488a4fa9a5_Gladstone%20Survey%20of%20AI.pdf)\n\n[56\\. Explainable Artificial Intelligence for Radio Resource Management Systems: A diverse feature importance approach](https://www.diva-portal.org/smash/get/diva2:1694443/FULLTEXT01.pdf)\n\n[57\\. End-to-End Principle](https://devopedia.org/end-to-end-principle)\n\n[58\\. V. Jacobson. “Congestion avoidance and control.” Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication](https://doi.org/10.1145/52324.52356)\n\n[59\\. J. Saltzer, D. Reed et al. “End-to-end arguments in system design.” TOCS](https://doi.org/10.1145/357401.357402)\n\n[60\\. C. Dovrolis, R. Prasad. “An evolutionary approach to improve end-to-end performance in tcp/ip networks.”](https://www.semanticscholar.org/paper/26cbd3122c03a5b7459a0957f21b267b484d72e7)\n\n[61\\. M. Mathis, J. Mahdavi et al. “TCP Selective Acknowledgement Options.”](https://www.semanticscholar.org/paper/f192b908014344086ef202147a8f27b7c4c32e2f)\n\n[62\\. TCP/IP Essentials: A Lab-Based Approach](http://core-cms.prod.aop.cambridge.org/core/search?filters%5BauthorTerms%5D=Shivendra%20S.%20Panwar&eventCode=SE-AU)\n\n[63\\. V. Jacobson, Robert T. Braden et al. “TCP Extensions for High Performance.” RFC](https://doi.org/10.17487/RFC1323)\n\n[64\\. TCP/IP Illustrated, Volume 1: The Protocols](https://www.r-5.org/files/books/computers/internals/net/Richard_Stevens-TCP-IP_Illustrated-EN.pdf)\n\n[65\\. ARGOS (Autumn-Ready Global Open Set)](https://files.quizbowlpackets.com/3318/Packet%207.docx)\n\n[66\\. Study of TCP Performance over Mobile Networks](http://web.cs.wpi.edu/~cs535/s03/krishnan.ppt)\n\n[67\\. Openness and neutrality in broadband networks - Norway](https://www.forskningsradet.no/siteassets/publikasjoner/1171917013192.pdf)\n\n[68\\. End-to-end principle](https://static.hlt.bme.hu/semantics/external/pages/deep_learning/en.wikipedia.org/wiki/End-to-end_principle.html)\n\n[69\\. End-to-end principle (Internet Architecture)](http://www.upcscavenger.com/wiki/end-to-end%20principle/)\n\n[70\\. Zhihai Wang, Zijie Geng et al. “Benchmarking End-To-End Performance of AI-Based Chip Placement Algorithms.”](https://arxiv.org/abs/2407.15026)\n\n[71\\. NSF Convergence Accelerator Workshop Report Inaugural Workshop on Provably Safe and Beneficial AI (PSBAI)](https://nsf-gov-resources.nsf.gov/2023-03/Provably%20Safe%20and%20Beneficial%20AI%20%28PSBAI%29%20Workshop%20Report_2230996_October%202022_Final.508.pdf?VersionId=.wZP19Vni_RHAq_xSjUg_Ofm6HdexWKD)\n\n[72\\. Dynamic modeling of Internet congestion control](https://people.kth.se/~kallej/grad_students/jacobsson_phdthesis08.pdf)\n\n[73\\. LAW AND INFORMATION PLATFORMS](http://www.jthtl.org/content/articles/V1I1/JTHTLv1i1_Weiser.PDF)\n\n[74\\. Open Access and Information Commons](http://benkler.org/Open%20Access%20Commons%20Oxford%20Handbook%20Prepub.pdf)\n\n[75\\. L. Grieco, S. Mascolo et al. “Additive increase adaptive decrease congestion control: a mathematical model and its experimental validation.” Proceedings ISCC 2002 Seventh International Symposium on Computers and Communications](https://doi.org/10.1109/ISCC.2002.1021772)\n\n[77\\. Marco Tulio Ribeiro, Sameer Singh et al. ““Why Should I Trust You?”: Explaining the Predictions of Any Classifier.” Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining](https://doi.org/10.1145/2939672.2939778)\n\n[78\\. Sally Floyd, Van Jacobson. “Random early detection gateways for congestion avoidance.” IEEE/ACM Trans. Netw.](https://doi.org/10.1109/90.251892)\n\n[79\\. V. Jacobson. “Congestion avoidance and control.” Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication](https://doi.org/10.1145/52324.52356)\n\n[80\\. A. Demers, S. Keshav et al. “Analysis and simulation of a fair queueing algorithm.” Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication](https://doi.org/10.1145/75246.75248)\n\n[81\\. K. Fall, S. Floyd. “Simulation-based comparisons of Tahoe, Reno and SACK TCP.” Comput. Commun. Rev.](https://doi.org/10.1145/235160.235162)\n\n[82\\. BASE PROSPECTUS](http://www.rns-pdf.londonstockexchange.com/rns/6200A_1-2025-3-13.pdf)\n\n[83\\. Automation for a Resilient Society](https://www.case2023.org/IEEECASE2023Handbook.pdf)\n\n[84\\. Explicit Allocation of Best-Effort Packet Delivery Service](http://www.nms.lcs.mit.edu/6829-papers/p362-clark.pdf)\n\n[85\\. D. Clark, Wenjia Fang. “Explicit allocation of best-effort packet delivery service.” IEEE/ACM Trans. Netw.](https://doi.org/10.1109/90.720870)\n\n[86\\. Artificial Intelligence in Communication Network Management](https://www.atlantis-press.com/article/126003614.pdf)\n\n[87\\. Yann LeCun, Yoshua Bengio et al. “Deep Learning.”](https://www.semanticscholar.org/paper/2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb)\n\n[88\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[89\\. David Silver, Julian Schrittwieser et al. “Mastering the game of Go without human knowledge.” Nature](https://doi.org/10.1038/nature24270)\n\n[90\\. David Silver, T. Hubert et al. “A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.” Science](https://doi.org/10.1126/science.aar6404)\n\n[91\\. Google DeepMind的人工智能之路](http://www.lamda.nju.edu.cn/qianh/CCCF-DeepMind.pdf)\n\n[92\\. Tim Miller. “Explanation in Artificial Intelligence: Insights from the Social Sciences.” Artif. Intell.](https://doi.org/10.1016/J.ARTINT.2018.07.007)\n\n[93\\. 哈萨比斯:不知AlphaGo有何弱点 将公布更多技术细节|AlphaG...](https://tech.sina.com.cn/it/2017-05-24/doc-ifyfkqks4529994.shtml)\n\n[94\\. RAPPORT au nom de L'OFFICE PARLEMENTAIRE D'ÉVALUATION DES CHOIX SCIENTIFIQUES ET TECHNOLOGIQUES sur les nouveaux développements de l'intelligence artificielle](https://www.senat.fr/rap/r24-170/r24-1701.pdf)\n\n[95\\. GitHub - brilee/MuGo: Replicating AlphaGo's architectu...](https://github.com/brilee/MuGo)\n\n[96\\. Yutian Chen, Aja Huang et al. “Bayesian Optimization in AlphaGo.” ArXiv](https://arxiv.org/abs/1812.06855)\n\n[97\\. 'A Game That Is Not a Game': The Sublime Limit of Human Intelligence and AI Through Go](https://culturemachine.net/wp-content/uploads/2021/09/Kwasu-Tembo.pdf)\n\n[99\\. A Bayesian end-to-end model with estimated uncertainties for simple question answering over knowledge bases](https://palm.seu.edu.cn/zhoudeyu/resources/files/publication/A%20Bayesian%20end-to-end%20model%20with%20estimated%20uncertainties%20for%20simple%20question%20answering%20over%20knowledge%20bases.pdf)\n\n[100\\. End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes](https://neurips.cc/media/neurips-2023/Slides/70635_DC54Jg0.pdf)\n\n[101\\. End-to-End Learning for Stochastic Optimization: A Bayesian Perspective](https://proceedings.mlr.press/v202/rychener23a/rychener23a.pdf)\n\n[102\\. Can Bayesian Neural Networks Explicitly Model Input...](http://arxiv.org/html/2501.08285v1)\n\n[103\\. Uncertainty-Aware CNNs for Depth Completion: Uncertainty from Beginning to End](https://openaccess.thecvf.com/content_CVPR_2020/papers/Eldesokey_Uncertainty-Aware_CNNs_for_Depth_Completion_Uncertainty_from_Beginning_to_End_CVPR_2020_paper.pdf)\n\n[104\\. Uncertainty quantification in scientific machine learning:](https://dl.acm.org/doi/10.1016/j.jcp.2022.111902)\n\n[105\\. Predictive uncertainty estimation via prior networks | Proceedings of ...](https://dl.acm.org/doi/10.5555/3327757.3327808)\n\n[106\\. Rhiannon Michelmore, Matthew Wicker et al. “Uncertainty Quantification with Statistical Guarantees in End-to-End Autonomous Driving Control.” 2020 IEEE International Conference on Robotics and Automation (ICRA)](https://doi.org/10.1109/ICRA40945.2020.9196844)\n\n[107\\. Y. Gal, Zoubin Ghahramani. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” International Conference on Machine Learning](https://arxiv.org/abs/1506.02142)\n\n[108\\. Can Bayesian Neural Networks Explicitly Model Input ...](https://arxiv.org/html/2501.08285v1)\n\n[109\\. Where to model the epistemic uncertainty of Bayesian convolutional neural networks for classification](https://dl.acm.org/doi/10.1016/j.neucom.2024.127568)\n\n[110\\. Balaji Lakshminarayanan, A. Pritzel et al. “Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles.” Neural Information Processing Systems](https://arxiv.org/abs/1612.01474)\n\n[111\\. Olga Graf, P. Flores et al. “Error-Aware B-PINNs: Improving Uncertainty Quantification in Bayesian Physics-Informed Neural Networks.” ArXiv](https://doi.org/10.48550/arXiv.2212.06965)\n\n[112\\. A Tutorial on Learning with Bayesian Networks](https://link.springer.com/chapter/10.1007/978-94-011-5014-9_11)\n\n[119\\. Sally Floyd, Van Jacobson. “Random early detection gateways for congestion avoidance.” IEEE/ACM Trans. Netw.](https://doi.org/10.1109/90.251892)\n\n[120\\. V. Jacobson. “Congestion avoidance and control.” Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication](https://doi.org/10.1145/52324.52356)\n\n[121\\. K. Fall, S. Floyd. “Simulation-based comparisons of Tahoe, Reno and SACK TCP.” Comput. Commun. Rev.](https://doi.org/10.1145/235160.235162)\n\n[122\\. Sally Floyd. “TCP and explicit congestion notification.” Comput. Commun. Rev.](https://doi.org/10.1145/205511.205512)\n\n[123\\. S. Floyd. “Connections with multiple congested gateways in packet-switched networks part 1: one-way traffic.” Comput. Commun. Rev.](https://doi.org/10.1145/122431.122434)\n\n[124\\. A NOVEL MODELING FOR INEXACT GATES AND ANALYSIS OF QUASI-TRNG APPLICATION](http://eece.cu.edu.eg/~hfahmy/thesis/2019_11_inexact.pdf)\n\n[125\\. Novel approximate adaptive carry lookahead adder for ...](https://www.nature.com/articles/s41598-025-03865-0)\n\n[126\\. Dynamic modeling of Internet congestion control](https://people.kth.se/~kallej/grad_students/jacobsson_phdthesis08.pdf)\n\n[127\\. B. Mittelstadt, Chris Russell et al. “Explaining Explanations in AI.” Proceedings of the Conference on Fairness, Accountability, and Transparency](https://doi.org/10.1145/3287560.3287574)\n\n[128\\. Model Context Protocol (MCP): Bridge between the AI ...](https://vivasoftltd.com/model-context-protocol-mcp/)\n\n[129\\. I. Goodfellow, Jonathon Shlens et al. “Explaining and Harnessing Adversarial Examples.” CoRR](https://arxiv.org/abs/1412.6572)\n\n[130\\. Nicholas Carlini, D. Wagner. “Towards Evaluating the Robustness of Neural Networks.” 2017 IEEE Symposium on Security and Privacy (SP)](https://doi.org/10.1109/SP.2017.49)\n\n[131\\. Mariusz Bojarski, D. Testa et al. “End to End Learning for Self-Driving Cars.” ArXiv](https://arxiv.org/abs/1604.07316)\n\n[132\\. Dario Amodei, C. Olah et al. “Concrete Problems in AI Safety.” ArXiv](https://arxiv.org/abs/1606.06565)\n\n[133\\. M. Cannarsa. “Ethics Guidelines for Trustworthy AI.” The Cambridge Handbook of Lawyering in the Digital Age](https://doi.org/10.1017/9781108936040.022)\n\n[134\\. Uncertainty Estimation Toward Safe AI](https://repository.upenn.edu/items/58fb5968-b882-417d-b798-b103fac14aaf)\n\n[135\\. Harald Ruess, Simon Burton. “Safe AI -- How is this Possible?.”](https://arxiv.org/abs/2201.10436)\n\n[136\\. Artificial Intelligence — Functional Safety and AI Systems](https://www.services.bis.gov.in/tmp/LITD38624862_23072024_2.pdf)\n\n[137\\. Assessment List for Trustworthy Artificial Intelligence (ALTAI)](https://airegio.ems-carsa.com/nfs/programme_5/call_3/call_preparation/ALTAI_final.pdf)\n\n[139\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[140\\. Zoubin Ghahramani. “Probabilistic machine learning and artificial intelligence.” Nature](https://doi.org/10.1038/nature14541)\n\n[141\\. Kevin P. Murphy. “Machine learning - a probabilistic perspective.” Adaptive computation and machine learning series](https://www.semanticscholar.org/paper/360ca02e6f5a5e1af3dce4866a257aafc2d6d6f5)\n\n[142\\. D. Koller, N. Friedman. “Probabilistic Graphical Models - Principles and Techniques.”](https://www.semanticscholar.org/paper/36eb6fea39ce06e2807f074fa3d5e79ed0f2bcef)\n\n[143\\. cornelio c 2021 - Reasoning with PCP-Nets](https://dl.acm.org/doi/10.1613/jair.1.13009)\n\n[144\\. AI Uncertainty](http://archive.gamedev.net/archive/reference/articles/article197.html)\n\n[145\\. Philipp Hennig, Michael A. Osborne et al. “Probabilistic numerics and uncertainty in computations.” Proceedings. Mathematical, Physical, and Engineering Sciences / The Royal Society](https://doi.org/10.1098/rspa.2015.0142)\n\n[146\\. COS 561: Advanced Computer Networks](https://www.cs.princeton.edu/courses/archive/fall16/cos561/docs/01Intro.ppt)\n\n[147\\. Polynomial semantics of tractable probabilistic circui...](https://dl.acm.org/doi/10.5555/3702676.3702696)\n\n[148\\. A copula-based uncertainty propagation method for stru...](https://dl.acm.org/doi/10.1016/j.ijar.2021.08.002)\n\n[149\\. Probability-interval hybrid uncertainty analysis for structures with both aleatory and epistemic uncertainties: a review](https://link.springer.com/article/10.1007/s00158-017-1864-4)\n\n[150\\. Propagating input uncertainties into parameter uncertainties and model prediction uncertainties-A review](https://ifp.hal.science/hal-04501643/document)\n\n[151\\. Predict-then-Optimize v/s Probabilistic Approximations: Tackling Uncertainties and Error Propagation](https://openreview.net/pdf?id=eLiFwgzB9O)\n\n[152\\. On the expressiveness of rule-based systems for reason...](https://dl.acm.org/doi/abs/10.5555/1856670.1856692)\n\n[153\\. S. Parsons. “Refining reasoning in qualitative probabilistic networks.” Conference on Uncertainty in Artificial Intelligence](https://arxiv.org/abs/1302.4975)\n\n[154\\. A. Hunter, Matthias Thimm. “Probabilistic Argumentation with Epistemic Extensions and Incomplete Information.” ArXiv](https://arxiv.org/abs/1405.3376)\n\n[155\\. Z. Xiao, Q. C. Zhao et al. “Probabilistic Analysis for Structures with Hybrid Uncertain Parameters.” Mathematical Problems in Engineering](https://doi.org/10.1155/2020/7953628)\n\n[156\\. End-to-End Principle](https://devopedia.org/end-to-end-principle)\n\n[157\\. N. Pedroni, E. Zio et al. “Hierarchical propagation of probabilistic and non-probabilistic uncertainty in the parameters of a risk model.” Computers & Structures](https://doi.org/10.1016/J.COMPSTRUC.2013.02.003)\n\n[158\\. D. Dubois, H. Prade. “Possibility Theory - An Approach to Computerized Processing of Uncertainty.”](https://doi.org/10.1007/978-1-4684-5287-7)\n\n[159\\. Y. Koren, Robert M. Bell et al. “Matrix Factorization Techniques for Recommender Systems.” Computer](https://doi.org/10.1109/MC.2009.263)\n\n[160\\. B. Sarwar, G. Karypis et al. “Item-based collaborative filtering recommendation algorithms.” The Web Conference](https://doi.org/10.1145/371920.372071)\n\n[161\\. Jonathan L. Herlocker, J. Konstan et al. “Evaluating collaborative filtering recommender systems.” ACM Trans. Inf. Syst.](https://doi.org/10.1145/963770.963772)\n\n[162\\. AI for Personalization in E-commerce and Recommendation System](https://propulsiontechjournal.com/index.php/journal/article/download/2217/1502/3811)\n\n[163\\. Upendra Shardanand, P. Maes. “Social information filtering: algorithms for automating “word of mouth”.” International Conference on Human Factors in Computing Systems](https://doi.org/10.1145/223904.223931)\n\n[164\\. P. Resnick. “An open architecture for collaborative filtering of netnews.” Computer Supported Cooperative Work](https://www.semanticscholar.org/paper/3746ef835cd7f1b8e1ef507f182828247be683c2)\n\n[165\\. ENHANCING HUMAN-AI COLLABORATION IN AI-ASSISTED DECISION-MAKING FOR INDIVIDUALS AND GROUPS](https://hammer.purdue.edu/ndownloader/files/53790800)\n\n[166\\. AI大模型正改变着推荐系统的未来-36氪](https://36kr.com/p/2805108795192961)\n\n[167\\. Top 8 AI Use Cases in Business with Real-World Examples ...](https://kaopiz.com/en/articles/ai-use-cases-in-business/)\n\n[168\\. Addressing Misalignment in Language Model Deployments through Context-Specific Evaluations](https://dspace.mit.edu/bitstream/handle/1721.1/156962/soni_prajna_sm_tpp_2024_thesis.pdf?sequence=1&isAllowed=y)\n\n[169\\. How AI Helps to Compile Human Intelligence: An Empirical Study of Emerging Augmented Intelligence for Medical Image Scanning](https://research.cbs.dk/files/112022386/Information_Systems_Journal_-_2025_-_Pieper_-_How_AI_Helps_to_Compile_Human_Intelligence_An_Empirical_Study_of_Emerging.pdf)\n\n[170\\. AI利活用と倫理](https://www.kumamoto-hsu.ac.jp/academics/pdf/materials02.pdf)\n\n[171\\. The limits of certainty](http://www.chriscorrigan.com/parkinglot/the-limits-of-certainty/)\n\n[172\\. Pan Li, Alexander Tuzhilin. “Latent Unexpected Recommendations.” ACM Transactions on Intelligent Systems and Technology (TIST)](https://doi.org/10.1145/3404855)\n\n[173\\. Qian Zhang, Jie Lu et al. “Artificial intelligence in recommender systems.” Complex & Intelligent Systems](https://doi.org/10.1007/s40747-020-00212-w)\n\n[174\\. Introduction](https://www.uni-due.de/imperia/md/content/computerlinguistik/aiama__2ed_2003__chap1)\n\n[175\\. G. Adomavicius, Alexander Tuzhilin. “Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions.” IEEE Transactions on Knowledge and Data Engineering](https://doi.org/10.1109/TKDE.2005.99)\n\n[176\\. Artificial Intelligence Ethics and Safety: Practical tools for creating “good” models](https://arxiv.org/vc/arxiv/papers/2112/2112.11208v1.pdf)\n\n[177\\. Enhancing Security in Industrial Application Development: Case Study on Self-Generating Artificial Intelligence Tools](https://rabida.uhu.es/dspace/bitstream/handle/10272/23587/applsci-14-03780.pdf?sequence=2)\n\n[178\\. AI史上的第一个成功的商业产品是怎样诞生的？](https://www.ainavi.top/archives/7402)\n\n[179\\. Enhancing Monte Carlo Dropout Performance for Uncertainty Quantification](https://arxiv.org/pdf/2505.15671)\n\n[180\\. Nitish Srivastava, Geoffrey E. Hinton et al. “Dropout: a simple way to prevent neural networks from overfitting.” J. Mach. Learn. Res.](https://doi.org/10.5555/2627435.2670313)\n\n[181\\. Applying Monte Carlo Dropout to Quantify the Uncertain...](https://www.mdpi.com/2079-9292/12/6/1453/xml)\n\n[182\\. Enhancing Monte Carlo Dropout Performance for ...](https://chatpaper.com/chatpaper/zh-CN/paper/139190)\n\n[183\\. Y. Gal, Zoubin Ghahramani. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” International Conference on Machine Learning](https://arxiv.org/abs/1506.02142)\n\n[184\\. Monte Carlo Dropout在预训练模型中的应用](https://transferlab.ai/seminar/2022/mc-dropout/)\n\n[199\\. Epistemic neural networks | Proceedings of the 37th...](https://dl.acm.org/doi/10.5555/3666122.3666246)\n\n[200\\. ACCRUE: ACCURATE AND RELIABLE UNCERTAINTY ESTIMATE IN DETERMINISTIC MODELS - 国际不确定性的量化期刊, 卷 11, 2021, 册 4 - Begell House Digital Library](https://www.dl.begellhouse.com/cn/journals/52034eb04b657aea,3ec0b84376cff3d2,1801e97431c5911b.html)\n\n[201\\. Predictive uncertainty estimation via prior networks | Proceedings of ...](https://dl.acm.org/doi/10.5555/3327757.3327808)\n\n[202\\. On Uncertainty In Natural Language Processing](https://en.itu.dk/-/media/EN/Research/PhD-Programme/PhD-defences/2024/Dennis-Ulmer-Thesis-pdf.pdf)\n\n[203\\. Bayesian Layers: A Module for Neural Network Uncertainty](https://proceedings.neurips.cc/paper/2019/file/154ff8944e6eac05d0675c95b5b8889d-Paper.pdf)\n\n[204\\. ...uncertainty assessment using Bayesian statistics: a...](https://dl.acm.org/doi/10.1162/0899766041941925)\n\n[205\\. Uncertainty Estimation for Data-Driven Visual Odometry...](https://dl.acm.org/doi/10.1109/TRO.2020.3001674)\n\n[206\\. Synaptic plasticity as Bayesian inference](https://www.nature.com/articles/s41593-021-00809-5?error=cookies_not_supported&code=28c12e9f-cb0c-495c-a588-74d324556129)\n\n[207\\. Bayesian Prompt Ensembles: Model Uncertainty Estimation for Black-Box Large Language Models](https://assets.amazon.science/38/93/884fe5a44f5b85623cbdf881f452/bayesian-prompt-ensembles-model-uncertainty-estimation-for-black-box-large-language-models.pdf)\n\n[208\\. Uncertainty-Aware Online Extrinsic Calibration: A Conf...](http://arxiv.org/html/2501.06878v1)\n\n[209\\. Where to model the epistemic uncertainty of Bayesian convolutional neural networks for classification](https://dl.acm.org/doi/10.1016/j.neucom.2024.127568)\n\n[210\\. C. Blundell, Julien Cornebise et al. “Weight Uncertainty in Neural Networks.” ArXiv](https://arxiv.org/abs/1505.05424)\n\n[211\\. Tim Pearce, Mohamed H. Zaki et al. “Uncertainty in Neural Networks: Bayesian Ensembling.” ArXiv](https://www.semanticscholar.org/paper/6f45c4c8912551c5d76c217245296fcf9a5db291)\n\n[212\\. Uncertainty Estimation for Twitter Inference](https://par.nsf.gov/servlets/purl/10340369)\n\n[213\\. ...Discriminant Deterministic Uncertainty | Computer V...](https://dl.acm.org/doi/10.1007/978-3-031-19775-8_15)\n\n[214\\. Epistemic Uncertainty Quantification For Pre-trained Neural Networks](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Epistemic_Uncertainty_Quantification_For_Pre-Trained_Neural_Networks_CVPR_2024_paper.pdf)\n\n[215\\. Uncertainty quantification for complex systems : application to the study of cities](https://dr.ntu.edu.sg/bitstream/10356/152120/2/phd_thesis_amendment_2.pdf)\n\n[216\\. Predictive Uncertainty Estimation via Prior Networks](http://papers.neurips.cc/paper/7936-predictive-uncertainty-estimation-via-prior-networks.pdf)\n\n[217\\. Depth uncertainty in neural networks | Proceedings of the 34th International Conference on Neural Information Processing Systems](https://dl.acm.org/doi/10.5555/3495724.3496615)\n\n[218\\. An adaptive Gaussian mixture method for nonlinear uncertainty propagation in neural networks | Neurocomputing](https://dl.acm.org/doi/abs/10.1016/j.neucom.2021.06.007)\n\n[219\\. G. Adomavicius, Alexander Tuzhilin. “Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions.” IEEE Transactions on Knowledge and Data Engineering](https://doi.org/10.1109/TKDE.2005.99)\n\n[220\\. B. Sarwar, G. Karypis et al. “Item-based collaborative filtering recommendation algorithms.” The Web Conference](https://doi.org/10.1145/371920.372071)\n\n[221\\. Jonathan L. Herlocker, J. Konstan et al. “Evaluating collaborative filtering recommender systems.” ACM Trans. Inf. Syst.](https://doi.org/10.1145/963770.963772)\n\n[222\\. P. Resnick. “An open architecture for collaborative filtering of netnews.” Computer Supported Cooperative Work](https://www.semanticscholar.org/paper/3746ef835cd7f1b8e1ef507f182828247be683c2)\n\n[223\\. Cai-Nicolas Ziegler, S. McNee et al. “Improving recommendation lists through topic diversification.” The Web Conference](https://doi.org/10.1145/1060745.1060754)\n\n[224\\. THE NEW APPROACH ON COLLABORATIVE FILTERING IN ENHANCING THE EFFICIENCY OF RECOMMENDATION SYSTEMS: A CASE STUDY IN THE E-COMMERCE INDUSTRY](https://www.jsju.org/index.php/journal/article/download/2003/1992)\n\n[225\\. Aplicación del Filtrado Colaborativo en un Sistema de Recomendación](https://biblus.us.es/bibing/proyectos/abreproy/95403/fichero/TFG-5403+Peinado+Ram%C3%ADrez.pdf)\n\n[226\\. Steffen Rendle, Walid Krichene et al. “Neural Collaborative Filtering vs. Matrix Factorization Revisited.” Proceedings of the 14th ACM Conference on Recommender Systems](https://doi.org/10.1145/3383313.3412488)\n\n[227\\. Personalized Recommendation Algorithms with Collaborative Filtering](https://ir.lib.hiroshima-u.ac.jp/37616/files/14539)\n\n[228\\. Serendipitous Recommendation in E-Commerce Using ...](https://pubmed.ncbi.nlm.nih.gov/29994495/)\n\n[229\\. Collaborative Filtering is Wrong and Here is Why - Springer](https://link.springer.com/chapter/10.1007/978-3-031-71079-7_4)\n\n[230\\. The effects of recommendation systems on sales forecasting and customer engagement in the fashion world](https://tesi.luiss.it/39120/1/747771_TERRULI_VINCENZO.pdf)\n\n[231\\. Monica Chew, J. D. Tygar. “Collaborative Filtering CAPTCHAs.” The Hip](https://doi.org/10.1007/11427896_5)\n\n[232\\. Trading Personalization for Accuracy: Data Debugging in Collaborative Filtering](https://cs.nju.edu.cn/yuanyao/static/neurips2020.pdf)\n\n[233\\. Collaborative Filtering: A Machine Learning Perspective (Formulations)](https://ics.uci.edu/~djp3/classes/2007_04_02_CS221/Lecture19/CFSara.pdf)\n\n[234\\. Collaborative filtering algorithms are prone to mainstream-taste bias | Proceedings of the 17th ACM Conference on Recommender Systems](https://dl.acm.org/doi/10.1145/3604915.3608825)\n\n[239\\. Enhancing Monte Carlo Dropout Performance for Uncertainty Quantification](https://arxiv.org/pdf/2505.15671)\n\n[240\\. Nitish Srivastava, Geoffrey E. Hinton et al. “Dropout: a simple way to prevent neural networks from overfitting.” J. Mach. Learn. Res.](https://doi.org/10.5555/2627435.2670313)\n\n[241\\. Y. Gal, Zoubin Ghahramani. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” International Conference on Machine Learning](https://arxiv.org/abs/1506.02142)\n\n[242\\. Predicting Safety Misbehaviours in Autonomous Driving Systems using Uncertainty Quantification](https://arxiv.org/pdf/2404.18573)\n\n[243\\. \\[论文审查\\] Enhancing Monte Carlo Dropout Performance ...](https://www.themoonlight.io/zh/review/enhancing-monte-carlo-dropout-performance-for-uncertainty-quantification)\n\n[244\\. Balaji Lakshminarayanan, A. Pritzel et al. “Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles.” Neural Information Processing Systems](https://arxiv.org/abs/1612.01474)\n\n[245\\. Alex Kendall, Y. Gal. “What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?.” ArXiv](https://arxiv.org/abs/1703.04977)\n\n[259\\. Zheng ZHANG's Homepage - UC Santa Barbara](https://web.ece.ucsb.edu/~zhengzhang/pubs.htm)\n\n[260\\. End-to-End Principle](https://devopedia.org/end-to-end-principle)\n\n[261\\. Bayesian learning for neural networks | Guide books](https://dl.acm.org/doi/10.5555/922680)\n\n[262\\. Uncertainty-Aware CNNs for Depth Completion: Uncertainty from Beginning to End](https://openaccess.thecvf.com/content_CVPR_2020/papers/Eldesokey_Uncertainty-Aware_CNNs_for_Depth_Completion_Uncertainty_from_Beginning_to_End_CVPR_2020_paper.pdf)\n\n[263\\. Bayesian Randomly Wired Neural Network with Variational Inference for Image Recognition](https://dl.acm.org/doi/abs/10.1007/978-3-030-63836-8_17)\n\n[264\\. Learning with Uncertainty – Gaussian Processes and Relevance Vector Machines](http://quinonero.net/Publications/quinonero04thesis.pdf)\n\n[265\\. End-to-End Learning for Stochastic Optimization: A Bayesian Perspective](https://proceedings.mlr.press/v202/rychener23a/rychener23a.pdf)\n\n[266\\. Priors for Infinite Networks](https://glizen.com/radfordneal/ftp/pin.pdf)\n\n[267\\. A Bayesian end-to-end model with estimated uncertainties for simple question answering over knowledge bases](https://palm.seu.edu.cn/zhoudeyu/resources/files/publication/A%20Bayesian%20end-to-end%20model%20with%20estimated%20uncertainties%20for%20simple%20question%20answering%20over%20knowledge%20bases.pdf)\n\n[268\\. Uncertainty quantification in scientific machine learning:](https://dl.acm.org/doi/10.1016/j.jcp.2022.111902)\n\n[269\\. End-to-end principle](https://static.hlt.bme.hu/semantics/external/pages/deep_learning/en.wikipedia.org/wiki/End-to-end_principle.html)\n\n[270\\. Error-Driven Uncertainty Aware Training](https://arxiv.org/pdf/2405.01205)\n\n[271\\. Sayna Ebrahimi, Mohamed Elhoseiny et al. “Uncertainty-guided Continual Learning with Bayesian Neural Networks.” ArXiv](https://arxiv.org/abs/1906.02425)\n\n[272\\. Epistemic Uncertainty Quantification For Pre-trained Neural Networks](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Epistemic_Uncertainty_Quantification_For_Pre-Trained_Neural_Networks_CVPR_2024_paper.pdf)\n\n[273\\. Epistemic neural networks | Proceedings of the 37th...](https://dl.acm.org/doi/10.5555/3666122.3666246)\n\n[274\\. Learning Bayesian Networks](https://analyticsconsultores.com.mx/wp-content/uploads/2019/04/Learning-Bayesian-Networks-R.E.-Neapolitan-Northeastern-Illinois-University.pdf)\n\n[275\\. Bayesian Neural Networks: An Introduction and Survey | SpringerLink](https://link.springer.com/chapter/10.1007/978-3-030-42553-1_3)\n\n[276\\. A Review of Variational Inference for Bayesian Neural Network - Springer](https://link.springer.com/chapter/10.1007/978-3-031-43520-1_20)\n\n[277\\. Uncertainty aware neural network from similarity and...](https://dl.acm.org/doi/10.1016/j.asoc.2023.111027)\n\n[278\\. Bayesian Surrogate Analysis and Uncertainty Propagation](https://www.mdpi.com/2673-9984/3/1/6)\n\n[279\\. The Engagement-Diversity Connection: Evidence from a Field Experiment on Spotify](https://ide.mit.edu/wp-content/uploads/2020/03/SSRN-id3555927.pdf)\n\n[280\\. G. Adomavicius, Alexander Tuzhilin. “Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions.” IEEE Transactions on Knowledge and Data Engineering](https://doi.org/10.1109/TKDE.2005.99)\n\n[281\\. B. Sarwar, G. Karypis et al. “Item-based collaborative filtering recommendation algorithms.” The Web Conference](https://doi.org/10.1145/371920.372071)\n\n[282\\. Matching and Recommendation Engine](https://www.theseus.fi/bitstream/10024/877114/2/Kumar_Rishu.pdf)\n\n[283\\. J. Breese, D. Heckerman et al. “Empirical Analysis of Predictive Algorithms for Collaborative Filtering.” Conference on Uncertainty in Artificial Intelligence](https://arxiv.org/abs/1301.7363)\n\n[284\\. David Goldberg, D. Nichols et al. “Using collaborative filtering to weave an information tapestry.” Commun. ACM](https://doi.org/10.1145/138859.138867)\n\n[285\\. Algorithms are not neutral: Bias in collaborative filtering](https://pmc.ncbi.nlm.nih.gov/articles/PMC8802245/)\n\n[286\\. Recommendation Systems And Algorithmic Bias](https://www.meegle.com/en_us/topics/recommendation-algorithms/recommendation-systems-and-algorithmic-bias)\n\n[287\\. A survey of multimodal recommendation systems](https://openreview.net/pdf?id=XEr9OZglab)\n\n[288\\. Try this! Researchers devise better recommendation algorithm](https://techxplore.com/news/2017-12-algorithm.html)\n\n[289\\. Advances in Collaborative Filtering](https://datajobs.com/data-science-repo/Collaborative-Filtering-%5BKoren-and-Bell%5D.pdf)\n\n[290\\. TRANSFER LEARNING IN COLLABORATIVE FILTERING](https://csse.szu.edu.cn/staff/panwk/publications/Thesis-12-TLCF.pdf)\n\n[291\\. Comparison of User Based and Item Based Collaborative Filtering Recommendation Services](https://kth.diva-portal.org/smash/get/diva2:1111865/FULLTEXT01.pdf)\n\n[292\\. An Analysis of Memory Based Collaborative Filtering Recommender Systems with Improvement Proposals](https://upcommons.upc.edu/bitstream/handle/2099.1/22602/102384.pdf?sequence=1&isAllowed=y)\n\n[293\\. Joint Embedding-Classifier Learning for Interpretable Collaborative Filtering](https://hal.science/hal-04625183/file/reda2024joint.pdf)\n\n[294\\. Personalized Recommendation Algorithms with Collaborative Filtering](https://ir.lib.hiroshima-u.ac.jp/37616/files/14539)\n\n[295\\. Collaborative filtering: How to build a recommender system](https://redis.io/blog/collaborative-filtering-how-to-build-a-recommender-system/)\n\n[296\\. One-Class Collaborative Filtering](https://shiftleft.com/mirrors/www.hpl.hp.com/techreports/2008/HPL-2008-48R1.pdf)\n\n[297\\. E. Candès, T. Tao. “The Power of Convex Relaxation: Near-Optimal Matrix Completion.” IEEE Transactions on Information Theory](https://doi.org/10.1109/TIT.2010.2044061)\n\n[298\\. Collaborative Filtering Vs Content-Based Filtering for Recommender Systems](https://analyticsindiamag.com/collaborative-filtering-vs-content-based-filtering-for-recommender-systems/)\n\n[299\\. Enhancing Monte Carlo Dropout Performance for Uncertainty Quantification](https://arxiv.org/pdf/2505.15671)\n\n[300\\. Nitish Srivastava, Geoffrey E. Hinton et al. “Dropout: a simple way to prevent neural networks from overfitting.” J. Mach. Learn. Res.](https://doi.org/10.5555/2627435.2670313)\n\n[301\\. Y. Gal, Zoubin Ghahramani. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” International Conference on Machine Learning](https://arxiv.org/abs/1506.02142)\n\n[302\\. Chuan Guo, Geoff Pleiss et al. “On Calibration of Modern Neural Networks.” International Conference on Machine Learning](https://arxiv.org/abs/1706.04599)\n\n[303\\. Balaji Lakshminarayanan, A. Pritzel et al. “Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles.” Neural Information Processing Systems](https://arxiv.org/abs/1612.01474)\n\n[304\\. Alex Kendall, Y. Gal. “What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?.” ArXiv](https://arxiv.org/abs/1703.04977)\n\n[305\\. \\[论文审查\\] Enhancing Monte Carlo Dropout Performance ...](https://www.themoonlight.io/zh/review/enhancing-monte-carlo-dropout-performance-for-uncertainty-quantification)\n\n[306\\. Applying Monte Carlo Dropout to Quantify the Uncertain...](https://www.mdpi.com/2079-9292/12/6/1453/xml)\n\n[319\\. A Bayesian end-to-end model with estimated uncertainties for simple question answering over knowledge bases](https://palm.seu.edu.cn/zhoudeyu/resources/files/publication/A%20Bayesian%20end-to-end%20model%20with%20estimated%20uncertainties%20for%20simple%20question%20answering%20over%20knowledge%20bases.pdf)\n\n[320\\. End-to-End Principle](https://devopedia.org/end-to-end-principle)\n\n[321\\. Y. Gal, Zoubin Ghahramani. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” International Conference on Machine Learning](https://arxiv.org/abs/1506.02142)\n\n[322\\. Predictive uncertainty estimation via prior networks | Proceedings of ...](https://dl.acm.org/doi/10.5555/3327757.3327808)\n\n[323\\. ACCRUE: ACCURATE AND RELIABLE UNCERTAINTY ESTIMATE IN DETERMINISTIC MODELS - 国际不确定性的量化期刊, 卷 11, 2021, 册 4 - Begell House Digital Library](https://www.dl.begellhouse.com/cn/journals/52034eb04b657aea,3ec0b84376cff3d2,1801e97431c5911b.html)\n\n[324\\. Jiaru Zhang, Yang Hua et al. “Information Bound and Its Applications in Bayesian Neural Networks.” European Conference on Artificial Intelligence](https://doi.org/10.3233/FAIA230617)\n\n[325\\. Alex Kendall, Y. Gal. “What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?.” ArXiv](https://arxiv.org/abs/1703.04977)\n\n[326\\. C. Blundell, Julien Cornebise et al. “Weight Uncertainty in Neural Networks.” ArXiv](https://arxiv.org/abs/1505.05424)\n\n[327\\. Bayesian Layers: A Module for Neural Network Uncertainty](https://proceedings.neurips.cc/paper/2019/file/154ff8944e6eac05d0675c95b5b8889d-Paper.pdf)\n\n[328\\. Meet P. Vadera, Benjamin M Marlin. “Assessing the Robustness of Bayesian Dark Knowledge to Posterior Uncertainty.” ArXiv](https://arxiv.org/abs/1906.01724)\n\n[329\\. Jie Jia, Honggang Zhou et al. “Using Deep Neural Network Approximate Bayesian Network.” ArXiv](https://arxiv.org/abs/1801.00282)\n\n[330\\. Alex Graves. “Practical Variational Inference for Neural Networks.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/5a9ef216bf11f222438fff130c778267d39a9564)\n\n[331\\. Journal of WSCG](http://wscg.zcu.cz/WSCG2024/2024-JWSCG.pdf)\n\n[332\\. Radford M. Neal. “Bayesian learning for neural networks.”](https://doi.org/10.1007/978-1-4612-0745-0)\n\n[333\\. Lingwei Wei, Dou Hu et al. “Towards Propagation Uncertainty: Edge-enhanced Bayesian Graph Convolutional Networks for Rumor Detection.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/2021.acl-long.297)\n\n[334\\. I. Khan, M. Raja et al. “Design of Neural Network With Levenberg-Marquardt and Bayesian Regularization Backpropagation for Solving Pantograph Delay Differential Equations.” IEEE Access](https://doi.org/10.1109/access.2020.3011820)\n\n[335\\. Andrew Y. K. Foong, Yingzhen Li et al. “'In-Between' Uncertainty in Bayesian Neural Networks.” ArXiv](https://arxiv.org/abs/1906.11537)\n\n[336\\. A Bayesian Method for the Induction of Probabilistic Networks from Data](https://link.springer.com/content/pdf/10.1007/BF00994110.pdf)\n\n[339\\. G. Adomavicius, Alexander Tuzhilin. “Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions.” IEEE Transactions on Knowledge and Data Engineering](https://doi.org/10.1109/TKDE.2005.99)\n\n[340\\. B. Sarwar, G. Karypis et al. “Item-based collaborative filtering recommendation algorithms.” The Web Conference](https://doi.org/10.1145/371920.372071)\n\n[341\\. J. Breese, D. Heckerman et al. “Empirical Analysis of Predictive Algorithms for Collaborative Filtering.” Conference on Uncertainty in Artificial Intelligence](https://arxiv.org/abs/1301.7363)\n\n[342\\. Matching and Recommendation Engine](https://www.theseus.fi/bitstream/10024/877114/2/Kumar_Rishu.pdf)\n\n[343\\. David Goldberg, D. Nichols et al. “Using collaborative filtering to weave an information tapestry.” Commun. ACM](https://doi.org/10.1145/138859.138867)\n\n[344\\. Try this! Researchers devise better recommendation algorithm](https://techxplore.com/news/2017-12-algorithm.html)\n\n[345\\. Netflix's Recommendation Systems: Entertainment Made ...](https://illumin.usc.edu/netflixs-recommendation-systems-entertainment-made-for-you/)\n\n[346\\. Advances in Collaborative Filtering](https://datajobs.com/data-science-repo/Collaborative-Filtering-%5BKoren-and-Bell%5D.pdf)\n\n[347\\. Recommender Systems in Practice](https://zhuanlan.zhihu.com/p/69953280)\n\n[348\\. Recommendation Systems And Algorithmic Bias](https://www.meegle.com/en_us/topics/recommendation-algorithms/recommendation-systems-and-algorithmic-bias)\n\n[349\\. alm411/Netflix-Recommendation-System-Development - GitHub](https://github.com/alm411/Netflix-Recommendation-System-Development)\n\n[350\\. Choice-Based Preference Elicitation for Collaborative Filtering Recommender Systems](https://interactivesystems.info/system/pdfs/297/original/chi-14-choice-based-preference-elicitation-for-collaborative-filtering-recommender-systems.pdf?1560346076)\n\n[351\\. Large-Scale Parallel Collaborative Filtering for the Netflix ...](https://link.springer.com/chapter/10.1007/978-3-540-68880-8_32)\n\n[352\\. An Analysis of Memory Based Collaborative Filtering Recommender Systems with Improvement Proposals](https://upcommons.upc.edu/bitstream/handle/2099.1/22602/102384.pdf?sequence=1&isAllowed=y)\n\n[353\\. T. Jambor, Jun Wang. “Goal-Driven Collaborative Filtering - A Directional Error Based Approach.” European Conference on Information Retrieval](https://doi.org/10.1007/978-3-642-12275-0_36)\n\n[354\\. G. Linden, Brent Smith et al. “Industry Report: Amazon.com Recommendations: Item-to-Item Collaborative Filtering.” IEEE Distributed Syst. Online](https://www.semanticscholar.org/paper/fdb98ca19262fa192db2494f4f64369351cb7d7c)\n\n[355\\. Collaborative models for Collaborative Filtering](https://n.ethz.ch/~piattigi/pdfs/CIL_paper.pdf)\n\n[356\\. Joosung Kim. “Collaborative Filtering in Recommendation Systems: Idea and Evaluation.”](https://www.semanticscholar.org/paper/c175dcf452fcc5d23acbbfd1c63939cd9866b462)\n\n[357\\. Netflix Challenge: Combined Collaborative Filtering](http://infolab.stanford.edu/~ullman/mining/2006/student_project_slides/nelson-sheinberg-Presentation.ppt)\n\n[358\\. Algorithms are not neutral: Bias in collaborative filtering](https://pmc.ncbi.nlm.nih.gov/articles/PMC8802245/)"
  },
  {
    "id": 30,
    "question": "Can Enterprises build better domain-specific models with their data, or will large general models always outperform them?",
    "response": "**Research Report**\n\n**To:** Interested Parties\n\n**From:** Expert Researcher\n\n**Date:** July 26, 2025\n\n**Subject:** Enterprise AI Strategy: A Comparative Analysis of Domain-Specific Models vs. General Foundation Models\n\n**1.0 Executive Summary**\n\nThe proliferation of large, general-purpose foundation models, such as those from OpenAI, has presented enterprises with a critical strategic choice: should they leverage these powerful, off-the-shelf APIs for their artificial intelligence needs, or should they invest in building custom, domain-specific models using their proprietary data? This report investigates this dilemma by synthesizing evidence from 2023 through mid-2025.\n\nOur research indicates that while large general models provide remarkable out-of-the-box capabilities and an accessible entry point for AI adoption, the strategic advantage is increasingly shifting towards custom, domain-specific models for core enterprise functions. For specialized, high-value, or high-volume tasks, enterprises can build models that are not only more accurate and reliable but also more cost-effective over the long term. This trend is powerfully accelerated by the maturation of efficient fine-tuning techniques that have democratized custom model development. The future of enterprise AI is not a binary choice but a hybrid ecosystem where general models serve as platforms and for broad applications, while a portfolio of specialized models drives competitive differentiation and operational excellence. As predicted by Gartner, the adoption of industry-specific GenAI models is on a sharp upward trajectory, projected to leap from 1% in 2023 to 50% by 2027 \\[2\\].\n\n**2.0 Performance and Accuracy: The Unassailable Value of Specialization**\n\nThe primary driver for an enterprise to build a custom model is the pursuit of superior performance. While general models are trained on vast swathes of public internet data, this breadth often comes at the cost of depth, leading to performance degradation when applied to specialized domains with unique jargon, processes, and data structures (finance/legal/healthcare search results).\n\n**2.1 Beyond General Benchmarks**\n\nThe evaluation of AI models has matured significantly. Early assessments relied on general benchmarks like HELM (Holistic Evaluation of Language Models) from Stanford, which provides a crucial, standardized view of a model's capabilities across broad metrics like accuracy, robustness, and fairness \\[163\\]\\[169\\]\\[224\\]. While useful for comparing foundational models, strong performance on these general tests does not guarantee success in specialized enterprise contexts \\[121\\].\n\nRecognizing this gap, the industry has rapidly developed domain-specific benchmarks. As of 2025, we see robust evaluation frameworks tailored to individual sectors:\n\n**Finance:** Benchmarks like FinMTEB, FinBen, FinEval, and FinanceBench are now used to assess reasoning and generation capabilities on financial tasks \\[111\\]\\[170\\]\\[170\\].\n\n**Healthcare:** The Open Medical LLM Leaderboard, MedBench, and DrBenchmark evaluate models on medical datasets, using accuracy and F1 scores on tasks like analyzing clinical notes and medical Q&A \\[111\\]\\[170\\]\\[126\\].\n\n**Legal:** Frameworks such as LegalEval-Q and LawBench assess the quality and relevance of LLM-generated legal text \\[125\\]\\[170\\].\n\nThis shift to specialized evaluation highlights a core finding: performance is context-dependent, and domain-specific models are consistently demonstrating superior results on these targeted benchmarks.\n\n**2.2 Accuracy, Precision, and Task-Specific Excellence**\n\nDomain-specific models, trained or fine-tuned on curated, high-quality enterprise data, are inherently optimized for precision and domain-relevant reasoning \\[3\\]\\[5\\]. This specialization yields tangible performance gains:\n\n**Code Generation:** GitHub Copilot, which evolved from the Codex model fine-tuned on code, enables developers to complete tasks up to 55% faster \\[3\\]. This demonstrates how a model specialized for a specific domain (software development) can dramatically improve productivity. Further analysis shows that while a general model like Gemini might excel at high-level system design, a more specialized model like Anthropic's Claude performs better at fine-grained code completion \\[16\\].\n\n**Healthcare:** SenseTime's \"Da Yi\" model, designed for the medical field, achieved 96% accuracy in generating medical records, while Google's Med-PaLM 2 scored 86.5% on the MedQA dataset (healthcare search results). These results significantly outperform what can be expected from a general model applied to the same complex medical tasks.\n\n**Financial Services:** Bloomberg's purpose-built model has been noted for its excellence in financial tasks, showcasing the advantages of training on domain-specific data from the ground up (finance search results).\n\nSmaller models, when fine-tuned on high-quality, in-domain data, can often outperform much larger, general-purpose models on specific tasks, delivering equivalent or superior performance with greater efficiency \\[11\\]\\[5\\]\\[255\\].\n\n**2.3 Mitigating Hallucinations: The Enterprise Imperative**\n\nModel \"hallucination\"—the generation of factually incorrect or nonsensical information—is a critical risk for any enterprise application. A financial model misreporting figures or a medical chatbot giving harmful advice can have severe liability and compliance consequences \\[105\\].\n\nThe comparison of hallucination rates between general and domain-specific models is nuanced. While some evidence suggests that top-tier closed-source general models have historically maintained an edge in accuracy and lower hallucination rates \\[15\\]\\[109\\]this advantage is eroding as enterprises adopt more sophisticated custom model strategies.\n\n**The Power of Grounding:** A domain-specific, corpus-grounded question-answering model demonstrated a hallucination rate of just 1-2%, compared to over 10% for a similarly sized general open-source model on the same benchmark \\[92\\].\n\n**Fine-Tuning for Factualness:** Fine-tuning on curated, domain-specific datasets is a powerful technique for minimizing factual errors. One study showed fine-tuning reduced a model's hallucination rate by a remarkable 89% (from 0.273% to 0.03%) without degrading quality \\[99\\]. Specialized benchmarks like HaluBench, which uses real-world examples from finance and medicine, are instrumental in measuring this improvement \\[142\\]\\[153\\]. On this benchmark, an open-source model (LYNX) significantly outperformed GPT-3.5-Turbo in identifying hallucinations \\[95\\].\n\n**The Role of RAG:** Retrieval-Augmented Generation (RAG) systems, which ground the model's responses in a specific corpus of enterprise documents, dramatically reduce hallucinations to as low as 3.4% \\[94\\].\n\nHowever, a crucial caveat exists: the quality of the fine-tuning data is paramount. Using data generated by other large models to fine-tune smaller ones can paradoxically _increase_ their tendency to hallucinate, by an average of 125% in one study \\[97\\]\\[98\\]. This underscores the value of an enterprise's own high-quality, human-verified data.\n\n**3.0 Economic Viability: The Total Cost of Ownership (TCO) Analysis**\n\nThe decision to build or buy is fundamentally an economic one. While detailed, verifiable 5-year TCO comparisons from Fortune 500 companies remain elusive in public documentation \\[182\\]\\[191\\]\\[287\\]available case studies and cost analyses provide a clear framework for this decision.\n\n**3.1 The \"Buy\" Option: API Subscription Costs**\n\nUsing a general model via API is an operational expense (OpEx) with predictable, recurring costs based on usage \\[82\\]. For example, GPT-4 API calls cost approximately 0.06 per 1,000 output tokens \\[28\\]. While this model offers flexibility and avoids large upfront capital expenditure, costs can escalate rapidly with high-volume usage and may ultimately exceed the cost of a custom solution over the long term \\[82\\].\n\n**3.2 The \"Build\" Option: Custom Model Development and Maintenance**\n\nBuilding a custom model involves a significant upfront investment in development or fine-tuning (a capital expense, or CapEx), followed by ongoing operational costs for maintenance, hosting, and updates.\n\n**Initial Investment:** The cost of full fine-tuning can be substantial. One analysis showed fine-tuning a model on biomedical data could cost 1,500,000 using a proprietary OpenAI model, but this dropped to just 43,550 when using an equivalent open-source model \\[36\\]. This demonstrates that the \"build\" path offers significant cost control depending on the chosen approach.\n\n**Long-Term Maintenance:** Ongoing maintenance for custom software is a critical factor, often estimated at 10-20% of the initial development cost per year \\[85\\]. The enterprise assumes full responsibility for updates, security patches, and knowledge retention if the original developers depart \\[85\\].\n\n**3.3 TCO Break-Even Analysis: Real-World Case Studies**\n\nThough formal 5-year studies are lacking, real-world cost comparisons illuminate the economic trade-offs:\n\n**Case Study: Financial Services Document Processing:** A financial firm processing 50,000 documents per month faced an annual cost of ~1.2 million for the first year (including development) but dropped to only ~$300,000 in subsequent years for maintenance and hosting. This resulted in a **break-even point at 27 months**, after which the custom model became significantly cheaper \\[182\\]\\[287\\].\n\n**Case Study: Customer Support Automation:** An e-commerce company handling 100,000 inquiries per month found a GPT-3.5 Turbo solution cost ~180,000 annually. A custom model was more expensive both initially (~900,000) and annually (~$220,000) and did not reach a break-even point. The study attributed this to the rapid evolution of general models for this specific, less-specialized use case \\[182\\].\n\nThe conclusion is clear: for high-volume, stable, and specialized applications, the TCO of a custom model is highly favorable over a 3-5 year horizon \\[234\\]. General model APIs are more economical for lower-volume, general-purpose, or highly dynamic use cases where the underlying technology is changing too quickly to justify a large custom build.\n\n**4.0 Feasibility and Accessibility: Democratizing Custom Model Development**\n\nHistorically, building proprietary models was the exclusive domain of tech giants with vast computational resources and data. However, the emergence of Parameter-Efficient Fine-Tuning (PEFT) methods has been a paradigm shift, making the \"build\" option feasible for a much broader range of enterprises.\n\nTechniques like **LoRA (Low-Rank Adaptation)** and **QLoRA (Quantized Low-Rank Adaptation)** are game-changers. Instead of retraining all billions of parameters in a foundation model, these methods freeze the original model's weights and train only a tiny fraction of new, \"adapter\" parameters \\[45\\]\\[47\\]\\[53\\]. QLoRA further enhances efficiency by using 4-bit quantization to drastically reduce the model's memory footprint \\[46\\]\\[49\\].\n\nThe benefits of these techniques are profound:\n\n**Massively Reduced Compute Costs:** They dramatically lower GPU memory requirements, making it possible to fine-tune large models on a single enterprise-grade GPU, or even consumer hardware in some cases \\[45\\]\\[52\\]\\[59\\].\n\n**Data Efficiency:** They can achieve strong performance with smaller, more focused datasets of a few hundred or thousand examples, rather than the millions previously required \\[45\\]\\[54\\].\n\n**High Performance:** Despite their efficiency, LoRA and QLoRA can achieve performance on par with or even exceeding traditional full fine-tuning methods \\[45\\]\\[59\\]\\[60\\].\n\n**Accelerated Development:** The lower resource requirements lead to faster training and iteration cycles, allowing teams to experiment and deploy custom solutions more quickly \\[45\\]\\[57\\].\n\nPEFT techniques have effectively lowered the barrier to entry, empowering enterprises to leverage their unique data to create high-performing, domain-specific models without prohibitive investment in data or infrastructure \\[45\\]\\[51\\]\\[57\\].\n\n**5.0 Scalability and Deployment in Complex Organizations**\n\nDeploying an AI model for a single use case is challenging; scaling it across multiple business units introduces far greater complexity. This involves managing distributed performance requirements, security models, and regional compliance needs \\[193\\]\\[204\\].\n\nWhen scaling from a single-domain implementation to a deployment across 10+ business units, latency and throughput become critical metrics.\n\n**Latency and Throughput Dynamics:** Domain-specific models, being smaller and more efficient, often exhibit lower latency for their designated tasks \\[255\\]\\[261\\]. However, scaling brings new challenges. Deploying multiple model instances on shared hardware can increase competition for resources, potentially decreasing the throughput for each individual model \\[297\\]. A system that performs well for a handful of users may see latency spike once it exceeds a certain threshold (e.g., 10-20 active users), necessitating a more complex distributed architecture \\[300\\].\n\n**Architectural Strategy:** A successful multi-unit deployment requires a cohesive strategy. While a single-domain model might seem simpler, it can lack the flexibility needed for replication to geographically dispersed offices with varying connectivity \\[200\\]. A recommended approach is to begin with a specific, high-value use case and deploy a custom model, evaluate its success, and then strategically scale the architecture and deployment processes to other business units \\[210\\].\n\nUltimately, the inherent efficiency of specialized models can provide a performance advantage at scale, but this benefit can only be realized with a thoughtful deployment architecture that addresses the distributed nature of a large enterprise.\n\n**6.0 Conclusion and Strategic Recommendation**\n\nThe question is not whether enterprises _can_ build better domain-specific models, but for which use cases they _should_. As of mid-2025, the evidence compellingly argues against a one-size-fits-all reliance on general foundation models. For applications that are core to an enterprise's value proposition, involve specialized knowledge, handle sensitive data, or operate at high volume, the strategic imperative is to build.\n\nOur research validates this conclusion on three key fronts:\n\n1.  **Superior Performance:** For specialized tasks in fields like finance, healthcare, and legal, custom-tuned models consistently deliver higher accuracy, greater task-specific utility, and critically, lower rates of dangerous hallucination when grounded in high-quality proprietary data \\[5\\]\\[92\\]\\[99\\].\n2.  **Favorable Long-Term Economics:** While requiring an initial investment, the total cost of ownership for custom models in high-volume, specialized scenarios becomes more favorable than API subscriptions, often reaching a break-even point within a 2-3 year timeframe \\[182\\]\\[287\\].\n3.  **Achievable Feasibility:** The widespread adoption of PEFT techniques like LoRA and QLoRA has democratized custom AI, drastically reducing the computational and data requirements that were once prohibitive barriers \\[45\\]\\[57\\].\n\nTherefore, the most effective enterprise AI strategy in 2025 is a hybrid one. Enterprises should continue to leverage general model APIs for rapid prototyping, general-purpose tasks, and areas of low specialization. Simultaneously, they must strategically identify their core, data-rich domains and invest in building a portfolio of lightweight, efficient, and highly accurate domain-specific models. This dual approach allows organizations to maximize both operational efficiency and long-term competitive advantage in the age of AI.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[2\\. Prospecting for Performance: Data Center Networking in 2025](https://arrcus-admin.prod.unomena.io/media/documents/AvidThnik_NGI_2025_DataCenter_Networking_AI_Cloud.pdf)\n\n[3\\. AI Models Comparison: Architecture, Features & Use Cases](https://www.edstellar.com/blog/ai-models-comparison)\n\n[4\\. Yaroslav Ganin, E. Ustinova et al. “Domain-Adversarial Training of Neural Networks.” Journal of machine learning research](https://doi.org/10.1007/978-3-319-58347-1_10)\n\n[5\\. Enterprise AI: Generative AI & Domain Specific Models for Enterprise](https://cdrdv2-public.intel.com/817880/EnterpriseAI%20Partner%20Enablement%20Package.pdf)\n\n[6\\. Martín Arjovsky, L. Bottou et al. “Invariant Risk Minimization.” ArXiv](https://arxiv.org/abs/1907.02893)\n\n[7\\. Rishi Bommasani, Kevin Klyman et al. “The Foundation Model Transparency Index.” ArXiv](https://doi.org/10.48550/arXiv.2310.12941)\n\n[8\\. The 2025 European Deep Tech Report](https://dealroom.co/uploaded/2024/11/2025-European-Deep-Tech-Report.pdf)\n\n[9\\. S. Kelly, Juha-Pekka Tolvanen. “Domain-Specific Modeling: Enabling Full Code Generation.”](https://doi.org/10.1108/k.2008.06737fae.001)\n\n[10\\. OECD Digital Economy Outlook 2024 (Volume 1): Embracing the Technology Frontier](https://www.oecd.org/content/dam/oecd/en/publications/reports/2024/05/oecd-digital-economy-outlook-2024-volume-1_d30a04c9/a1689dc5-en.pdf)\n\n[11\\. 2023 in (expert.ai) Review](https://www.expert.ai/blog/2023-in-expert-ai-review/)\n\n[12\\. 2022 Annual Report](http://notice.10jqka.com.cn/api/pdf/7c5f47e3407aef8d.pdf)\n\n[13\\. Enterprise Data and AI Trends 2025: Intelligentsia, Platforms ...](https://www.aisharenet.com/en/2025-nianqiyeshuju/)\n\n[14\\. The 2023 Foundation Model Transparency Index](https://openreview.net/pdf/032d164d6447b2f8fcfc5133d5471ea6baf356fe.pdf)\n\n[15\\. 2025 Tech Trends](https://trendsunplugged.io/wp-content/uploads/2024/11/2025-TechTrends.pdf)\n\n[16\\. 16 Changes to AI in the Enterprise: 2025 Edition](https://a16z.com/ai-enterprise-2025/)\n\n[17\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2024.” ArXiv](https://doi.org/10.48550/arXiv.2405.19522)\n\n[18\\. Domain-Specific AI Models: The Future of Industry AI](https://www.iamdave.ai/blog/domain-specific-ai-models-explained-the-future-of-business-ai/)\n\n[19\\. Hamza Riaz, A. Smeaton. “Vision Based Machine Learning Algorithms for Out-of-Distribution Generalisation.” ArXiv](https://doi.org/10.1007/978-3-031-37963-5_60)\n\n[21\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[22\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[23\\. Applying Large Language Models to Issue Classification: Revisiting with Extended Data and New Models](https://www.arxiv.org/pdf/2506.00128)\n\n[24\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[25\\. OpenAI Josh Achiam, Steven Adler et al. “GPT-4 Technical Report.”](https://arxiv.org/abs/2303.08774)\n\n[26\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[27\\. The Machine Learning Solutions Architect Handbook](https://sciendo.com/2/v2/download/chapter/9781805124825/10.0000/9781805124825-001.pdf?Token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VycyI6W3sic3ViIjoyNTY3ODUxNywicHVicmVmIjoiNzY0NDg4IiwibmFtZSI6Ikdvb2dsZSBHb29nbGVib3QgLSBXZWIgQ3Jhd2xlciBTRU8iLCJ0eXBlIjoiaW5zdGl0dXRpb24iLCJsb2dvdXRfbGluayI6Imh0dHBzOi8vY29ubmVjdC5saWJseW54LmNvbS9sb2dvdXQvNjgxNDQzNDBlOGI4ZmY4OTY2ZmZjNWUyNTk3NWU4NTMiLCJhdXRoX21ldGhvZCI6ImlwIiwiaXAiOiI2Ni4yNDkuNzkuMiIsImNvdW50ZXJwYXJ0eV9pZCI6Ijc2NDQ4OCJ9XSwiaWF0IjoxNzQ2MTYxMjIxLCJleHAiOjE3NDczNzA4MjF9.3EgYWpVJfB7rVtMuBptYg975z4pyU_8mBg8xfdy20E0)\n\n[28\\. AIGC场景应用展望研究报告](https://runwise.oss-accelerate.aliyuncs.com/sites/15/2024/01/2024-01-15-2023%E5%B9%B4AIGC%E5%9C%BA%E6%99%AF%E5%BA%94%E7%94%A8%E5%B1%95%E6%9C%9B%E7%A0%94%E7%A9%B6%E6%8A%A5%E5%91%8A-%E8%89%BE%E7%91%9E%E5%92%A8%E8%AF%A2.pdf)\n\n[29\\. Domain specific models outperform large vision language models on cytomorphology tasks](https://www.medrxiv.org/content/medrxiv/early/2025/05/06/2025.05.05.25326989.full.pdf)\n\n[30\\. Dong Chen, Shuo Zhang et al. “Improving Large Models with Small models: Lower Costs and Better Performance.” ArXiv](https://doi.org/10.48550/arXiv.2406.15471)\n\n[31\\. AI Exploration and Innovation for the Clinical Data Scientist](https://phuse.s3.eu-central-1.amazonaws.com/Archive/2024/Connect/US/Bethesda/PAP_ET08.pdf)\n\n[32\\. FINE-TUNING VS CONTEXT-INJECTION: USING GPT FOR AMBIGUOUS QUESTION-ANSWERING ON PROPRIETARY DATA](https://arti.franklin.uga.edu/sites/default/files/inline-files/theses/vanhorn_rex_202308_ms.pdf)\n\n[33\\. Hazards from Increasingly Accessible Fine-Tuning of Downloadable Foundation Models](https://openreview.net/pdf?id=QbXf5BqyXp)\n\n[34\\. Fine-tuning Best Practices Chapter 2: Models - OpenPipe](https://openpipe.ai/blog/fine-tuning-best-practices-chapter-2-models)\n\n[35\\. AI Large Models Bring Great Opportunities to Reusable Design of CAD Software](http://elib.mi.sanu.ac.rs/files/journals/csis/66/csisn68p1523-1546.pdf)\n\n[36\\. GENERATIVE AI / THE RACE IS ON FOUR ADOPTION PATTERNS FOR YOUR ENTERPRISE](https://info.softserveinc.com/hubfs/files/generative-ai/softserve-four-adoption-patterns-for-your-enterprise.pdf)\n\n[37\\. The Big Book of GenAI](https://www.hkdca.com/wp-content/uploads/2024/08/big-book-generative-ai-databricks.pdf)\n\n[38\\. Wenqi Shi, Ran Xu et al. “MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning.” ArXiv](https://doi.org/10.48550/arXiv.2405.03000)\n\n[39\\. Closed API vs. Open-Source API: Gen AI Cost Strategies](https://addepto.com/blog/closed-api-vs-open-source-api-gen-ai-cost-strategies/)\n\n[40\\. Namgyu Ho, Laura Schmid et al. “Large Language Models Are Reasoning Teachers.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.48550/arXiv.2212.10071)\n\n[41\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[42\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[43\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[44\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[45\\. Scaling Large Language Models: Effective Strategies for ...](https://antematter.io/blogs/llm-scalability)\n\n[46\\. QLoRA: Efficient Finetuning of Quantized LLMs](https://zhuanlan.zhihu.com/p/649327877)\n\n[47\\. Fine-Tuning and Deploying Phi-3.5 Model with Azure and AI...](https://techcommunity.microsoft.com/t5/educator-developer-blog/fine-tuning-and-deploying-phi-3-5-model-with-azure-and-ai/ba-p/4364312)\n\n[48\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[49\\. Cost-Effective LLMs: Leveraging Generative AI with Limited Hardware](https://deepsense.ai/how-to-reduce-the-cost-of-llms)\n\n[50\\. Accessible Foundation Models: Systems, Algorithms, and Science](https://digital.lib.washington.edu/server/api/core/bitstreams/63e54e0c-bcaa-42e8-b164-5a8e4db7a715/content)\n\n[51\\. Medication information extraction using local large language models](https://www.medrxiv.org/content/10.1101/2025.03.28.25324847v1.full.pdf)\n\n[52\\. Large Language Models in Finance](https://probability.nl/wp-content/uploads/2024/01/LLMs-in-Finance-Probability-Partners.pdf)\n\n[53\\. Low-Rank Adaptation (LoRa): Revolutionizing Model Optimization in Deep Learning](https://iaeme.com/MasterAdmin/Journal_uploads/IJCET/VOLUME_15_ISSUE_4/IJCET_15_04_047.pdf)\n\n[54\\. Technologies Driving Enhanced On-device Generative AI Experiences: LoRA](https://www.edge-ai-vision.com/2024/05/technologies-driving-enhanced-on-device-generative-ai-experiences-lora/)\n\n[55\\. Finetuning of Open Source LLMs for Specific Domains](https://pyxida.aueb.gr/bitstreams/6caaf2dc-7a4c-4dd5-8292-76a4d20eee14/download)\n\n[56\\. CheonSu Jeong. “Fine-tuning and Utilization Methods of Domain-specific LLMs.” ArXiv](https://doi.org/10.13088/jiis.2024.30.1.093)\n\n[57\\. Low-rank Adaption: Efficient Strategies For Language Models](https://clavrit.com/blogs/lora-low-rank-adaption/)\n\n[58\\. QLORA and Fine-Tuning of Quantized Language Models (LMs)](https://training.continuumlabs.ai/training/the-fine-tuning-process/parameter-efficient-fine-tuning/qlora-and-fine-tuning-of-quantized-language-models-lms)\n\n[59\\. Generative AI in the Enterprise with AMD Accelerators](https://media.bitpipe.com/io_31x/io_315126/item_2828616/Generative%20AI%20in%20the%20Enterprise%20With%20AMD%20Accelerators.pdf)\n\n[60\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[61\\. 不断发展的科学考试](https://www.xueshuxiangzi.com/downloads/2025_7_23/2507.16514.pdf)\n\n[62\\. THE CHALLENGER REPORT March 2025](https://fedprimerate.com/docs/challenger-job-cuts/----%5b--%5d--Fed-Prime-Rate---CHALLENGER----JOB--CUT--REPORT----MARCH----2025.pdf)\n\n[63\\. Best Tools for Domain-Specific LLM Benchmarking - Ghost](https://latitude-blog.ghost.io/blog/best-tools-for-domain-specific-llm-benchmarking/)\n\n[64\\. 北京首都在线科技股份有限公司 2024 年年度报告](http://notice.10jqka.com.cn/api/pdf/d1690c1b660b0a2b.pdf)\n\n[65\\. TRENDS IN GLOBAL M&A 2024 - 2025](https://albiacapital.com/wp-content/uploads/2025/02/Middle-Market-MA-Global-Trends-2024-25-Report-IMAP-and-Capstone-Partners-1.pdf)\n\n[66\\. 2025易凯资本中国健康产业白皮书医疗与健康服务篇](http://oss.ceccapitalgroup.com/portal/index/2025%E4%B8%AD%E5%9B%BD%E5%81%A5%E5%BA%B7%E4%BA%A7%E4%B8%9A%E7%99%BD%E7%9A%AE%E4%B9%A6_%E5%8C%BB%E7%96%97%E4%B8%8E%E5%81%A5%E5%BA%B7%E6%9C%8D%E5%8A%A1%E7%AF%87.pdf)\n\n[67\\. Macroeconomic and Equity Capital Markets Update](https://docs.londonstockexchange.com/sites/default/files/documents/macro-and-ecm-updated-november-final-version-2025.pdf)\n\n[68\\. Quarterly equity market insights from Capital Group](https://www.capitalgroup.com/content/dam/cgc/tenants/eacg/documents/insights/en/2025/q1/ii-perspectives-on-equity-markets-for-q1-2025-and-beyond.pdf)\n\n[69\\. Know The Markets](https://www.kasikornasset.com/SiteCollectionDocuments/kasset-jpmam/pdf/know_the_markets/KnowTheMarkets-Q12025.pdf)\n\n[70\\. On the Effectiveness of Large Language Models in Domain-Specific Code Generation](https://arxiv.org/pdf/2312.01639)\n\n[71\\. 2024年全球AIGC产业全景报告](https://hulianhutongshequ.cn/upload/tank/report/2024/202411/1/88649e79666e404fb928e4222acac325.pdf)\n\n[72\\. Checklist for Domain-Specific LLM Fine-Tuning - Ghost](https://latitude-blog.ghost.io/blog/checklist-for-domain-specific-llm-fine-tuning/)\n\n[73\\. Strategy | Market Outlook](https://www.hlib.com.my/Published/Download.ashx?ArticleId=25417&mode=view)\n\n[74\\. DAILY MARKET RECAP](https://www.vndirect.com.vn/cmsupload/beta/Vietnam-Daily-Market-Recap-February-19-2025-ETF-Report-Vinfast-Analyst-note.pdf)\n\n[75\\. 全球金融科技每周动态 Weekly Update of Global Fintech](https://www.afca-asia.org/cM07_UploadFile.do?method=downloadFile&storeFileName=1737689884453.pdf)\n\n[76\\. AI First并行伙伴说：特邀对话澜舟科技合伙人马永亮③](https://www.bilibili.com/video/av113366685191985)\n\n[77\\. 2025 Earnings preview Delivery matters](https://am.jpmorgan.com/content/dam/jpm-am-aem/emea/regional/en/investment-themes/2025-earnings-delivery-matters.pdf)\n\n[78\\. Domain-specific large vision models (LVMs) simplified](https://datasciencedojo.com/blog/large-vision-models-lvms-simplified/)\n\n[81\\. Data Management Buyers Guide: How to Compare the TCO of Cloud-Native, On-premise, and Hosted Data Platforms](https://www.reltio.com/wp-content/uploads/2020/02/Data-Management-Buyers-Guide-Compare-TCO.pdf)\n\n[82\\. Subscription Licensing Model](https://www.revenera.de/software-monetization/glossary/subscription-licensing-model)\n\n[83\\. Perpetual Versus Subscription Licensing Under Quality Uncertainty and Network Externality Effects](http://neconomides.stern.nyu.edu/networks/Zhang_JMIS2010.pdf)\n\n[84\\. ServiceNow Pricing 2025: The Complete Breakdown](https://www.desk365.io/blog/servicenow-pricing/)\n\n[85\\. Greenback Expat Tax Services Case Study: How Greenback Tax Services used Declarative Webhooks to never have an incorrect invoice again](https://appexchange.salesforce.com/partners/servlet/servlet.FileDownload?file=00P4V000011m1qKUAQ)\n\n[86\\. Perpetual vs Subscription - RISA Technologies: RISA-3D](https://www.eng-tips.com/viewthread.cfm?qid=435447)\n\n[87\\. GSMA Open Gateway: State of the Market, H2 2024](https://eu-assets.contentstack.com/v3/assets/blt23eb5bbc4124baa6/blt85b8246f902c5786/6761e86ae2ba1d6d60f7629b/111224-Open-Gateway-State-of-the-Market-H2-2024.pdf)\n\n[88\\. Financial growth plan](https://www.temenos.com/wp-content/uploads/2022/02/cmd-2022-presentation-financial-growth-plan-wojwg8imr.pdf)\n\n[89\\. Java SE 许可和成本](https://redresscompliance.com/java-se-licensing-and-costs/)\n\n[90\\. 软件许可模型详解](https://www.waybinary.com/what-are-software-licensing-models/)\n\n[91\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[92\\. Merlyn Mind的教育专用语言模型](https://www.merlynmind.ai/blog/merlyn-minds-education-specific-language-models)\n\n[93\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[94\\. Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA](https://arxiv.org/pdf/2502.10497)\n\n[95\\. Lynx: An Open Source Hallucination Evaluation Model](https://uploads-ssl.webflow.com/64e655d42d3be60f582d0472/668fa6d33f471278ecac72ec_Lynx_Hallucination_Model.pdf)\n\n[96\\. A dataset for evaluating clinical research claims in large language models](https://www.medrxiv.org/content/10.1101/2024.10.08.24315103v1.full.pdf)\n\n[97\\. Exploring the Knowledge Mismatch Hypothesis: Hallucination Propensity in Small Models Fine-tuned on Data from Larger Models](https://linnk.ai/insight/natural-language-processing/fine-tuning-small-language-models-on-data-from-larger-models-increases-hallucination-U4GQ2OVz/)\n\n[98\\. Hallucination Propensity in Small Models Fine-tuned on ...](https://arxiv.org/html/2411.00878v1)\n\n[99\\. Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused Preference Optimization](https://openreview.net/pdf/e5d6c798ff69947edd3d75c3662f802004b63deb.pdf)\n\n[100\\. Hallucinations: What Are They, Why Do They Happen, How to Fix Them?](https://lawdroid.com/wp-content/uploads/2024/08/Hallucinations_-What-Are-They-Why-Do-They-Happen-How-to-Fix-Them.pdf)\n\n[101\\. CTGT AI Platform Removes Bias, Hallucinations in ...](https://www.secureitworld.com/news-post/ctgts-new-ai-platform-eliminates-bias-hallucinations-in-ai-models-like-deepseek/)\n\n[102\\. Reducing LLM Hallucinations: A Developer's Guide - Zep](https://www.getzep.com/ai-agents/reducing-llm-hallucinations/)\n\n[103\\. HALLUCINATION IN LVLMs: FICTITIOUS PRESUPPOSITION QUESTIONS, BENCHMARK, AND SOLUTION](https://openreview.net/pdf/eb5476001fe2206eb341b539ffb085530fa1afc5.pdf)\n\n[104\\. Ziwei Ji, Nayeon Lee et al. “Survey of Hallucination in Natural Language Generation.” ACM Computing Surveys](https://doi.org/10.1145/3571730)\n\n[105\\. Domain-Specific LLM Fine-Tuning - Rohan's Bytes](https://www.rohan-paul.com/p/domain-specific-llm-fine-tuning)\n\n[106\\. Stephanie C. Lin, Jacob Hilton et al. “TruthfulQA: Measuring How Models Mimic Human Falsehoods.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/2022.acl-long.229)\n\n[107\\. An empirical study of business process models and model clones on GitHub](https://research.tue.nl/files/349441603/s10664-024-10584-z.pdf)\n\n[108\\. Fine-tuning language models for the enterprise: What you need to know](https://alan.app/blog/fine-tuning-language-models-for-the-enterprise-what-you-need-to-know/)\n\n[109\\. 2025 Tech Trends](https://trendsunplugged.io/wp-content/uploads/2024/11/2025-TechTrends.pdf)\n\n[111\\. Learnware of Language Models: Specialized Small Language Models Can Do Big](https://www.arxiv.org/pdf/2505.13425)\n\n[112\\. LLMs Evaluation: Benchmarks, Challenges, and Future ...](https://blog.premai.io/llms-evaluation-benchmarks-challenges-and-future-trends/)\n\n[113\\. DO WE NEED DOMAIN-SPECIFIC EMBEDDING MODELS? AN EMPIRICAL INVESTIGATION ON FINANCE DOMAIN AND A DOMAIN-MTEB BENCHMARK](https://openreview.net/pdf?id=powufeT93G)\n\n[114\\. Martín Arjovsky, L. Bottou et al. “Invariant Risk Minimization.” ArXiv](https://arxiv.org/abs/1907.02893)\n\n[115\\. Ishaan Gulrajani, David Lopez-Paz. “In Search of Lost Domain Generalization.” ArXiv](https://arxiv.org/abs/2007.01434)\n\n[116\\. Standardized Benchmarks](https://oumi.ai/docs/en/latest/user_guides/evaluate/standardized_benchmarks.html)\n\n[117\\. Best Tools for Domain-Specific LLM Benchmarking - Ghost](https://latitude-blog.ghost.io/blog/best-tools-for-domain-specific-llm-benchmarking/)\n\n[118\\. Shiori Sagawa, Pang Wei Koh et al. “Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization.” ArXiv](https://arxiv.org/abs/1911.08731)\n\n[119\\. Da Li, Yongxin Yang et al. “Deeper, Broader and Artier Domain Generalization.” 2017 IEEE International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV.2017.591)\n\n[120\\. G. Feder. “Development and validation of an international appraisal instrument for assessing the quality of clinical practice guidelines: the AGREE project.” Quality and Safety in Health Care](https://doi.org/10.1136/qhc.12.1.18)\n\n[121\\. The Model Trust Score: The Framework for Strategic Enterprise AI Model Selection](https://www.aigl.blog/content/files/2025/04/The-Model-Trust-Score--The-Framework-for-Strategic-Enterprise-AI-Model-Selection.pdf)\n\n[122\\. Healthcare Benchmarks Initiative Setting 2026-2030 Benchmarks](https://portal.ct.gov/ohs/-/media/ohs/cost-growth-benchmark/2026-2030-proposed-benchmarks/ohs-healthcare-benchmark-initiative-proposed-2026-2030-benchmarks-and-technical-team-recommendations.pdf?rev=5409dd31d6c145c6a8a1e45f296a0067)\n\n[123\\. LLM Evaluation: Benchmarks to Test Model Quality](https://labelyourdata.com/articles/llm-evaluation)\n\n[124\\. GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI](https://proceedings.neurips.cc/paper_files/paper/2024/file/ab7e02fd60e47e2a379d567f6b54f04e-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[125\\. LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text](https://fetcher.alphaxiv.org/v2/pdf/2505.24826v1)\n\n[126\\. Evaluating fine-tuned GPT models on different datasets in the healthcare domain](https://worldscientific.com/doi/pdf/10.1142/S2737599425500124?download=true)\n\n[127\\. DATAMAN: DATA MANAGER FOR PRE-TRAINING LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2502.19363)\n\n[131\\. AI Model Pricing: Comparing GPT, Claude, and Custom ...](https://www.getmonetizely.com/articles/ai-model-pricing-comparing-gpt-claude-and-custom-models-for-enterprise-decision-makers)\n\n[132\\. Perpetual Versus Subscription Licensing Under Quality Uncertainty and Network Externality Effects](http://neconomides.stern.nyu.edu/networks/Zhang_JMIS2010.pdf)\n\n[133\\. Total Cost of Ownership Analysis: Actual vs. Perpetual U.S., Europe, UK & Japan](https://adsknews.autodesk.com/app/uploads/2022/06/TCO-Analysis-US_UK_Europe_JP-June_2022-FINAL.pdf)\n\n[134\\. LJ Loek Jongen. “A generic model for comparing suppliers on total cost of ownership with focus on cost of non-quality.”](https://www.semanticscholar.org/paper/2dd7e40b0f79bb28dfd8475a652ace6696972b11)\n\n[135\\. Continuous API Management: Making the Right Decisions in an Evolving Landscape](https://raw.githubusercontent.com/beamnanda/redhatfuse/main/continuous-api-mgmt-2e.pdf)\n\n[136\\. Autodesk与建筑行业的互动](https://adsknews.autodesk.com/pt-br/views/autodesk-and-the-architecture-industry/)\n\n[141\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[142\\. Patronus AI launches new open-source 'hallucination detection' model](https://tfir.io/patronus-ai-launches-new-open-source-hallucination-detection-model/)\n\n[143\\. Lynx: An Open Source Hallucination Evaluation Model](https://uploads-ssl.webflow.com/64e655d42d3be60f582d0472/668fa6d33f471278ecac72ec_Lynx_Hallucination_Model.pdf)\n\n[144\\. MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models](https://arxiv.org/pdf/2412.18947)\n\n[145\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[146\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[147\\. A Survey on Hallucination in Large Language and Foundation Models](https://www.preprints.org/frontend/manuscript/7b49e7437a3bd104a0b6008f20667fba/download_pub)\n\n[148\\. Improving the Reliability of LLMs: Combining Chain-of-Thought Reasoning and Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.09031)\n\n[149\\. HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild](https://openreview.net/pdf?id=sjwX4Vif03)\n\n[150\\. HaluEval-Wild: Evaluating Hallucinations of Language Models in the...](https://openreview.net/forum?id=q1KnButNt9E)\n\n[151\\. Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models](https://www.summarizepaper.com/en/arxiv-id/2308.11764v4/)\n\n[152\\. HALLUSIONBENCH: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Guan_HallusionBench_An_Advanced_Diagnostic_Suite_for_Entangled_Language_Hallucination_and_CVPR_2024_paper.pdf)\n\n[153\\. Patronus AI open-sources Lynx, a real-time LLM-based judge of AI ...](https://siliconangle.com/2024/07/11/patronus-ai-open-sources-lynx-real-time-llm-based-judge-ai-hallucinations/)\n\n[154\\. Open-Sourced Training Datasets for Large Language Models (LLMs)](https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models)\n\n[155\\. Evaluating Hallucinations of Language Models in the Wild](https://arxiv.org/html/2403.04307v3)\n\n[161\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[162\\. Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective](https://arxiv.org/pdf/2503.01933)\n\n[163\\. Evaluating Large Reasoning Model Performance on Complex Medical Scenarios In The MMLU-Pro Benchmark](https://www.medrxiv.org/content/10.1101/2025.04.07.25325385v2.full.pdf)\n\n[164\\. HELM safety scores across leading AI models 2025](https://www.statista.com/statistics/1612840/helm-safety-ai-models/)\n\n[165\\. Alex Wang, Amanpreet Singh et al. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” BlackboxNLP@EMNLP](https://doi.org/10.18653/v1/W18-5446)\n\n[166\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[167\\. Aarohi Srivastava, Abhinav Rastogi et al. “Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models.” ArXiv](https://arxiv.org/abs/2206.04615)\n\n[168\\. Artificial Intelligence Index Report 2025: Chapter 3 Responsible AI](https://hai.stanford.edu/assets/files/hai_ai-index-report-2025_chapter3_final.pdf)\n\n[169\\. AI by AI Weekly Top 5: 02.17-23, 2025 - Champaign Magazine](https://champaignmagazine.com/2025/02/24/ai-by-ai-weekly-top-5-02-17-23-2025/)\n\n[170\\. FinMTEB: Finance Massive Text Embedding Benchmark](https://arxiv.org/pdf/2502.10990)\n\n[171\\. Percy Liang, Rishi Bommasani et al. “Holistic Evaluation of Language Models.” Annals of the New York Academy of Sciences](https://doi.org/10.1111/nyas.15007)\n\n[172\\. tinyBenchmarks: evaluating LLMs with fewer examples](https://raw.githubusercontent.com/mlresearch/v235/main/assets/maia-polo24a/maia-polo24a.pdf)\n\n[173\\. What Are the Best AI Benchmarks in 2025? - KDCube.Tech](https://kdcube.tech/what-are-the-best-ai-benchmarks-in-2025/)\n\n[174\\. Best Tools for Domain-Specific LLM Benchmarking - Ghost](https://latitude-blog.ghost.io/blog/best-tools-for-domain-specific-llm-benchmarking/)\n\n[175\\. Magenta: Metrics and Evaluation Framework for Generative Agents Based on LLMs](https://openaccess-api.cms-conferences.org/articles/download/978-1-958651-95-7_14)\n\n[176\\. A domain-specific language for monitoring ML model performance](https://cea.hal.science/cea-04485024/document)\n\n[177\\. DO WE NEED DOMAIN-SPECIFIC EMBEDDING MODELS? AN EMPIRICAL INVESTIGATION](https://arxiv.org/pdf/2409.18511v1)\n\n[178\\. DO WE NEED DOMAIN-SPECIFIC EMBEDDING MODELS? AN EMPIRICAL INVESTIGATION ON FINANCE DOMAIN AND A DOMAIN-MTEB BENCHMARK](https://openreview.net/pdf?id=powufeT93G)\n\n[179\\. A Survey on Large Language Models for Critical Societal Domains: Finance, Healthcare, and Law](https://openreview.net/pdf/2a94d2314673972d399c929a8a22e26e872091b4.pdf)\n\n[181\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[182\\. AI Model Pricing: Comparing GPT, Claude, and Custom ...](https://www.getmonetizely.com/articles/ai-model-pricing-comparing-gpt-claude-and-custom-models-for-enterprise-decision-makers)\n\n[183\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[184\\. Nils Reimers, Iryna Gurevych. “Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D19-1410)\n\n[185\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[186\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[187\\. 腾讯全球数字生态大会：大模型赋能产业智能化](https://pdf.dfcfw.com/pdf/H3_AP202309081598088427_1.pdf)\n\n[188\\. 2024 generative AI predictions](https://assets.super.so/c564254b-123a-497f-84cb-5c9a2f3b6e1c/files/255bdff2-cd19-49fa-9c9e-40719babf3f9.pdf)\n\n[189\\. Market Case Study: Perplexity](https://assets.nextleap.app/submissions/MarketCaseStudy_Perplexity-f00d74d8-89e5-4526-a23d-1ac4f85f4ea7.pdf)\n\n[190\\. \\[2025 UPDATED\\] OpenAI GPT-4o API Pricing](https://blog.laozhang.ai/ai/openai-gpt-4o-api-pricing-guide/)\n\n[191\\. 定期出版：年度/季度投资策略](https://www.citics.com.hk/upload/files/report/20241016033938_6007.pdf)\n\n[192\\. Gemini Pro vs GPT 4：AI模型的性能与应用比较](https://www.insnapsys.com/blog/developing-with-ai-gemini-pro-vs-gpt-4-for-programmers/)\n\n[193\\. Engineering Virtual Domain-Specific Service Platforms](https://sse.uni-hildesheim.de/media/fb4/informatik/AG_SSE/Forschung/Projekte/INDENICA/Deliverables/D2.2.1_-_Variability_Implementation_Techniques_-_Interim.pdf)\n\n[194\\. K. Czarnecki, U. Eisenecker. “Generative Programming: Methods, Tools, and Applications.”](https://www.semanticscholar.org/paper/268a30e57724f7e3b92b16f19b82f003ba725778)\n\n[195\\. M. Mernik, J. Heering et al. “When and how to develop domain-specific languages.” ACM Comput. Surv.](https://doi.org/10.1145/1118890.1118892)\n\n[196\\. D. Schmidt. “Model-Driven Engineering.”](https://www.semanticscholar.org/paper/3f21283d368ca0c88ea4f93a8afd1d4a84538467)\n\n[197\\. Building Domain-Specific LLMs: A Comprehensive Guide ...](https://arya.ai/blog/building-domain-specific-llms-for-enterprise-leaders)\n\n[198\\. S. Kelly, Juha-Pekka Tolvanen. “Domain-Specific Modeling: Enabling Full Code Generation.”](https://doi.org/10.1108/k.2008.06737fae.001)\n\n[199\\. Practical Domain-Driven Design in Enterprise Java: Using Jakarta EE, Eclipse MicroProfile, Spring Boot, and the Axon Framework](https://content.e-bookshelf.de/media/reading/L-12255977-b0f0f6a51b.pdf)\n\n[200\\. 8.1 Active Directory Domains](http://etutorials.org/Server+Administration/dns+windows+server/Chapter+8.+Integrating+with+Active+Directory/8.1+Active+Directory+Domains/)\n\n[201\\. The International Exascale Software Project RoadMap](https://exascale.org/mediawiki/images/a/a2/Iesp-roadmap-draft-0.935-complete.pdf)\n\n[202\\. Implementing modular product architectures in mid-sized companies](https://orbit.dtu.dk/files/253915459/PhD_Thesis_Christoffer_Askh_j.pdf)\n\n[203\\. DATA EXCHANGE BETWEEN ARCHITECTURAL DESIGN AND STRUCTURAL ANALYSIS MODELS](https://repositum.tuwien.at/bitstream/20.500.12708/80605/1/Sibenik%20Goran%20-%202022%20-%20Data%20exchange%20between%20architectural%20design%20and%20structural...pdf)\n\n[204\\. Implementation Guide: Success by Design](https://download.microsoft.com/download/c/2/4/c24f97f7-00a6-46db-9b50-8ff6e03e9d45/Dynamics%20365%20Implementation%20Guide%20v1-2.pdf)\n\n[205\\. Domain-Specific Data Models (DSDMs)](https://tes-savi.com/wp-content/uploads/2021/09/TES-SAVi-WP-2020-Jun-1r3s.pdf)\n\n[206\\. G. Sibenik, I. Kovacic. “Interpreted open data exchange between architectural design and structural analysis models.” J. Inf. Technol. Constr.](https://doi.org/10.36680/J.ITCON.2021.004)\n\n[207\\. Implementing Domain Driven Design](https://abp.io/e9f5b0c2-e5e7-41bb-a484-7b87a88c1bba/Implementing_Domain_Driven_Design.pdf)\n\n[208\\. Traian-Radu Ploscă, Christian-Daniel Curiac et al. “Investigating Semantic Differences in User-Generated Content by Cross-Domain Sentiment Analysis Means.” Applied Sciences](https://doi.org/10.3390/app14062421)\n\n[209\\. Implementing Domain-Driven Design](https://dl.ebooksworld.ir/motoman/AW.Implementing.Domain-Driven.Design.www.EBooksWorld.ir.pdf)\n\n[210\\. Building a Domain-Specific LLM | toloka.ai](https://toloka.ai/blog/domain-specific-llm/)\n\n[211\\. Open Enterprise Server 24.4 Domain Services for Windows Administration Guide](https://www.microfocus.com/documentation/open-enterprise-server/24.4/pdfdoc/acc_dsfw_lx/acc_dsfw_lx.pdf)\n\n[213\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[214\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[215\\. HELM safety scores across leading AI models 2025](https://www.statista.com/statistics/1612840/helm-safety-ai-models/)\n\n[216\\. Evaluating Large Reasoning Model Performance on Complex Medical Scenarios In The MMLU-Pro Benchmark](https://www.medrxiv.org/content/10.1101/2025.04.07.25325385v2.full.pdf)\n\n[217\\. Leveraging Large Language Models for Legal Document Understanding and Software System Analysis: Addressing Key Challenges](https://www.rivas.ai/pdfs/quevedo2024llms.pdf)\n\n[218\\. Artificial Intelligence Index Report 2025: Chapter 3 Responsible AI](https://hai.stanford.edu/assets/files/hai_ai-index-report-2025_chapter3_final.pdf)\n\n[219\\. Metritocracy: Representative Metrics for Lite Benchmarks](https://procaccia.info/wp-content/uploads/2025/05/metritocracy.pdf)\n\n[220\\. What Are the Best AI Benchmarks in 2025? - KDCube.Tech](https://kdcube.tech/what-are-the-best-ai-benchmarks-in-2025/)\n\n[221\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[222\\. FinMTEB: Finance Massive Text Embedding Benchmark](https://arxiv.org/pdf/2502.10990)\n\n[223\\. Aarohi Srivastava, Abhinav Rastogi et al. “Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models.” ArXiv](https://arxiv.org/abs/2206.04615)\n\n[224\\. AI by AI Weekly Top 5: 02.17-23, 2025 - Champaign Magazine](https://champaignmagazine.com/2025/02/24/ai-by-ai-weekly-top-5-02-17-23-2025/)\n\n[225\\. Percy Liang, Rishi Bommasani et al. “Holistic Evaluation of Language Models.” Annals of the New York Academy of Sciences](https://doi.org/10.1111/nyas.15007)\n\n[226\\. Evaluating Large Language Models for Public Health Classification and Extraction Tasks](https://arxiv.org/pdf/2405.14766)\n\n[227\\. Rethinking Model Evaluation as Narrowing the Socio-Technical Gap](https://arxiv.org/pdf/2306.03100)\n\n[228\\. HELIX-mRNA: A HYBRID FOUNDATION MODEL FOR FULL SEQUENCE mRNA THERAPEUTICS](https://fetcher.alphaxiv.org/v2/pdf/2502.13785v2)\n\n[229\\. 中国信通院：大模型基准测试体系研究报告（2024年）（52页）.pdf](https://mip.sgpjbg.com/baogao/168027.html)\n\n[230\\. Magenta: Metrics and Evaluation Framework for Generative Agents Based on LLMs](https://openaccess-api.cms-conferences.org/articles/download/978-1-958651-95-7_14)\n\n[231\\. FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models](https://arxiv.org/pdf/2308.09975.pdf)\n\n[232\\. A Survey on Large Language Models for Critical Societal Domains: Finance, Healthcare, and Law](https://openreview.net/pdf/2a94d2314673972d399c929a8a22e26e872091b4.pdf)\n\n[233\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[234\\. AI Model Pricing: Comparing GPT, Claude, and Custom ...](https://www.getmonetizely.com/articles/ai-model-pricing-comparing-gpt-claude-and-custom-models-for-enterprise-decision-makers)\n\n[235\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[236\\. Nils Reimers, Iryna Gurevych. “Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D19-1410)\n\n[237\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[238\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[239\\. AI, 제 5의 유틸리티](https://www.eugenefn.com/common/files/amail/20250512_B45_Juhyeonglee_2.pdf)\n\n[240\\. 2024 generative AI predictions](https://neuro-hub.ru/wp-content/uploads/2024/02/Gen-AI-Predictions-2024.pdf)\n\n[241\\. Enterprise LLM APIs: Top Choices for Powering LLM ... - Unite.AI](https://www.unite.ai/enterprise-llm-apis-top-choices-for-powering-llm-applications/)\n\n[242\\. 腾讯全球数字生态大会：大模型赋能产业智能化](https://pdf.dfcfw.com/pdf/H3_AP202309081598088427_1.pdf)\n\n[243\\. AI Agent：基于大模型的自主智能体，在探索AGI的道路上前进](https://www.hangxincap.com/wp-content/uploads/2023/11/20230825-%E4%BF%A1%E6%81%AF%E7%A7%91%E6%8A%80-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A1%8C%E4%B8%9A%E6%B7%B1%E5%BA%A6%E6%8A%A5%E5%91%8A%EF%BC%9AAI-Agent%EF%BC%8C%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%87%AA%E4%B8%BB%E6%99%BA%E8%83%BD%E4%BD%93%EF%BC%8C%E5%9C%A8%E6%8E%A2%E7%B4%A2AGI%E7%9A%84%E9%81%93%E8%B7%AF%E4%B8%8A%E5%89%8D%E8%BF%9B.pdf)\n\n[244\\. Gen AI에 올라타! 한미중 Gen AI 밸류체인 & AI 반도체](http://file.myasset.com/sitemanager/upload/2024/0403/112243/20240403112243290_0_ko.pdf)\n\n[245\\. GENERATIVE AI AND LLM OPTIMIZING TECHNIQUES FOR DEVELOPING COST EFFECTIVE ENTERPRISE APPLICATIONS](https://iaeme.com/MasterAdmin/Journal_uploads/IJAIAP/VOLUME_2_ISSUE_1/IJAIAP_02_01_004.pdf)\n\n[246\\. \\[2025 UPDATED\\] OpenAI GPT-4o API Pricing](https://blog.laozhang.ai/ai/openai-gpt-4o-api-pricing-guide/)\n\n[247\\. DeepSeek AI: A comprehensive guide for enterprise implementation](https://aigc.idigital.com.cn/djyanbao/%E3%80%90DeepSeek%20AI%E3%80%91%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%9E%E6%96%BD%E5%85%A8%E9%9D%A2%E6%8C%87%E5%8D%97-2025-06-02.pdf)\n\n[248\\. 大模型应用百花齐放，AI发展进入新时代](https://pdf.dfcfw.com/pdf/H3_AP202303281584600914_1.pdf?1679996015000.pdf)\n\n[249\\. 2024年8月期 第2四半期 決算説明会資料](https://finance-frontend-pc-dist.west.edge.storage-yahoo.jp/disclosure/20240411/20240411569096.pdf)\n\n[250\\. Billing overview - Guides - Coze](https://assets.goodfirms.co/software_files/6840ba34c0d275737c51f362/Billing%20overview%20-%20Guides%20-%20Coze.pdf)\n\n[251\\. GPT-4震撼发布，AI算法之巅](https://pdf.dfcfw.com/pdf/H301_AP202303151584293213_1.pdf)\n\n[252\\. Anthony Dewayne Hunt. “Bitcoin: A Peer-to-Peer Electronic Cash System.”](https://doi.org/10.2139/ssrn.3440802)\n\n[253\\. Throughput and latency in Watson OpenScale model ...](https://dataplatform.cloud.ibm.com/docs/content/wsj/model/wos-model-health-throughput-latency.html?locale=ru&context=cpdaas)\n\n[254\\. S. Kelly, Juha-Pekka Tolvanen. “Domain-Specific Modeling: Enabling Full Code Generation.”](https://doi.org/10.1108/k.2008.06737fae.001)\n\n[255\\. Specializing Language Models for Domain-Specific Tasks](https://cdn.prod.website-files.com/63401dfb6edad1c1702f685c/67ad627babcfaacf16f9e6dd_f85b3088b643b58527bcf775cfeecf8a_TSMPaper.pdf)\n\n[256\\. Hosting FMs on Amazon SageMaker for scale and performance](https://d1.awsstatic.com/events/Summits/reinvent2023/AIM345_Hosting-FMs-on-Amazon-SageMaker-for-scale-and-performance.pdf)\n\n[257\\. Engineering Virtual Domain-Specific Service Platforms](https://sse.uni-hildesheim.de/media/fb4/informatik/AG_SSE/Forschung/Projekte/INDENICA/Deliverables/D2.2.1_-_Variability_Implementation_Techniques_-_Interim.pdf)\n\n[258\\. Frame-level throughput and latency metrics - proposed text.](https://www.cse.wustl.edu/~jain/atmf/ftp/af9606-2.pdf)\n\n[259\\. Latency and Throughput](https://www.diskodev.com/posts/latency-and-throughput/)\n\n[260\\. Selecting Transformer Model Size & Complexity for Deployment](https://www.rohan-paul.com/p/selecting-transformer-model-size)\n\n[261\\. LLM Comparison: Choosing the Right Model for Your Use Case - Botscrew](https://botscrew.com/blog/llm-comparison-choosing-the-right-model/)\n\n[263\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[264\\. Evaluating Large Reasoning Model Performance on Complex Medical Scenarios In The MMLU-Pro Benchmark](https://www.medrxiv.org/content/10.1101/2025.04.07.25325385v2.full.pdf)\n\n[265\\. STYLE OUTWEIGHS SUBSTANCE: FAILURE MODES OF LLM JUDGES IN ALIGNMENT BENCHMARKING](https://openreview.net/pdf?id=MzHNftnAM1)\n\n[266\\. HELM safety scores across leading AI models 2025](https://www.statista.com/statistics/1612840/helm-safety-ai-models/)\n\n[267\\. Leveraging Large Language Models for Legal Document Understanding and Software System Analysis: Addressing Key Challenges](https://www.rivas.ai/pdfs/quevedo2024llms.pdf)\n\n[268\\. AI by AI Weekly Top 5: 02.17-23, 2025 - Champaign Magazine](https://champaignmagazine.com/2025/02/24/ai-by-ai-weekly-top-5-02-17-23-2025/)\n\n[269\\. Artificial Intelligence Index Report 2025: Chapter 3 Responsible AI](https://hai.stanford.edu/assets/files/hai_ai-index-report-2025_chapter3_final.pdf)\n\n[270\\. Alex Wang, Amanpreet Singh et al. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” BlackboxNLP@EMNLP](https://doi.org/10.18653/v1/W18-5446)\n\n[271\\. Metritocracy: Representative Metrics for Lite Benchmarks](https://procaccia.info/wp-content/uploads/2025/05/metritocracy.pdf)\n\n[272\\. IBM/helm-enterprise-benchmark](https://github.com/IBM/helm-enterprise-benchmark)\n\n[273\\. Benchmarking Chinese Medical LLMs: A MEDBench-Based Analysis of Performance Gaps and Hierarchical Optimization Strategies](https://arxiv.org/pdf/2503.07306)\n\n[274\\. Evaluating Large Language Models for Public Health Classification and Extraction Tasks](https://arxiv.org/pdf/2405.14766)\n\n[275\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[276\\. What Are the Best AI Benchmarks in 2025? - KDCube.Tech](https://kdcube.tech/what-are-the-best-ai-benchmarks-in-2025/)\n\n[277\\. Peer-Reviewed Publications - 2025](https://www.beg.utexas.edu/publications)\n\n[278\\. Aarohi Srivastava, Abhinav Rastogi et al. “Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models.” ArXiv](https://arxiv.org/abs/2206.04615)\n\n[279\\. DO WE NEED DOMAIN-SPECIFIC EMBEDDING MODELS? AN EMPIRICAL INVESTIGATION](https://arxiv.org/pdf/2409.18511v1)\n\n[280\\. Percy Liang, Rishi Bommasani et al. “Holistic Evaluation of Language Models.” Annals of the New York Academy of Sciences](https://doi.org/10.1111/nyas.15007)\n\n[283\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[284\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[285\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[286\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[287\\. AI Model Pricing: Comparing GPT, Claude, and Custom ...](https://www.getmonetizely.com/articles/ai-model-pricing-comparing-gpt-claude-and-custom-models-for-enterprise-decision-makers)\n\n[288\\. Lianmin Zheng, Wei-Lin Chiang et al. “Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.” ArXiv](https://doi.org/10.48550/arXiv.2306.05685)\n\n[289\\. AI, 제 5의 유틸리티](https://www.eugenefn.com/common/files/amail/20250512_B45_Juhyeonglee_2.pdf)\n\n[290\\. 2024 generative AI predictions](https://assets.super.so/c564254b-123a-497f-84cb-5c9a2f3b6e1c/files/255bdff2-cd19-49fa-9c9e-40719babf3f9.pdf)\n\n[291\\. Enterprise LLM APIs: Top Choices for Powering LLM ... - Unite.AI](https://www.unite.ai/enterprise-llm-apis-top-choices-for-powering-llm-applications/)\n\n[292\\. DeepSeek AI: A comprehensive guide for enterprise implementation](https://aigc.idigital.com.cn/djyanbao/%E3%80%90DeepSeek%20AI%E3%80%91%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%9E%E6%96%BD%E5%85%A8%E9%9D%A2%E6%8C%87%E5%8D%97-2025-06-02.pdf)\n\n[293\\. 腾讯全球数字生态大会：大模型赋能产业智能化](https://pdf.dfcfw.com/pdf/H3_AP202309081598088427_1.pdf?1694167708000.pdf)\n\n[294\\. AI at Turning Point](https://www.eugenefn.com/common/files/amail/20250319_B45_tena_14.pdf)\n\n[295\\. OpenAI Josh Achiam, Steven Adler et al. “GPT-4 Technical Report.”](https://arxiv.org/abs/2303.08774)\n\n[297\\. Emerging Jets Search, Triton Server Deployment, and Track Quality Development: Machine Learning Applications in High Energy Physics](https://lss.fnal.gov/archive/thesis/2000/fermilab-thesis-2024-09.pdf)\n\n[298\\. D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models](https://openreview.net/pdf?id=JzKFN5fWOk)\n\n[299\\. Selecting Transformer Model Size & Complexity for Deployment](https://www.rohan-paul.com/p/selecting-transformer-model-size)\n\n[300\\. Scaling Your Deployment for Large Data Volumes](https://www.cisco.com/c/en/us/td/docs/security/cisco-secure-cloud-app/user-guide/cisco-security-cloud-user-guide/m_scaling_your_deployment_for_large_data_volumes.pdf)\n\n[301\\. Scaling Out Bioinformatics in the Datacenter](https://infoscience.epfl.ch/record/268461/files/EPFL_TH9451.pdf)\n\n[302\\. Engineering Virtual Domain-Specific Service Platforms](https://sse.uni-hildesheim.de/media/fb4/informatik/AG_SSE/Forschung/Projekte/INDENICA/Deliverables/D2.2.1_-_Variability_Implementation_Techniques_-_Interim.pdf)\n\n[303\\. Holistic Approaches to Performance Optimization in Decentralized Systems: A Study of Hyperledger Fabric](https://mediatum.ub.tum.de/doc/1745563/1745563.pdf)"
  },
  {
    "id": 31,
    "question": "What are the specific technological/policy challenges in maintaining AI leadership while avoiding self-harm through overregulation?",
    "response": "**Research Report: The AI Leadership Tightrope**\n\n**Date:** July 26, 2025\n\n**Author:** Expert Researcher\n\n**Topic:** What are the specific technological/policy challenges in maintaining AI leadership while avoiding self-harm through overregulation?\n\n**Executive Summary**\n\nAs of mid-2025, the global race for leadership in Artificial Intelligence (AI) has entered a precarious new phase, defined by a fundamental tension: the drive for innovation versus the imperative for safety and control. Nations are grappling with how to establish rules that foster public trust and mitigate catastrophic risk without stifling the very technological dynamism that underpins economic competitiveness and national security. This report analyzes the primary technological and policy challenges inherent in this balancing act, focusing on the divergent approaches of the United States (US) and the European Union (EU), with reference to China's distinct trajectory.\n\nOur analysis reveals two core areas of friction. First, **policy challenges** stem from fundamentally different regulatory philosophies. The EU has adopted a comprehensive, risk-based, and legally binding framework in its AI Act, aiming to set a global standard—the \"Brussels Effect\"—at the potential cost of innovation speed and flexibility \\[44\\]\\[286\\]. The US, through its Executive Order and subsequent agency actions, has pursued a more flexible, principles-based, and pro-innovation approach that encourages industry standards but risks creating a fragmented and potentially less protective regulatory patchwork \\[44\\]\\[47\\].\n\nSecond, these policy choices create and exacerbate significant **technological challenges**. Regulations are mandating technical solutions for deeply complex issues like model transparency, bias mitigation, and content authentication, often ahead of proven, scalable, and robust technologies. Case studies on AI watermarking and real-time biometric identification reveal a critical gap between regulatory intent and engineering reality. For instance, mandates for watermarking AI-generated content face unresolved technical hurdles in robustness and reliability \\[88\\]\\[90\\]\\[300\\]while the legal exceptions for using biometric surveillance for counter-terrorism lack the clear, quantitative technical thresholds (e.g., error rates) needed for compliant implementation \\[202\\]\\[218\\]\\[359\\].\n\nEarly indicators from 2023 through early 2025 suggest these divergent approaches are having a measurable impact. The US continues to dominate in AI investment, particularly in later-stage funding for generative AI, widening its lead over the EU \\[69\\]\\[121\\]. Concurrently, China has established a formidable lead in the sheer volume of AI patent filings \\[64\\]\\[72\\]. While definitive data on talent migration remains nascent, the US has explicitly integrated talent attraction into its AI strategy \\[230\\]\\[222\\]while Europe has historically seen a net loss of AI researchers \\[162\\].\n\nUltimately, maintaining AI leadership while avoiding self-harm requires a nuanced navigation of this complex landscape. Overly prescriptive regulation risks ceding the innovation frontier, while a hands-off approach could lead to systemic risks and a loss of public trust. The central challenge lies in crafting policies that are adaptive, technically informed, and foster an ecosystem of responsible innovation rather than one of prohibitive compliance.\n\n**1\\. The Global Regulatory Landscape: Divergent Philosophies**\n\nThe period between 2023 and 2025 has been characterized by the crystallization of distinct national and regional approaches to AI governance. The two most influential models, those of the European Union and the United States, represent a fundamental split in regulatory philosophy, creating a complex and sometimes contradictory global landscape \\[24\\]\\[32\\].\n\n**1.1 The European Union's Risk-Based, Comprehensive Model: The AI Act**\n\nThe EU's landmark AI Act, which passed its final parliamentary vote on March 13, 2024 \\[338\\], represents the world's most comprehensive and prescriptive legal framework for AI \\[71\\]. Its core design principle is a **risk-based classification system** that sorts AI applications into tiers of risk: unacceptable, high, limited, and minimal \\[44\\]\\[46\\]\\[47\\].\n\n**Prohibited AI:** The Act outright bans applications deemed to pose an \"unacceptable risk,\" such as social scoring by governments, untargeted scraping of facial images from the internet, and emotion recognition in workplaces and schools \\[41\\]\\[48\\]\\[53\\].\n\n**High-Risk Systems:** This is the most heavily regulated category. AI systems used in critical infrastructure, employment, law enforcement, and administration of justice are subject to stringent obligations _before_ they can be placed on the market. These include establishing robust risk management systems, ensuring high levels of data quality, maintaining detailed technical documentation (to be kept for 10 years), enabling automatic event logging, and guaranteeing human oversight \\[41\\]\\[47\\]\\[51\\]. Remote biometric identification systems fall into this category, with their use strictly curtailed \\[142\\].\n\n**Transparency Obligations:** For systems with \"limited risk,\" such as chatbots or AI generating \"deepfakes,\" the primary obligation is transparency—users must be clearly informed that they are interacting with an AI system or viewing synthetic content \\[50\\]\\[53\\].\n\nThe EU AI Act is characterized by its **extraterritorial reach**, applying to any entity providing an AI system or its output within the EU market, regardless of where the provider is based \\[43\\]\\[50\\]. This, combined with severe penalties for non-compliance—fines up to €35 million or 7% of global annual turnover—is designed to create a powerful \"Brussels Effect,\" compelling global companies to adopt EU standards worldwide \\[44\\]\\[53\\]\\[286\\]. While this approach aims to create a harmonized and predictable legal framework to foster trust \\[91\\], it has also raised significant concerns about imposing heavy compliance burdens, particularly on small and medium-sized enterprises (SMEs) and startups, potentially slowing innovation \\[21\\]\\[27\\]\\[28\\].\n\n**1.2 The United States' Principles-Based, Pro-Innovation Model**\n\nIn contrast to the EU's single legislative text, the US has pursued a more flexible, sector-specific, and guidance-oriented approach, crystallized in President Biden’s Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence, signed on October 30, 2023 \\[222\\]\\[343\\]\\[348\\].\n\nThe US model is fundamentally **principles-based and pro-innovation**, seeking to guide responsible development without imposing broad, pre-market legal mandates \\[44\\]\\[47\\]. Its key tenets include:\n\n**Guidance and Best Practices:** The Executive Order directs federal agencies, such as the National Institute of Standards and Technology (NIST), to develop standards, guidelines, and best practices for AI safety, security, and testing \\[49\\]\\[60\\]. This includes producing guidance on AI watermarking and content authentication to detect AI-generated content \\[49\\]\\[60\\]. The NIST AI Risk Management Framework (RMF) is a central pillar of this approach, providing a voluntary \"how-to\" guide for responsible AI development \\[62\\]\\[185\\].\n\n**Focus on Government and Large Models:** A primary thrust of the Order is compelling developers of the most powerful foundation models to share their safety test results and other critical information with the government \\[49\\]\\[60\\]. It also establishes new governance structures within the federal government itself, such as Chief AI Officers for every agency \\[285\\].\n\n**Promoting Competition and Talent:** A significant portion of the Executive Order is dedicated to maintaining US leadership by promoting innovation and competition. It includes initiatives to attract and retain global AI talent, streamline visa processes, provide technical assistance to smaller developers, and launch a national AI research resource \\[49\\]\\[57\\]\\[222\\].\n\nThis approach relies more heavily on **industry self-regulation and a federalist system** where federal, state, and local laws combine with sector-specific standards \\[43\\]\\[47\\]. The advantage is agility; regulations can adapt more quickly to the rapid pace of technological change \\[30\\]\\[34\\]. The risk, however, is a fragmented and inconsistent landscape that can create compliance challenges for multinational companies and may lead to \"forum shopping,\" where companies gravitate toward jurisdictions with the least regulatory friction \\[23\\]\\[24\\]\\[33\\].\n\n**2\\. Technological Hurdles Exacerbated by Regulation**\n\nPolicy decisions do not exist in a vacuum; they create direct engineering and research challenges. The current wave of AI regulation risks oversimplifying complex technical realities, potentially worsening existing vulnerabilities or creating new obstacles to innovation.\n\n**2.1 The \"Black Box\" Problem: Mandating Unachievable Transparency**\n\nFoundation models are often described as \"black boxes\" because their decision-making processes, derived from patterns in petabytes of data, are not readily interpretable by humans \\[3\\]\\[4\\]. Regulations, particularly the EU AI Act's requirements for high-risk systems, demand transparency and explainability \\[47\\]\\[51\\]. However, mandating explainability without providing clear technical standards or acknowledging the inherent opacity of current deep learning architectures creates a major hurdle \\[4\\]. This can force developers into a compliance paradox: either use less powerful, more interpretable models, thus falling behind competitively, or struggle to reverse-engineer justifications for a black-box model's output, a process that may not reflect its actual reasoning.\n\n**2.2 Security and Integrity: The Risk of Regulated Backdoors**\n\nAI models are susceptible to security vulnerabilities, including malicious backdoors implanted during training that could allow attackers to misuse the system or extract sensitive data \\[9\\]. Overly restrictive regulations that limit collaborative research, open-sourcing of model weights, or security auditing could inadvertently protect these vulnerabilities from discovery \\[2\\]\\[7\\]. For example, a regulation that stifles the open, collaborative \"red-teaming\" of models could allow state-sponsored actors to discover and exploit flaws that a broader community of security researchers would have otherwise found and fixed.\n\n**2.3 Data Privacy and Algorithmic Monoculture**\n\nFoundation models carry inherent risks of leaking private information from their training data or user prompts and infringing on intellectual property by reproducing copyrighted material \\[3\\]\\[8\\]. While regulation rightly seeks to address this, a clumsy approach can backfire. For instance, if regulations make it excessively difficult or legally risky for diverse organizations to train their own models, it could accelerate the trend toward an **\"algorithmic monoculture\"** \\[4\\]. In such a scenario, a handful of dominant, US-based foundation models become the underpinning for most applications globally. This not only centralizes power and stifles competition \\[7\\] but also creates a systemic risk: a single flaw or bias in a dominant model could have cascading negative effects across society \\[4\\].\n\n**3\\. Case Study 1: The Watermarking and Logging Conundrum**\n\nThe divergent approaches of the US and EU are clearly illustrated in their respective technical mandates for content and system traceability: AI watermarking and automatic logging. Both aim to increase transparency and accountability but are fraught with technical and practical challenges.\n\n**3.1 AI Watermarking: A Mandate in Search of a Robust Solution**\n\nBoth the US Executive Order and the EU AI Act call for methods to identify AI-generated content, with watermarking being a primary candidate \\[49\\]\\[76\\]\\[83\\]. The EU AI Act (Article 52) mandates that synthetic content be marked in a machine-readable format that is \"effective, interoperable, robust, and reliable\" \\[183\\]\\[186\\]\\[192\\].\n\nHowever, as of 2025, the technology has not caught up to the regulatory ambition. The search for a universal, robust watermarking technique remains elusive \\[256\\].\n\n**Technical Fragility:** Current watermarking techniques display \"strong technical limitations and drawbacks\" \\[88\\]\\[90\\]. Visible watermarks are easily cropped or removed. Invisible watermarks can often be degraded or destroyed by simple image compression, filtering, or re-encoding—common processes in online content distribution \\[246\\]\\[300\\].\n\n**Lack of Deployed, Compliant Architectures:** Despite the regulatory push, there is a notable absence of publicly available technical whitepapers or case studies demonstrating deployed watermarking systems that verifiably meet both the EU AI Act's strict criteria and the risk-management principles of the US NIST AI RMF, particularly in high-stakes, real-time environments like live news broadcasting \\[171\\]\\[185\\]\\[242\\]. Efforts to map the NIST RMF to the EU AI Act are underway \\[185\\]\\[185\\]but concrete, deployed examples remain theoretical.\n\nThis gap between policy and technology creates a significant challenge. Forcing the adoption of immature technology could create a false sense of security, where malicious actors easily circumvent the watermarks, while legitimate innovators are burdened with implementing ineffective and costly compliance measures.\n\n**3.2 Automatic Logging: A Vague but Costly Requirement**\n\nThe EU AI Act requires that high-risk AI systems implement **\"automatic logging\"** capabilities to record events throughout the system's lifecycle \\[79\\]\\[82\\]\\[78\\]. The goal is to ensure traceability and facilitate post-market monitoring. However, the requirement's technical specificity is low. The Act states logs must be kept for an \"appropriate period\" based on the system's purpose, a vague directive that creates uncertainty for developers \\[79\\]\\[79\\]. The computational and storage overhead for logging every relevant event in a large-scale AI system can be immense, posing a significant, continuous cost that may disproportionately affect smaller companies.\n\n**4\\. Case Study 2: The Biometric Identification Tightrope**\n\nPerhaps no area highlights the conflict between security imperatives and fundamental rights more than real-time remote biometric identification (RBI). The EU AI Act's handling of this technology is a prime example of a policy attempting to walk a very fine technical tightrope.\n\n**4.1 The General Prohibition and Its Narrow Exceptions**\n\nThe EU AI Act establishes a general prohibition on the use of real-time RBI systems in publicly accessible spaces by law enforcement, citing the significant risks of bias and infringements on fundamental rights \\[95\\]\\[142\\]\\[147\\]. However, it carves out narrow, \"strictly necessary\" exceptions for severe situations, including:\n\nThe prevention of a \"genuine and present or foreseeable threat of a terrorist attack\" \\[95\\]\\[100\\]\\[206\\].\n\nThe search for victims of serious crimes like trafficking \\[202\\]\\[206\\].\n\nThe localization of suspects for serious crimes punishable by at least four years of imprisonment \\[205\\]\\[207\\].\n\n**4.2 The Engineering Challenge of Vague Legal Thresholds**\n\nThese exceptions create profound engineering challenges. The law requires that systems be used only when \"strictly necessary,\" a legal concept that is difficult to translate into precise, algorithmic triggers \\[98\\]\\[111\\]. The system must be designed to integrate with legal frameworks requiring prior judicial authorization, adding a layer of sociotechnical complexity \\[95\\]\\[98\\]\\[107\\].\n\nMost critically, the EU AI Act and its supporting documents from bodies like CEN-CENELEC or ENISA **fail to specify maximum allowable error rates** (e.g., False Acceptance Rates or False Positive Rates) for RBI systems used under these counter-terrorism exceptions \\[202\\]\\[218\\]\\[312\\]. While other EU systems, like the Entry/Exit System (EES) for border control, specify a maximum false positive rate of 0.1% \\[204\\]\\[323\\]no such clear technical threshold exists for these high-stakes domestic deployments. This regulatory gap leaves manufacturers and law enforcement agencies in a state of uncertainty, forcing them to define their own performance standards and defend them during a fundamental rights impact assessment, creating significant legal and operational risk \\[98\\]\\[104\\].\n\n**5\\. Measuring the Impact: Early Indicators of Divergence (2023-2025)**\n\nWhile the full impact of these regulatory frameworks will take years to manifest (the EU AI Act's full enforcement begins in 2026 \\[222\\]\\[238\\], analysis of investment, intellectual property, and talent flow data from 2023 to early 2025 provides crucial leading indicators of how these divergent paths are shaping the global AI landscape.\n\n**5.1 Investment and Funding: The US Widens its Lead**\n\nThe United States has solidified its position as the dominant force in AI investment. In 2023, US-based AI startups received 31 billion in financing, a 14% year-on-year increase, accounting for nearly half of all AI transactions globally \\[69\\]. The gap is particularly stark in generative AI; in 2023, US investment in this subfield (22.46 billion) dwarfed that of the EU/UK (0.65 billion) combined \\[121\\]\\[121\\]. While the search results do not provide a clean regression analysis directly linking this funding disparity to regulation, the trend suggests that the US's flexible, pro-innovation environment remains highly attractive to venture capital, especially for later-stage (Series B+) investments where regulatory risk is a greater consideration.\n\n**5.2 Patent Filings: China's Volume-Driven Strategy**\n\nIn terms of intellectual property generation, China presents a different picture. Chinese institutions have dramatically outpaced their US counterparts in the volume of AI-related patent applications. In 2022, Chinese entities filed nearly 80% more AI-related patents than US filers \\[64\\]. Over the last decade, China has accumulated approximately six times more AI invention patents than the US (38,210 to 6,276) \\[72\\]\\[73\\]. This suggests a state-driven strategy focused on accumulating a massive portfolio of IP, which could confer significant strategic advantages regardless of immediate commercialization or startup funding metrics.\n\n**5.3 Talent Migration: An Unfolding Story**\n\nThe global competition for AI talent is fierce. The US remains the top destination for top-tier AI talent, hosting 60% of the world's leading AI institutions \\[163\\]. LinkedIn data from 2019-2023 shows a historical pattern of Europe experiencing a net loss of AI talent \\[162\\], while countries like India and Israel have seen declining net AI talent migration figures more recently \\[169\\]. The US Executive Order explicitly aims to leverage this advantage by including provisions to attract and retain skilled AI professionals \\[230\\]\\[222\\].\n\nHowever, the direct impact of the 2023-2024 regulatory announcements on talent flows is still difficult to quantify. The search results explicitly note the absence of empirical data or regression analyses directly correlating monthly AI researcher relocation patterns with the EU AI Act's final vote or the signing of the US Executive Order \\[219\\]\\[238\\]\\[276\\]. This remains a critical area for future observation as the full weight of these regulations comes into force.\n\n**6\\. Conclusion and Strategic Outlook**\n\nAs of July 2025, the challenge of maintaining AI leadership while mitigating existential risks has no simple solution. The world is witnessing a real-time experiment with two dominant, divergent regulatory models.\n\nThe **EU's comprehensive, risk-based approach** is ambitious and aims to build a global gold standard for trustworthy AI. However, it risks becoming a \"compliance moat\" that protects incumbents and stifles the agile, iterative development at which startups excel \\[21\\]\\[28\\]. The technical vagueness in critical areas like biometric error rates and the practical difficulties of mandates like watermarking demonstrate a potential disconnect between policy goals and technological feasibility.\n\nThe **US's flexible, principles-based strategy** is explicitly designed to foster innovation and maintain a competitive edge \\[49\\]\\[57\\]. Its success hinges on the ability of industry self-regulation and voluntary standards like the NIST AI RMF to effectively manage risk. The primary danger is a fragmented system that fails to prevent systemic harms and cedes the power of setting global norms to more assertive regulatory blocs \\[23\\]\\[33\\].\n\nTo navigate this leadership tightrope, policymakers must move beyond a simple \"regulation vs. innovation\" dichotomy. The future will likely belong to the region that can best foster **responsible innovation**. This requires adaptive regulations that are co-developed with technical experts, the use of regulatory sandboxes to test rules before wide-scale implementation \\[22\\]\\[26\\]\\[27\\]and a focus on establishing clear technical standards for critical functions. Without this deep integration of policy and technology, nations risk either stifling their own progress through well-intentioned but technically naive overregulation or failing to erect the guardrails necessary to prevent the technology from causing irreparable self-harm.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[2\\. Dual-Use Foundation Models with Widely Available Model Weights](https://www.ntia.doc.gov/sites/default/files/publications/ntia-ai-open-model-report.pdf)\n\n[3\\. Companies using Gen AI weigh regulatory and compliance risks](https://www.legaldive.com/news/companies-weigh-genai-regulatory-compliance-risks-wsj-gowling-zoominfo-nist-gdpr/715401/)\n\n[4\\. Regulation Priorities for Artificial Intelligence Foundation Models](https://cdn.vanderbilt.edu/vu-URL/wp-content/uploads/sites/356/2023/11/28023547/Gaske_PDF_FINAL.pdf)\n\n[5\\. FAQs on Foundation Models and Generative AI](https://ai.gov/wp-content/uploads/2023/09/FAQs-on-Foundation-Models-and-Generative-AI.pdf)\n\n[6\\. OpenAI Josh Achiam, Steven Adler et al. “GPT-4 Technical Report.”](https://arxiv.org/abs/2303.08774)\n\n[7\\. Issues Paper: Competition and Generative Artificial Intelligence](https://www.concorrencia.pt/sites/default/files/documentos/Issues%20Paper%20-%20Competition%20and%20Generative%20Artificial%20Intelligence.pdf)\n\n[8\\. aws-samples/bias-mitigation-foundation-models](https://github.com/aws-samples/bias-mitigation-foundation-models)\n\n[9\\. 2025 \"人工智能+\" 行业发展蓝皮书](http://www.sccio.cn/uploads/20250522/8696496143e2c9e662e2e45890c9c1b4.pdf)\n\n[10\\. Hazards from Increasingly Accessible Fine-Tuning of Downloadable Foundation Models](https://openreview.net/pdf?id=QbXf5BqyXp)\n\n[11\\. A new type of powerful artificial intelligence could make EU’s new law obsolete](https://sciencebusiness.net/news/new-type-powerful-artificial-intelligence-could-make-eus-new-law-obsolete?utm_source=Science%7CBusiness+Newsletters&utm_campaign=fb8ecf568d-EMAIL_CAMPAIGN_4_26_2021_17_43_COPY_01&utm_medium=email&utm_term=0_179178d214-fb8ecf568d-138624831)\n\n[12\\. A pro-innovation approach to AI regulation White Paper](https://www.equalityhumanrights.com/sites/default/files/2023/Department%20for%20Science,%20Innovation%20and%20Technology%20-%20A%20pro-innovation%20approach%20to%20AI%20regulation%20White%20Paper%20consultation%20response,%2023%20June%202023.docx)\n\n[13\\. Rishi Bommasani, Drew A. Hudson et al. “On the Opportunities and Risks of Foundation Models.” ArXiv](https://arxiv.org/abs/2108.07258)\n\n[14\\. AI Foundation Models – Explained](https://ccianet.org/wp-content/uploads/2023/09/AI_Foundation_Models_Explained.pdf)\n\n[15\\. Pengfei Liu, Weizhe Yuan et al. “Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing.” ACM Computing Surveys](https://doi.org/10.1145/3560815)\n\n[16\\. Market Concentration Implications of Foundation Models: The Invisible Hand of ChatGPT](https://www.economic-policy.org/wp-content/uploads/2024/03/EcPol-2023-183.R1_Proof_hi_Korinek_Vipra.pdf)\n\n[21\\. AI regulation versus innovation – finding the balance](https://www.thenationalnews.com/uae/advertorial/2024/03/20/ai-regulation-versus-innovation-finding-the-balance/)\n\n[22\\. Regulating AI on Latin America's Terms - Americas Quarterly](https://americasquarterly.org/article/regulating-ai-on-latin-americas-terms/#:~:text=For%20Latin%20America,%20smart%20AI,over-%20or%20under-regulating.)\n\n[23\\. SANDBOXES FOR AI: TOOLS FOR A NEW FRONTIER](https://www.thedatasphere.org/wp-content/uploads/2025/02/Report-Sandboxes-for-AI-2025.pdf)\n\n[24\\. Promoting Investor Success, Industry Innovation, and Efficiency with AI](https://www.sifma.org/wp-content/uploads/2024/09/AI-Whitepaper-Promoting-Investor-Success-Industry-Innovation-and-Efficiency-with-AI.pdf)\n\n[25\\. A Comparative Framework for AI Regulatory Policy: Phase 2](https://ceimia.org/wp-content/uploads/2024/06/a-comparative-framework-for-ai-regulatory-policy_-phase-2-docx-merged.pdf)\n\n[26\\. AI-Related Disciplines: A Comparative Analysis of Regional Trade Agreements and National Regulatory Approaches](https://ciwto.uibe.edu.cn/docs/2025-03/5cd4b83d97fe489ba8d1fa9075b3687f.pdf)\n\n[27\\. Cohere Response to the Request for Information on the Development of an Artificial Intelligence (AI) Action Plan](https://files.nitrd.gov/90-fr-9088/Cohere-AI-RFI-2025.pdf)\n\n[28\\. Point Zero Forum](https://www.pointzeroforum.com/hubfs/Point%20Zero%20Forum/PZF%202025/forum%20guide/Point%20Zero%20Forum%202025_Forum%20Guide.pdf)\n\n[29\\. Flexible, Pro-Innovation Governance Strategies for Artificial Intelligence](https://www.rstreet.org/wp-content/uploads/2023/04/Final_Study283.pdf)\n\n[30\\. Governing AI – attempting to herd cats? Introduction to the special issue on the Governance of Artificial Intelligence](https://mediatum.ub.tum.de/doc/1695221/document.pdf)\n\n[31\\. Startup Model Law Framework](https://144526406.fs1.hubspotusercontent-eu1.net/hubfs/144526406/StartupModelLawFramework_summary.pdf)\n\n[32\\. Gaining National Competitive Advantage through Artificial Intelligence (AI): Policy Making & National AI Strategies](https://www.pwc.lu/en/technology/docs/gaining-national-competitive-advantage-through-ai.pdf)\n\n[33\\. Key Insights on President Trump’s New AI Executive Order and Policy & Regulatory Implications](https://www.squirepattonboggs.com/-/media/files/insights/publications/2025/02/key-insights-on-president-trumps-new-ai-executive-order-and-policy--regulatory-implications/president_trumps_new_ai_eo.pdf?rev=e146494e56c5479fbd1bc5af27ad671e&sc_lang=en&hash=65F06034A2720C1FC1C3DF9DD6BCB6AF)\n\n[34\\. Comments of the International Center for Law & Economics, Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights](https://laweconcenter.org/wp-content/uploads/2024/03/ICLE-NTIA-COMMENTS-RFC-Open-Foundation-Models-2024.pdf)\n\n[35\\. Roundup: U.S. experts urge flexible AI regulation to spur ...](https://english.news.cn/20241009/5b4c0eee8138479b8586d4cb5760551c/c.html)\n\n[36\\. A Complex Adaptive System Framework to Regulate AI](https://eacpm.gov.in/wp-content/uploads/2023/10/EACPM-WP26-A-Complex-Adaptive-System-Framework-to-Regulate-AI.pdf)\n\n[37\\. Alessio Tartaro, A. Smith et al. “Assessing the impact of regulations and standards on innovation in the field of AI.” ArXiv](https://doi.org/10.48550/arXiv.2302.04110)\n\n[38\\. 美国专家呼吁灵活的人工智能监管以促进发展](https://franktalknow.com/u-s-experts-urge-flexible-ai-regulation-to-spur-development/)\n\n[41\\. 欧盟人工智能监管实践](https://zhuanlan.zhihu.com/p/680218099)\n\n[42\\. Impact, Opportunity and Challenges of Generative AI](https://indiaai.s3.ap-south-1.amazonaws.com/docs/generative-ai-report.pdf)\n\n[43\\. EU and US AI Regulatory Push Overlaps Across Global Business](https://regulatingai.org/eu-and-us-ai-regulatory-push-overlaps-across-global-business/)\n\n[44\\. A Tale of Two Policies: The EU AI Act and the U.S. AI ... - Trilligent](https://trilligent.com/a-tale-of-two-policies-the-eu-ai-act-and-the-us-ai-executive-order-in-focus/)\n\n[45\\. Albane Gourdol, Katarzyna Kraszewska et al. “European Commission.” Integrating Environmental and Climate Action into Development Co-operation](https://doi.org/10.1787/9789264265189-13-en)\n\n[46\\. EVERYTHING YOU NEED TO KNOW ABOUT AI AND ITS REGULATION IN 15 MINUTES](https://www.drewnapier.com/DrewNapier/media/DrewNapier/Everything-you-need-to-know-about-AI-and-its-regulation-in-15-minutes.pdf)\n\n[47\\. 人工智能：人工智能开启创新发展新时代](https://www.nbdx.cn/module/download/downfile.jsp?classid=0&showname=%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%882024%E5%B9%B4%E7%AC%AC2%E6%9C%9F%EF%BC%89.pdf&filename=84850167a21b416baf9bddf0d75866ea.pdf)\n\n[48\\. ​ 欧盟人工智能法案获批之际,创作者要求对权利人提供更强...](https://mp.weixin.qq.com/s?__biz=MjM5NjE5NzE4NQ%3D%3D&mid=2651432279&idx=2&sn=21ab61827210b071a1672cfa26a74045&chksm=bd11e3798a666a6f4a4ee016f332868b45abfd16f8e6befabb0936f496b55b33d09ee1eb40fb&scene=27)\n\n[49\\. 【最新】美国发布AI相关行政令,以多项措施监管人工智能风险](https://mp.weixin.qq.com/s?__biz=MzU4NTU0NTQ1Ng%3D%3D&mid=2247551108&idx=2&sn=1ea4a9a63972970e265ee7e2376e763a&chksm=fd8aab1bcafd220da03053ea6ca9295dec0dafdc3daadb3c9879428d5542377ce129a88c89bb&scene=27)\n\n[50\\. EU Retains Role of Lead AI Regulator with Signing of EU AI Act](https://www.afslaw.com/perspectives/ai-law-blog/eu-retains-role-lead-ai-regulator-signing-eu-ai-act)\n\n[51\\. The EU Artificial Intelligence Act - Herzog's Guide](https://herzoglaw.co.il/wp-content/uploads/2024/03/The-AI-Act-Herzogs-Guide.pdf)\n\n[52\\. Comparing the EU AI Act to Proposed AI-Related Legislation in the US](https://businesslawreview.uchicago.edu/sites/default/files/2024-03/Sofia%20Gracias.pdf)\n\n[53\\. Policy Note on Artificial Intelligence](https://depdev.gov.ph/wp-content/uploads/2025/02/Policy-Note-on-Artificial-Intelligence.pdf)\n\n[54\\. 知识产权海外风险预警专刊](http://ipr.mofcom.gov.cn/hwwq_2/zhuankan/file/2024/2024-08.pdf)\n\n[55\\. EMOTIONAL AI AND DATA PROTECTION: RELEVANT IMPLICATIONS FOR OLDER ADULTS AND OTHER VULNERABLE SUBJECTS](https://revistaseug.ugr.es/index.php/delegeferenda/article/download/30946/28066/134797)\n\n[56\\. 美国白宫提出十条AI监管原则,避免政府过度干预AI发展 - 腾...](https://www.cloud.tencent.com/developer/news/568970)\n\n[57\\. The Tale of Two Approaches to Artificial Intelligence – EU AI Act & U.S. Executive Order on Safe, Secure, and Trustworthy AI](https://cybersecurityadvisors.network/2023/11/08/the-tale-of-two-approaches-ai/)\n\n[58\\. AI法案监管过严，阿斯麦等多家企业呼吁欧盟推迟实施](https://www.sohu.com/a/910359653_211762)\n\n[59\\. Comparative Table on Approaches to AI Regulation in the EU, US and UK](https://www.steptoe.com/a/web/mdWt44Hg7mmGXDAcLsAr7n/comparative-table-on-approaches-to-ai-regulation-in-the-eu-us-and-uk-2.pdf)\n\n[60\\. 拜登签署首份关于AI的行政令!美国将推动A技术全面监管](https://finance.sina.com.cn/jjxw/2023-10-31/doc-imzsxxcw8020217.shtml)\n\n[61\\. Albane Gourdol, Katarzyna Kraszewska et al. “European Commission.” Integrating Environmental and Climate Action into Development Co-operation](https://doi.org/10.1787/9789264265189-13-en)\n\n[62\\. 海内外人工智能监管政策进展几何？](https://pdf.dfcfw.com/pdf/H301_AP202304201585627803_1.pdf)\n\n[63\\. 欧美人工智能监管模式及政策启示](http://xxbzz.xxbcm.com/xxbzz/20250210/096.pdf)\n\n[64\\. China, EU are suggested to strengthen cooperation on AI ...](https://www.globaltimes.cn/page/202312/1303343.shtml)\n\n[65\\. 2023 人工智能发展白皮书 The White Book on the Development of Artificial Intelligence in 2023](https://www.shujiaowang.cn/uploads/20230910/efd63c1d975dd5940eb40dd53dd36365.pdf)\n\n[66\\. A. Bradford. “The Brussels Effect.” The Brussels Effect](https://doi.org/10.1093/oso/9780190088583.001.0001)\n\n[67\\. New AI Legal Developments in the U.S., the EU, the UK, and...](https://mp.weixin.qq.com/s?__biz=MzI0NzQ0OTUwNg%3D%3D&mid=2247485561&idx=1&sn=20cbb6b6cc83446837f7d1d8a0ec4028&chksm=e9ae9ab4ded913a26e63c65d9ddfe7a18294faa7618b424a9ce64a78b69476d60c2113762f9a&scene=27)\n\n[68\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2024.” ArXiv](https://doi.org/10.48550/arXiv.2405.19522)\n\n[69\\. AI Financing Declining in China, Widening Gap in AI Between...](https://www.tmtpost.com/baidu/6939710.html)\n\n[70\\. Hu Zhong, Eamonn O'Neill et al. “Regulating AI: Applying Insights from Behavioural Economics and Psychology to the Application of Article 5 of the EU AI Act.” AAAI Conference on Artificial Intelligence](https://doi.org/10.1609/aaai.v38i18.29977)\n\n[71\\. Sophia Hatz, Noemi Dreksler et al. “Local US officials' views on the impacts and governance of AI: Evidence from 2022 and 2023 survey waves.”](https://arxiv.org/abs/2501.09606)\n\n[72\\. 台媒：中美AI比一比，中国未来具优势](https://www.bilibili.com/video/av113991888143249)\n\n[73\\. AI专利中国是美国6倍，AI企业，美国是中国2倍！](https://www.bilibili.com/video/av112766513842230)\n\n[75\\. European AI Standards – Technical Standardization and Implementation Challenges under the EU AI Act](https://ki-verband.de/wp-content/uploads/2025/03/Study_European-AI-Standards_FINAL_20250325.pdf)\n\n[76\\. European Union Artificial Intelligence Act: ガイド](https://www.twobirds.com/-/media/new-website-content/pdfs/capabilities/artificial-intelligence/eu-ai-act-guide-japanese-version.pdf)\n\n[77\\. EU AI Act Update: New Watermarking Requirements for AI- ...](https://www.imatag.com/blog/eu-ai-act-update-new-watermarking-requirements-for-ai-generated-content)\n\n[78\\. The EU AI Act Regulation (EU) 2024/1689 English / German](https://assets.contentstack.io/v3/assets/blt3de4d56151f717f2/bltdcbb64d7c8139417/EU%20AI%20Act%20-%20%20English%20German.pdf)\n\n[79\\. The EU AI Act: Guide for In-House Lawyers](https://www.hunton.com/assets/htmldocuments/ai-act-guide.pdf)\n\n[80\\. Implementation Requirements for EU AI Act](https://www.hopsworks.ai/faq-ai-act/implementation-requirements-for-eu-ai-act-and-how-to-approach-them)\n\n[81\\. European Union Artificial Intelligence Act: a guide](https://www.twobirds.com/-/media/new-website-content/pdfs/capabilities/artificial-intelligence/european-union-artificial-intelligence-act-guide.pdf)\n\n[82\\. The EU AI Act Compliance through Observability](https://newrelic.com/blog/best-practices/the-eu-artificial-intelligence-act-and-observability)\n\n[83\\. Position: Technical Research and Talent is Needed for Effective AI Governance](https://openreview.net/pdf?id=Be2B6f0ps1)\n\n[84\\. Harmonised Standards for the European AI Act](https://publications.jrc.ec.europa.eu/repository/bitstream/JRC139430/JRC139430_01.pdf)\n\n[85\\. Massive lost mountain cities revealed by lasers](https://www.nature.com/articles/d41586-024-03463-6)\n\n[86\\. EU AI Act: Final Text Published in the Official Journal of the EU with Interactive Table of Contents](https://ai-regulation.com/wp-content/uploads/2025/02/EU-AI-Act-Interactive-ToC.pdf)\n\n[87\\. Add blogpost \"Open Source Developers Guide to the EU AI Act\" (](https://github.com/ruanhuhu/huggingface-blog/commit/e3e672ed7183fc5eeb1b95af261b1e8804dbbd65)\n\n[88\\. The AI Act's AI Watermarking Requirement Is a Misstep in the ...](https://datainnovation.org/2024/07/the-ai-acts-ai-watermarking-requirement-is-a-misstep-in-the-quest-for-transparency/#:~:text=The%20AI%20Act,%20formally%20adopted,concerns%20like%20deepfakes%20and%20misinformation.)\n\n[89\\. The EU Artificial Intelligence Act - Herzog's Guide](https://herzoglaw.co.il/wp-content/uploads/2024/03/The-AI-Act-Herzogs-Guide.pdf)\n\n[90\\. Generative AI and watermarking - European Sources Online](https://www.europeansources.info/record/generative-ai-and-watermarking/)\n\n[91\\. The Tale of Two Approaches to Artificial Intelligence – EU AI Act & U.S. Executive Order on Safe, Secure, and Trustworthy AI](https://cybersecurityadvisors.network/2023/11/08/the-tale-of-two-approaches-ai/)\n\n[92\\. 10: EU AI Act – What are the obligations for “high-risk AI ...](https://www.aoshearman.com/en/insights/ao-shearman-on-tech/zooming-in-on-ai-10-eu-ai-act-what-are-the-obligations-for-high-risk-ai-systems)\n\n[95\\. Artificial Intelligence, EU Regulation and Competition Law ...](https://www.quinnemanuel.com/the-firm/publications/artificial-intelligence-eu-regulation-and-competition-law-enforcement-addressing-emerging-challenges/)\n\n[96\\. AI Act and the Prohibition of Real-Time Biometric ...](https://verfassungsblog.de/ai-act-and-the-prohibition-of-real-time-biometric-identification/)\n\n[97\\. Brussels is proposing the first European legal framework for the use of artificial intelligence](https://dih.um.si/en/brussels-is-proposing-the-first-european-legal-framework-for-the-use-of-artificial-intelligence/)\n\n[98\\. The EU AI Act Regulation (EU) 2024/1689 English / German](https://assets.contentstack.io/v3/assets/blt3de4d56151f717f2/bltdcbb64d7c8139417/EU%20AI%20Act%20-%20%20English%20German.pdf)\n\n[99\\. Navigating data governance risks: Facial recognition in law enforcement under EU legislation](https://policyreview.info/pdf/policyreview-2024-3-1798.pdf)\n\n[100\\. Artificial Intelligence Act: what is the European Union regulating?](https://wikimedia.brussels/artificial-intelligence-act-what-is-the-european-union-regulating/)\n\n[101\\. Lei da UE sobre IA: primeira regulamentação de inteligência artificial](https://www.europarl.europa.eu/pdfs/news/expert/2023/6/story/20230601STO93804/20230601STO93804_pt.pdf)\n\n[102\\. How the European Union's AI Act Provides Insufficient ...](https://www.law.upenn.edu/live/news/16742-how-the-european-unions-ai-act-provides)\n\n[103\\. The EU AI Act: What It Means for Your Compliance](https://www.centraleyes.com/the-eu-ai-act/)\n\n[104\\. The EU AI Act: State of Play and the Potential for Regulatory Globalisation](https://www.cife.eu/Ressources/FCK/files/publications/policy%20paper/2024/Alma_2024_EU_AI_Act_Policy_Paper_CIFE.pdf)\n\n[105\\. AI Act: how the EU wants to put artificial intelligence in its place](https://indico.cern.ch/event/1426918/attachments/2875787/5054819/Digitec-AI-EU.docx)\n\n[106\\. Artificial Intelligence: Legal Challenges and Emerging Solutions](https://www.hansonbridgett.com/sites/default/files/2024-03/Presentation_2024-AI-Seminar-Slides_3-19-2024.pdf)\n\n[107\\. EU Parliament Contemplates Limited Use of Real-Time Biometric Tech Under AI Act](https://dig.watch/updates/eu-parliament-contemplates-limited-use-of-real-time-biometric-tech-under-ai-act)\n\n[108\\. The EU Artificial Intelligence Act: our 16 key takeaways](https://www.stibbe.com/publications-and-insights/the-eu-artificial-intelligence-act-our-16-key-takeaways)\n\n[109\\. EU AI Act: first regulation on artificial intelligence](https://www.europarl.europa.eu/pdfs/news/expert/2023/6/story/20230601STO93804/20230601STO93804_en.pdf)\n\n[110\\. The EU AI Act: key impacts for the Public Sector](https://www.eu-online-academy.org/EOA/Ressources/FCK/files/Master%20Thesis%20Final%20Paper/2024%20Van%20Pinxteren%20The%20AI%20Act%20Key%20impacts%20for%20the%20Public%20Sector.pdf)\n\n[111\\. CEN CWA 18028 WORKSHOP AGREEMENT](https://www.cencenelec.eu/media/CEN-CENELEC/CWAs/RI/cwa18028.pdf)\n\n[112\\. Biometrics under the EU AI Act](https://www.twobirds.com/en/insights/2023/global/biometrics-under-the-eu-ai-act)\n\n[113\\. The Human Rights Risks of Facial Recognition AI Tech in Policing and Immigration Must be Properly Recognised in the EU AI Act](https://cdt.org/wp-content/uploads/2023/02/CDT-Europe-Facial-recognition-EU-AI-Act-issue-brief.pdf)\n\n[115\\. Miles Brundage, S. Avin et al. “The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation.” ArXiv](https://doi.org/10.17863/CAM.22520)\n\n[116\\. Investing in the Future: M31 Capital Gathers 300 Pioneering ...](https://www.m31capital.com/investing-in-the-future-m31-capital-gathers-300-pioneering-minds-to-unlock-cross-border-opportunities/)\n\n[117\\. Vincent Mortier: U.S. Exceptionalism Faces Valuation Risks ...](https://www.21jingji.com/article/20250722/herald/3e4bd19a3b87382feae4e1d6e7f4d16a.html)\n\n[118\\. Iyad Rahwan. “Society-in-the-loop: programming the algorithmic social contract.” Ethics and Information Technology](https://doi.org/10.1007/s10676-017-9430-8)\n\n[119\\. Sophia Hatz, Noemi Dreksler et al. “Local US officials' views on the impacts and governance of AI: Evidence from 2022 and 2023 survey waves.”](https://arxiv.org/abs/2501.09606)\n\n[120\\. China promotes coordination of AI governance - Chinada...](http://www.chinadaily.com.cn/a/202407/02/WS668369c2a31095c51c50be31.html)\n\n[121\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2024.” ArXiv](https://doi.org/10.48550/arXiv.2405.19522)\n\n[122\\. 人工智能全域变革图景展望：跃迁点来临（2023）](https://www.bing.com/ck/a?!&p=87c49edad300a5a4bdc7e4008e77c1db7bca69baba58fb50526986556ce30516JmltdHM9MTc0Nzg3MjAwMA&ptn=3&ver=2&hsh=4&fclid=1c300984-fc54-639a-12da-1c71fd4f62df&u=a1aHR0cHM6Ly9hc3NldHMua3BtZy5jb20vY29udGVudC9kYW0va3BtZy9jbi9wZGYvemgvMjAyMy8xMi9wcm9zcGVjdHMtZm9yLXRoZS1nbG9iYWwtdHJhbnNmb3JtYXRpb24tb2YtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UucGRm)\n\n[123\\. Defining moment - Opinion](https://www.chinadaily.com.cn/a/202507/23/WS6880277ea310ad07b5d915e5.html)\n\n[124\\. 韩媒：全球AI投资62%集中在美国，欧盟8%，中国呢？](https://www.bilibili.com/video/av1906441632)\n\n[125\\. Robot Maker Deep Robotics Completes Series B+ Funding...](https://www.tmtpost.com/7206115.html)\n\n[126\\. Robot Maker Deep Robotics Completes Series B+ Funding ...](https://new.qq.com/rain/a/20240812A04VZN00)\n\n[127\\. 2023 人工智能发展白皮书 The White Book on the Development of Artificial Intelligence in 2023](https://www.shujiaowang.cn/uploads/20230910/efd63c1d975dd5940eb40dd53dd36365.pdf)\n\n[131\\. Margaret Mitchell, Simone Wu et al. “Model Cards for Model Reporting.” Proceedings of the Conference on Fairness, Accountability, and Transparency](https://doi.org/10.1145/3287560.3287596)\n\n[132\\. Sandra Wachter, B. Mittelstadt et al. “Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation.” International Data Privacy Law](https://doi.org/10.2139/SSRN.2903469)\n\n[133\\. M. Cannarsa. “Ethics Guidelines for Trustworthy AI.” The Cambridge Handbook of Lawyering in the Digital Age](https://doi.org/10.1017/9781108936040.022)\n\n[134\\. Artificial Intelligence Act: committees confirm landmark ...](https://www.ciplawyer.cn/html_e/Highlights/20240314/153077.shtml)\n\n[135\\. Francesco Sovrano, Salvatore Sapienza et al. “Metrics, Explainability and the European AI Act Proposal.” J](https://doi.org/10.3390/j5010010)\n\n[136\\. L. Floridi. “Translating Principles into Practices of Digital Ethics: Five Risks of Being Unethical.” Philosophy & Technology](https://doi.org/10.1007/s13347-019-00354-x)\n\n[137\\. T. Gils, Frederic Heymans et al. “From Policy to Practice: Prototyping The EU AI Act’s Transparency Requirements.” SSRN Electronic Journal](https://doi.org/10.2139/ssrn.4714345)\n\n[138\\. Xuhong Wang, Haoyu Jiang et al. “Building Intelligence Identification System via Large Language Model Watermarking: A Survey and Beyond.”](https://arxiv.org/abs/2407.11100)\n\n[139\\. Bill Marino, Preslav Aleksandrov et al. “Compliance Cards: Computational Artifacts for Automated AI Regulation Compliance.” ArXiv](https://doi.org/10.48550/arXiv.2406.14758)\n\n[140\\. Bill Marino, Yaqub Chaudhary et al. “Compliance Cards: Automated EU AI Act Compliance Analyses amidst a Complex AI Supply Chain.”](https://arxiv.org/abs/2406.14758)\n\n[142\\. The EU AI Act Regulation (EU) 2024/1689 English / German](https://assets.contentstack.io/v3/assets/blt3de4d56151f717f2/bltdcbb64d7c8139417/EU%20AI%20Act%20-%20%20English%20German.pdf)\n\n[143\\. EU clarifies AI model thresholds in new regulatory guidelines](https://ppc.land/eu-clarifies-ai-model-thresholds-in-new-regulatory-guidelines/)\n\n[144\\. Landmark Agreement on EU AI Act](https://www.lumenova.ai/blog/eu_ai_act/)\n\n[145\\. Lei da UE sobre IA: primeira regulamentação de inteligência artificial](https://www.europarl.europa.eu/pdfs/news/expert/2023/6/story/20230601STO93804/20230601STO93804_pt.pdf)\n\n[146\\. AI Act and the Prohibition of Real-Time Biometric ...](https://verfassungsblog.de/ai-act-and-the-prohibition-of-real-time-biometric-identification/)\n\n[147\\. News on the AI Act: Logbook on the planned EU Regulation](https://www.srd-rechtsanwaelte.de/en/blog/ai-act-news)\n\n[148\\. The EU AI Act: Political Agreement reached on EU Artificial Intelligence Act](https://www.csbgroup.com/malta-news/the-eu-ai-act-political-agreement-reached-on-eu-artificial-intelligence-act/)\n\n[149\\. The EU Artificial Intelligence Act: our 16 key takeaways](https://www.stibbe.com/publications-and-insights/the-eu-artificial-intelligence-act-our-16-key-takeaways)\n\n[150\\. The EU Agrees on a Path Forward for the AI Act - Gibson Dunn](https://www.gibsondunn.com/eu-agrees-on-a-path-forward-for-the-ai-act/)\n\n[151\\. Pioneering the Future: The European Union’s Regulatory Framework for Artificial Intelligence](https://taoma-partners.fr/en/category/it/)\n\n[152\\. The Human Rights Risks of Facial Recognition AI Tech in Policing and Immigration Must be Properly Recognised in the EU AI Act](https://cdt.org/wp-content/uploads/2023/02/CDT-Europe-Facial-recognition-EU-AI-Act-issue-brief.pdf)\n\n[153\\. EU AI Regulation - Usercentrics Guide To The EU AI Act](https://usercentrics.com/knowledge-hub/artificial-intelligence-ai-and-consent/)\n\n[154\\. AI and Systemic Oversight Mechanisms in Criminal Justice](https://www.lco-cdo.org/wp-content/uploads/2025/04/LCO-AI-in-Criminal-Justice-Paper-5-Systemic-Oversight-1.pdf)\n\n[155\\. EU AI Act: the world’s first legal framework for artificial intelligence](https://gsk.de/wp-content/uploads/2024/03/GSK-Update_EU-KI-Verordnung_EN.pdf)\n\n[156\\. The AI Act: deregulation in disguise](https://www.socialeurope.eu/the-ai-act-deregulation-in-disguise)\n\n[157\\. EU AI Act - Mifsud & Mifsud Advocates](https://www.mifsudadvocates.com.mt/eu-ai-act/)\n\n[158\\. European Parliament - Spokesperson, Jaume Duch Guillot. “EU AI Act: first regulation on artificial intelligence.”](https://www.semanticscholar.org/paper/80527ed02db8fe7574f676ed2aa573eb1ae252a0)\n\n[159\\. Artificial Intelligence, EU Regulation and Competition Law ...](https://www.quinnemanuel.com/the-firm/publications/artificial-intelligence-eu-regulation-and-competition-law-enforcement-addressing-emerging-challenges/)\n\n[162\\. China to overtake Europe in AI research](https://english.www.gov.cn/news/top_news/2018/12/13/content_281476431771924.htm)\n\n[163\\. The Global AI Talent Tracker 2.0](https://mp.weixin.qq.com/s?__biz=MzA3NDI1NDk3Mg%3D%3D&mid=2650145922&idx=2&sn=c74dc0ffbdcd4dfafa27105d35746c11&chksm=87003b91b077b287826a3765231212e4d760e6fea7f63d6dc00499869ab3dc3ba2d7f7b33334&scene=27)\n\n[164\\. Artificial Intelligence Index Report 2023](https://event-cdn.baai.ac.cn/file/file-browser/C7FA4aFhrT2Hrnm77AKZPww62Ywm7Pyk.pdf)\n\n[165\\. China Narrows AI Talent Gap With U.S. as Research Enters...](http://applocal.myzaker.com/news/article.php?pk=686793688e9f096bd063a038)\n\n[166\\. China promotes coordination of AI governance](http://epaper.chinadaily.com.cn/a/202407/02/WS66833439a3106431fe82cd06.html)\n\n[167\\. Sophia Hatz, Noemi Dreksler et al. “Local US officials' views on the impacts and governance of AI: Evidence from 2022 and 2023 survey waves.”](https://arxiv.org/abs/2501.09606)\n\n[168\\. ChinaJOB - Working in China,Start Here! - Home](https://www.chinajob.com/)\n\n[169\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2024.” ArXiv](https://doi.org/10.48550/arXiv.2405.19522)\n\n[170\\. 人工智能全域变革图景展望：跃迁点来临（2023）](https://runwise.oss-accelerate.aliyuncs.com/sites/15/2023/12/2023-12-22-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%85%A8%E5%9F%9F%E5%8F%98%E9%9D%A9%E5%9B%BE%E6%99%AF%E5%B1%95%E6%9C%9B%EF%BC%9A%E8%B7%83%E8%BF%81%E7%82%B9%E6%9D%A5%E4%B8%B4%EF%BC%882023%EF%BC%89-%E6%AF%95%E9%A9%AC%E5%A8%81.pdf)\n\n[171\\. The LinkedIn AI Talent Index: Tracking the global AI talent ecosystem](https://economicgraph.linkedin.com/content/dam/me/economicgraph/en-us/PDF/li-ai-talent-index.pdf)\n\n[182\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[183\\. European AI Act: Mandatory Labeling for AI-Generated ...](https://www.imatag.com/blog/ai-act-legal-requirement-to-label-ai-generated-content)\n\n[184\\. AI as a Public Good: Ensuring Democratic Control of AI in the Information Space](https://informationdemocracy.org/wp-content/uploads/2024/03/ID-AI-as-a-Public-Good-Feb-2024.pdf)\n\n[185\\. Navigating the EU AI Act: Proposed Compliance Measures for AI Providers and Deployers](https://epub.jku.at/download/pdf/10576761.pdf)\n\n[186\\. The AI Act's AI Watermarking Requirement Is a Misstep in the ...](https://datainnovation.org/2024/07/the-ai-acts-ai-watermarking-requirement-is-a-misstep-in-the-quest-for-transparency/#:~:text=The%20AI%20Act,%20formally%20adopted,concerns%20like%20deepfakes%20and%20misinformation.)\n\n[187\\. Margaret Mitchell, Simone Wu et al. “Model Cards for Model Reporting.” Proceedings of the Conference on Fairness, Accountability, and Transparency](https://doi.org/10.1145/3287560.3287596)\n\n[188\\. ByteDance Launches OmniHuman-1: AI Tool Turns Photos ...](https://opentools.ai/news/bytedance-launches-omnihuman-1-ai-tool-turns-photos-into-lifelike-videos)\n\n[189\\. EU AI Act Update: New Watermarking Requirements for AI- ...](https://www.imatag.com/blog/eu-ai-act-update-new-watermarking-requirements-for-ai-generated-content)\n\n[190\\. AI-generated journalism: Do the transparency provisions in the AI Act give news readers what they hope for?](https://policyreview.info/pdf/policyreview-2024-4-1810.pdf)\n\n[191\\. AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap](https://hdsr.mitpress.mit.edu/pub/aelql9qy/download/pdf)\n\n[192\\. European Union Artificial Intelligence Act: ガイド](https://www.twobirds.com/-/media/new-website-content/pdfs/capabilities/artificial-intelligence/eu-ai-act-guide-japanese-version.pdf)\n\n[193\\. E. Mitchell, Yoonho Lee et al. “DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature.” International Conference on Machine Learning](https://doi.org/10.48550/arXiv.2301.11305)\n\n[194\\. Vinu Sankar Sadasivan, Aounon Kumar et al. “Can AI-Generated Text be Reliably Detected?.” ArXiv](https://doi.org/10.48550/arXiv.2303.11156)\n\n[195\\. Watermarking in Images Will Not Solve AI-Generated ...](https://itif.org/publications/2024/08/15/watermarking-images-will-not-solve-ai-generated-content-abuse/)\n\n[196\\. EU AI Act: Navigating a Brave New World](https://www.lw.com/admin/upload/SiteAttachments/EU-AI-Act-Navigating-a-Brave-New-World.pdf)\n\n[197\\. Generative AI and watermarking](https://www.europarl.europa.eu/RegData/etudes/BRIE/2023/757583/EPRS_BRI%282023%29757583_EN.pdf)\n\n[202\\. Artificial Intelligence, EU Regulation and Competition Law ...](https://www.quinnemanuel.com/the-firm/publications/artificial-intelligence-eu-regulation-and-competition-law-enforcement-addressing-emerging-challenges/)\n\n[203\\. Commission Guidelines on prohibited artificial intelligence practices established by Regulation (EU) 2024/1689 (AI Act)](https://www.key4biz.it/wp-content/uploads/2025/02/C_2025_884_1_EN_annexe_acte_autonome_cp_part1_v4_4YAkRZf5bMIWOuMqiKMIGySPo_112367.pdf)\n\n[204\\. Artificial intelligence at EU borders: Overview of applications and key issues](https://www.europarl.europa.eu/RegData/etudes/IDAN/2021/690706/EPRS_IDA%282021%29690706_EN.pdf)\n\n[205\\. 欧盟人工智能法案的最终协议达成](https://www.dataprotectionreport.com/2023/12/the-eus-ai-act-the-position-is-agreed/)\n\n[206\\. CEN CWA 18028 WORKSHOP AGREEMENT](https://www.cencenelec.eu/media/CEN-CENELEC/CWAs/RI/cwa18028.pdf)\n\n[207\\. The EU Agrees on a Path Forward for the AI Act - Gibson Dunn](https://www.gibsondunn.com/eu-agrees-on-a-path-forward-for-the-ai-act/)\n\n[208\\. The EU AI Act Regulation (EU) 2024/1689 English / German](https://assets.contentstack.io/v3/assets/blt3de4d56151f717f2/bltdcbb64d7c8139417/EU%20AI%20Act%20-%20%20English%20German.pdf)\n\n[209\\. The EU Artificial Intelligence Act: our 16 key takeaways](https://www.stibbe.com/publications-and-insights/the-eu-artificial-intelligence-act-our-16-key-takeaways)\n\n[210\\. Understanding the impact of the AI act for Law Enforcement in 7 points and ...](https://www.timelex.eu/en/blog/understanding-impact-ai-act-law-enforcement-7-points-and-7-key-takeaways#:~:text=Notably,%20the%20AI%20Act%20makes,an%20offender%20or%20re-offending.)\n\n[211\\. EU AI Act: Final Text Published in the Official Journal of the EU with Interactive Table of Contents](https://ai-regulation.com/wp-content/uploads/2025/02/EU-AI-Act-Interactive-ToC.pdf)\n\n[212\\. Article 5: Prohibited Artificial Intelligence Practices | EU AI Act](https://securiti.ai/eu-ai-act/article-5/)\n\n[213\\. Amendment 808 Artificial Intelligence Act](https://www.europarl.europa.eu/doceo/document/A-9-2023-0188-AM-808-808_EN.pdf)\n\n[214\\. A. Juels, M. Wattenberg. “A fuzzy commitment scheme.” Conference on Computer and Communications Security](https://doi.org/10.1145/319709.319714)\n\n[215\\. First Milestone in the Implementation of the EU AI Act...](https://www.alstonprivacy.com/first-milestone-in-the-implementation-of-the-eu-ai-act/)\n\n[216\\. The AI Act: deregulation in disguise](https://www.socialeurope.eu/the-ai-act-deregulation-in-disguise)\n\n[217\\. HITRUST AI 安全规范草案反馈](https://blog.stackaware.com/p/hitrust-ai-security-specification-technical-controls-specifications-risks)\n\n[218\\. AI Act and the Prohibition of Real-Time Biometric ...](https://verfassungsblog.de/ai-act-and-the-prohibition-of-real-time-biometric-identification/)\n\n[219\\. The EU AI Act: Political Agreement reached on EU Artificial Intelligence Act](https://www.csbgroup.com/malta-news/the-eu-ai-act-political-agreement-reached-on-eu-artificial-intelligence-act/)\n\n[220\\. European Parliament - Spokesperson, Jaume Duch Guillot. “EU AI Act: first regulation on artificial intelligence.”](https://www.semanticscholar.org/paper/80527ed02db8fe7574f676ed2aa573eb1ae252a0)\n\n[222\\. Understanding the Future of Artificial Intelligence Governance: Comparing the EU AI Act and U.S. Executive Order on Safe AI](https://stpp.fordschool.umich.edu/sites/stpp/files/2024-06/stpp-future-of-ai-governance.pdf)\n\n[223\\. 一、美國AI行政命令與歐盟AI法草案之管理差異](https://web.wtocenter.org.tw/Page/17435/391300)\n\n[224\\. Generative AI: Europe’s Quest for Regulation and Industry Leadership](https://www.eitdigital.eu/fileadmin/2024/ecosystem/downloads/EIT-Digital_AiNed_Generative-AI-Report-web.pdf)\n\n[225\\. The EU AI Act and the Shifting Global AI Policy Landscape](https://www.conference-board.org/pdfdownload.cfm?masterProductID=57243)\n\n[226\\. CHARTING PATHS FOR A GLOBAL GOVERNANCE OF AI](https://static.ie.edu/CGC/CGC-policy-brief_Global-Governance-of-AI_Final.pdf)\n\n[227\\. Making the most of the AI opportunity: The challenges of regulating AI](https://www.pc.gov.au/research/completed/making-the-most-of-the-ai-opportunity/ai-paper2-regulating.pdf)\n\n[228\\. Comparing the EU AI Act to Proposed AI-Related Legislation in the US](https://businesslawreview.uchicago.edu/sites/default/files/2024-03/Sofia%20Gracias.pdf)\n\n[229\\. Additional Comments on the “White Paper: On Artificial Intelligence - A European approach to excellence and trust”.](https://www.semanticscholar.org/paper/b24b06069392197c5835983f4aba41853fbc1b62)\n\n[230\\. International Talent Flows to the United States](https://nap.nationalacademies.org/resource/27787/Neufeld_and_Kaushik_ITP_Commissioned_Paper.pdf)\n\n[231\\. Global AI Governance: Key Steps for Transatlantic Cooperation](https://www.gmfus.org/sites/default/files/2024-11/ECA%20AI%20POLICY%20REPORT%20VER%206%5B46%5D.pdf)\n\n[232\\. Biden's Executive Order on AI vs the EU's AI Act](https://commongroundeurope.eu/blog/bidens-executive-order-on-ai-and-the-e-u-s-ai-act-a-comparative-analysis/)\n\n[233\\. Successful and timely uptake of Artificial Intelligence in science in the EU](https://www.kowi.de/Portaldata/2/Resources/HEU/sam-scientific-opinion-artificial-intelligence.pdf)\n\n[234\\. Foreshadowing Biden’s AI Executive Order](https://www.mintz.com/insights-center/viewpoints/2191/2023-10-20-foreshadowing-bidens-ai-executive-order-ai-washington)\n\n[235\\. Navigating the EU AI Act: Proposed Compliance Measures for AI Providers and Deployers](https://epub.jku.at/download/pdf/10576761.pdf)\n\n[236\\. STABILITY AND ADAPTABILITY Selected doctoral studies 'ÁLLANDÓSÁG ÉS ALKALMAZKODÁS' Válogatott doktorandusz tanulmányok](https://publikacio.ppke.hu/id/eprint/1477/1/Allandosag_es_alakalmazkodas_Dokt_Tan_09.pdf)\n\n[237\\. Navigating the New Frontier: Safeguarding Student Data in AI-Driven Education Systems Post-February 2nd, 2025 EU AI Act Regulations integrating UNESCO’s AI frameworks](https://ai4edu.eu/wp-content/uploads/2025/04/9a.-Safeguarding-student-data-in-AI-driven-education-EUAI-ACt-and-UNESCOs-Frameworks.pdf)\n\n[238\\. Data Protection & Privacy 2024](https://practiceguides.chambers.com/practice-guides/data-protection-privacy-2024)\n\n[242\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[243\\. I. Cox, J. Kilian et al. “Secure spread spectrum watermarking for multimedia.” IEEE transactions on image processing : a publication of the IEEE Signal Processing Society](https://doi.org/10.1109/83.650120)\n\n[244\\. Navigating the EU AI Act: Proposed Compliance Measures for AI Providers and Deployers](https://epub.jku.at/download/pdf/10576761.pdf)\n\n[245\\. I. Cox, J. Linnartz. “Some general methods for tampering with watermarks.” IEEE J. Sel. Areas Commun.](https://doi.org/10.1109/49.668980)\n\n[246\\. Hugging Face Information for NIST “Related to NIST’s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence”](https://downloads.regulations.gov/NIST-2023-0009-0149/attachment_1.pdf)\n\n[247\\. F. Petitcolas, Ross J. Anderson et al. “Attacks on Copyright Marking Systems.” Information Hiding](https://doi.org/10.1007/3-540-49380-8_16)\n\n[248\\. Sztuczna inteligencja w systemach radiowych – I](https://krit2024.put.poznan.pl/assets/doc/Przeglad%20Telekomunikacyjny%20sesje%20zoptymali.pdf)\n\n[249\\. I. Cox, J. Kilian et al. “A Secure, Robust Watermark for Multimedia.” Information Hiding](https://doi.org/10.1007/3-540-61996-8_41)\n\n[250\\. A hitchhiker’s guide to white-box neural network watermarking robustness](https://hal.science/hal-04230306/document)\n\n[251\\. AI RMF 1.0 Controls Checklist](https://www.aigl.blog/ai-rmf-1-0-controls-checklist/)\n\n[252\\. EU AI Act Update: New Watermarking Requirements for AI- ...](https://www.imatag.com/blog/eu-ai-act-update-new-watermarking-requirements-for-ai-generated-content)\n\n[253\\. Watermarks are Just One of Many Tools Needed for Effective Use of AI in News](https://innovating.news/wp-content/uploads/2024/04/CNTI_Watermarks_are_Just_One_of_Many_Tools_Needed.pdf)\n\n[254\\. P. Termont, L. D. Strycker et al. “Performance measurements of a real-time digital watermarking system for broadcast monitoring.” Proceedings IEEE International Conference on Multimedia Computing and Systems](https://doi.org/10.1109/MMCS.1999.778284)\n\n[255\\. T. Kalker, G. Depovere et al. “Video watermarking system for broadcast monitoring.” Electronic imaging](https://doi.org/10.1117/12.344661)\n\n[256\\. NIST Trustworthy and Responsible AI NIST AI 100-2e2025 Adversarial Machine Learning A Taxonomy and Terminology of Attacks and Mitigations](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2025.pdf)\n\n[257\\. European Union Artificial Intelligence Act: a guide](https://www.twobirds.com/-/media/new-website-content/pdfs/capabilities/artificial-intelligence/european-union-artificial-intelligence-act-guide.pdf)\n\n[258\\. How do you know if content is AI-created ? Generative AI ...](https://www.lexology.com/library/detail.aspx?g=8e996bd8-19c0-402b-af4a-283ec152d201)\n\n[259\\. The AI Act's AI Watermarking Requirement Is a Misstep in the ...](https://datainnovation.org/2024/07/the-ai-acts-ai-watermarking-requirement-is-a-misstep-in-the-quest-for-transparency/#:~:text=The%20AI%20Act,%20formally%20adopted,concerns%20like%20deepfakes%20and%20misinformation.)\n\n[262\\. The EU AI Act Regulation (EU) 2024/1689 English / German](https://assets.contentstack.io/v3/assets/blt3de4d56151f717f2/bltdcbb64d7c8139417/EU%20AI%20Act%20-%20%20English%20German.pdf)\n\n[263\\. Michael Veale, Frederik J. Zuiderveen Borgesius. “Demystifying the Draft EU Artificial Intelligence Act — Analysing the good, the bad, and the unclear elements of the proposed approach.” Computer Law Review International](https://doi.org/10.9785/cri-2021-220402)\n\n[264\\. NIST Contribution to ISO/IEC JTC1 SC37 WG 6, 24779: Pictograms, Icons and Symbols for use with Biometric Systems](https://www.nist.gov/document/nist-contribution-isoiec-jtc1-sc27-wg6-icons-symbols-and-pictograms-use-biometric-systems)\n\n[265\\. Commission Guidelines on prohibited artificial intelligence practices established by Regulation (EU) 2024/1689 (AI Act)](https://www.key4biz.it/wp-content/uploads/2025/02/C_2025_884_1_EN_annexe_acte_autonome_cp_part1_v4_4YAkRZf5bMIWOuMqiKMIGySPo_112367.pdf)\n\n[266\\. Artificial Intelligence, EU Regulation and Competition Law ...](https://www.quinnemanuel.com/the-firm/publications/artificial-intelligence-eu-regulation-and-competition-law-enforcement-addressing-emerging-challenges/)\n\n[267\\. Proposition de Règlement du Parlement européen et du Conseil établissant des règles harmonisées concernant l'intelligence artificielle (législation sur l'intelligence artificielle, AIA) et modifiant certains actes législatifs de l'Union - Texte de compromis de la présidence - Version consolidée](https://artificialintelligenceact.eu/wp-content/uploads/2022/06/AIA-FRA-Consolidated-Version-15-June.pdf)\n\n[268\\. EU AI Regulation](https://compliance-made-simple.ch/ai-regulation/)\n\n[269\\. I. Hupont, Songül Tolan et al. “The landscape of facial processing applications in the context of the European AI Act and the development of trustworthy systems.” Scientific Reports](https://doi.org/10.1038/s41598-022-14981-6)\n\n[270\\. The Human Rights Risks of Facial Recognition AI Tech in Policing and Immigration Must be Properly Recognised in the EU AI Act](https://cdt.org/wp-content/uploads/2023/02/CDT-Europe-Facial-recognition-EU-AI-Act-issue-brief.pdf)\n\n[271\\. The pre-final text of the EU’s AI Act leaked online](https://service.betterregulation.com/sites/default/files/2024-02/whitecase.com-The%20pre-final%20text%20of%20the%20EUs%20AI%20Act%20leaked%20online.pdf)\n\n[272\\. The EU AI Act](https://assets.contentstack.io/v3/assets/blt3de4d56151f717f2/blt1414a3a56ef681b9/EU%20AI%20Act%20-%20%20English%20Dutch.pdf)\n\n[273\\. 欧盟人工智能法案(中、英对照)(目录、1-5条)](https://www.douban.com/note/865686695/)\n\n[274\\. Artificial Intelligence Act](https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.docx)\n\n[275\\. AI Act and the Prohibition of Real-Time Biometric ...](https://verfassungsblog.de/ai-act-and-the-prohibition-of-real-time-biometric-identification/)\n\n[276\\. Artificial Intelligence – Questions and Answers](https://ec.europa.eu/commission/presscorner/api/files/document/print/en/qanda_21_1683/QANDA_21_1683_EN.pdf)\n\n[282\\. AI in the EU: 2024 Trends and Insights from LinkedIn](https://economicgraph.linkedin.com/content/dam/me/economicgraph/en-us/PDF/AI-in-the-EU-Report.pdf)\n\n[283\\. Work Change Report: AI Is Coming to Work](https://economicgraph.linkedin.com/content/dam/me/economicgraph/en-us/PDF/Work-Change-Report.pdf)\n\n[284\\. China important AI professionals' destination: Report...](http://www.chinadaily.com.cn/business/tech/2017-07/07/content_30027334.htm)\n\n[285\\. 国外行业热点洞察](https://ydma.oss-cn-shanghai.aliyuncs.com/chinade100/LearningMaterials/20240611-%E6%95%B0%E7%99%BE%E4%BC%9A%E5%9B%BD%E5%A4%96%E8%A1%8C%E4%B8%9A%E7%83%AD%E7%82%B9%E6%B4%9E%E5%AF%9F%EF%BC%882024%E5%B9%B4%E7%AC%AC17%E6%9C%9F%EF%BC%89.pdf)\n\n[286\\. 欧盟人工智能政策分析](https://pdf.erytis.cn/cs/CS.8014.pdf)\n\n[287\\. T. Matsunaga, K. Takagi. “State of implementation of the OECD AI Principles.” OECD Digital Economy Papers](https://doi.org/10.1787/1cd40c44-en)\n\n[288\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2024.” ArXiv](https://doi.org/10.48550/arXiv.2405.19522)\n\n[289\\. 欧盟 AI 法案立法观察](https://www.hankunlaw.com/upload/portal/20230518/f3ce659dbd57842f57380510bb6712dd.pdf)\n\n[292\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[293\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[294\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[295\\. European Union Artificial Intelligence Act: ガイド](https://www.twobirds.com/-/media/new-website-content/pdfs/capabilities/artificial-intelligence/eu-ai-act-guide-japanese-version.pdf)\n\n[296\\. Margaret Mitchell, Simone Wu et al. “Model Cards for Model Reporting.” Proceedings of the Conference on Fairness, Accountability, and Transparency](https://doi.org/10.1145/3287560.3287596)\n\n[297\\. 欧盟人工智能法案：指南](https://www.twobirds.com/-/media/new-website-content/pdfs/capabilities/artificial-intelligence/eu-ai-act-guide-chinese-version.pdf)\n\n[298\\. AI RMF 1.0 Controls Checklist](https://www.aigl.blog/ai-rmf-1-0-controls-checklist/)\n\n[299\\. E. Mitchell, Yoonho Lee et al. “DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature.” International Conference on Machine Learning](https://doi.org/10.48550/arXiv.2301.11305)\n\n[300\\. European Union Artificial Intelligence Act: a guide](https://www.twobirds.com/-/media/new-website-content/pdfs/capabilities/artificial-intelligence/european-union-artificial-intelligence-act-guide.pdf)\n\n[301\\. European AI Act: Mandatory Labeling for AI-Generated ...](https://www.imatag.com/blog/ai-act-legal-requirement-to-label-ai-generated-content)\n\n[302\\. EU AI Act Update: New Watermarking Requirements for AI- ...](https://www.imatag.com/blog/eu-ai-act-update-new-watermarking-requirements-for-ai-generated-content)\n\n[303\\. ByteDance Launches OmniHuman-1: AI Tool Turns Photos ...](https://opentools.ai/news/bytedance-launches-omnihuman-1-ai-tool-turns-photos-into-lifelike-videos)\n\n[304\\. Test Criteria Catalogue for AI Systems in Finance](https://www.bsi.bund.de/SharedDocs/Downloads/EN/BSI/KI/AI-Finance_Test-Criteria.pdf?__blob=publicationFile&v=3)\n\n[305\\. Mitigating Disinformation Risks in the Run-Up to 2024 European Elections: A Three-Step Action Plan against AI-Generated Disinformation](https://www.coleurope.eu/sites/default/files/uploads/page/natolin_policy_papers_vol_1_24%20%281%29.pdf)\n\n[306\\. ANNEX to the Communication to the Commission](https://ec.europa.eu/newsroom/repository/document/2024-13/C_2024_2121_1_EN_annexe_acte_autonome_cp_part1_v3_tpHHZgYyBGFMF8J5rE0OR1GdOis_103911.pdf)\n\n[307\\. Hugging Face Information for NIST “Related to NIST’s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning Artificial Intelligence”](https://downloads.regulations.gov/NIST-2023-0009-0149/attachment_1.pdf)\n\n[308\\. Navigating the EU AI Act: Proposed Compliance Measures for AI Providers and Deployers](https://epub.jku.at/download/pdf/10576761.pdf)\n\n[309\\. Making sense of AI rules: EU AI Act, NIST AI RMF, and ISO ...](https://verifywise.ai/making-sense-of-ai-rules-eu-ai-act-nist-ai-rmf-and-iso-42001/)\n\n[310\\. EU’s draft election security guidelines for tech giants take aim at political deepfakes](https://gayello.com/eus-draft-election-security-guidelines-for-tech-giants-take-aim-at-political-deepfakes/)\n\n[311\\. The AI Act's AI Watermarking Requirement Is a Misstep in the ...](https://datainnovation.org/2024/07/the-ai-acts-ai-watermarking-requirement-is-a-misstep-in-the-quest-for-transparency/#:~:text=The%20AI%20Act,%20formally%20adopted,concerns%20like%20deepfakes%20and%20misinformation.)\n\n[312\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[313\\. 信息安全技术 远程人脸识别系统技术要求](http://m.jsntspa.com/sitefiles/services/cms/utils.aspx?type=Download&publishmentSystemID=23&channelID=68&contentID=715&fileUrl=o4HSZYW5MypkIf6GKhtiI1Vmar234248YUITuXBgum6Cb4LoTjgedVOk6DpWzh1sthOC82g1Yd7wqLM7l9AgbwFZJ39He9rAj9D9Cso5ZK0nC0nfa0slash0539l1HbotAXD75)\n\n[314\\. I. Goodfellow, Jonathon Shlens et al. “Explaining and Harnessing Adversarial Examples.” CoRR](https://arxiv.org/abs/1412.6572)\n\n[315\\. Florian Schroff, Dmitry Kalenichenko et al. “FaceNet: A unified embedding for face recognition and clustering.” 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2015.7298682)\n\n[316\\. Jiankang Deng, J. Guo et al. “ArcFace: Additive Angular Margin Loss for Deep Face Recognition.” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2019.00482)\n\n[317\\. Gary B. Huang, Marwan A. Mattar et al. “Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments.”](https://www.semanticscholar.org/paper/c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3)\n\n[318\\. 人脸识别系统技术要求](https://static1.tianyancha.com/czd_file/fs/8ca7b9c2-21fa-437d-82c2-2672e759c28f.pdf)\n\n[319\\. OPTIMIZING THE INTEGRATION OF FINGERPRINT AND FACIAL RECOGNITION TECHNOLOGIES FOR REAL-TIME ATTENDANCE SYSTEMS](https://ssaapublications.com/index.php/sjasor/article/download/586/588)\n\n[320\\. Commission Guidelines on prohibited artificial intelligence practices established by Regulation (EU) 2024/1689 (AI Act)](https://www.key4biz.it/wp-content/uploads/2025/02/C_2025_884_1_EN_annexe_acte_autonome_cp_part1_v4_4YAkRZf5bMIWOuMqiKMIGySPo_112367.pdf)\n\n[321\\. New rules for Artificial Intelligence – EU Commission Q&A](https://portal.ieu-monitoring.com/editorial/new-rules-for-artificial-intelligence-eu-commission-qa/7089?utm_source=ieu-portal)\n\n[322\\. 信息安全技术 虹膜识别系统技术要求](https://www.tc260.org.cn/servlet/Download?path=GB&filename=20140928111715215311.doc)\n\n[323\\. Artificial intelligence at EU borders: Overview of applications and key issues](https://www.europarl.europa.eu/RegData/etudes/IDAN/2021/690706/EPRS_IDA%282021%29690706_EN.pdf)\n\n[324\\. 信息安全技术 指纹识别系统技术要求](https://www.tc260.org.cn/file/20160701091238021286.doc)\n\n[325\\. Apple iOS 15: iPhones Security Target](https://www.commoncriteriaportal.org/nfs/ccpfiles/files/epfiles/st_vid11237-st.pdf)\n\n[326\\. Min Ren, Yunlong Wang et al. “Artificial Immune System of Secure Face Recognition Against Adversarial Attacks.” ArXiv](https://doi.org/10.1007/s11263-024-02153-0)\n\n[332\\. Albane Gourdol, Katarzyna Kraszewska et al. “European Commission.” Integrating Environmental and Climate Action into Development Co-operation](https://doi.org/10.1787/9789264265189-13-en)\n\n[333\\. The EU AI Act and the Shifting Global AI Policy Landscape](https://www.conference-board.org/pdfdownload.cfm?masterProductID=57243)\n\n[334\\. Understanding the Future of Artificial Intelligence Governance: Comparing the EU AI Act and U.S. Executive Order on Safe AI](https://stpp.fordschool.umich.edu/sites/stpp/files/2024-06/stpp-future-of-ai-governance.pdf)\n\n[335\\. OECD-Bericht zu Künstlicher Intelligenz in Deutschland](https://www.oecd.org/content/dam/oecd/de/publications/reports/2024/06/oecd-artificial-intelligence-review-of-germany_c1c35ccf/8fd1bd9d-de.pdf)\n\n[336\\. EU AI Act 最新进展](https://legalbriefs.deloitte.com/post/102ii7b/latest-on-the-eus-ai-act)\n\n[337\\. Policy Guide: The EU AI Act](https://www.gsma.com/about-us/regions/europe/wp-content/uploads/2024/10/Policy_Guide_-EU_AI_Act.pdf)\n\n[338\\. The EU AI Act: two steps forward, one step back](https://www.globalgovernance.eu/publications/the-eu-ai-act-two-steps-forward-one-step-back)\n\n[339\\. Petra Ahrens, Lise Rolandsen Agustín. “European Parliament.” International and Comparative Law Quarterly](https://doi.org/10.1007/978-94-009-5452-6_5)\n\n[340\\. 欧盟2024年人工智能法案：全球AI法律发展的里程碑](https://natlawreview.com/article/worldwide-first-ai-legal-developments-european-unions-2024-artificial-intelligence)\n\n[341\\. Successful and timely uptake of Artificial Intelligence in science in the EU](https://www.kowi.de/Portaldata/2/Resources/HEU/sam-scientific-opinion-artificial-intelligence.pdf)\n\n[342\\. A. Bradford. “The Brussels Effect.” Columbia Law School](https://www.semanticscholar.org/paper/a76e70cca7ed03ed587c645403cb489df14c15f7)\n\n[343\\. Making the most of the AI opportunity: The challenges of regulating AI](https://www.pc.gov.au/research/completed/making-the-most-of-the-ai-opportunity/ai-paper2-regulating.pdf)\n\n[344\\. Artificial Intelligence & AI Act](https://www.taylorwessing.com/zh-hant/interface/2023/predictions-2023-part-2/artificial-intelligence-and-data-act)\n\n[345\\. Successful and timely uptake of artificial intelligence in science in the EU](https://allea.org/wp-content/uploads/2024/04/ai-in-science-err.pdf)\n\n[346\\. OnPolicy March 2024: AI aye aye](https://onfido.com/blog/onpolicy-march-2024-ai-aye-aye/)\n\n[347\\. Global Employer 2023 in Review and 2024 Preview](https://www.dlapiper.com/en-be/insights/publications/2023/12/global-employer-2023-in-review-2024-preview)\n\n[348\\. Evolving Legal Norms for Artificial Intelligence in the EU and the US](https://www.braumillerlaw.com/evolving-legal-norms-artificial-intelligence-eu-us/)\n\n[349\\. Q3/2023 – Executive Summary](https://blog.denic.de/en/q3-2023/)\n\n[352\\. I. Cox, J. Kilian et al. “Secure spread spectrum watermarking for multimedia.” IEEE transactions on image processing : a publication of the IEEE Signal Processing Society](https://doi.org/10.1109/83.650120)\n\n[353\\. W. Bender, Daniel F. Gruhl et al. “Techniques for data hiding.” Electronic imaging](https://doi.org/10.1117/12.205315)\n\n[354\\. Technical Review](https://tech.ebu.ch/docs/techreview/trev_bestof08.pdf)\n\n[355\\. L. D. Strycker, P. Termont et al. “Implementation of a real-time digital watermarking process for broadcast monitoring on a TriMedia VLIW processor.”](https://doi.org/10.1049/IP-VIS:20000580)\n\n[356\\. Sztuczna inteligencja w systemach radiowych – I](https://krit2024.put.poznan.pl/assets/doc/Przeglad%20Telekomunikacyjny%20sesje%20zoptymali.pdf)\n\n[357\\. T. Kalker, G. Depovere et al. “Video watermarking system for broadcast monitoring.” Electronic imaging](https://doi.org/10.1117/12.344661)\n\n[358\\. M. Maes, T. Kalker et al. “Exploiting shift invariance to obtain a high payload in digital image watermarking.” Proceedings IEEE International Conference on Multimedia Computing and Systems](https://doi.org/10.1109/MMCS.1999.779112)\n\n[359\\. DIGITAL AUDIO WATERMARKING FOR BROADCAST MONITORING AND CONTENT IDENTIFICATION](https://mural.maynoothuniversity.ie/1971/1/Thesis_FINAL_submission_010610.pdf)\n\n[360\\. AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap](https://hdsr.mitpress.mit.edu/pub/aelql9qy/download/pdf)\n\n[361\\. T. Kalker, J. Linnartz et al. “On the reliability of detecting electronic watermarks in digital images.” 9th European Signal Processing Conference (EUSIPCO 1998)](https://doi.org/10.5281/ZENODO.36534)\n\n[362\\. NIST AI 100-1 AI Risk Management Framework (AI RMF 1.0)](https://aisi.go.jp/assets/pdf/NIST_RMF_Japanese_ED_updated_20240626.pdf)\n\n[363\\. AI RMF 1.0 Controls Checklist](https://www.aigl.blog/ai-rmf-1-0-controls-checklist/)\n\n[364\\. Noise sources in robust uncompressed video watermarking](https://theses.hal.science/tel-00541755v1/file/TheseDUMITRU.pdf)\n\n[365\\. S. Mohanty, E. Kougianos. “Real-time perceptual watermarking architectures for video broadcasting.” J. Syst. Softw.](https://doi.org/10.1016/j.jss.2010.12.012)\n\n[372\\. Publication of the CEN-CENELEC work programme for 2024](https://www.kan.de/en/help-advice/news/detailansicht-en/cen-cenelec-arbeitsprogramm-2024-veroeffentlicht)\n\n[373\\. European AI Standards – Technical Standardization and Implementation Challenges under the EU AI Act](https://ki-verband.de/wp-content/uploads/2025/03/Study_European-AI-Standards_FINAL_20250325.pdf)\n\n[374\\. Commission Guidelines on prohibited artificial intelligence practices established by Regulation (EU) 2024/1689 (AI Act)](https://www.key4biz.it/wp-content/uploads/2025/02/C_2025_884_1_EN_annexe_acte_autonome_cp_part1_v4_4YAkRZf5bMIWOuMqiKMIGySPo_112367.pdf)\n\n[375\\. Facial Recognition: Facing up to terrorism](https://counterterrorbusiness.com/features/facial-recognition-facing-terrorism)\n\n[376\\. The Human Rights Risks of Facial Recognition AI Tech in Policing and Immigration Must be Properly Recognised in the EU AI Act](https://cdt.org/wp-content/uploads/2023/02/CDT-Europe-Facial-recognition-EU-AI-Act-issue-brief.pdf)\n\n[377\\. European standardisation synergy: towards the AI Act](https://docbox.etsi.org/MTS/MTS/05-CONTRIBUTIONS/2023/MTS%2823%29000053_European_standardisation_synergy_-_Towards_the_AI_act.pdf)\n\n[378\\. Artificial Intelligence Act](https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.docx)\n\n[379\\. Analysis of the preliminary AI standardisation work plan in support of the AI Act](https://publications.jrc.ec.europa.eu/repository/bitstream/JRC132833/JRC132833_01.pdf)\n\n[380\\. European Union Artificial Intelligence Act: ガイド](https://www.twobirds.com/-/media/new-website-content/pdfs/capabilities/artificial-intelligence/eu-ai-act-guide-japanese-version.pdf)\n\n[381\\. A inteligência artificial no combate ao terrorismo em Portugal – estudo exploratório](https://comum.rcaap.pt/bitstream/10400.26/41570/1/3420_%20Disserta%C3%A7%C3%A3o%20de%20Mestrado_In%C3%AAsProen%C3%A7a.pdf)\n\n[382\\. EU AI Act: Key provisions now in force](https://www.lexology.com/library/detail.aspx?g=b3fed5bd-a3b8-4ab4-a4d7-b1f03f60bbfb)\n\n[383\\. I. Hupont, Songül Tolan et al. “The landscape of facial processing applications in the context of the European AI Act and the development of trustworthy systems.” Scientific Reports](https://doi.org/10.1038/s41598-022-14981-6)\n\n[384\\. SPROSTOWANIE](https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138-FNL-COR01_PL.pdf)\n\n[385\\. Article 5 - Prohibited AI practices AI Act](https://www.activemind.legal/legislation/ai-act/article-5/)\n\n[386\\. European Union Artificial Intelligence Act: a guide](https://www.twobirds.com/-/media/new-website-content/pdfs/capabilities/artificial-intelligence/european-union-artificial-intelligence-act-guide.pdf)"
  },
  {
    "id": 32,
    "question": "How do you see AI 'getting better' - what does 'better' mean when correctness isn't the primary metric?",
    "response": "**Research Report**\n\n**To:** Interested Parties\n\n**From:** Expert Researcher\n\n**Date:** July 26, 2025\n\n**Subject:** Redefining \"Better\" AI: A 2025 Perspective on Progress Beyond Correctness\n\n**Introduction**\n\nFor decades, the primary benchmark for artificial intelligence (AI) has been a seemingly simple one: correctness. A \"better\" model was one that achieved a higher accuracy score, a lower error rate, or a superior performance on a standardized test. However, as AI systems become more deeply embedded in the fabric of society—powering everything from autonomous vehicles and medical diagnostics to financial markets and customer interactions—this one-dimensional view of progress is proving profoundly inadequate.\n\nAs of mid-2025, the definition of a \"better\" AI system has evolved into a far more sophisticated and multi-faceted concept. Progress is no longer a linear march toward perfect accuracy. Instead, \"better\" now describes a delicate and deliberate balancing act. It encompasses a complex interplay of operational efficiency, system robustness, security, user experience, and, most critically, ethical alignment. In this new paradigm, correctness is merely one of several vital, and often competing, objectives.\n\nThis report analyzes the contemporary understanding of AI improvement, moving beyond the narrow confines of accuracy. It explores the key dimensions of non-correctness metrics that now define AI quality, the technical methodologies developed to measure them, the sophisticated frameworks for managing their inherent trade-offs, and the real-world implications of this shift in key industrial sectors.\n\n**Part 1: The Expanding Definition of \"Better\" - A Taxonomy of Non-Correctness Metrics**\n\nThe evolution of AI has necessitated a corresponding evolution in how we measure its quality. Industry and research have moved to adopt a diverse suite of metrics that capture a more holistic view of an AI system's value and impact. These can be broadly categorized into performance, security, and ethical dimensions.\n\n**1.1 Performance and Operational Efficiency**\n\nIn many commercial applications, the speed and reliability of an AI system are more critical to its success than a marginal gain in prediction accuracy. A model that is 99% accurate but takes ten seconds to respond is often less useful than one that is 98% accurate and responds in milliseconds.\n\n**Latency and Throughput:** Latency, which quantifies the delay in a system's response, is a primary metric influenced by model architecture and processing infrastructure \\[19\\]. Closely related is throughput, which measures the number of requests a system can handle in a given timeframe, reflecting its overall efficiency \\[19\\]. In customer service, for example, key metrics include response time and the speed of query resolution \\[2\\].\n\n**Operational Reliability:** Beyond speed, system stability is paramount. Metrics borrowed from traditional engineering, such as **Mean Time Between Failures (MTBF)** and **Mean Time To Repair (MTTR)**, are used to evaluate an AI system's dependability \\[6\\]. In service-oriented applications, metrics like the **First-Time Fix Rate (FTFR)** —the percentage of issues resolved on the first attempt—are crucial indicators of operational effectiveness \\[6\\].\n\n**1.2 Security and Robustness**\n\nA theoretically \"correct\" model is useless if it is easily broken or manipulated. Robustness measures an AI's ability to maintain consistent performance when faced with diverse, unexpected, or even malicious inputs \\[19\\].\n\n**Adversarial Performance:** This metric assesses how well a system performs under adversarial conditions, such as inputs deliberately crafted to deceive the model \\[14\\].\n\n**Threat Detection:** In cybersecurity applications, improvement is measured by the **precision of threat detection** and, crucially, a low **false negative rate**, as failing to detect a genuine threat can have catastrophic consequences \\[14\\].\n\n**1.3 Ethical Alignment and Fairness**\n\nPerhaps the most significant expansion of the term \"better\" lies in the domain of ethics and fairness. The field of \"AI alignment research\" is dedicated to ensuring that AI systems behave in ways that are consistent with human values and objectives \\[23\\]\\[25\\]\\[26\\]. This moves the goalposts from simple correctness to normative alignment.\n\n**Bias Metrics:** These metrics are essential for evaluating fairness. They are used to detect and quantify biases in AI systems, such as racial bias in healthcare algorithms or gender bias in classification systems \\[8\\]\\[11\\]. The goal is to ensure that a model's performance does not disproportionately harm or benefit specific demographic groups.\n\n**Safety and Harmfulness:** For generative AI, a key metric is a \"safety score,\" which measures the model's propensity to generate content related to harmful or sensitive topics \\[19\\].\n\n**Avoiding Evaluation Bias:** A critical meta-level concern is \"evaluation bias,\" which arises when the metrics or procedures used to assess a model are not properly aligned with the dataset or the real-world population, leading to a misleading assessment of its quality \\[12\\].\n\n**Part 2: The Challenge of the Trade-Off - Balancing Competing Objectives**\n\nThe expansion of metrics introduces a fundamental challenge: these new dimensions of \"better\" are often in conflict with one another and with traditional correctness. Improving fairness might reduce accuracy; increasing complexity for robustness might increase latency. The central task for AI designers in 2025 is not to maximize a single metric, but to skillfully navigate these complex trade-offs.\n\n**2.1 The Myth of \"Sacrifice\" vs. the Reality of Optimization**\n\nA search for documented case studies from 2024-2025 where companies _explicitly and deliberately sacrificed_ accuracy to improve metrics like efficiency or user experience yields no clear examples \\[43\\]\\[54\\]\\[62\\]. This lack of evidence is itself a significant finding. It suggests that mature AI development does not view these choices as a simple \"sacrifice\" of one metric for another. Instead, the paradigm is one of multi-objective optimization: finding a solution that provides the best possible balance across all critical dimensions for a given application.\n\nThe domain of autonomous vehicles (AVs) provides a powerful illustration. Low latency is safety-critical; a delay of even a fraction of a second in a collision avoidance system can be catastrophic \\[126\\]. Concurrently, high object detection accuracy is also non-negotiable. The engineering challenge is not to trade accuracy for speed, but to achieve extreme speed _while maintaining_ extremely high accuracy. As of 2025, advanced edge computing architectures in AVs are reducing decision latency to as low as 20-50 milliseconds while simultaneously achieving object detection accuracy of 98.3% \\[122\\]\\[129\\]. While increased latency can cause a 5-10% drop in detection accuracy in some scenarios \\[195\\], this is viewed as a performance failure to be engineered around, not a desirable trade-off to be designed for.\n\n**2.2 Key Trade-Off Frontiers**\n\nNavigating this multi-dimensional space requires a deep understanding of the key trade-off frontiers where objectives conflict.\n\n**Fairness vs. Accuracy:** This is one of the most studied trade-offs. Implementing fairness constraints to ensure equitable outcomes across demographic groups can sometimes lead to a reduction in the model's overall predictive accuracy \\[101\\]\\[107\\]. The **Chouldechova Impossibility Theorem** even proves that it is mathematically impossible to satisfy multiple common fairness definitions simultaneously, forcing designers to make explicit choices about which fairness metrics to prioritize \\[119\\].\n\n**Explainability vs. Accuracy:** Highly complex models like deep neural networks are often the most accurate but are also \"black boxes,\" making their reasoning difficult to interpret. Simpler, less accurate models may be preferred in high-stakes domains where explainability is paramount for trust and accountability \\[103\\]\\[113\\].\n\n**Privacy vs. Fairness:** Techniques like **differential privacy**, which add noise to data to protect individual privacy, can have the unintended consequence of reducing a model's fairness, particularly for underrepresented minority groups whose statistical signals may be obscured by the noise \\[103\\].\n\n**Latency vs. Complexity/Accuracy:** As seen in the AV example, more complex models that might offer higher accuracy or robustness often require more computational resources, thereby increasing latency \\[115\\].\n\n**Part 3: Technical Frameworks and Algorithms for Multi-Objective AI Improvement**\n\nTo manage the complex trade-offs inherent in modern AI, designers are increasingly relying on principled frameworks and advanced optimization algorithms that move beyond single-metric maximization.\n\n**3.1 Frameworks for Principled Design**\n\nTo avoid treating ethics and other non-correctness metrics as an afterthought, structured frameworks are being integrated early into the AI development lifecycle.\n\n**Fairness in Design (FID):** FID is a methodology that aims to facilitate discussions about fairness concerns at the beginning of the design process, building fairness metrics and considerations into the system from its inception rather than trying to patch them in later \\[108\\]\\[112\\].\n\n**Behavioral and Trustworthy AI Frameworks:** Methodologies like the European Commission's **Assessment List for Trustworthy AI (ALTAI)** incorporate principles from behavioral science to ensure AI systems align with human values and account for their impact on human decision-making \\[93\\].\n\n**3.2 Optimization at the Algorithmic Level**\n\nAt a technical level, the challenge of balancing multiple objectives is addressed by a class of techniques known as multi-objective optimization, with **Pareto optimization** being a central concept. A solution is \"Pareto optimal\" if no single objective can be improved without degrading at least one other objective \\[222\\]\\[225\\]. The goal is to identify the \"Pareto frontier,\" the set of all such optimal solutions, from which a human designer can select the most appropriate trade-off for their specific context.\n\nSeveral families of algorithms are used to find or approximate this frontier:\n\n**Efficiency Optimization:** Techniques like **quantization** (reducing model precision) and **pruning** (removing redundant parameters) can shrink models and increase inference speed, often with minimal impact on accuracy \\[161\\].\n\n**Evolutionary Algorithms:** The **Non-dominated Sorting Genetic Algorithm II (NSGA-II)** is a powerful algorithm used to find the Pareto frontier by simulating evolutionary processes \\[221\\].\n\n**Preference-Based Reinforcement Learning:** As of 2025, novel algorithms like **ParetoPrompt Direct Preference Optimization/Identity Preference Optimization (PP-DPO/IPO)** are emerging. These methods use preference data to guide a model towards the Pareto front without the biases inherent in traditional scalarization techniques (which collapse multiple objectives into a single score) \\[225\\]\\[225\\]. While pioneered for optimizing LLM prompts across objectives like accuracy and fluency \\[225\\], their principles are generalizable to other fairness-accuracy trade-offs. However, documented benchmark results comparing these advanced methods to traditional ones in live production APIs, such as for financial fraud detection, remain limited \\[393\\]\\[409\\].\n\n**3.3 Case Study: Improving Fairness with Synthetic Data and Fair-GAN**\n\nOne of the most promising practical applications of these principles is the use of synthetic data to enhance AI fairness. Since real-world data often contains historical biases, training models on it can perpetuate and even amplify those biases. Synthetic data generation offers a way to create balanced, fair datasets from the ground up.\n\nThe **Fair-GAN** framework is a prime example. It is a Generative Adversarial Network (GAN) specifically designed to generate synthetic data that adheres to fairness constraints \\[382\\]. The technical implementation involves adding a penalty term to the generator's loss function, which rewards the generator for producing data that meets specific fairness criteria \\[372\\]\\[378\\].\n\nCrucially, Fair-GAN enables measurement and optimization using quantitative bias metrics that go beyond simple statistical parity. These include:\n\n**Equal Opportunity Difference:** Measures whether the model performs equally well for a favorable outcome (e.g., loan approval) across different protected groups \\[271\\].\n\n**Disparate Impact:** Assesses whether the rate of selection for a particular outcome is substantially different across groups \\[271\\].\n\n**Average Odds Difference:** A metric that combines checks for equality in both false positive and false positive rates across groups \\[271\\].\n\nIn frameworks like Fair-GAN and the related TabFairGAN, a hyperparameter, often denoted as λ, allows designers to explicitly control the trade-off between data utility (accuracy) and fairness, empowering them to find the optimal balance for their needs \\[374\\]\\[372\\].\n\n**Part 4: Conclusion - The Future of \"Better\" AI**\n\nAs of July 2025, the pursuit of a \"better\" AI has matured significantly. The narrow focus on correctness has given way to a sophisticated, multi-objective optimization problem. Progress is now measured across a broad spectrum of metrics encompassing efficiency, robustness, security, and ethical alignment. The defining characteristic of a state-of-the-art AI system is no longer its singular performance on one axis, but its gracefully balanced performance across all of them.\n\nThe paradigm has shifted from accuracy maximization to holistic trade-off management. The absence of documented cases where accuracy is deliberately \"sacrificed\" underscores this maturity; the goal is not a simplistic exchange but a comprehensive improvement of the entire system's character.\n\nThe theoretical foundations (Pareto optimization), technical frameworks (Fairness in Design), and specific algorithms (PP-IPO, Fair-GAN) are advancing at a remarkable pace. The next great challenge for the field of AI will be the widespread and principled implementation of these tools in live production systems. The future of AI will be defined by how effectively organizations can deploy this new understanding of \"better\" to build systems that are not just technically correct, but are also efficient, resilient, fair, and demonstrably aligned with fundamental human values.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. AI-enhanced Cyber-Physical Systems in Automotive Industry: Integration of CPS and Application Artificial Intelligence Technologies in Automotive Paint Shop Process](https://www.tdx.cat/bitstream/handle/10803/690335/TEMSG1de1.pdf?sequence=4&isAllowed=y)\n\n[2\\. 10 Metrics for AI Service Optimization](https://blog.naitive.cloud/10-metrics-for-ai-service-optimization/)\n\n[3\\. Assessing the Reliability of Artificial Intelligence Systems: Challenges, Metrics, and Future Directions](https://www.ijimes.ir/index.php/ijimes/article/download/133/254/710)\n\n[4\\. IMPLEMENTATION OF AN AI-POWERED SURVEILLANCE SYSTEM FOR INDUSTRIAL FIRE DETECTION WITH YOLO-V8](https://www.ijprems.com/uploadedfiles/paper/issue_11_november_2024/36900/final/fin_ijprems1732083637.pdf)\n\n[5\\. AI Performance Metrics: The Science & Art of Measuring AI - Version 1](https://www.version1.com/blog-ai-performance-metrics-the-science-and-art-of-measuring-ai/)\n\n[6\\. The Need for Standardized Metrics and KPI's for AI Performance](https://liftescalatorlibrary.org/paper_indexing/papers/00000549.pdf)\n\n[7\\. U.S. Leadership in AI: A Plan for Federal Engagement in Developing Technical Standards and Related Tools](https://www.nist.gov/document/report-plan-federal-engagement-developing-technical-standards-and-related-tools)\n\n[8\\. Z. Obermeyer, Brian W. Powers et al. “Dissecting racial bias in an algorithm used to manage the health of populations.” Science](https://doi.org/10.1126/science.aax2342)\n\n[9\\. Liability for AI Agents](https://scholarship.law.unc.edu/cgi/viewcontent.cgi?article=1508&context=ncjolt)\n\n[10\\. AI-Driven Design Systems: The Future of Scalable UI Frameworks](https://eajournals.org/ejcsit/wp-content/uploads/sites/21/2025/05/AI-Driven-Design-Systems.pdf)\n\n[11\\. Joy Buolamwini, Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” FAT](https://www.semanticscholar.org/paper/18858cc936947fc96b5c06bbe3c6c2faa5614540)\n\n[12\\. From Inception to Retirement: Addressing Bias Throughout the Lifecycle of AI Systems](https://rhite.tech/files/From-Inception-to-Retirement-Addressing-Bias-Throughout-the-Lifecycle-of-AI-Systems.pdf)\n\n[13\\. 测量人工智能性能的重要性与最佳解决方案](https://jahanitech.ir/best-artificial-intelligence-performance-measurement-solution-in-2023/)\n\n[14\\. AI security in different industries: A comprehensive review of vulnerabilities and mitigation strategies](https://ijsra.net/sites/default/files/IJSRA-2024-1923.pdf)\n\n[15\\. Technical Assessment of High-Risk AI Systems: State of Play and Challenges](https://www.tuev-lab.ai/fileadmin/user_upload/AI_Lab/TUEV_AI_Lab_Whitepaper_Technical_Assessment_of_AI_Systems.pdf)\n\n[16\\. AI Performance Metrics: The Science & Art of Measuring AI](https://www.version1.com/en-us/en-ushttps:/www.version1.com/blog/ai-performance-metrics-the-science-and-art-of-measuring-ai/)\n\n[17\\. Robustness and performance of AI applications](https://www.sgs.com/-/media/sgscorp/documents/corporate/brochures/sgs-dti-ai-robustness-and-performance-of-ai-applications-en.cdn.en-MU.pdf)\n\n[18\\. AI RMF PLAYBOOK](https://www.privacysecurityacademy.com/wp-content/uploads/2024/10/AI_RMF_Playbook-1.pdf)\n\n[19\\. Key AI Performance Metrics (AI KPIs) Businesses Should ...](https://www.neurond.com/blog/ai-performance-metrics)\n\n[20\\. How to Measure AI Performance: Key Metrics and Best Practices](https://neontri.com/blog/measure-ai-performance/#:~:text=Precision%20and%20recall:%20These%20two,catching%20all%20the%20real%20issues.)\n\n[21\\. Dario Amodei, C. Olah et al. “Concrete Problems in AI Safety.” ArXiv](https://arxiv.org/abs/1606.06565)\n\n[22\\. Dylan Hadfield-Menell, Stuart J. Russell et al. “Cooperative Inverse Reinforcement Learning.” Neural Information Processing Systems](https://arxiv.org/abs/1606.03137)\n\n[23\\. I asked chatGPT: how are you?](https://www.whitehatstoic.com/p/i-asked-chatgpt-how-are-you)\n\n[24\\. Anna Jobin, M. Ienca et al. “Artificial Intelligence: the global landscape of ethics guidelines.” ArXiv](https://doi.org/10.1038/s42256-019-0088-2)\n\n[25\\. AI alignment vs AI ethical treatment: Ten challenges](https://globalprioritiesinstitute.org/wp-content/uploads/Bradley-and-Saad-AI-alignment-vs-AI-ethical-treatment_-Ten-challenges.pdf)\n\n[26\\. A Multi-Level Framework for the AI Alignment Problem](https://openreview.net/pdf?id=Hx8yuAB__kw)\n\n[27\\. ChatBench: From Static Benchmarks to Human-AI Evaluation](http://jakehofman.com/pdfs/chatbench.pdf)\n\n[28\\. B. Mittelstadt. “Principles alone cannot guarantee ethical AI.” Nature Machine Intelligence](https://doi.org/10.1038/s42256-019-0114-4)\n\n[29\\. Iason Gabriel. “Artificial Intelligence, Values, and Alignment.” Minds and Machines](https://doi.org/10.1007/s11023-020-09539-2)\n\n[30\\. There and Back Again: The AI Alignment Paradox](https://arxiv.org/pdf/2405.20806)\n\n[31\\. Travis LaCroix, A. Luccioni. “Metaethical Perspectives on 'Benchmarking' AI Ethics.” ArXiv](https://doi.org/10.48550/arXiv.2204.05151)\n\n[32\\. Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?](https://arxiv.org/pdf/2407.21792)\n\n[33\\. AI Alignment: Risks, Approaches, Challenges and Benefits](https://tracker.holisticai.com/feed/ai-alignment?ref=/feed)\n\n[34\\. How to measure value alignment in AI | AI and Ethics](https://link.springer.com/article/10.1007/s43681-023-00357-7)\n\n[35\\. Building Human-AI Alignment: Specifying, Inspecting, Modeling, and Revising AI Behaviors](https://slbooth.com/assets/docs/faculty_apps_2023/Serena_Booth_2023_Research.pdf)\n\n[36\\. AI IN ASSESSMENT: ETHICS, INNOVATION AND RESEARCH](https://www.nbme.org/sites/default/files/2024-03/AI_in_Assessment_Executive_Summary.pdf)\n\n[37\\. Integrating ethics in AI development: a qualitative study](https://bmcmedethics.biomedcentral.com/counter/pdf/10.1186/s12910-023-01000-0.pdf)\n\n[38\\. Measuring Human-AI Value Alignment in Large Language Models](https://ojs.aaai.org/index.php/AIES/article/download/31703/33870/35767)\n\n[39\\. AI alignment - EA Forum](https://forum.effectivealtruism.org/topics/ai-alignment)\n\n[41\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[42\\. T. Lillicrap, Jonathan J. Hunt et al. “Continuous control with deep reinforcement learning.” CoRR](https://arxiv.org/abs/1509.02971)\n\n[43\\. AI-Driven Design Systems: The Future of Scalable UI Frameworks](https://eajournals.org/ejcsit/wp-content/uploads/sites/21/2025/05/AI-Driven-Design-Systems.pdf)\n\n[44\\. Revolutionary Success in Real-World AI Case Studies 2025](https://fixusglobal.com/case-studies/revolutionary-success-in-real-world-ai-case-studies-2025/)\n\n[45\\. 20 Generative AI Case Studies \\[2024\\] - DigitalDefynd](https://digitaldefynd.com/IQ/generative-ai-case-studies/)\n\n[46\\. AI 科普丨都2025年了,人们到底在用AI做什么?国外大牛总结...](https://mp.weixin.qq.com/s?__biz=MjM5ODIwNjEzNQ%3D%3D&mid=2649904618&idx=3&sn=c72608dff07b87abc1f7781e9a19017d&chksm=bf0a8208f6eda572a06d84ec4a1547369e53d37e754b3764f1cc7b755e4fd489e37e6a1f8b4c&scene=27)\n\n[47\\. Top 10 Generative AI Use Cases for 2024 :: IgniteTech](http://ignitetech.com/about/blogs/top-10-generative-ai-use-cases-2024)\n\n[48\\. 5 Innovative AI Case Studies of 2024 - Success Stories](https://www.biz4group.com/blog/innovative-ai-case-studies)\n\n[49\\. Ziyun Wang, T. Schaul et al. “Dueling Network Architectures for Deep Reinforcement Learning.” International Conference on Machine Learning](https://arxiv.org/abs/1511.06581)\n\n[50\\. The State of Salesforce 2024-2025: 3 priorities for driving an AI advantage](https://www.techresearchinfo.com/whitepaper/IBV%20-State-ofSalesforce2024-2025-report.pdf)\n\n[51\\. 100+ AI Use Cases & Applications: In-Depth Guide for 2024 - AIMultiple](https://research.aimultiple.com/ai-usecases/)\n\n[52\\. 10 Best AI Use Cases in 2024](https://maticz.com/ai-use-cases)\n\n[53\\. Marc G. Bellemare, Yavar Naddaf et al. “The Arcade Learning Environment: An Evaluation Platform for General Agents.” ArXiv](https://doi.org/10.1613/jair.3912)\n\n[54\\. AI-Powered Customer Experience: Top 5 Trends for 2025](https://blog.ecosystm.io/wp-content/uploads/2024/12/AI-Powered-Customer-Experience-Top-5-Trends-2025.pdf)\n\n[55\\. Unlocking AI and Generative AI Use Cases with AI-Ready Hybrid Cloud Infrastructure](https://cdrdv2-public.intel.com/833841/Nutanix%20CP%20AI%20WP.pdf)\n\n[56\\. 2024 → 2025: The Year Ahead in AI - by Nikunj Kothari](https://writing.nikunjk.com/p/2024-2025)\n\n[57\\. REVOLUTIONIZING CUSTOMER SUPPORT: A DEEP DIVE INTO SALESFORCE AGENTFORCE AND LAM TECHNOLOGY](https://www.irjmets.com/uploadedfiles/paper/issue_2_february_2025/67118/final/fin_irjmets1739032501.pdf)\n\n[58\\. How AI Is Changing User Experience In 2024 - Inkbot De...](https://inkbotdesign.com/how-ai-is-changing-user-experience/)\n\n[59\\. Beyond the noise: Orchestrating AI-driven customer excellence](https://assets.kpmg.com/content/dam/kpmg/ie/pdf/2024/11/ie-customer-experience-excellence-report-2024-2.pdf)\n\n[61\\. John D. Lee, Katrina A. See. “Trust in Automation: Designing for Appropriate Reliance.” Human Factors: The Journal of Human Factors and Ergonomics Society](https://doi.org/10.1518/hfes.46.1.50_30392)\n\n[62\\. AI-Driven Design Systems: The Future of Scalable UI Frameworks](https://eajournals.org/ejcsit/wp-content/uploads/sites/21/2025/05/AI-Driven-Design-Systems.pdf)\n\n[63\\. Berkeley J. Dietvorst, J. Simmons et al. “Algorithm Aversion: People Erroneously Avoid Algorithms after Seeing Them Err.” CSN: Business (Topic)](https://doi.org/10.2139/ssrn.2466040)\n\n[64\\. Gagan Bansal, Tongshuang Sherry Wu et al. “Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance.” Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems](https://doi.org/10.1145/3411764.3445717)\n\n[65\\. How We Can Make AI Smarter, Safer, and More Human-Friendly](https://iircj.org/wp-content/uploads/46.-How-We-Can-Make-AI-Smarter-Safer-and-More-Human-Friendly.pdf)\n\n[66\\. Yunfeng Zhang, Q. Liao et al. “Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making.” Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency](https://doi.org/10.1145/3351095.3372852)\n\n[67\\. Gagan Bansal, Besmira Nushi et al. “Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance.” AAAI Conference on Human Computation & Crowdsourcing](https://doi.org/10.1609/hcomp.v7i1.5285)\n\n[68\\. The State of AI in 2025](https://www.genaicollective.ai/s/The-State-of-AI-in-2025.pdf)\n\n[69\\. REVOLUTIONIZING CUSTOMER SUPPORT: A DEEP DIVE INTO SALESFORCE AGENTFORCE AND LAM TECHNOLOGY](https://www.irjmets.com/uploadedfiles/paper/issue_2_february_2025/67118/final/fin_irjmets1739032501.pdf)\n\n[70\\. AI Case Studies That Revolutionised Businesses for 2025](https://www.growthjockey.com/blogs/ai-case-study)\n\n[71\\. 20 Generative AI Case Studies \\[2024\\] - DigitalDefynd](https://digitaldefynd.com/IQ/generative-ai-case-studies/)\n\n[72\\. 2024 Was a Watershed Moment for Artificial Intelligenc...](https://hackernoon.com/2024-was-a-watershed-moment-for-artificial-intelligence)\n\n[73\\. 盘点2024年的“AI 事故”:内容垃圾、幻觉与滥用](http://h5.ifeng.com/c/vivo/v002T8flgcFBxhrkIf7ZdIOZ-_DzqVJFa0VVS12EMcGGnCLk__)\n\n[74\\. Artificial Intelligence Index Report 2024](https://www.hkdca.com/wp-content/uploads/2024/06/ai-index-report-2024-hai.pdf)\n\n[75\\. Qatar University Research Magazine](https://www.qu.edu.qa/ar/research/publications/documents/english-22.pdf)\n\n[76\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[77\\. Scaling AI in 2025: The IT trends that separate AI Scalers from Nonscalers](https://assets.asana.biz/m/768c1d54926072a/original/-LAB26-IT-ScalingAIin2025.pdf)\n\n[78\\. 都2025年了,人们到底在用AI做什么?国外大牛总结了100个案例](https://www.360doc.cn/article/29699259_1153059101.html)\n\n[79\\. Advancements in Explainable Artificial Intelligence for Enhanced Transparency and Interpretability across Business Applications](https://www.astesj.com/publications/ASTESJ_090502.pdf)\n\n[80\\. Biases in Data in AI and Strategies to Tackle Them](https://advantiss.com/biases-in-data-in-ai-and-strategies-to-tackle-them/)\n\n[81\\. What Is AI Alignment? - IBM](https://www.ibm.com/think/topics/ai-alignment#:~:text=For%20example,%20if%20you%20ask,phase%20of%20model%20fine-tuning.)\n\n[82\\. AI Super Alignment: Aristotle’s Enduring Ethical Blueprint](https://adammvictor.com/ai/ai-super-alignment-aristotles-enduring-ethical-blueprint/)\n\n[83\\. Anna Jobin, M. Ienca et al. “Artificial Intelligence: the global landscape of ethics guidelines.” ArXiv](https://doi.org/10.1038/s42256-019-0088-2)\n\n[84\\. AI alignment vs AI ethical treatment: Ten challenges](https://globalprioritiesinstitute.org/wp-content/uploads/Adam-Bradley-and-Bradford-Saad-AI-alignment-vs-AI-ethical-treatment_-Ten-challenges.pdf)\n\n[85\\. This AI Paper from KAIST AI Unveils ORPO: Elevating Preference Alignment in Language Models to New Heights](https://www.marktechpost.com/2024/03/20/this-ai-paper-from-kaist-ai-unveils-orpo-elevating-preference-alignment-in-language-models-to-new-heights/)\n\n[86\\. Ethical AI in practice: Balancing technological advancements with human values](https://ijsra.net/sites/default/files/IJSRA-2024-0218.pdf)\n\n[87\\. E. Awad, Sohan Dsouza et al. “The Moral Machine experiment.” Nature](https://doi.org/10.1038/s41586-018-0637-6)\n\n[88\\. D. Greene, A. Hoffmann et al. “Better, Nicer, Clearer, Fairer: A Critical Assessment of the Movement for Ethical Artificial Intelligence and Machine Learning.” Hawaii International Conference on System Sciences](https://doi.org/10.24251/HICSS.2019.258)\n\n[89\\. Technical AI Ethics](https://aiindex.stanford.edu/wp-content/uploads/2023/04/HAI_AI-Index-Report-2023_CHAPTER_3.pdf)\n\n[90\\. Thilo Hagendorff. “The Ethics of AI Ethics: An Evaluation of Guidelines.” Minds and Machines](https://doi.org/10.1007/s11023-020-09517-8)\n\n[91\\. Jon Chun, Katherine Elkins. “Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative Values.” ArXiv](https://doi.org/10.48550/arXiv.2402.01651)\n\n[92\\. AI Innovation and Ethics with AI Safety and Alignment](https://www.fiddler.ai/blog/ai-innovation-and-ethics-with-ai-safety-and-alignment)\n\n[93\\. Ethical AI Governance: Methods for Evaluating Trustworthy AI](https://ceur-ws.org/Vol-3948/paper7.pdf)\n\n[94\\. B. Mittelstadt. “Principles alone cannot guarantee ethical AI.” Nature Machine Intelligence](https://doi.org/10.1038/s42256-019-0114-4)\n\n[95\\. Freedom from Fear Magazine](https://www.bullseyeproject.eu/wp-content/uploads/2022/01/F3-The-past-the-present-and-the-future-in-our-hands.pdf)\n\n[96\\. Beyond Preferences in AI Alignment](https://arxiv.org/pdf/2408.16984)\n\n[97\\. 知情人工智能监管：使用基于道德的审计来评估道德推理和规范价值，比较领先的 LLM 聊天机器人的道德框架 \\*](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_2_6/2402.01651.pdf)\n\n[98\\. Thilo Hagendorff, Sarah Fabi. “Methodological reflections for AI alignment research using human feedback.” ArXiv](https://doi.org/10.48550/arXiv.2301.06859)\n\n[99\\. Aligning artificial intelligence with moral intuitions: an intuitionist approach to the alignment problem | AI and Ethics](https://link.springer.com/article/10.1007/s43681-024-00496-5)\n\n[101\\. Security of AI-Systems: Fundamentals](https://www.bsi.bund.de/SharedDocs/Downloads/EN/BSI/Publications/Studies/KI/P464_Provision_use_external_data_trained_models.pdf?__blob=publicationFile&v=7)\n\n[102\\. Fairness in design : a tool for guidance for ethical artificial intelligence design](https://dr.ntu.edu.sg/bitstream/10356/154153/2/NTU_Thesis.pdf)\n\n[103\\. Implementing Responsible AI: Tensions and Trade-Offs Between Ethics Aspects](https://arxiv.org/pdf/2304.08275)\n\n[104\\. Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)](https://www.csiro.au/-/media/D61/Reports/Artificial-Intelligence-ethics-framework.pdf)\n\n[105\\. Ninareh Mehrabi, Fred Morstatter et al. “A Survey on Bias and Fairness in Machine Learning.” ACM Computing Surveys (CSUR)](https://doi.org/10.1145/3457607)\n\n[106\\. Moritz Hardt, Eric Price et al. “Equality of Opportunity in Supervised Learning.” ArXiv](https://arxiv.org/abs/1610.02413)\n\n[107\\. Paths to AI Accountability: Design, Measurement, and the Law](https://dspace.mit.edu/bitstream/handle/1721.1/158476/cen-shcen-phd-eecs-2024-thesis.pdf?sequence=-1&isAllowed=y)\n\n[108\\. Fairness in Design: A Framework for Facilitating Ethical Artificial Intelligence Designs](https://www.sciopen.com/article_pdf/1630468431960289282.pdf)\n\n[109\\. C. Dwork, Moritz Hardt et al. “Fairness through awareness.” ArXiv](https://doi.org/10.1145/2090236.2090255)\n\n[110\\. Methodology and Tools For Designing Ethical Artificial Intelligence Systems](https://dr.ntu.edu.sg/bitstream/10356/169362/4/Thesis_Report_Zhang_Jiehuang.pdf)\n\n[111\\. Fairness in AI: Impact and Opportunities](https://aiasiapacific.org/wp-content/uploads/2023/08/Fairness-in-AI-AIAPI-Report-2023.pdf)\n\n[112\\. Fairness in design: a framework for facilitating ethical artificial intelligence designs](https://dr.ntu.edu.sg/bitstream/10356/174587/2/Fairness_in_Design_A_Framework_for_Facilitating_Ethical_Artificial_Intelligence_Designs.pdf)\n\n[113\\. Implementing Responsible AI: Tensions and Trade-Offs ...](https://montrealethics.ai/implementing-responsible-ai-tensions-and-trade-offs-between-ethics-aspects/)\n\n[114\\. D1.1 User Requirements and Architecture](https://mammoth-ai.eu/wp-content/uploads/2023/11/d1.1-v1.0.pdf)\n\n[115\\. FINANCE21](https://worldcommercereview.com/wp-content/uploads/2025/03/Finance21Spring2025.pdf)\n\n[116\\. Ethical AI in Life and Non-Life Insurance: A Framework for Mapping Ethical Trade-offs in AI Use](https://fogp.dk/media/n23fabwb/ethical-ai-in-life-and-non-life-insurance-final-oktober-2024.pdf)\n\n[117\\. J. Kleinberg, S. Mullainathan et al. “Inherent Trade-Offs in the Fair Determination of Risk Scores.” Information Technology Convergence and Services](https://doi.org/10.4230/LIPIcs.ITCS.2017.43)\n\n[118\\. S. Corbett-Davies, E. Pierson et al. “Algorithmic Decision Making and the Cost of Fairness.” Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining](https://doi.org/10.1145/3097983.309809)\n\n[119\\. A Legal Framework for Artificial Intelligence Fairness Reporting](https://law.nus.edu.sg/trail/wp-content/uploads/sites/9/2022/06/07_2022_Ernest.pdf)\n\n[121\\. Marco Tulio Ribeiro, Sameer Singh et al. ““Why Should I Trust You?”: Explaining the Predictions of Any Classifier.” Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining](https://doi.org/10.1145/2939672.2939778)\n\n[122\\. The 2025 Edge AI Technology Report](https://www.hkdca.com/wp-content/uploads/2025/06/edge-ai-technology-report-2025.pdf)\n\n[123\\. PLATFORM ENGINEERING AND SERVICE AUTOMATION IN HEALTHCARE: A CASE STUDY APPROACH](https://www.irjmets.com/uploadedfiles/paper/issue_3_march_2025/69851/final/fin_irjmets1744008557.pdf)\n\n[124\\. Top AI Agents Use case for Healthcare in 2025](https://www.upskillist.com/blog/top-ai-agents-use-case-for-healthcare-in-2025/)\n\n[125\\. 2025-2030年人工智能在医疗诊断中的应用研究报告](https://www.chinairn.com/userfiles/20250307/20250307144018598.pdf)\n\n[126\\. AI-driven cloud-edge synergy in telecom: An approach for real-time data processing and latency optimization](https://journalwjaets.com/sites/default/files/fulltext_pdf/WJAETS-2025-0166.pdf)\n\n[127\\. Alexey Dosovitskiy, G. Ros et al. “CARLA: An Open Urban Driving Simulator.” Conference on Robot Learning](https://arxiv.org/abs/1711.03938)\n\n[128\\. Z. Obermeyer, Brian W. Powers et al. “Dissecting racial bias in an algorithm used to manage the health of populations.” Science](https://doi.org/10.1126/science.aax2342)\n\n[129\\. Edge computing and ai for real-time enterprise innovation: Transforming business operations through low-latency analytics](https://journalwjarr.com/sites/default/files/fulltext_pdf/WJARR-2025-1180.pdf)\n\n[130\\. Advances in Artificial Intelligence: Current Trends and Future Directions](http://sprcopen.org/index.php/FAIR/article/download/36/32)\n\n[131\\. AI in 2025: Structured Strategic Insights for Decision-Makers](https://ugc.production.linktr.ee/db4b3471-a59d-492a-ab25-19d4df2c6039_AI-in-2025--Structured-Strategic-Insights-for-Decision-Makers--1-.pdf)\n\n\\[132. 15+ Groundbreaking AI Use Cases in Healthcare [2025 ...](https://trreta.com/blog/ai-healthcare-use-cases)\n\n[133\\. Revolutionizing Healthcare Diagnostics: Sophisticated AI methodologies for accuracy and accuracy in chronic disease forecasting](https://www.irjet.net/archives/V12/i2/IRJET-V12I226.pdf)\n\n[134\\. AI Model Optimization Techniques for Enhanced Performance in 2025](https://www.netguru.com/blog/ai-model-optimization#:~:text=AI%20model%20optimization%20is%20the,ability%20to%20perform%20tasks%20well.)\n\n[135\\. Case Studies and Success Stories of a Decade of Research in Applied Artificial Intelligence](https://easychair.org/publications/preprint/hnFM/open)\n\n[136\\. AI IN HEALTHCARE: THE NEXT FRONTIER IN MEDICAL DIAGNOSIS](https://philpapers.org/archive/MOHAIH.pdf)\n\n[137\\. Implementation of ai in healthcare in 2025](https://callin.io/implementation-of-ai-in-healthcare/)\n\n[141\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[142\\. I. Goodfellow, Jean Pouget-Abadie et al. “Generative adversarial networks.” Communications of the ACM](https://doi.org/10.1145/3422622)\n\n[143\\. Reinforcement Learning From Human Feedback For Ethically Robust Ai Decision-Making](https://stars.library.ucf.edu/cgi/viewcontent.cgi?article=1164&context=hut2024)\n\n[144\\. P. Christiano, J. Leike et al. “Deep Reinforcement Learning from Human Preferences.” ArXiv](https://arxiv.org/abs/1706.03741)\n\n[145\\. Dario Amodei, C. Olah et al. “Concrete Problems in AI Safety.” ArXiv](https://arxiv.org/abs/1606.06565)\n\n[146\\. Top Metrics for Evaluating Ethical AI Frameworks](https://magai.co/evaluating-ethical-ai-frameworks/)\n\n[147\\. Technical AI Ethics](https://aiindex.stanford.edu/wp-content/uploads/2023/04/HAI_AI-Index-Report-2023_CHAPTER_3.pdf)\n\n[148\\. AI Performance Metrics: The Science & Art of Measuring AI - Version 1](https://www.version1.com/blog-ai-performance-metrics-the-science-and-art-of-measuring-ai/)\n\n[149\\. What Is AI Alignment?](https://www.ibm.com/think/topics/ai-alignment)\n\n[150\\. Leadership in the age of AI: Review of quantitative models and visualization for managerial decision-making](https://hal.science/hal-05052839v1/document)\n\n[151\\. Anna Jobin, M. Ienca et al. “Artificial Intelligence: the global landscape of ethics guidelines.” ArXiv](https://doi.org/10.1038/s42256-019-0088-2)\n\n[152\\. Ruibo Liu, Jerry Wei et al. “Best Practices and Lessons Learned on Synthetic Data for Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2404.07503)\n\n[153\\. From Instructions to Basic Human Values: A Survey of Alignment Goals for Big Models](https://openreview.net/pdf/17341b7efbc6ff01fb2cbc3ac6ec4f8d4063c916.pdf)\n\n[154\\. UK Intelligence Community Postdoctoral Research Fellowships 2025](https://raeng.org.uk/media/wqldcci3/uk-ic-postdoc-rf-topic-list-2025.pdf)\n\n[155\\. IBM Synthetic Data Sets](https://www.redbooks.ibm.com/redpapers/pdfs/redp5748.pdf)\n\n[156\\. Developing Ethical AI Frameworks: A Comparative Analysis of Global Standards and Practices](https://tijer.org/tijer/papers/TIJER2403133.pdf)\n\n[157\\. Is Synthetic Data all We Need? Benchmarking the Robustness of Models Trained with Synthetic Images](https://openaccess.thecvf.com/content/CVPR2024W/SyntaGen/papers/Singh_Is_Synthetic_Data_all_We_Need_Benchmarking_the_Robustness_of_CVPRW_2024_paper.pdf)\n\n[158\\. TRIAGE: Ethical Benchmarking of AI Models Through Mass Casualty Simulations](https://openreview.net/pdf?id=hFtmNJj4do)\n\n[161\\. AI Model Optimization Techniques for Enhanced Performance in 2025](https://www.netguru.com/blog/ai-model-optimization#:~:text=AI%20model%20optimization%20is%20the,ability%20to%20perform%20tasks%20well.)\n\n[162\\. Ninareh Mehrabi, Fred Morstatter et al. “A Survey on Bias and Fairness in Machine Learning.” ACM Computing Surveys (CSUR)](https://doi.org/10.1145/3457607)\n\n[163\\. Moritz Hardt, Eric Price et al. “Equality of Opportunity in Supervised Learning.” ArXiv](https://arxiv.org/abs/1610.02413)\n\n[164\\. Addressing interpretability fairness & privacy in machine learning through combinatorial optimization methods](https://theses.hal.science/tel-04429697v2/file/2023TOU30199a.pdf)\n\n[165\\. Artificial Intelligence Index Report 2023](https://event-cdn.baai.ac.cn/file/file-browser/C7FA4aFhrT2Hrnm77AKZPww62Ywm7Pyk.pdf)\n\n[166\\. C. Dwork, Moritz Hardt et al. “Fairness through awareness.” ArXiv](https://doi.org/10.1145/2090236.2090255)\n\n[167\\. Evaluating AI Agents in 2025 - by Nilesh Barla - Adaline Labs](https://labs.adaline.ai/p/evaluating-ai-agents-in-2025)\n\n[168\\. Interpretable and Automated Bias Detection for AI in Healthcare](https://dspace.mit.edu/bitstream/handle/1721.1/158474/alexiev-calexiev-sm-eecs-2024-thesis.pdf?sequence=-1&isAllowed=y)\n\n[169\\. NIST Trustworthy and Responsible AI NIST AI 100-2e2025 Adversarial Machine Learning A Taxonomy and Terminology of Attacks and Mitigations](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2025.pdf)\n\n[170\\. A Universal Fairness Evaluation Framework For Resource Allocation In Cloud Computing](http://www.ijies.net/finial-docs/finial-manuscripts/21592618052023M.Tech_CSE.Review-Paper.docx)\n\n[171\\. A. Chouldechova. “Fair prediction with disparate impact: A study of bias in recidivism prediction instruments.” Big data](https://doi.org/10.1089/big.2016.0047)\n\n[172\\. 9 Best LLM Evaluation Tools of 2025 - thedatascientist.com](https://thedatascientist.com/9-best-llm-evaluation-tools-of-2025/)\n\n[173\\. Hana Samad, Michael Akinwumi et al. “Selecting for Less Discriminatory Algorithms: A Relational Search Framework for Navigating Fairness-Accuracy Trade-offs in Practice.”](https://arxiv.org/abs/2506.01594)\n\n[174\\. Exploring Accuracy-Fairness Trade-off in Large Language ...](https://arxiv.org/html/2411.14500v1)\n\n[175\\. J. Kleinberg, S. Mullainathan et al. “Inherent Trade-Offs in the Fair Determination of Risk Scores.” Information Technology Convergence and Services](https://doi.org/10.4230/LIPIcs.ITCS.2017.43)\n\n[176\\. Best AI Fairness Tools for 2025](https://www.byteplus.com/en/topic/381760)\n\n[177\\. Advanced Payments & Fintech Report 2025](https://hkifoa.com/wp-content/uploads/2024/12/advanced-payments-fintech-report-2025-edgar-dunn.pdf)\n\n[178\\. Dynamic Partitioning of AI Models Across Clusters](https://www.modular.com/ai-resources/dynamic-partitioning-of-ai-models-across-server-clusters)\n\n[181\\. THE HEART OF INFOTAINMENT SYSTEMS: VEHICLE INTERFACE PROCESSOR (VIP) ON SOC](https://iaeme.com/MasterAdmin/Journal_uploads/IJITMIS/VOLUME_16_ISSUE_2/IJITMIS_16_02_042.pdf)\n\n[182\\. Research on Safety Testing Method for Autonomous Driving Based on Visual System in Complex Urban Road Conditions](http://www.hume-ai.hk/index.php/zk/article/download/80/47/363)\n\n[183\\. Alexey Dosovitskiy, G. Ros et al. “CARLA: An Open Urban Driving Simulator.” Conference on Robot Learning](https://arxiv.org/abs/1711.03938)\n\n[184\\. Mariusz Bojarski, D. Testa et al. “End to End Learning for Self-Driving Cars.” ArXiv](https://arxiv.org/abs/1604.07316)\n\n[185\\. On Green Edge Computing with Machine Learning Applications](https://dspace.networks.imdea.org/bitstream/handle/20.500.12761/1835/Francesco_Spinelli_thesis%202024.pdf?sequence=1)\n\n[186\\. Autonomous Vehicle Safety Systems in 2025](https://motorwatt.com/ev-blog/trends/autonomous-vehicle-safety-systems-2025)\n\n[187\\. PERCEPTION SYSTEMS FOR AUTONOMOUS VEHICLES: RECENT ADVANCES IN SENSORS, DATA FUSION, AND AI](https://www.irjmets.com/uploadedfiles/paper/issue_3_march_2025/68587/final/fin_irjmets1741356658.pdf)\n\n[188\\. ZeyuLing (zyling) · GitHub](https://github.com/ZeyuLing)\n\n[189\\. Yuchi Tian, Kexin Pei et al. “DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars.” 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)](https://doi.org/10.1145/3180155.3180220)\n\n[190\\. Behavior-Driven Falsification of Autonomous Driving Stack Systems with LLM-Generated Scenarios](https://jeevithanmahenthran.com/docs/cse233_final_report.pdf)\n\n[191\\. Driving Intelligence Validation Platform 令和5年度「無人自動運転等のCASE対応に向けた実証・支援事業(仮想空間での自動運転安全性評価環境の構築)」\\_2023年度成果報告](https://divp.net/cms/wp-content/uploads/2025/01/2023%E5%B9%B4%E5%BA%A6_%E6%B4%BB%E5%8B%95%E5%A0%B1%E5%91%8A%E6%9B%B8.pdf)\n\n[192\\. Guy Katz, Clark W. Barrett et al. “Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks.” ArXiv](https://doi.org/10.1007/978-3-319-63387-9_5)\n\n[193\\. A case study of unavoidable accidents of autonomous vehicles](http://www.ircobi.org/wordpress/downloads/irc23/pdf-files/23115.pdf)\n\n[194\\. Moritz Klischat, M. Althoff. “Generating Critical Test Scenarios for Automated Vehicles with Evolutionary Algorithms.” 2019 IEEE Intelligent Vehicles Symposium (IV)](https://doi.org/10.1109/IVS.2019.8814230)\n\n[195\\. Data Fusion and Optimization Techniques for Enhancing Autonomous Vehicle Performance in Smart Cities](https://woodyinternational.com/index.php/jaii/article/download/50/40/52)\n\n[196\\. Use Cases](https://elsa-ai.eu/use-cases/)\n\n[197\\. AdvSim: Generating Safety-Critical Scenarios for Self-Driving Vehicles](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_AdvSim_Generating_Safety-Critical_Scenarios_for_Self-Driving_Vehicles_CVPR_2021_paper.pdf)\n\n[198\\. Autonomous Vehicle Decision-Making and Control in Complex and Unconventional Scenarios—A Review](https://mdpi-res.com/d_attachment/machines/machines-11-00676/article_deploy/machines-11-00676.epub)\n\n[201\\. Ninareh Mehrabi, Fred Morstatter et al. “A Survey on Bias and Fairness in Machine Learning.” ACM Computing Surveys (CSUR)](https://doi.org/10.1145/3457607)\n\n[202\\. Joy Buolamwini, Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” FAT](https://www.semanticscholar.org/paper/18858cc936947fc96b5c06bbe3c6c2faa5614540)\n\n[203\\. Dario Amodei, C. Olah et al. “Concrete Problems in AI Safety.” ArXiv](https://arxiv.org/abs/1606.06565)\n\n[204\\. Developing Ethical AI Frameworks: A Comparative Analysis of Global Standards and Practices](https://tijer.org/tijer/papers/TIJER2403133.pdf)\n\n[205\\. AAAI Lab for Innovative Uses of Synthetic Data](https://www.vanderschaar-lab.com/wp-content/uploads/2022/08/AAAI_Synthetic-Data-Tutorial.pdf)\n\n[206\\. Top Metrics for Evaluating Ethical AI Frameworks](https://magai.co/evaluating-ethical-ai-frameworks/)\n\n[207\\. ETHICAL AI IMPLEMENTATION IN ENTERPRISE SOFTWARE: A TECHNICAL ANALYSIS OF SALESFORCE'S TRUST-BUILDING FRAMEWORK AND EINSTEIN GPT ARCHITECTURE](https://www.irjmets.com/uploadedfiles/paper/issue_2_february_2025/68209/final/fin_irjmets1740666172.pdf)\n\n[208\\. D. Pedreschi, S. Ruggieri et al. “Discrimination-aware data mining.” Knowledge Discovery and Data Mining](https://doi.org/10.1145/1401890.1401959)\n\n[209\\. Research: Ethical and Human-Centered AI/Process Frameworks](https://meta.wikimedia.org/wiki/Research:Ethical_and_human-centered_AI/Process_frameworks)\n\n[210\\. AI Super Alignment: Aristotle’s Enduring Ethical Blueprint](https://adammvictor.com/ai/ai-super-alignment-aristotles-enduring-ethical-blueprint/)\n\n[211\\. Harini Suresh, J. Guttag. “A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle.” Equity and Access in Algorithms, Mechanisms, and Optimization](https://doi.org/10.1145/3465416.3483305)\n\n[212\\. A Framework to Measure Ethical, Equitable, and Integrated AI Algorithms](https://dt4regions.eu/dt-book/dt-stories/framework-measure-how-ethic-equitable-and-integrated-ai-algorithm)\n\n[213\\. THE SOCIETAL IMPACT OF GENERATIVE AI IN NETWORK COMMUNICATIONS](https://www.irjmets.com/uploadedfiles/paper/issue_3_march_2025/68497/final/fin_irjmets1741158776.pdf)\n\n[214\\. IBM Synthetic Data Sets](https://www.redbooks.ibm.com/redpieces/pdfs/redp5748.pdf)\n\n[215\\. A. Patrikar, Arjuna Mahenthiran et al. “Leveraging synthetic data for AI bias mitigation.” Defense + Commercial Sensing](https://doi.org/10.1117/12.2662276)\n\n[216\\. Towards Trusted AI: A Blueprint for Ethics Assessment in Practice](https://drops.dagstuhl.de/storage/01oasics/oasics-vol126-saia2024/OASIcs.SAIA.2024.7/OASIcs.SAIA.2024.7.pdf)\n\n[217\\. Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms](https://siliconflatirons.org/wp-content/uploads/2021/02/Algorithmic-bias-detection-and-mitigation_-Best-practices-and-policies-to-reduce-consumer-harms.pdf)\n\n[218\\. Towards an End-to-End Personal Fine-Tuning Framework for AI Value Alignment](https://lukaspetersson.com/assets/pdf/ftva_paper.pdf)\n\n[219\\. Toward Ethical Artificial Intelligence in International Development](https://www.dai.com/uploads/ethical-ai.pdf)\n\n[221\\. K. Deb, S. Agrawal et al. “A fast and elitist multiobjective genetic algorithm: NSGA-II.” IEEE Trans. Evol. Comput.](https://doi.org/10.1109/4235.996017)\n\n[222\\. On Accelerating Edge AI: Optimizing Resource-Constrained Environments](https://arxiv.org/pdf/2501.15014)\n\n[223\\. NIST Trustworthy and Responsible AI NIST AI 100-2e2025 Adversarial Machine Learning A Taxonomy and Terminology of Attacks and Mitigations](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2025.pdf)\n\n[224\\. Moritz Hardt, Eric Price et al. “Equality of Opportunity in Supervised Learning.” ArXiv](https://arxiv.org/abs/1610.02413)\n\n[225\\. PARETO PROMPT OPTIMIZATION](https://openreview.net/pdf?id=HGCk5aaSvE)\n\n[226\\. Ai scheduling algorithms in 2025](https://callin.io/ai-scheduling-algorithms/)\n\n[227\\. syftr: Pareto-Optimal Generative AI](https://arxiv.org/pdf/2505.20266)\n\n[228\\. Michael Feldman, Sorelle A. Friedler et al. “Certifying and Removing Disparate Impact.” Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining](https://doi.org/10.1145/2783258.2783311)\n\n[229\\. To the Fairness Frontier and Beyond: Identifying, Quantifying, and Optimizing the Fairness-Accuracy Pareto Frontier](https://repository.rice.edu/bitstreams/6afa7a05-3827-4b62-982f-5e3d1a5cb1cc/download)\n\n[230\\. M. B. Zafar, Isabel Valera et al. “Fairness Constraints: Mechanisms for Fair Classification.” ArXiv](https://www.semanticscholar.org/paper/4c64ee852f082f8f4e0113cfc1302b34e31539a2)\n\n[231\\. Geoff Pleiss, Manish Raghavan et al. “On Fairness and Calibration.” The Societal Impacts of Algorithmic Decision-Making](https://doi.org/10.1145/3603195.3603198)\n\n[241\\. Joseph Redmon, Ali Farhadi. “YOLOv3: An Incremental Improvement.” ArXiv](https://arxiv.org/abs/1804.02767)\n\n[242\\. Model Compression Techniques for Seamless Cloud-to-Edge AI Development](https://ijsrm.net/index.php/ijsrm/article/download/4624/3799/17926)\n\n[243\\. Object Detection Techniques in Autonomous Driving](https://www.itm-conferences.org/articles/itmconf/pdf/2025/01/itmconf_dai2024_01019.pdf)\n\n[244\\. PERCEPTION SYSTEMS FOR AUTONOMOUS VEHICLES: RECENT ADVANCES IN SENSORS, DATA FUSION, AND AI](https://www.irjmets.com/uploadedfiles/paper/issue_3_march_2025/68587/final/fin_irjmets1741356658.pdf)\n\n[245\\. Andreas Geiger, Philip Lenz et al. “Vision meets robotics: The KITTI dataset.” The International Journal of Robotics Research](https://doi.org/10.1177/0278364913491297)\n\n[246\\. Autonomous Vehicle Safety Systems in 2025](https://motorwatt.com/ev-blog/trends/autonomous-vehicle-safety-systems-2025)\n\n[247\\. Survey of Autonomous Vehicles' Collision Avoidance Algorithms](https://pdfs.semanticscholar.org/d5a3/956fa4a3bd43a8628c143f37fc3fc548841f.pdf)\n\n[248\\. On Green Edge Computing with Machine Learning Applications](https://dspace.networks.imdea.org/bitstream/handle/20.500.12761/1835/Francesco_Spinelli_thesis%202024.pdf?sequence=1)\n\n[249\\. Yin Zhou, Oncel Tuzel. “VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection.” 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2018.00472)\n\n[250\\. Object Detection: State-of-the-Art Models in 2025 - HiringNet](https://hiringnet.com/object-detection-state-of-the-art-models-in-2025)\n\n[251\\. Deep learning for autonomous driving systems: technological innovations, strategic implementations, and business implications - a comprehensive review](https://f.oaes.cc/xmlpdf/b76b7b03-f5b2-4241-97d4-84d1faa6fb86/ces4083_down.pdf?v=46)\n\n[252\\. The 2025 Edge AI Technology Report](https://www.ceva-ip.com/wp-content/uploads/2025-Edge-AI-Technology-Report.pdf)\n\n[253\\. Deep Learning for Real Time Object Detection in Autonomous Vehicles](https://ijrpr.com/uploads/V5ISSUE12/IJRPR36039.pdf)\n\n[261\\. Ninareh Mehrabi, Fred Morstatter et al. “A Survey on Bias and Fairness in Machine Learning.” ACM Computing Surveys (CSUR)](https://doi.org/10.1145/3457607)\n\n[262\\. Moritz Hardt, Eric Price et al. “Equality of Opportunity in Supervised Learning.” ArXiv](https://arxiv.org/abs/1610.02413)\n\n[263\\. C. Dwork, Moritz Hardt et al. “Fairness through awareness.” ArXiv](https://doi.org/10.1145/2090236.2090255)\n\n[264\\. Michael Feldman, Sorelle A. Friedler et al. “Certifying and Removing Disparate Impact.” Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining](https://doi.org/10.1145/2783258.2783311)\n\n[265\\. Towards Privacy-preserving and Fairness-aware Federated Learning Framework](https://petsymposium.org/popets/2025/popets-2025-0044.pdf)\n\n[266\\. B. Zhang, Blake Lemoine et al. “Mitigating Unwanted Biases with Adversarial Learning.” Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society](https://doi.org/10.1145/3278721.3278779)\n\n[267\\. Fair Synthetic Time Series Data for Predictive Maintenance](https://uu.diva-portal.org/smash/get/diva2:1899960/FULLTEXT01.pdf)\n\n[268\\. Imposing Fairness Constraints in Synthetic Data Generation](https://proceedings.mlr.press/v238/abroshan24a/abroshan24a.pdf)\n\n[269\\. Implementing ethical principles in AI: an initial discussion](https://link.springer.com/article/10.1007/s43681-025-00710-y)\n\n[270\\. Fair synthetic data and ethical AI in healthcare with Humana](https://mostly.ai/data-democratization-podcast/fair-synthetic-data-and-ethical-ai-in-healthcare)\n\n[271\\. A. Patrikar, Arjuna Mahenthiran et al. “Leveraging synthetic data for AI bias mitigation.” Defense + Commercial Sensing](https://doi.org/10.1117/12.2662276)\n\n[272\\. R. Ramachandranpillai, Md Fahim Sikder et al. “Bt-GAN: Generating Fair Synthetic Healthdata via Bias-transforming Generative Adversarial Networks.” ArXiv](https://doi.org/10.1613/jair.1.15317)\n\n[273\\. Depeng Xu, Shuhan Yuan et al. “FairGAN: Fairness-aware Generative Adversarial Networks.” 2018 IEEE International Conference on Big Data (Big Data)](https://doi.org/10.1109/BigData.2018.8622525)\n\n[274\\. Counterfactual Fairness in Synthetic Data Generation](https://par.nsf.gov/servlets/purl/10447890)\n\n[275\\. Improving Fair Training under Correlation Shifts](https://proceedings.mlr.press/v202/roh23a/roh23a.pdf)\n\n[276\\. GAN-based Fairness-Aware Recommendation for Enhancing ...](https://dl.acm.org/doi/10.1145/3675417.3675468)\n\n[277\\. Exploring the Role of Generative Adversarial Networks (GANs) and Generative AI for Synthetic Data Generation and Augmentation in Machine Learning](https://jisem-journal.com/index.php/journal/article/download/5563/2613/9225)\n\n[278\\. Improved Value Alignment in Large Language Models Using Variational Best-of-N Techniques](https://assets-eu.researchsquare.com/files/rs-4794797/v1_covered_3163667d-ce4d-4901-9d8c-ed52e07e2edf.pdf)\n\n[281\\. K. Deb, S. Agrawal et al. “A fast and elitist multiobjective genetic algorithm: NSGA-II.” IEEE Trans. Evol. Comput.](https://doi.org/10.1109/4235.996017)\n\n[282\\. Breakthrough Advancements in Real-time Payment Processing Optimization](https://eajournals.org/ejcsit/wp-content/uploads/sites/21/2025/05/Breakthrough-Advancements.pdf)\n\n[283\\. PARETO PROMPT OPTIMIZATION](https://openreview.net/pdf?id=HGCk5aaSvE)\n\n[284\\. Moritz Hardt, Eric Price et al. “Equality of Opportunity in Supervised Learning.” ArXiv](https://arxiv.org/abs/1610.02413)\n\n[285\\. Michael Feldman, Sorelle A. Friedler et al. “Certifying and Removing Disparate Impact.” Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining](https://doi.org/10.1145/2783258.2783311)\n\n[286\\. M. B. Zafar, Isabel Valera et al. “Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment.” Proceedings of the 26th International Conference on World Wide Web](https://doi.org/10.1145/3038912.3052660)\n\n[287\\. M. B. Zafar, Isabel Valera et al. “Fairness Constraints: Mechanisms for Fair Classification.” ArXiv](https://www.semanticscholar.org/paper/4c64ee852f082f8f4e0113cfc1302b34e31539a2)\n\n[288\\. Optimizing fairness and accuracy: a Pareto optimal approach for ...](https://link.springer.com/article/10.1007/s43681-024-00508-4)\n\n[289\\. Implementing machine learning algorithms to detect and prevent financial fraud in real-time](https://www.fepbl.com/index.php/csitrj/article/view/1274/1507)\n\n[290\\. Pareto Optimization in Practice](https://www.numberanalytics.com/blog/pareto-optimization-practice)\n\n[291\\. Synergistic Optimization and Risk Control: Integrated Price Forecasting Models](https://ojs.apspublisher.com/index.php/jaet/article/download/361/323)\n\n[301\\. PERCEPTION SYSTEMS FOR AUTONOMOUS VEHICLES: RECENT ADVANCES IN SENSORS, DATA FUSION, AND AI](https://www.irjmets.com/uploadedfiles/paper/issue_3_march_2025/68587/final/fin_irjmets1741356658.pdf)\n\n[302\\. Autonomous vehicles' object detection architectures ranking based on ...](https://link.springer.com/article/10.1007/s41870-023-01517-y)\n\n[303\\. O. Khatib. “Real-Time Obstacle Avoidance for Manipulators and Mobile Robots.” The International Journal of Robotics Research](https://doi.org/10.1177/027836498600500106)\n\n[304\\. Autonomous Vehicle Safety Systems in 2025](https://motorwatt.com/ev-blog/trends/autonomous-vehicle-safety-systems-2025)\n\n[305\\. Survey of Autonomous Vehicles' Collision Avoidance Algorithms](https://pdfs.semanticscholar.org/d5a3/956fa4a3bd43a8628c143f37fc3fc548841f.pdf)\n\n[306\\. J. Kuchar, L. Yang. “A review of conflict detection and resolution modeling methods.” IEEE Trans. Intell. Transp. Syst.](https://doi.org/10.1109/6979.898217)\n\n[307\\. Santokh Singh. “Critical Reasons for Crashes Investigated in the National Motor Vehicle Crash Causation Survey.”](https://www.semanticscholar.org/paper/4532a641cb16fe38d256a740093e8219c9bc3dd8)\n\n[308\\. You Li, J. Ibañez-Guzmán. “Lidar for Autonomous Driving: The Principles, Challenges, and Trends for Automotive Lidar and Perception Systems.” IEEE Signal Processing Magazine](https://doi.org/10.1109/MSP.2020.2973615)\n\n[321\\. I. Goodfellow, Jean Pouget-Abadie et al. “Generative adversarial networks.” Communications of the ACM](https://doi.org/10.1145/3422622)\n\n[322\\. Imposing Fairness Constraints in Synthetic Data Generation](https://proceedings.mlr.press/v238/abroshan24a/abroshan24a.pdf)\n\n[323\\. R. Ramachandranpillai, Md Fahim Sikder et al. “Bt-GAN: Generating Fair Synthetic Healthdata via Bias-transforming Generative Adversarial Networks.” ArXiv](https://doi.org/10.1613/jair.1.15317)\n\n[324\\. DECAF: Generating Fair Synthetic Data Using Causally-Aware Generative Networks](https://papers.nips.cc/paper/2021/file/ba9fab001f67381e56e410575874d967-Paper.pdf)\n\n[325\\. Depeng Xu, Shuhan Yuan et al. “FairGAN: Fairness-aware Generative Adversarial Networks.” 2018 IEEE International Conference on Big Data (Big Data)](https://doi.org/10.1109/BigData.2018.8622525)\n\n[326\\. Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic Data Generation and Fairness Algorithms](https://arxiv.org/pdf/2501.01785)\n\n[327\\. amirarsalan90/TabFairGAN](https://github.com/amirarsalan90/TabFairGAN)\n\n[328\\. Fair Synthetic Time Series Data for Predictive Maintenance](https://uu.diva-portal.org/smash/get/diva2:1899960/FULLTEXT01.pdf)\n\n[329\\. Generative AI mitigates representation bias and improves model fairness through synthetic health data](https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1013080&type=printable)\n\n[330\\. Fairness-Optimized Synthetic EHR Generation for Arbitrary Downstream Predictive Tasks](https://arxiv.org/pdf/2406.02510)\n\n[331\\. Synthetic Data Generation for Emergency Medical Systems: A Systematic Comparison of Tabular GAN Extensions](https://www.scitepress.org/Papers/2025/133072/133072.pdf)\n\n[332\\. Diving Deep Into Fair Synthetic Data Generation (Fairness Series Part 5)](https://mostly.ai/blog/diving-deep-into-fair-synthetic-data-generation-fairness-series-part-5/)\n\n[341\\. Richard B. Carter, Steven Manaster. “Initial Public Offerings and Underwriter Reputation.” Journal of Finance](https://doi.org/10.1111/J.1540-6261.1990.TB02426.X)\n\n[342\\. Kevin F. Rock. “Why new issues are underpriced.” Journal of Financial Economics](https://doi.org/10.1016/0304-405X%2886%2990054-1)\n\n[343\\. Randolph P. Beatty, J. Ritter. “INVESTMENT BANKING, REPUTATION, AND THE UNDERPRICING OF INITIAL PUBLIC OFFERINGS\\*.” Journal of Financial Economics](https://doi.org/10.1016/0304-405X%2886%2990055-3)\n\n[344\\. Lawrence M. Benveniste, P. Spindt. “How investment bankers determine the offer price and allocation of new issues.” Journal of Financial Economics](https://doi.org/10.1016/0304-405X%2889%2990051-2)\n\n[345\\. R. Ibbotson, J. Sindelar et al. “INITIAL PUBLIC OFFERINGS.” Journal of Applied Corporate Finance](https://doi.org/10.1111/J.1745-6622.1988.TB00164.X)\n\n[346\\. Fraud Detection Agent vs Traditional Monitoring in 2025](https://vlinkinfo.com/blog/fraud-detection-agent-vs-traditional-risk-monitoring)\n\n[347\\. Fairness-Accuracy Trade-Offs: A Causal Perspective](https://arxiv.org/abs/2405.15443)\n\n[348\\. Top 6 API Integration Advancements for Secure Banking ...](https://www.numberanalytics.com/blog/secure-api-advancements-banking-services)\n\n[349\\. Product Differentiation, Benchmarking, and Corporate Fraud](https://acfr.aut.ac.nz/__data/assets/pdf_file/0003/188607/Product-Market-Competition-and-Corporate-Fraud_July-2018.pdf)\n\n[351\\. Survey of Autonomous Vehicles' Collision Avoidance Algorithms](https://pdfs.semanticscholar.org/d5a3/956fa4a3bd43a8628c143f37fc3fc548841f.pdf)\n\n[352\\. Vehicle Detection Techniques for Collision Avoidance S...](https://dl.acm.org/doi/10.1109/TITS.2015.2409109)\n\n[353\\. Intelligent Collision Prevention System for Enhanced Road Safety](https://ijmtst.com/volume11/issue03/13IJMTST1103064.pdf)\n\n[354\\. The 2025 Edge AI Technology Report](https://www.hkdca.com/wp-content/uploads/2025/06/edge-ai-technology-report-2025.pdf)\n\n[355\\. ACAS Guide Airborne Collision Avoidance Systems](https://skybrary.aero/sites/default/files/bookshelf/34051.pdf)\n\n[356\\. REACT: Runtime-Enabled Active Collision-avoidance ...](https://arxiv.org/abs/2505.11474)\n\n[357\\. Obstacle Detection and Track Detection in Autonomous Cars](https://www.intechopen.com/chapters/69747)\n\n[358\\. Improvement in Collision Avoidance in Cut-In Maneuvers...](https://www.mdpi.com/2624-6511/8/1/15/xml)\n\n[359\\. Collision-Avoidance System](https://encyclopedia2.thefreedictionary.com/collision-avoidance+system)\n\n[371\\. Individualized Treat, Jinsung Yoon. “GENERATIVE ADVERSARIAL NETS.”](https://www.semanticscholar.org/paper/c68796f833a7151f0a63d1d1608dc902b4fdc9b6)\n\n[372\\. Imposing Fairness Constraints in Synthetic Data Generation](https://proceedings.mlr.press/v238/abroshan24a/abroshan24a.pdf)\n\n[373\\. DECAF: Generating Fair Synthetic Data Using Causally-Aware Generative Networks](https://papers.nips.cc/paper/2021/file/ba9fab001f67381e56e410575874d967-Paper.pdf)\n\n[374\\. amirarsalan90/TabFairGAN](https://github.com/amirarsalan90/TabFairGAN)\n\n[375\\. Fairness-Optimized Synthetic EHR Generation for Arbitrary Downstream Predictive Tasks](https://arxiv.org/pdf/2406.02510)\n\n[376\\. Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic Data Generation and Fairness Algorithms](https://arxiv.org/pdf/2501.01785)\n\n[377\\. GANs for Synthetic Data Generation](https://towardsai.net/p/l/gans-for-synthetic-data-generation)\n\n[378\\. Fair Synthetic Time Series Data for Predictive Maintenance](https://uu.diva-portal.org/smash/get/diva2:1899960/FULLTEXT01.pdf)\n\n[379\\. Diving Deep Into Fair Synthetic Data Generation (Fairness Series Part 5)](https://mostly.ai/blog/diving-deep-into-fair-synthetic-data-generation-fairness-series-part-5/)\n\n[380\\. Generative AI mitigates representation bias and improves model fairness through synthetic health data](https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1013080&type=printable)\n\n[381\\. Ninareh Mehrabi, Fred Morstatter et al. “A Survey on Bias and Fairness in Machine Learning.” ACM Computing Surveys (CSUR)](https://doi.org/10.1145/3457607)\n\n[382\\. Depeng Xu, Shuhan Yuan et al. “FairGAN: Fairness-aware Generative Adversarial Networks.” 2018 IEEE International Conference on Big Data (Big Data)](https://doi.org/10.1109/BigData.2018.8622525)\n\n[391\\. Joy Buolamwini, Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” FAT](https://www.semanticscholar.org/paper/18858cc936947fc96b5c06bbe3c6c2faa5614540)\n\n[392\\. Moritz Hardt, Eric Price et al. “Equality of Opportunity in Supervised Learning.” ArXiv](https://arxiv.org/abs/1610.02413)\n\n[393\\. B. A. Jain, Omesh Kini. “The Post-Issue Operating Performance of IPO Firms.” Journal of Finance](https://doi.org/10.1111/J.1540-6261.1994.TB04778.X)\n\n[394\\. Geoff Pleiss, Manish Raghavan et al. “On Fairness and Calibration.” The Societal Impacts of Algorithmic Decision-Making](https://doi.org/10.1145/3603195.3603198)\n\n[395\\. R. Ibbotson, J. Sindelar et al. “INITIAL PUBLIC OFFERINGS.” Journal of Applied Corporate Finance](https://doi.org/10.1111/J.1745-6622.1988.TB00164.X)\n\n[396\\. 7 predictions for 2025 - The year AI fights back against scams](https://www.sardine.ai/es/blog/2025-fraud-compliance-predictions)\n\n[397\\. Liveness Detection: A Complete Guide for Fraud ...](https://sumsub.com/blog/face-liveness-detection/)\n\n[398\\. ROBUST LOSS AND PENALTY FOR FAIR MACHINE LEARNING](https://www.cos.ufrj.br/uploadfile/publicacao/3171.pdf)\n\n[399\\. Fraud Detection Agent vs Traditional Monitoring in 2025](https://vlinkinfo.com/blog/fraud-detection-agent-vs-traditional-risk-monitoring)\n\n[400\\. Peer-to-Peer Payment App Statistics 2025](https://coinlaw.io/peer-to-peer-payment-app-statistics/)\n\n[401\\. Fraud Detection in Banking: 2025 Future Trends & ...](https://trustdecision.com/resources/blog/fraud-detection-in-banking-2025-future-trends-predictions)\n\n[402\\. Top 10 MLOps Platforms for Scalable AI in 2025](https://azumo.com/artificial-intelligence/ai-insights/mlops-platforms)\n\n[403\\. Usage-Based vs. Outcome-Based Pricing for APIs](https://www.moesif.com/blog/api-monetization/api-strategy/Usage-Based-vs-Outcome-Based-Pricing-For-APIs/)\n\n[404\\. Empirical Analysis of Privacy-Fairness-Accuracy Trade-offs ...](https://arxiv.org/html/2503.16233v1)\n\n[405\\. Financial Fraud Detection Using Explainable AI and ...](https://arxiv.org/html/2505.10050v1)\n\n[406\\. Automation In Finance Statistics 2025: AI, RPA, etc.](https://coinlaw.io/automation-in-finance-statistics/?utm_campaign=missinglettr&utm_medium=social_link&utm_source=missinglettr-instagram)\n\n[407\\. Optimization of Learning Workflows at Large Scale on High-Performance Computing Systems](https://theses.hal.science/tel-04636586v1/file/144195_EGELE_2024_archivage.pdf)\n\n[408\\. Fairness-Accuracy Trade-Offs: A Causal Perspective](https://arxiv.org/abs/2405.15443)\n\n[409\\. API - The Power that Drives New-Age Banking Ecosystems](https://m2pfintech.com/blog/api-the-power-that-drives-new-age-banking-ecosystems/)"
  },
  {
    "id": 33,
    "question": "Why choose a general model approach over domain-specific solutions, given the industry trend toward narrow AI applications?",
    "response": "**Research Report: The Strategic Imperative for General AI Models in an Era of Specialization**\n\n**Report Date:** July 26, 2025\n\n**Prepared by:** Expert Researcher\n\n**Executive Summary**\n\nThe contemporary Artificial Intelligence (AI) landscape is characterized by a strong and justifiable trend toward narrow AI applications. These domain-specific solutions, honed for singular tasks, consistently demonstrate superior accuracy, computational efficiency, and immediate return on investment (ROI) in targeted industry applications. However, a countervailing strategic paradigm is emerging, centered on the adoption of general, large-scale foundation models. This report argues that despite the proven success of narrow AI, the choice to adopt a general model approach is a crucial strategic decision for enterprises seeking long-term adaptability, development velocity, and economies of scale.\n\nThis research analyzes the technical and economic trade-offs between these two approaches. While domain-specific models currently lead in quantifiable short-term metrics, general models offer unparalleled advantages in lowering implementation barriers through transfer learning, accelerating the development of diverse applications, and providing the flexibility to adapt to unforeseen market and regulatory shifts. The report concludes that the most effective long-term strategy will involve leveraging general foundation models as a platform to rapidly develop and deploy a portfolio of fine-tuned, high-performing applications, thereby synthesizing the benefits of both paradigms. A significant finding of this research is the notable absence of long-term, comparative ROI and Total Cost of Ownership (TCO) studies, indicating the industry is still in the nascent stages of understanding the full economic lifecycle of enterprise-scale general AI.\n\n**1\\. The Prevailing Paradigm: The Dominance and Allure of Domain-Specific AI**\n\nThe current industry preference for domain-specific AI is not arbitrary; it is rooted in clear, measurable, and compelling advantages. These specialized models are engineered for precision and performance within a defined operational context, making them the default choice for organizations seeking immediate, quantifiable results.\n\n**1.1. Unmatched Performance and Accuracy**\n\nThe primary driver for domain-specific AI is its superior performance on targeted tasks. By training on highly curated, industry-specific datasets, these models develop a deep \"understanding\" of the relevant context, terminology, and data patterns.\n\n**Quantitative Superiority:** Research and practical applications consistently show that custom-built or domain-specific models outperform their generalist counterparts. For example, custom models can improve accuracy by a significant 20-30% and process queries 50% faster than general alternatives \\[8\\]. In the highly specialized field of medicine, a model like IBM's Watson for Oncology, which is optimized with vast amounts of medical literature and patient data, logically achieves higher diagnostic accuracy than a general AI without that specific training \\[45\\].\n\n**Industry Validation:** This performance boost translates directly into tangible business value. Across sectors, from legal firms reporting 40% productivity gains with custom AI \\[103\\] to healthcare providers improving diagnostic accuracy \\[114\\], the narrative is consistent: specialization yields precision \\[6\\]\\[11\\]\\[49\\].\n\n**1.2. Favorable Economics: Cost-Efficiency and Return on Investment (ROI)**\n\nThe economic case for domain-specific AI is currently more straightforward and easier to quantify.\n\n**Computational Efficiency:** Smaller, specialized models often require less computational power for inference—the day-to-day operational use of the model. This translates directly to lower running costs, a critical factor for applications that handle high volumes of transactions \\[9\\]\\[11\\]\\[16\\].\n\n**Demonstrable ROI:** This combination of high accuracy and operational efficiency creates a direct path to positive ROI. The search results are replete with evidence that organizations using tailored AI solutions are seeing measurable returns that often exceed expectations \\[106\\]. Whether in manufacturing through predictive maintenance that slashes downtime \\[109\\], or in insurance through automated claims adjudication \\[100\\]\\[110\\]specialized GenAI applications are proving their financial worth \\[101\\]. The initial investment, while potentially significant, is often justified by clear, near-term cost savings and performance gains \\[14\\].\n\n**2\\. The Emerging Challenger: The Strategic Case for General Models**\n\nWhile domain-specific models solve today's problems with precision, general foundation models represent a strategic investment in solving the problems of tomorrow. These massive models, pre-trained on a diverse, internet-scale corpus of data \\[25\\]\\[30\\]\\[80\\]are not designed for a single task but as a versatile platform for countless applications. Their value proposition is not precision out-of-the-box, but rather profound adaptability.\n\n**2.1. The Transformative Power of Transfer Learning**\n\nThe core technical advantage of foundation models is transfer learning. By absorbing general patterns, language structures, and reasoning abilities during pre-training, these models can be adapted to new, specialized tasks with remarkable efficiency \\[21\\]\\[28\\]\\[29\\].\n\n**Lowering Implementation Barriers:** The single greatest hurdle in AI adoption has historically been the availability of large, high-quality, labeled datasets. Foundation models dramatically lower this barrier. They can achieve high performance on new tasks with minimal task-specific data, a technique known as \"few-shot learning\" \\[22\\]\\[29\\]\\[34\\]. This democratizes AI development, particularly in niche domains where data is scarce \\[37\\].\n\n**Accelerating Development and Reducing Costs:** Building a sophisticated AI model from scratch is a resource-intensive endeavor, often costing hundreds of thousands of dollars and months of expert time \\[159\\]. Fine-tuning a pre-trained foundation model is vastly cheaper and faster \\[28\\]\\[37\\]\\[74\\]. The astronomical cost of training a frontier model from scratch (estimates for GPT-4 range from hundreds of millions to billions of dollars) \\[91\\]\\[164\\]makes leveraging existing models the only viable path for most enterprises \\[163\\]. Furthermore, advanced techniques like Parameter-Efficient Transfer Learning (PETL) methods (e.g., LoRA) further reduce the computational and storage costs of adaptation \\[33\\]\\[36\\]\\[35\\].\n\n**2.2. Long-Term Strategic Advantages: Adaptability and Scalability**\n\nThe choice of a general model is a strategic one, prioritizing long-term flexibility over short-term optimization.\n\n**Future-Proofing and Adaptability:** A narrow AI model is brittle; it excels at its designated task but breaks when the task changes. A business environment is anything but static. A general foundation model provides a flexible core that can be repurposed and re-tuned as business priorities, market conditions, and regulatory landscapes evolve \\[41\\]\\[51\\]. This agility is a powerful competitive advantage.\n\n**Platform for Growth and Economies of Scale:** A domain-specific approach can lead to a fragmented ecosystem of siloed AI tools, each requiring separate maintenance, monitoring, and expertise. A general model approach creates a centralized \"AI backbone.\" This platform can be used to spin up dozens of different applications, amortizing the cost and complexity of the core infrastructure across the entire organization. This creates economies of scale in development, governance, and talent.\n\n**The Trajectory of Compute:** Some research posits that as parallel computing capabilities continue their exponential growth, general solution techniques will eventually become more powerful and efficient than hand-engineered, domain-specific solutions \\[7\\]\\[44\\]. Adopting a general model architecture today positions an organization to ride this technological wave rather than be displaced by it.\n\n**3\\. Re-examining the Economics: Total Cost of Ownership and ROI**\n\nA direct comparison of the financial performance between the two approaches reveals a significant gap in available long-term data. The ROI for domain-specific models is immediate and tactical, while the ROI for general models is delayed and strategic.\n\n**3.1. The Total Cost of Ownership (TCO) Conundrum**\n\nCalculating a 5-year TCO for either approach is fraught with complexity, and direct comparative studies are conspicuously absent from the research landscape.\n\n**Development and Implementation:** For a single application, fine-tuning a general model is often cheaper upfront than building a custom model from scratch \\[157\\]\\[204\\]\\[214\\]. A study on a healthcare AI platform estimated a five-year cost of $1.78 million for a single system \\[67\\], illustrating the high investment required for custom builds.\n\n**Operational Costs:** This is the domain-specific model's strength. Large foundation models can be expensive to run for inference \\[91\\]\\[123\\]. However, costs are falling, and smaller, fine-tuned versions of open-source models like Llama 3 are becoming remarkably cost-effective \\[204\\]\\[214\\].\n\n**Maintenance and Retraining Costs:** This is a critical, often underestimated, long-term cost. AI models require continuous monitoring and retraining to combat data drift and adapt to new information, such as evolving regulatory requirements like HIPAA. Foundation models are designed for inexpensive adaptation with less local data \\[155\\]\\[156\\]suggesting a potential long-term TCO advantage. Retraining a custom model can incur significant recurring costs \\[83\\]\\[159\\]. However, this report's research queries for specific quarterly HIPAA retraining costs for hospital systems (e.g., at Mayo Clinic, Johns Hopkins, Cleveland Clinic) versus fine-tuned general models yielded no quantitative data, highlighting a major blind spot in current TCO analysis.\n\n**3.2. The Divergent Paths to Return on Investment (ROI)**\n\nThe search for documented, long-term, comparative ROI case studies proved fruitless. Extensive queries targeting specific industries (finance, manufacturing, healthcare) and leading companies (JPMorgan Chase, Bank of America, BMW, Mercedes-Benz, Toyota) failed to uncover a single published case study or audited financial report that quantitatively proves a general AI model delivered a higher 5-year ROI than a domain-specific solution.\n\n**The Missing Evidence:** This lack of evidence is a pivotal finding. It suggests that while the strategic argument for general models is strong, the industry has yet to complete the multi-year deployment and measurement cycles necessary to produce hard financial proof of their long-term superiority.\n\n**The Strategic ROI of General Models:** The ROI of a general model should not be measured on a single application. Its true return is strategic: the \"option value\" of its adaptability, the cumulative cost savings from accelerating the development of ten or twenty applications, and the risk mitigation of not being locked into a brittle, outdated technology. It is an investment in the organization's overall capacity to innovate.\n\n**4\\. Analysis of Industry Use Cases**\n\nExamining specific industries reveals the practical tension between the two approaches.\n\n**Healthcare:** This sector is a prime example of the dichotomy. On one side are deeply integrated, highly regulated, and expensive systems like Epic's prediction models for sepsis or hospital readmissions \\[259\\]\\[289\\]. On the other is the immense potential to rapidly fine-tune models like Llama 3 or GPT-4 on local hospital data for a fraction of the cost and time \\[156\\]\\[203\\]yet with significant unknowns around the costs of ensuring and maintaining HIPAA compliance.\n\n**Financial Services:** Banks like JPMorgan Chase and HSBC have achieved massive efficiencies in Anti-Money Laundering (AML) by using AI to slash false positives \\[233\\]\\[234\\]\\[249\\]. While the results do not specify the nature of these systems, the emergence of general platforms like Palantir Foundry \\[278\\] and generalized AML models \\[190\\]\\[307\\]points toward a platform-based future. The core value proposition is faster time-to-value and adaptability in the face of constantly evolving criminal tactics and regulations.\n\n**Manufacturing:** The benefits of AI-driven predictive maintenance are undeniable, with companies like GM and Bosch reporting millions in savings and dramatic reductions in costly downtime \\[181\\]\\[294\\]. The conceptual difference between a foundation model and a fine-tuned one is clear \\[174\\]\\[221\\]but no available case studies from leaders like Toyota, BMW, or Mercedes-Benz quantify the 5-year TCO difference between a third-party service and an in-house fine-tuned model.\n\n**5\\. Conclusion and Strategic Outlook**\n\nThe decision between a general model approach and a domain-specific solution is not merely a technical choice but a fundamental strategic one. The industry trend toward narrow AI is a rational response to the demand for immediate, reliable, and high-performing solutions for well-defined problems. The evidence overwhelmingly supports the superiority of domain-specific models in these contexts based on current, short-term metrics.\n\nHowever, choosing a general model approach is a strategic bet on a future defined by rapid change and escalating complexity. The rationale for this choice is threefold:\n\n1.  **Development Velocity:** To dramatically reduce the time and cost required to move from idea to deployed application across a wide range of business functions.\n2.  **Long-Term Adaptability:** To build a resilient, future-proof AI infrastructure that can evolve with the business rather than constraining it.\n3.  **Economies of Scale:** To create a central AI platform that compounds in value as more applications are built upon it, creating a powerful and defensible competitive advantage.\n\nUltimately, the perceived conflict between general and specific AI is dissolving. The most powerful and pragmatic path forward is a synthesis of the two: **using general foundation models as a universal launchpad to rapidly develop and deploy fleets of fine-tuned, domain-aware applications.** This hybrid strategy captures the development speed and adaptability of the general approach while delivering the high-end performance and precision that businesses demand. As of July 2025, while the long-term financial proof is still forthcoming, the strategic logic is compelling and clear.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[2\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[3\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[4\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[5\\. Xing Hao, Guigang Zhang et al. “Deep Learning.” Int. J. Semantic Comput.](https://doi.org/10.1142/S1793351X16500045)\n\n[6\\. Domain-Specific AI Models: The Future of Industry AI](https://www.iamdave.ai/blog/domain-specific-ai-models-explained-the-future-of-business-ai/)\n\n[7\\. Learning and Control for Interactions in Mixed Human-Robot Environments](https://dspace.mit.edu/bitstream/handle/1721.1/130771/1252062025-MIT.pdf?sequence=1&isAllowed=y)\n\n[8\\. Custom AI Models for Industry-Specific Jargon](https://dialzara.com/blog/custom-ai-models-for-industry-specific-jargon/)\n\n[9\\. Generative AI models like DSLMs outperform LLMs in value](https://www.techtarget.com/searchenterpriseai/news/366620407/Generative-AI-models-like-DSLMs-outperform-LLMs-in-value)\n\n[10\\. MoDEM: Mixture of Domain Expert Models](https://alta2024.alta.asn.au/assets/papers/31.pdf)\n\n[11\\. THE NEXT WAVE OF GENERATIVE AI: 5 KEY TRENDS](https://www.alvarezandmarsal.com/sites/default/files/2025-02/The%20Next%20Wave%20of%20Generative%20AI%20-%20Five%20Key%20Trends.pdf)\n\n[12\\. SOP-AGENT: EMPOWER GENERAL PURPOSE AI AGENT WITH DOMAIN-SPECIFIC SOPs](https://openreview.net/pdf/0c005083882e42e4cc53d3b2d932167b7ded8420.pdf)\n\n[13\\. Generic AI Models Save Time; Prebuilt AI Models for Verticals Save More](https://edisonlabs.net/artificial-intelligence-news/generic-ai-models-save-time-prebuilt-ai-models-for-verticals-save-more/)\n\n[14\\. Domain Specific AI: A Complete Guide to Specialized ...](https://www.getguru.com/reference/domain-specific-ai)\n\n[15\\. AI Large Models Bring Great Opportunities to Reusable Design of CAD Software](http://elib.mi.sanu.ac.rs/files/journals/csis/66/csisn68p1523-1546.pdf)\n\n[16\\. Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective](https://arxiv.org/pdf/2503.01933)\n\n[21\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[22\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[23\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[24\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[25\\. Foundation models in the public sector](https://www.adalovelaceinstitute.org/wp-content/uploads/2023/11/ALI_Foundation-models-evidence-review_2023.pdf)\n\n[26\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[27\\. Low-Resource Vision Challenges for Foundation Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Low-Resource_Vision_Challenges_for_Foundation_Models_CVPR_2024_paper.pdf)\n\n[28\\. Scientific Progress in Artificial Intelligence: History, Status, and Futures](https://www.cs.cmu.edu/~tom/pubs/AI_Overview_History_Status_Futures_February_2024.pdf)\n\n[29\\. Foundation Models for AI - GeeksforGeeks](https://www.geeksforgeeks.org/foundation-models-for-ai/)\n\n[30\\. Keeping an eye on AI: Approaches to government monitoring of the AI landscape](https://www.adalovelaceinstitute.org/wp-content/uploads/2024/08/Ada-Lovelace-Institute-Keeping-an-eye-on-AI-150824.pdf)\n\n[31\\. Exploring In-Context Learning Capabilities of Foundation Models for Generating Knowledge Graphs from Text](https://ceur-ws.org/Vol-3447/Text2KG_Paper_9.pdf)\n\n[32\\. Foundation Models: A New Paradigm for Artificial Intelligence](https://d-nb.info/132563624X/34)\n\n[33\\. Parameter-Efficient Transfer Learning for Large Foundation Models](https://www.iis.sinica.edu.tw/en/page/Events/data/DJ220105.html)\n\n[34\\. Artificial Intelligence in the Biopharmaceutical Industry](https://www.bioprocessintl.com/information-technology/artificial-intelligence-in-the-biopharmaceutical-industry-treacherous-or-transformative-)\n\n[35\\. Efficient Transfer Learning for Video-language Foundation Models](https://arxiv.org/pdf/2411.11223)\n\n[36\\. Robust and generalizable AI for medical image processing](https://kth.diva-portal.org/smash/get/diva2:1905784/SUMMARY01.pdf)\n\n[37\\. Foundation Models: The Building Blocks of Next-Gen AI](https://arize.com/blog-course/foundation-models-ai/)\n\n[41\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[42\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[43\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[44\\. Learning and Control for Interactions in Mixed Human-Robot Environments](https://dspace.mit.edu/bitstream/handle/1721.1/130771/1252062025-MIT.pdf?sequence=1&isAllowed=y)\n\n[45\\. Domain-Specific AI Models: The Future of Industry AI](https://www.iamdave.ai/blog/domain-specific-ai-models-explained-the-future-of-business-ai/)\n\n[46\\. Eric Tzeng, Judy Hoffman et al. “Adversarial Discriminative Domain Adaptation.” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2017.316)\n\n[47\\. Mingsheng Long, Yue Cao et al. “Learning Transferable Features with Deep Adaptation Networks.” ArXiv](https://arxiv.org/abs/1502.02791)\n\n[48\\. General AI vs. narrow AI comes down to adaptability](https://www.techtarget.com/searchenterpriseai/feature/General-AI-vs-narrow-AI-comes-down-to-adaptability)\n\n[49\\. Specialized AI: Transforming Industries with Domain-Specific Solutions](https://www.uipath.com/ai/specialized-ai)\n\n[50\\. Generic AI Models Save Time; Prebuilt AI Models for Verticals Save More](https://edisonlabs.net/artificial-intelligence-news/generic-ai-models-save-time-prebuilt-ai-models-for-verticals-save-more/)\n\n[51\\. Research & Policy Note on Generative Artificial Intelligence for Customs](https://www.wcoomd.org/-/media/wco/public/global/pdf/topics/research/papers/researchpolicynote_genai_en_06122023.pdf?la=fi)\n\n[52\\. Engineering Applications of Artificial Intelligence](https://ak-tyagi.com/static/pdf/149.pdf)\n\n[53\\. Why custom AI models matter](https://novacene.ai/generic-vs-custom-ai/)\n\n[54\\. Fine-Tuning Pre-Trained AI Models for Industry-Specific AI ...](https://www.sapien.io/blog/fine-tuning-pre-trained-models-for-industry-specific-ai-applications)\n\n[55\\. Enterprise AI: Generative AI & Domain Specific Models for Enterprise](https://cdrdv2-public.intel.com/817880/EnterpriseAI%20Partner%20Enablement%20Package.pdf)\n\n[56\\. Artificial General Intelligence through Large-Scale, Multimodal Bayesian Learning](https://people.csail.mit.edu/milch/papers/agi08.pdf)\n\n[57\\. AI Large Models Bring Great Opportunities to Reusable Design of CAD Software](http://elib.mi.sanu.ac.rs/files/journals/csis/66/csisn68p1523-1546.pdf)\n\n[58\\. S. Kanazawa. “General intelligence as a domain-specific adaptation..” Psychological review](https://doi.org/10.1037/0033-295X.111.2.512)\n\n[59\\. Integrating Formal Verification with Large Language Models for the Satisfiability Modulo Theories Problem](https://honors.libraries.psu.edu/files/final_submissions/10273)\n\n[60\\. THE NEXT WAVE OF GENERATIVE AI: 5 KEY TRENDS](https://www.alvarezandmarsal.com/sites/default/files/2025-02/The%20Next%20Wave%20of%20Generative%20AI%20-%20Five%20Key%20Trends.pdf)\n\n[61\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[62\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[63\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[64\\. AI Model Pricing: Comparing GPT, Claude, and Custom ...](https://www.getmonetizely.com/articles/ai-model-pricing-comparing-gpt-claude-and-custom-models-for-enterprise-decision-makers)\n\n[65\\. OpenAI Josh Achiam, Steven Adler et al. “GPT-4 Technical Report.”](https://arxiv.org/abs/2303.08774)\n\n[66\\. Rishi Bommasani, Drew A. Hudson et al. “On the Opportunities and Risks of Foundation Models.” ArXiv](https://arxiv.org/abs/2108.07258)\n\n[67\\. AI with agency: a vision for adaptive, efficient, and ethical healthcare](https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2025.1600216/pdf)\n\n[68\\. RESEARCH STATEMENT](http://web.stanford.edu/~jfries/assets/pdfs/research_statement_jfries.pdf)\n\n[69\\. Large AI Models for Germany](https://leam.ai/wp-content/uploads/2023/05/LEAM-Feasibility-STudy.pdf)\n\n[70\\. How Foundation Models Can Advance AI in Healthcare](https://hai.stanford.edu/news/how-foundation-models-can-advance-ai-healthcare)\n\n[71\\. The Hidden Costs, Challenges, and Total Cost of Ownership of Generative AI Adoption in the Enterprise](https://ai-infrastructure.org/wp-content/uploads/2023/09/AIIA-ClearML-Survey-Report-Sept-2023.pdf)\n\n[72\\. AI adoption can save up to $360 billion in healthcare costs](https://www.karlsnotes.com/ai-adoption-can-save-up-to-360-billion-in-healthcare/)\n\n[73\\. Generative artificial intelligence, patient safety and healthcare quality: a review](https://qualitysafety.bmj.com/content/qhc/33/11/748.full.pdf)\n\n[74\\. The AI Paradigm: A Special Report](https://lab45thinktank.com/wp-content/uploads/2024/04/The-AI-paradigm-04Apr2024.pdf)\n\n[75\\. 超越AI大模型的“加拉帕戈斯”效应_\\_财经头条](https://t.cj.sina.com.cn/articles/view/3009742660/b365074401901llu0)\n\n[76\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[77\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[78\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[79\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[80\\. Rishi Bommasani, Drew A. Hudson et al. “On the Opportunities and Risks of Foundation Models.” ArXiv](https://arxiv.org/abs/2108.07258)\n\n[81\\. Paving the path towards general purpose AI systems regulation in the AI Act: an analysis of the Parliament’s and Council’s proposals](https://www.medialaws.eu/wp-content/uploads/2024/01/3-23-Olivato.pdf)\n\n[82\\. National Tech Rhetoric in a Global AI Race. Smart Futures, Public Goods and Fierce Geopolitics.](https://publikationen.bibliothek.kit.edu/1000180457/158291979)\n\n[83\\. AI Development Costs - Detailed Guide & Breakdown](https://sumatosoft.com/blog/ai-development-costs)\n\n[84\\. CMS Artificial Intelligence Playbook](https://ai.cms.gov/assets/CMS_AI_Playbook.pdf)\n\n[85\\. Get AI Ready: Action Plan for IT Leaders](https://www.gartner.com.au/en/information-technology/topic/ai-readiness)\n\n[86\\. Rationales, Mechanisms, and Challenges to Regulating AI: A Concise Guide and Explanation](https://ai.gov/wp-content/uploads/2023/07/Rationales-Mechanisms-Challenges-Regulating-AI-NAIAC-Non-Decisional.pdf)\n\n[87\\. Explainer: What is a foundation model? | Ada Lovelace Institute](https://www.adalovelaceinstitute.org/resource/foundation-models-explainer/)\n\n[88\\. Navigating the AI Revolution: Key Updates for Today’s CPA](https://www.cpacanada.ca/-/media/site/operational/rg-research-guidance-and-support/docs/02473-rg-navigating-ai-revolution.pdf)\n\n[89\\. The AI Paradigm: A Special Report](https://lab45thinktank.com/wp-content/uploads/2024/04/The-AI-paradigm-04Apr2024.pdf)\n\n[90\\. Lessons from GDPR for AI Policymaking](https://laweconcenter.org/wp-content/uploads/2023/08/Lessons-from-GDPR-for-AI-Policymaking.pdf)\n\n[91\\. Competition Between AI Foundation Models: Dynamics and Policy Recommendations](https://ide.mit.edu/wp-content/uploads/2024/01/SSRN-id4493900.pdf?x41178)\n\n[92\\. The Machine Learning Solutions Architect Handbook](https://sciendo.com/2/v2/download/chapter/9781805124825/10.0000/9781805124825-001.pdf?Token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VycyI6W3sic3ViIjoyNTY3ODUxNywicHVicmVmIjoiNzY0NDg4IiwibmFtZSI6Ikdvb2dsZSBHb29nbGVib3QgLSBXZWIgQ3Jhd2xlciBTRU8iLCJ0eXBlIjoiaW5zdGl0dXRpb24iLCJsb2dvdXRfbGluayI6Imh0dHBzOi8vY29ubmVjdC5saWJseW54LmNvbS9sb2dvdXQvNjgxNDQzNDBlOGI4ZmY4OTY2ZmZjNWUyNTk3NWU4NTMiLCJhdXRoX21ldGhvZCI6ImlwIiwiaXAiOiI2Ni4yNDkuNzkuMiIsImNvdW50ZXJwYXJ0eV9pZCI6Ijc2NDQ4OCJ9XSwiaWF0IjoxNzQ2MTYxMjIxLCJleHAiOjE3NDczNzA4MjF9.3EgYWpVJfB7rVtMuBptYg975z4pyU_8mBg8xfdy20E0)\n\n[93\\. Accounting for the Development of Generative AI Software Products](https://iasplus.com/content/bf25d4a6-d909-4fad-8d64-70c1046f6ec4)\n\n[94\\. Artificial Intelligence Act](https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236_EN.docx)\n\n[95\\. Regulating AI in the financial sector: recent developments and main challenges](https://www.bis.org/fsi/publ/insights63.pdf)\n\n[96\\. Xing Hao, Guigang Zhang et al. “Deep Learning.” Int. J. Semantic Comput.](https://doi.org/10.1142/S1793351X16500045)\n\n[97\\. Top AI Use Cases: Real-World Examples Across Industries ...](https://www.shopify.com/sg/blog/ai-use-cases)\n\n[98\\. The ROI of Gen AI: A global survey of enterprise adoption and value](https://digisoft.in/gsuite/pdf/gemini_for_google_workspace/The_ROI_of_Gen_AI.pdf)\n\n[99\\. Driving ROI Through AI](https://thoughtlabgroup.com/wp-content/uploads/2021/07/ESITL_Driving-ROI-through-AI_FINAL_September-2020.pdf)\n\n[100\\. Specialized AI: Transforming Industries with Domain-Specific Solutions](https://www.uipath.com/ai/specialized-ai)\n\n[101\\. 2025，CISO的AI元年](https://www.goupsec.com/news/18302.html)\n\n[102\\. Domain-Specific AI Models: The Future of Industry AI](https://www.iamdave.ai/blog/domain-specific-ai-models-explained-the-future-of-business-ai/)\n\n[103\\. Custom AI Models for Industry-Specific Jargon](https://dialzara.com/blog/custom-ai-models-for-industry-specific-jargon/)\n\n[104\\. Domain Specific AI: A Complete Guide to Specialized ...](https://www.getguru.com/reference/domain-specific-ai)\n\n[105\\. Enterprise AI: Generative AI & Domain Specific Models for Enterprise](https://cdrdv2-public.intel.com/817880/EnterpriseAI%20Partner%20Enablement%20Package.pdf)\n\n[106\\. Making Reinvention Real with Gen AI: From experimentation to impact](https://www.accenture.com/content/dam/accenture/final/industry/cross-industry/document/Making-Reinvention-Real-With-GenAI-TL.pdf)\n\n[107\\. 如何选择和部署行业特定的人工智能模型](https://techcrunch.com/2021/04/12/how-to-choose-and-deploy-industry-specific-ai-models/)\n\n[108\\. 计算机行业 2025 年度投资策略](https://pdf.dfcfw.com/pdf/H3_AP202412211641385756_1.pdf?1734863652000.pdf)\n\n[109\\. Real-World Case Studies of AI Automation Success](https://threesixtyvue.com/blog/maximizing-roi-real-world-case-studies-of-ai-automation-success)\n\n[110\\. 5 industry-leading AI use cases in customer experience](https://cdn.twimbit.com/uploads/2025/04/02202834/5-industry-leading-AI-use-cases-in-customer-experience-.pdf)\n\n[111\\. Domain-Specific Bots vs. Generic Bots](https://dealerai.com/domain-specific-bots-vs-generic-bots/)\n\n[112\\. Revolutionizing business consulting with generative AI: Exploring transformative models for strategic decision-making, innovation, and operational excellence](https://wjarr.com/sites/default/files/WJARR-2024-1276.pdf)\n\n[113\\. STUDY ON ROI ALIGN](https://www.irjmets.com/uploadedfiles/paper/issue_1_january_2024/48480/final/fin_irjmets1705588256.pdf)\n\n[114\\. 2025年的AI投資回報，企業如何衡量AI的真正價值？ - AI行業應用](https://www.infoai.com.tw/blog/ai-roi-2025-business-value)\n\n[116\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[117\\. Rishi Bommasani, Drew A. Hudson et al. “On the Opportunities and Risks of Foundation Models.” ArXiv](https://arxiv.org/abs/2108.07258)\n\n[118\\. Revisiting Fine-Tuning: A Survey of Parameter-Efficient Techniques for Large AI Models](https://www.preprints.org/manuscript/202504.0743/v1/download)\n\n[119\\. Strategizing with AI: Foundation vs Fine-Tuned Models for Business Success](https://www.multimodal.dev/post/strategizing-with-ai-foundation-vs-fine-tuned-models-for-business-success)\n\n[120\\. 9 Prominent AI Use Cases in Manufacturing](https://svitla.com/blog/ai-use-cases-in-manufacturing/)\n\n[121\\. Gian Antonio Susto, A. Schirru et al. “Machine Learning for Predictive Maintenance: A Multiple Classifier Approach.” IEEE Transactions on Industrial Informatics](https://doi.org/10.1109/TII.2014.2349359)\n\n[122\\. AI Applications in Manufacturing and Production: An Exercise of Technology Monitoring and Assessment](https://thesis.unipd.it/retrieve/53a91454-2996-427a-9010-45d79caa92a8/Woldehanna%20Natnael%20Tsegaye%2C.pdf)\n\n[123\\. The AI Paradigm: A Special Report](https://lab45thinktank.com/wp-content/uploads/2024/04/The-AI-paradigm-04Apr2024.pdf)\n\n[124\\. AWS Cloud Adoption Framework for Artificial Intelligence, Machine Learning, and Generative AI](https://docs.aws.amazon.com/pdfs/whitepapers/latest/aws-caf-for-ai/aws-caf-for-ai.pdf)\n\n[125\\. Market Concentration Implications of Foundation Models: The Invisible Hand of ChatGPT](https://www.economic-policy.org/wp-content/uploads/2024/03/EcPol-2023-183.R1_Proof_hi_Korinek_Vipra.pdf)\n\n[126\\. Artificial Intelligence, Data and Competition](https://read.oecd.org/10.1787/e7e88884-en)\n\n[127\\. AI-POWERED PREDICTIVE MAINTENANCE](https://www.pi.exchange/hubfs/Case%20Studies/PI.EXCHANGE_AI&A-ENGINE_CaseStudy_PredictiveMaintenance_PI01_01.pdf?utm_campaign=Lead+Generation+Ad+Manufacturing+Predictive+Maintenance+%28Dev%29+-+10/12/2021&utm_source=linkedin&utm_medium=paid&hsa_acc=508314885&hsa_cam=617202866&hsa_grp=171490246&hsa_ad=144092536&hsa_net=linkedin&hsa_ver=3&trk=test)\n\n[128\\. Blueprinting AI Economics: Cost Assessment Framework for Business Stakeholders to Navigate Key Aspects in Prompt Engineering, Prompt Automation, and Fine-tuning LLMs](https://dspace.mit.edu/bitstream/handle/1721.1/155634/sulaiman-azfar-sm-idm-2024-thesis.pdf?sequence=1&isAllowed=y)\n\n[129\\. AI-Driven Business Model Innovation in Manufacturing Industry: An In-Depth Look at Siemens](https://vbn.aau.dk/ws/files/718505163/Davor_Androcec_Master_Thesis.pdf)\n\n[130\\. Artificial Intelligence in Manufacturing](https://aim-net.eu/wp-content/uploads/2024/01/AIM-NET-Artificial-Intelligence-in-Manufacturing-white-paper.pdf)\n\n[131\\. T. Carvalho, Fabrízzio Soares et al. “A systematic literature review of machine learning methods applied to predictive maintenance.” Comput. Ind. Eng.](https://doi.org/10.1016/j.cie.2019.106024)\n\n[132\\. Foundation Model Fine Tuning: A Comprehensive Guide](https://www.labellerr.com/blog/beginners-guide-using-foundation-models-in-ml-projects/)\n\n[133\\. GENERATIVE AI / THE RACE IS ON FOUR ADOPTION PATTERNS FOR YOUR ENTERPRISE](https://info.softserveinc.com/hubfs/files/generative-ai/softserve-four-adoption-patterns-for-your-enterprise.pdf)\n\n[134\\. AI Driven Predictive Maintenance: Reducing Downtime and Enhancing Productivity in Manufacturing Environments](https://www.preprints.org/frontend/manuscript/b12db330c002b9f598cb8cab4669a05f/download_pub)\n\n[136\\. Financial Services Regulation 2025 – New Year briefing](https://www.traverssmith.com/media/4wtooqtx/new-year-briefing-2025-travers-smith-v2.pdf)\n\n[137\\. Real-World Case Studies of AI Automation Success](https://threesixtyvue.com/blog/maximizing-roi-real-world-case-studies-of-ai-automation-success)\n\n[138\\. Case Studies: Successful AI Implementations in Various ...](https://www.capellasolutions.com/blog/case-studies-successful-ai-implementations-in-various-industries)\n\n[139\\. GenAI: The Path Forward Part 2 – AI ROI (\"ROAI\")](https://avityim.com/wp-content/uploads/2025/02/FINAL-FINAL-PDF-VERSION-ROAI.pdf)\n\n[140\\. AI-Driven Compliance: Case Studies & Success Stories](https://www.nanomatrixsecure.com/ai-driven-compliance-case-studies-success-stories/)\n\n[141\\. AI agents: Top 5 practical use cases](https://sema4.ai/wp-content/uploads/2024/10/Sema4_AI_Agents_Top5_Ebook_v2.1.pdf)\n\n[142\\. AI in Financial Services: Top Use Cases You Need To Know](https://smartdev.com/ai-use-cases-in-financial-services/)\n\n[143\\. AI in 2025: Structured Strategic Insights for Decision-Makers](https://ugc.production.linktr.ee/db4b3471-a59d-492a-ab25-19d4df2c6039_AI-in-2025--Structured-Strategic-Insights-for-Decision-Makers--1-.pdf)\n\n[144\\. AI in Compliance: Top Use Cases You Need To Know](https://smartdev.com/ai-use-cases-in-compliance/)\n\n[145\\. 5 Top Myths and Facts about AI Implementation in AML Programs](https://www.tookitaki.com/blog/5-top-myths-and-facts-about-ai-implementation-in-aml-programs)\n\n[146\\. AI ROI: How to Measure and Maximize Your Return on Investment in ...](https://smartdev.com/ai-return-on-investment-roi-unlocking-the-true-value-of-artificial-intelligence-for-your-business/#:~:text=Moreover,%20AI%20ROI%20plays%20a,to%20measurable%20gains%20in%20productivity.)\n\n[147\\. Munivel Devan, Samir Vinayak Bayani et al. “AI-driven Solutions for Cloud Compliance Challenges.” Advanced International Journal of Multidisciplinary Research](https://doi.org/10.62127/aijmr.2024.v02i02.1037)\n\n[148\\. AI Solutions for Financial Services: A Smarter Approach to ...](https://www.mesh-ai.com/blog-posts/ai-solutions-for-financial-services-a-smarter-approach-to-regulatory-compliance)\n\n[149\\. AI Case Studies: Highlighting Breakthrough Innovations](https://blog.emb.global/ai-case-studies-breakthrough-innovations/)\n\n[150\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[151\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[152\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[153\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[154\\. Alistair E. W. Johnson, T. Pollard et al. “MIMIC-III, a freely accessible critical care database.” Scientific Data](https://doi.org/10.1038/sdata.2016.35)\n\n[155\\. Foundation Models Could Help Advance AI in Healthcare](https://govhealth.distilinfo.com/2023/01/11/foundation-models-could-help-advance-ai-in-healthcare/)\n\n[156\\. How Foundation Models Can Advance AI in Healthcare](https://hai.stanford.edu/news/how-foundation-models-can-advance-ai-healthcare)\n\n[157\\. AI Development Cost: A Comprehensive Overview for 2025](https://www.prismetric.com/ai-development-cost/)\n\n[158\\. The AI Paradigm: A Special Report](https://lab45thinktank.com/wp-content/uploads/2024/04/The-AI-paradigm-04Apr2024.pdf)\n\n[159\\. Assessing the Cost of Implementing AI in Healthcare](https://itrexgroup.com/blog/assessing-the-costs-of-implementing-ai-in-healthcare/#:~:text=The%20costs%20of%20AI-based,custom-made%20deep%20learning%20solution.)\n\n[160\\. Papers with Code - Health AI Developer Foundations](https://paperswithcode.com/paper/health-ai-developer-foundations)\n\n[161\\. Navigating the AI Revolution: Key Updates for Today’s CPA](https://www.cpacanada.ca/-/media/site/operational/rg-research-guidance-and-support/docs/02473-rg-navigating-ai-revolution.pdf)\n\n[162\\. A differentiated approach to AI foundation models](https://www.techresearchinfo.com/whitepaper/ibm-watsonx-a-differentiated-approach-to-ai-foundation-models.pdf)\n\n[163\\. Your AI Architecture is Your Cost Model](https://www.seekr.com/blog/enterprise-ai-costs/)\n\n[164\\. Competition Between AI Foundation Models: Dynamics and Policy Recommendations](https://ide.mit.edu/wp-content/uploads/2024/01/SSRN-id4493900.pdf?x41178)\n\n[165\\. Cost of Implementing AI in Healthcare in 2025](https://perimattic.com/cost-of-implementing-ai-in-healthcare/)\n\n[166\\. AI Governance Alliance Briefing Paper Series](https://c4ir.rs/wp-content/uploads/2024/01/WEF_AI_Governance_Alliance_Briefing_Paper_Series_2024.pdf)\n\n[167\\. AI-POWERED PREDICTIVE MAINTENANCE](https://www.pi.exchange/hubfs/Case%20Studies/PI.EXCHANGE_AI&A-ENGINE_CaseStudy_PredictiveMaintenance_PI01_01.pdf?utm_campaign=Lead+Generation+Ad+Manufacturing+Predictive+Maintenance+%28Dev%29+-+10/12/2021&utm_source=linkedin&utm_medium=paid&hsa_acc=508314885&hsa_cam=617202866&hsa_grp=171490246&hsa_ad=144092536&hsa_net=linkedin&hsa_ver=3&trk=test)\n\n[168\\. AI-Driven Predictive Maintenance in IoT-Enabled Industrial Systems](https://www.irejournals.com/formatedpaper/1701235.pdf)\n\n[169\\. AI Driven Predictive Maintenance: Reducing Downtime and Enhancing Productivity in Manufacturing Environments](https://www.preprints.org/frontend/manuscript/b12db330c002b9f598cb8cab4669a05f/download_pub)\n\n[170\\. A. Jardine, Daming Lin et al. “A review on machinery diagnostics and prognostics implementing condition-based maintenance.” Mechanical Systems and Signal Processing](https://doi.org/10.1016/J.YMSSP.2005.09.012)\n\n[171\\. INTEGRATION OF ARTIFICIAL INTELLIGENCE IN PREDICTIVE MAINTENANCE FOR MECHANICAL AND INDUSTRIAL ENGINEERING](https://www.irjmets.com/uploadedfiles/paper/issue_3_march_2025/70039/final/fin_irjmets1742721796.pdf)\n\n[172\\. 9 Prominent AI Use Cases in Manufacturing](https://svitla.com/blog/ai-use-cases-in-manufacturing/)\n\n[173\\. Gian Antonio Susto, A. Schirru et al. “Machine Learning for Predictive Maintenance: A Multiple Classifier Approach.” IEEE Transactions on Industrial Informatics](https://doi.org/10.1109/TII.2014.2349359)\n\n[174\\. Strategizing with AI: Foundation vs Fine-Tuned Models for Business Success](https://www.multimodal.dev/post/strategizing-with-ai-foundation-vs-fine-tuned-models-for-business-success)\n\n[175\\. How AI Transforms Automotive Manufacturing: Visual ... - Opsio](https://opsiocloud.com/in/blogs/how-ai-transforms-automotive-manufacturing-visual-inspection-predictive-maintenance/)\n\n[176\\. Predictive Maintenance and Equipment Monitoring Using AI](https://easychair.org/publications/preprint/rV4s/open)\n\n[177\\. AI-driven predictive maintenance in Manufacturing](https://www.koerber-digital.com/blog/ai-driven-predictive-maintenance-in-manufacturing)\n\n[178\\. AI-Powered Predictive Maintenance in Manufacturing](https://www.pi.exchange/use-cases/ai-powered-predictive-maintenance)\n\n[179\\. Top Use Cases of AI in Automotive Industry](https://www.matellio.com/blog/ai-use-cases-in-automotive-industry/)\n\n[180\\. T. Carvalho, Fabrízzio Soares et al. “A systematic literature review of machine learning methods applied to predictive maintenance.” Comput. Ind. Eng.](https://doi.org/10.1016/j.cie.2019.106024)\n\n[181\\. Predictive Maintenance Case Studies: How Companies ...](https://www.provalet.io/guides-posts/predictive-maintenance-case-studies)\n\n[187\\. Scott M. Lundberg, Su-In Lee. “A Unified Approach to Interpreting Model Predictions.” Neural Information Processing Systems](https://arxiv.org/abs/1705.07874)\n\n[188\\. Moritz Hardt, Eric Price et al. “Equality of Opportunity in Supervised Learning.” ArXiv](https://arxiv.org/abs/1610.02413)\n\n[189\\. Alejandro Barredo Arrieta, Natalia Díaz Rodríguez et al. “Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI.” Inf. Fusion](https://doi.org/10.1016/j.inffus.2019.12.012)\n\n[190\\. 5 Top Myths and Facts about AI Implementation in AML Programs](https://www.tookitaki.com/blog/5-top-myths-and-facts-about-ai-implementation-in-aml-programs)\n\n[191\\. AI Enhances Compliance for Financial Institutions - Lawsnote Shares ...](https://voiceofasean.com/newsroom/pr-business/ai-enhances-compliance-for-financial-institutions-lawsnote-shares-successful-experience-at-hong-kong-pan-asian-regulatory-summit-2024/)\n\n[192\\. Automating financial compliance with AI: A New Era in regulatory technology (RegTech)](https://ijsra.net/sites/default/files/IJSRA-2024-0040.pdf)\n\n[193\\. A $200-million warning for financial institutions: Cloud communications simplify compliance](https://nikishevdevelopment.com/a-200-million-warning-for-financial-institutions-cloud-communications-simplify-compliance/)\n\n[194\\. The ROI of Data Security: How Hospitals and Health Systems Can Turn Compliance into Competitive Advantage](https://www.researchcorridor.org/index.php/mjh/article/download/397/379/1095)\n\n[195\\. Cloudera Achieves PCI DSS 4.0 Compliance to Unlock Business Value from AI for Financial Institutions](https://www.aap.com.au/aapreleases/cision20240822ae88871/)\n\n[196\\. EXPLAINABLE AI (XAI) FOR TRANSPARENT FINANCIAL DECISION-MAKING: A TECHNICAL FRAMEWORK](https://iaeme.com/MasterAdmin/Journal_uploads/IJRCAIT/VOLUME_7_ISSUE_2/IJRCAIT_07_02_131.pdf)\n\n[197\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[198\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[199\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[200\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[201\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[202\\. AI, 제 5의 유틸리티](https://www.eugenefn.com/common/files/amail/20250512_B45_Juhyeonglee_2.pdf)\n\n[203\\. Fine-tuning a local LLaMA-3 large language model for automated privacy-preserving physician letter generation in radiation oncology](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1493716/pdf)\n\n[204\\. The Business Case for LLAMA 3 in Modern Data & AI Workflows](https://cdn.prod.website-files.com/625447c67b621ab49bb7e3e5/665fd26c1fa981930940aab5_LI-PDF-llama3.pdf)\n\n[205\\. Assessing the Cost of Implementing AI in Healthcare - ITRex Group](https://itrexgroup.com/blog/assessing-the-costs-of-implementing-ai-in-healthcare/#:~:text=McKinsey%20suggests%20that%20AI%20can,annual%20savings%20of%20$150%20billion.)\n\n[206\\. The State of AI in Insurance: A Comparison of LLM Performance (Vol. V)](https://8595762.fs1.hubspotusercontent-na1.net/hubfs/8595762/The%20State%20of%20AI%20in%20Insurance_A%20Comparison%20of%20LLMs_Vol%20V%20%28Feb%202025%29.pdf)\n\n[207\\. Llama 3.2 models comparison, use cases, fine-tuning - Onegen](https://onegen.ai/llama-3-2-models-comparison-use-cases-and-fine-tuning/)\n\n[208\\. Comparing pruned Llama 3 and Llama 2 models for on-device AI assistants](https://assets-eu.researchsquare.com/files/rs-4927672/v1_covered_f491a376-a2fc-4d70-be1d-261f14e10964.pdf)\n\n[209\\. Automating Evaluation of AI Text Generation in Healthcare with a Large Language Model (LLM)-as-a-Judge](https://www.medrxiv.org/content/medrxiv/early/2025/04/22/2025.04.22.25326219.full.pdf)\n\n[210\\. Fine-tuning Llama-3 to get 90% of GPT-4’s performance at a fraction of the cost](https://www.together.ai/blog/finetuning)\n\n[211\\. Summarizing Clinical Notes using LLMs for ICU Bounceback and Length-of-Stay Prediction](https://www.medrxiv.org/content/10.1101/2025.01.19.25320797v1.full.pdf)\n\n[212\\. The State of AI in Insurance: A Comparison of LLMs (Vol. II)](https://8595762.fs1.hubspotusercontent-na1.net/hubfs/8595762/The%20State%20of%20AI%20in%20Insurance_LLM%20Comparison%20Vol%20II_%28July%202024%29.pdf)\n\n[213\\. Llama 3.2 Integrations in 2025](https://slashdot.org/software/p/Llama-3.2/integrations/)\n\n[214\\. The Business Case for Fine-Tuning Llama 3 Today | Shakudo](https://www.shakudo.io/blog/business-case-fine-tuning-llama3-today)\n\n[216\\. A. Jardine, Daming Lin et al. “A review on machinery diagnostics and prognostics implementing condition-based maintenance.” Mechanical Systems and Signal Processing](https://doi.org/10.1016/J.YMSSP.2005.09.012)\n\n[217\\. Kevin A. Kaiser, N. Gebraeel. “Predictive Maintenance Management Using Sensor-Based Degradation Models.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans](https://doi.org/10.1109/TSMCA.2009.2016429)\n\n[218\\. T. Carvalho, Fabrízzio Soares et al. “A systematic literature review of machine learning methods applied to predictive maintenance.” Comput. Ind. Eng.](https://doi.org/10.1016/j.cie.2019.106024)\n\n[219\\. Using AI in Predictive Maintenance: What You Need to K...](https://www.oracle.com/nl/scm/ai-predictive-maintenance/)\n\n[220\\. R. Mobley. “An introduction to predictive maintenance.”](https://doi.org/10.1016/b978-0-7506-7531-4.x5000-3)\n\n[221\\. Strategizing with AI: Foundation vs Fine-Tuned Models for Business Success](https://www.multimodal.dev/post/strategizing-with-ai-foundation-vs-fine-tuned-models-for-business-success)\n\n[222\\. AI-POWERED PREDICTIVE MAINTENANCE](https://www.pi.exchange/hubfs/Case%20Studies/PI.EXCHANGE_AI&A-ENGINE_CaseStudy_PredictiveMaintenance_PI01_01.pdf?utm_campaign=Lead+Generation+Ad+Manufacturing+Predictive+Maintenance+%28Dev%29+-+10/12/2021&utm_source=linkedin&utm_medium=paid&hsa_acc=508314885&hsa_cam=617202866&hsa_grp=171490246&hsa_ad=144092536&hsa_net=linkedin&hsa_ver=3&trk=test)\n\n[223\\. Variance Analysis in Production Operations](https://praxie.com/variance-analysis-in-production-operations/)\n\n[224\\. How AI Transforms Automotive Manufacturing: Visual ... - Opsio](https://opsiocloud.com/in/blogs/how-ai-transforms-automotive-manufacturing-visual-inspection-predictive-maintenance/)\n\n[225\\. Predictive Maintenance and Equipment Monitoring Using AI](https://easychair.org/publications/preprint/rV4s/open)\n\n[226\\. Manufacturing Efficiency: AI and MIxed Reality Applications](https://www.brandxr.io/manufacturing-efficiency-ai-and-augmented-and-virtual-reality-applications)\n\n[227\\. Predictive Maintenance](https://www2.deloitte.com/content/dam/Deloitte/de/Documents/deloitte-analytics/Deloitte_Predictive-Maintenance_PositionPaper.pdf)\n\n[228\\. INTEGRATION OF ARTIFICIAL INTELLIGENCE IN PREDICTIVE MAINTENANCE FOR MECHANICAL AND INDUSTRIAL ENGINEERING](https://www.irjmets.com/uploadedfiles/paper/issue_3_march_2025/70039/final/fin_irjmets1742721796.pdf)\n\n[229\\. AI-enhanced predictive maintenance systems for critical infrastructure: Cloud-native architectures approach](https://wjaets.com/sites/default/files/WJAETS-2024-0552.pdf)\n\n[232\\. William L. Hamilton, Z. Ying et al. “Inductive Representation Learning on Large Graphs.” Neural Information Processing Systems](https://arxiv.org/abs/1706.02216)\n\n[233\\. JPMorgan Chase Uses AI to Sharpen Anti-Money Laundering ...](https://ai.business/case-studies/ai-to-improve-anti-money-laundering-procedures/)\n\n[234\\. Artificial Intelligence and Anti-Money Laundering](https://sanctionscanner.com/blog/artificial-intelligence-and-anti-money-laundering-17)\n\n[235\\. Defending Your Finances: The AI Revolution in Fraud Prevention](https://clouddevs.com/ai/fraud-prevention/)\n\n[236\\. Anti-Money Laundering Compliance: AI's Impact - Transform FinCrime ...](https://lucinity.com/blog/reshaping-anti-money-laundering-compliance-the-impact-of-ai-on-financial-integrity)\n\n[237\\. Where Will JPMorgan Chase Be in 5 Years?](https://www.fool.com/investing/2019/08/27/where-will-jpmorgan-chase-be-in-5-years.aspx)\n\n[238\\. HSBC's AI-Driven Success: A 50% Increase in Suspicious ...](https://www.ainvest.com/news/hsbc-s-ai-driven-success-a-50-increase-in-suspicious-transactions-intercepted-25011010c0870efdf702da3c/)\n\n[239\\. TRANSFORMING ANTI-MONEY LAUNDERING COMPLIANCE IN BANKING WITH AI-DRIVEN ROBOTIC PROCESS AUTOMATION](https://www.irjet.net/archives/V11/i11/IRJET-V11I1148.pdf)\n\n[240\\. J. Kingdon. “AI Fights Money Laundering.” IEEE Intell. Syst.](https://doi.org/10.1109/MIS.2004.1)\n\n[241\\. A. F. Colladon, Elisa Remondi. “Using social network analysis to prevent money laundering.” Expert Syst. Appl.](https://doi.org/10.1016/j.eswa.2016.09.029)\n\n[242\\. HSBC leads UK banks in global AI race](https://assetfinanceconnect.com/hsbc-leads-uk-banks-in-global-ai-race/)\n\n[243\\. JP Morgan Chase通过机器学习降低成本并提升效率](https://signalscout.io/jp-morgan-chase-reduces-legal-costs-and-more-with-machine-learning/)\n\n[244\\. Rafał Dreżewski, Jan Sepielak et al. “The application of social network analysis algorithms in a system supporting money laundering detection.” Inf. Sci.](https://doi.org/10.1016/j.ins.2014.10.015)\n\n[245\\. Banking on AI: How U.S. Financial Institutions are Transforming with Artificial Intelligence](https://pyramidci.com/wp-content/uploads/2025/04/PCI_032025_Whitepaper_Banking-on-AI-1.pdf)\n\n[246\\. David C. Hicks, Adam Graycar. “International Crime and Justice: Money Laundering.”](https://doi.org/10.1017/CBO9780511762116.028)\n\n[247\\. Advances in Economics, Management and Political Sciences](https://www.ewadirect.com/media/var/media/upload/vol_pdf/aemps/115.pdf)\n\n[248\\. AI and machine learning for risk management](https://www.garp.org/hubfs/Whitepapers/a1Z1W0000054wumUAA.pdf)\n\n[249\\. AI in 2025: Structured Strategic Insights for Decision-Makers](https://ugc.production.linktr.ee/db4b3471-a59d-492a-ab25-19d4df2c6039_AI-in-2025--Structured-Strategic-Insights-for-Decision-Makers--1-.pdf)\n\n[250\\. How banks can mitigate fraud & financial crimes with AI](https://www.financealliance.io/ai-in-risk-management-how-banks-can-mitigate-fraud-and-financial-crimes/#:~:text=AI%20and%20ML%20offer%20adaptive,detection%20in%20the%20banking%20sector.)\n\n[251\\. JPMorgan Chase's IT, AI bets: Where the returns are](https://www.constellationr.com/blog-news/insights/jpmorgan-chase-s-it-ai-bets-where-returns-are)\n\n[252\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[253\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[254\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[255\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[256\\. J. Kaplan, Sam McCandlish et al. “Scaling Laws for Neural Language Models.” ArXiv](https://arxiv.org/abs/2001.08361)\n\n[257\\. Cerner与Epic EMR定价与演示比较](https://www.ezineposting.com/cerner-and-epic-emr-pricing-and-demo/)\n\n[258\\. Epic Implementation: A Comprehensive Guide](https://www.osplabs.com/insights/epic-implementation/)\n\n[259\\. Best Practices in Training Nurses to Use Electronic Health ...](https://cjni.net/journal/?p=14284)\n\n[260\\. FLAN-T5 and Llama 3 Cost Comparison - BytePlus](https://www.byteplus.com/en/topic/500568#:~:text=FLAN-T5%20excels%20in%20its,and%20a%20broader%20knowledge%20base.)\n\n[261\\. Comparative Analysis of Foundation Models for Hospital Integration](https://web.stanford.edu/class/cs224n/final-reports/256722488.pdf)\n\n[262\\. A. Jardine, Daming Lin et al. “A review on machinery diagnostics and prognostics implementing condition-based maintenance.” Mechanical Systems and Signal Processing](https://doi.org/10.1016/J.YMSSP.2005.09.012)\n\n[263\\. Gian Antonio Susto, A. Schirru et al. “Machine Learning for Predictive Maintenance: A Multiple Classifier Approach.” IEEE Transactions on Industrial Informatics](https://doi.org/10.1109/TII.2014.2349359)\n\n[264\\. Fine-Tuning AI Models: Comparing the Costs of OpenAI vs Azure OpenAI](https://vladiliescu.net/finetuning-costs-openai-vs-azure-openai/)\n\n[265\\. T. Carvalho, Fabrízzio Soares et al. “A systematic literature review of machine learning methods applied to predictive maintenance.” Comput. Ind. Eng.](https://doi.org/10.1016/j.cie.2019.106024)\n\n[266\\. Data & Infrastructure Migration Assessment](https://www.microsoft.com/content/dam/microsoft/final/en-us/microsoft-brand/documents/mcaps-ja-jp-biz-assessment-Sample-Report-Data-and-Infrastructure-Migration-Assessment.pdf)\n\n[267\\. R. Mobley. “An introduction to predictive maintenance.”](https://doi.org/10.1016/b978-0-7506-7531-4.x5000-3)\n\n[268\\. Jong-Ho Shin, H. Jun. “On condition based maintenance policy.” J. Comput. Des. Eng.](https://doi.org/10.1016/j.jcde.2014.12.006)\n\n[269\\. ...tuning-with-function-calling-on-azure-openai-servic...](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/fine-tuning-with-function-calling-on-azure-openai-service/4065968)\n\n[270\\. Azure OpenAI fine-tuned model cross-tenant deployment...](https://learn.microsoft.com/en-us/answers/questions/1846210/azure-openai-fine-tuned-model-cross-tenant-deploym)\n\n[271\\. Fine Tuning: now available with Azure OpenAI Service](https://argonsys.com/microsoft-cloud/library/fine-tuning-now-available-with-azure-openai-service/)\n\n[272\\. Initial Report: Palantir Technologies (PLTR), 162% 5-yr ... - 雪球](https://xueqiu.com/1764132429/283188553)\n\n[273\\. Palantir Foundry for Customer-Centric Banking](https://www.palantir.com/assets/xrfr7uokpv1b/5WzUXzX6u89lsU3omgAR5p/36ed087996f049e8811e33e2395bfacd/Whitepaper_-_Palantir_Foundry_for_Customer-Centric_Banking.pdf)\n\n[274\\. Palantir Technologies的未来五年：AI投资洞察](https://intellectia.ai/blog/palantir-stock-forecast)\n\n[275\\. Better Artificial Intelligence Stock: Palantir vs. Microsoft](https://www.fool.com/investing/2024/11/10/better-artificial-intelligence-stock-palantir-vs-m/)\n\n[276\\. Essential disadvantages of Palantir for European ...](https://xpert.digital/en/ai-platform-disadvantages/)\n\n[277\\. Palantir Technologies Inc. (PLTR) Poised to Become the Dominant AI/ML Software Company](https://uniteconomics.com/wp-content/uploads/2024/10/Palantir-Buy-Initiation.pdf)\n\n[278\\. Palantir Foundry for AML](https://www.palantir.com/assets/xrfr7uokpv1b/1faMo2Wb4LJzUZNt3tOmTm/14cc66723edced7355e90c6ef1b56246/Foundry_for_AML.pdf)\n\n[279\\. Artificial Intelligence: Five Trends to Watch in 2025](https://www.globalxetfs.com/content/files/Artificial-Intelligence-Five-Trends-to-Watch-in-2025-Final1.pdf)\n\n[280\\. Palantir Technologies：对AI策略的洞察力- 人工智能领域的 ...](https://xpert.digital/zh-cn/%E6%B7%B1%E5%85%A5%E4%BA%86%E8%A7%A3ai%E7%AD%96%E7%95%A5/)\n\n[282\\. M. Shankar-Hari, Gary S. Phillips et al. “Developing a New Definition and Assessing New Clinical Criteria for Septic Shock: For the Third International Consensus Definitions for Sepsis and Septic Shock (Sepsis-3)..” JAMA](https://doi.org/10.1001/jama.2016.0289)\n\n[283\\. D. Angus, W. Linde‐Zwirble et al. “Epidemiology of severe sepsis in the United States: Analysis of incidence, outcome, and associated costs of care.” Critical Care Medicine](https://doi.org/10.1097/00003246-200107000-00002)\n\n[284\\. Adil Rafiq Rather, B. Kasana. “The Third International Consensus Definitions for Sepsis and Septic Shock (Sepsis-3).” The Journal of Men's Studies](https://doi.org/10.33883/JMS.V18I2.269)\n\n[285\\. C. Paoli, M. A. Reynolds et al. “Epidemiology and Costs of Sepsis in the United States—An Analysis Based on Timing of Diagnosis and Severity Level\\*.” Critical Care Medicine](https://doi.org/10.1097/CCM.0000000000003342)\n\n[286\\. C. Torio, R. Andrews. “National Inpatient Hospital Costs: The Most Expensive Conditions by Payer, 2011.”](https://www.semanticscholar.org/paper/25c5ae0936f34b383d5f68712436b5794dbe292e)\n\n[287\\. Epic Sepsis Model Inpatient Predictive Analytic Tool](https://pmc.ncbi.nlm.nih.gov/articles/PMC10317482/)\n\n[288\\. Journal of Health & Medical Informatics](https://www.hilarispublisher.com/proceedings/an-eicuicu-collaborative-to-reduce-sepsis-mortality-12047.html)\n\n[289\\. External validation of the Epic sepsis predictive model in 2 ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC11560849/)\n\n[290\\. Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis](https://dspace.mit.edu/bitstream/handle/1721.1/155176/3613904.3642343.pdf?sequence=1&isAllowed=y)\n\n[291\\. Intelligent Prediction Platform for Sepsis Risk Based on Real ...](https://medinform.jmir.org/2025/1/e74940/)\n\n[292\\. Gian Antonio Susto, A. Schirru et al. “Machine Learning for Predictive Maintenance: A Multiple Classifier Approach.” IEEE Transactions on Industrial Informatics](https://doi.org/10.1109/TII.2014.2349359)\n\n[293\\. R. Mobley. “An introduction to predictive maintenance.”](https://doi.org/10.1016/b978-0-7506-7531-4.x5000-3)\n\n[294\\. AI Driven Predictive Maintenance: Reducing Downtime and Enhancing Productivity in Manufacturing Environments](https://www.preprints.org/frontend/manuscript/b12db330c002b9f598cb8cab4669a05f/download_pub)\n\n[295\\. Introdução de Tempos de Manutenção Planeada ao abrigo do TPM: caso de estudo na Bosch Car Multimédia](https://estudogeral.uc.pt/bitstream/10316/82878/1/Tese%20Nuno%20Branco.pdf)\n\n[296\\. Gemini Versus Vertex AI: A Comparison of Two Google AI Tools](https://promevo.com/blog/google-gemini-vs-vertex-ai)\n\n[297\\. What Is Vertex AI? Streamlining ML Workflows on Google ...](https://cloudchipr.com/blog/vertex-ai)\n\n[298\\. INTEGRATION OF ARTIFICIAL INTELLIGENCE IN PREDICTIVE MAINTENANCE FOR MECHANICAL AND INDUSTRIAL ENGINEERING](https://www.irjmets.com/uploadedfiles/paper/issue_3_march_2025/70039/final/fin_irjmets1742721796.pdf)\n\n[299\\. Unlock AI Use Cases in Manufacturing: The Ultimate Guide](https://smartdev.com/ai-use-cases-in-manufacturing/)\n\n[300\\. 5 ways Mercedes Benz is using AI \\[Case Study\\] \\[2025\\]](https://digitaldefynd.com/IQ/mercedes-benz-using-ai-case-study/)\n\n[301\\. Domino Compared with Google Cloud Vertex AI](https://6816846.fs1.hubspotusercontent-na1.net/hubfs/6816846/Product-Comparisons/DDL-Google%20Vertex%20AI%20Datasheet.pdf)\n\n[302\\. Lukas Ryll, Mary Emma Barton et al. “Transforming Paradigms: A Global AI in Financial Services Survey.” Electronic](https://doi.org/10.2139/ssrn.3532038)\n\n[303\\. D. Zetzsche, D. Arner et al. “Artificial Intelligence in Finance: Putting the Human in the Loop.” Risk](https://www.semanticscholar.org/paper/d8d69ae667f8c42e9fecbf09a65903af10bf1097)\n\n[304\\. Bart. Van Liebergen. “Machine learning: A revolution in risk management and compliance?.” Journal of financial transformation](https://www.semanticscholar.org/paper/ef8a87960d1402226e126858276d34f85efc40a0)\n\n[305\\. L. Wall. “Some financial regulatory implications of artificial intelligence.” Journal of Economics and Business](https://doi.org/10.1016/J.JECONBUS.2018.05.003)\n\n[306\\. Using AI and machine learning to build an effective anti- ...](https://globalinvestigationsreview.com/guide/the-guide-anti-money-laundering/third-edition/article/using-ai-and-machine-learning-build-effective-anti-money-laundering-compliance-programme)\n\n[307\\. 5 Top Myths and Facts about AI Implementation in AML Programs](https://www.tookitaki.com/blog/5-top-myths-and-facts-about-ai-implementation-in-aml-programs)\n\n[308\\. AI vs. Traditional Compliance Methods](https://quidget.ai/blog/ai-automation/ai-vs-traditional-compliance-methods/)\n\n[309\\. J. Truby, R. Brown et al. “Banking on AI: mandating a proactive approach to AI regulation in the financial sector.” Law and Financial Markets Review](https://doi.org/10.1080/17521440.2020.1760454)\n\n[310\\. With Watson Financial Services, IBM Launches Cognitive Era of RegTech](https://www.channelpronetwork.com/2017/06/17/with-watson-financial-services-ibm-launches-cognitive-era-of-regtech/)\n\n[311\\. TRANSFORMING ANTI-MONEY LAUNDERING COMPLIANCE IN BANKING WITH AI-DRIVEN ROBOTIC PROCESS AUTOMATION](https://www.irjet.net/archives/V11/i11/IRJET-V11I1148.pdf)\n\n[312\\. D. Hosmer, S. Lemeshow et al. “Applied Logistic Regression: Hosmer/Applied Logistic Regression.”](https://doi.org/10.1002/9781118548387)\n\n[313\\. D. Kansagara, Honora Englander et al. “Risk prediction models for hospital readmission: a systematic review..” JAMA](https://doi.org/10.1001/jama.2011.1515)\n\n[314\\. S. Jencks, Mark V. Williams et al. “Rehospitalizations among patients in the Medicare fee-for-service program..” The New England journal of medicine](https://doi.org/10.1056/NEJMsa0803563)\n\n[315\\. J. Donzé, D. Aujesky et al. “Potentially avoidable 30-day hospital readmissions in medical patients: derivation and validation of a prediction model..” JAMA internal medicine](https://doi.org/10.1001/jamainternmed.2013.3023)\n\n[316\\. C. van Walraven, I. Dhalla et al. “Derivation and validation of an index to predict early death or unplanned readmission after discharge from hospital to the community.” Canadian Medical Association Journal](https://doi.org/10.1503/cmaj.091117)\n\n[317\\. 大模型生态加速突破，2024年应用元年有望到来](https://pdf.dfcfw.com/pdf/H3_AP202403101626199132_1.pdf)\n\n[318\\. Cleveland Clinic应对HIPAA Omnibus规则的合规措施](https://omnibus.healthcareinfosecurity.com/cleveland-clinic-tackles-hipaa-omnibus-a-5666)\n\n[319\\. A. Montero, J. Stevenson et al. “Reducing Unplanned Medical Oncology Readmissions by Improving Outpatient Care Transitions: A Process Improvement Project at the Cleveland Clinic..” Journal of oncology practice](https://doi.org/10.1200/JOP.2015.007880)\n\n[320\\. Blinatumomab with chemotherapy for consolidation treatment of Philadelphia-chromosome-negative CD19-positive B-precursor acute lymphoblastic leukaemia with no measurable residual disease \\[ID6405\\]](https://www.nice.org.uk/guidance/ta1049/documents/1)\n\n[321\\. Anthropic发布Claude 3模型系列](https://zhuanlan.zhihu.com/p/686876483)"
  },
  {
    "id": 34,
    "question": "What new types of 'creative infrastructure' does the web need to support AI-generated 3D/immersive experiences while maintaining open standards?",
    "response": "**Research Report**\n\n**To:** Interested Parties\n\n**From:** Expert Researcher\n\n**Date:** July 26, 2025\n\n**Subject:** The Next-Generation Creative Infrastructure for AI-Generated Immersive Web Experiences\n\n**1.0 Introduction: The Confluence of AI and the Immersive Web**\n\nThe digital landscape is undergoing a paradigm shift, driven by the convergence of two powerful forces: generative Artificial Intelligence (AI) and the burgeoning immersive web. As of 2025, AI models are demonstrating a remarkable capacity to generate complex, high-fidelity 3D assets and environments from simple text or image prompts. This revolution in content creation promises to democratize the development of rich, interactive virtual and augmented reality (VR/AR) experiences, moving us closer to the vision of a persistent, open metaverse.\n\nHowever, this creative explosion is running headlong into the technical limitations of our current web infrastructure. The foundational technologies that support 3D content on the web were not designed for the unique demands of AI-generated assets, which are often characterized by massive geometric complexity, unconventional topology, and an urgent need for verifiable provenance \\[271\\].\n\nThis report investigates the new types of 'creative infrastructure' the web must develop to fully support AI-generated 3D and immersive experiences while steadfastly maintaining a commitment to open, interoperable standards. We will analyze the shortfalls of the current ecosystem and propose a multi-pillar framework for the future, encompassing a modernized rendering and computation layer, a distributed and scalable compute fabric, a robust trust and provenance layer, and the governance models required to hold it all together.\n\n**2.0 The Current Landscape: An Infrastructure Under Strain**\n\nThe existing web infrastructure for 3D and immersive content, primarily built upon WebGL and the WebXR Device API, is showing its age when confronted with the scale and nature of AI-generated content.\n\n**2.1 Limitations of the Rendering and Compute Layer**\n\n**WebGL**, the long-standing JavaScript API for rendering 3D graphics, presents several critical bottlenecks. As a low-level API modeled on the older OpenGL ES standard, it struggles with performance on devices with limited GPU memory and presents a steep learning curve for developers \\[2\\]\\[4\\]\\[5\\]. More fundamentally, WebGL's reliance on 16-bit identifiers imposes a hard limit of 65,536 vertices per draw call, which is grossly insufficient for the multi-million polygon models that generative AI can produce \\[9\\]\\[20\\]. While workarounds like mesh splitting exist, they are computationally expensive and add significant complexity \\[172\\].\n\n**WebXR**, the standard for accessing VR and AR devices, is built upon WebGL and thus inherits many of its limitations \\[13\\]. A significant and pressing issue is that **WebGPU**, the modern successor to WebGL, currently does not have a standardized integration with WebXR \\[18\\]. This means that even as browsers adopt the more powerful WebGPU API, immersive experiences are stuck using the less performant WebGL pipeline, creating a critical performance gap \\[18\\]\\[63\\]. WebGPU was designed specifically to harness modern GPU capabilities, including compute shaders for general-purpose GPU (GPGPU) computation, which are essential for advanced physics, simulation, and AI-driven rendering techniques—all crucial for next-generation immersive worlds \\[63\\]\\[204\\]. The lack of a clear path for WebXR to leverage WebGPU is perhaps the single greatest technical roadblock to progress.\n\n**2.2 Inadequacies in Asset Handling and Provenance**\n\nStandard 3D asset formats for the web, such as GLB (the binary format for glTF) and Apple's USDZ, are well-suited for traditional, human-created models \\[183\\]\\[189\\]. However, they lack native, standardized mechanisms for handling two key challenges posed by AI generation:\n\n1.  **Extreme Complexity:** AI models can generate meshes with millions or even billions of polygons. Transmitting and rendering such assets on consumer hardware, especially mobile devices, is often impossible without significant pre-processing and optimization \\[164\\]\\[164\\].\n2.  **Lack of Provenance:** When an AI generates a 3D model, critical questions arise: Which model created it? What were the input prompts or source assets? What are the licensing terms? Who is the creator? Without a standardized way to embed this information securely within the asset itself, we risk a future of unverifiable, untrustworthy, and legally ambiguous content \\[29\\].\n\n**3.0 Pillar 1: A Modernized, AI-Aware Rendering Pipeline**\n\nTo overcome the limitations of the current stack, the web needs a new rendering pipeline built on modern APIs and optimized for the unique characteristics of AI-generated content.\n\n**3.1 The WebGPU-Powered WebXR Imperative**\n\nThe first and most urgent infrastructural need is the formal standardization and browser implementation of a **WebXR-WebGPU binding**. This integration, currently a major gap, is non-negotiable for achieving real-time performance in complex, AI-driven immersive scenes. The W3C's \"GPU for the Web\" working group, which oversees both standards, must prioritize this work \\[202\\]\\[339\\]. A WebGPU-powered WebXR would unlock:\n\n**Higher Frame Rates:** By providing more direct, low-overhead access to the GPU, it will enable the rendering of more complex scenes at the stable 90+ frames per second required for comfortable VR \\[201\\]\\[204\\].\n\n**Compute Shaders:** This allows developers to offload general-purpose parallel computations—like physics simulations, procedural generation, and AI inference—to the GPU, freeing up the CPU and enabling richer, more dynamic worlds \\[63\\].\n\n**3.2 Intelligent, AI-Enhanced Mesh Optimization**\n\nGiven that AI-generated models often have unoptimized, high-density geometry, the web infrastructure must include robust, automated optimization techniques. This goes beyond simple polygon reduction. The new frontier is using AI to optimize AI-generated assets.\n\nResearch and development are pointing towards several key techniques that need to be standardized and integrated into browser engines or high-level frameworks like Three.js:\n\n**AI-Driven Mesh Segmentation and Simplification:** New algorithms can use machine learning, such as Graph Convolutional Networks (GCNs), to intelligently segment a 3D mesh into meaningful parts \\[220\\]. This allows for more nuanced optimization, where geometrically important features are preserved while flat or less critical areas are heavily simplified \\[213\\]\\[213\\]. Tools are emerging that can identify object types within a scene and apply bespoke optimization strategies accordingly \\[163\\]\\[266\\].\n\n**Automated Level of Detail (LOD) Generation:** Instead of requiring manual creation of LODs, the infrastructure should support AI-powered systems that analyze a model and automatically generate a series of progressive meshes \\[166\\]\\[171\\]\\[264\\]. This allows the browser to dynamically select the appropriate model resolution based on the viewer's distance and screen space, dramatically reducing the rendering load \\[217\\].\n\n**AI-Optimized Mesh Splitting:** To overcome per-draw-call vertex limits, mesh splitting is necessary. Emerging research describes intelligent splitting algorithms that use neural networks to determine the optimal \"splitting lines\" to partition a mesh into efficient, GPU-friendly chunks, a process far more advanced than naive geometric division \\[326\\].\n\n**4.0 Pillar 2: A Distributed and Scalable Computation Fabric**\n\nEven with a fully optimized client-side pipeline, the computational demands of generating and rendering vast, persistent immersive worlds in real-time will exceed the capacity of any single device. The future infrastructure must therefore be distributed.\n\n**4.1 Hybrid Cloud-Edge Rendering**\n\nThe web is moving towards an **end-cloud collaborative rendering framework**, where the computational load is dynamically balanced between the client device (edge) and powerful cloud servers \\[54\\]. In this model, heavy tasks like path tracing, complex physics, or AI-based scene generation can be performed on the server, with the results streamed to the client, which handles final composition and user interaction \\[42\\]\\[54\\]. This approach reduces hardware barriers for users and allows for a level of fidelity previously only seen in offline rendering \\[46\\].\n\n**4.2 Decentralized GPU Networks and Open Standards**\n\nA powerful emerging paradigm is the use of blockchain-based **decentralized GPU networks**, such as the Render Network \\[48\\]\\[56\\]. These platforms create a global marketplace for idle GPU compute power, allowing developers to access nearly unlimited, scalable rendering resources at a fraction of the cost of traditional cloud providers.\n\nFor these networks to function as open infrastructure, they must rely on open standards for interoperability. The **ORBX file format**, used by the Render Network, is a key example \\[151\\]. ORBX acts as an open-source container format that abstracts scene data from over 20 different Digital Content Creation (DCC) tools, allowing a complex scene to be packaged and distributed for rendering across a heterogeneous network of GPUs without dependencies on local software \\[152\\]\\[153\\]\\[203\\]. It supports industry-standard sub-formats like glTF, OpenVDB, and OSL, ensuring broad compatibility \\[151\\]\\[253\\].\n\nHowever, a significant governance gap exists. While formats like ORBX are open source, there are no formal governance frameworks that mandate their compatibility or co-evolution with W3C standards like WebGPU \\[9\\]\\[15\\]\\[299\\]. For a truly open and interoperable immersive web, bridges must be built between the standards bodies governing the web (W3C) and the protocols governing these new decentralized compute fabrics.\n\n**5.0 Pillar 3: A Foundational Layer for Trust and Provenance**\n\nAs AI becomes a primary author of digital content, the ability to verify an asset's origin, history, and licensing becomes paramount. The web needs a dedicated, standardized infrastructure for content provenance.\n\n**5.1 C2PA as the Open Standard for Provenance**\n\nThe **Coalition for Content Provenance and Authenticity (C2PA)** has established the leading open technical standard for this purpose \\[32\\]\\[186\\]\\[330\\]. C2PA allows a creator (or an AI service) to attach a cryptographically signed, tamper-evident \"manifest\" to a digital asset \\[194\\]. This manifest functions like a digital nutrition label, containing assertions about the asset’s creation, the tools used (including specific AI models), source materials (\"ingredients\"), and any subsequent edits \\[32\\]\\[332\\].\n\n**5.2 Technical Implementation for 3D Assets (GLB/USDZ)**\n\nA critical infrastructure project is to define and standardize the technical implementation for embedding C2PA manifests within core 3D web formats like GLB and USDZ. While the C2PA specification details embedding for images, videos, and PDFs \\[192\\]\\[281\\]explicit guidance for 3D formats is currently lacking in public documentation \\[8\\]\\[231\\]\\[243\\]. Based on the C2PA framework and the structure of these formats, several technical proposals are viable:\n\n1.  **Embedding in the GLB JSON Chunk:** The GLB format contains a JSON chunk that describes the scene's structure \\[284\\]\\[286\\]. A standardized extension could be defined to add a c2paManifest property to this JSON, containing either the full manifest or, more practically, a URI reference to the manifest stored in a binary data box within the file or externally \\[329\\]\\[334\\].\n2.  **Embedding within the USDZ Archive:** The USDZ format is fundamentally a ZIP archive \\[184\\]\\[185\\]. A straightforward approach would be to standardize the inclusion of a C2PA manifest file (e.g., c2pa.dat) within the archive, with a clear linking mechanism from the main USD scene description file.\n3.  **Leveraging Existing Metadata Standards:** C2PA manifests can incorporate other data types \\[329\\]. They could reference or contain data structured according to older 3D metadata schemas like Microformat3D, bridging the old with the new and providing rich semantic context alongside the core provenance data \\[22\\]\\[22\\]\\[22\\].\n\nStandardizing these embedding methods is an essential step to making provenance a native feature of the 3D web.\n\n**6.0 Conclusion: Building the Open Future**\n\nThe proliferation of AI-generated 3D content is not a distant future; it is a present-day reality that demands an immediate and thoughtful evolution of our web infrastructure. To support this new creative paradigm while upholding the web's core values of openness and interoperability, a multi-faceted approach is required.\n\nThe new 'creative infrastructure' must be built upon four pillars:\n\n1.  **A Modernized Rendering Pipeline:** Transitioning from WebGL to a fully integrated **WebGPU-powered WebXR**, augmented with **AI-enhanced mesh optimization** techniques to handle the complexity of generated assets.\n2.  **A Distributed Computation Fabric:** Embracing **hybrid cloud-edge rendering models** and leveraging the power of **decentralized GPU networks** through open interoperability standards like ORBX.\n3.  **A Trust and Provenance Layer:** Integrating the **C2PA standard** natively into 3D asset formats like GLB and USDZ to provide verifiable, transparent content histories.\n4.  **Collaborative Governance:** Fostering cooperation between standards bodies like the **W3C** and the communities building decentralized protocols to ensure the entire ecosystem evolves in a cohesive, open, and interoperable manner.\n\nBuilding this infrastructure is a collective responsibility. It requires focused effort from browser vendors, standards bodies, framework developers, and the wider creative technology community. By investing in these foundational pillars, we can build a web that is not only more immersive and creative but also more transparent, trustworthy, and accessible to all.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. bioWeb3D: an online webGL 3D data visualisation tool](http://www.biomedcentral.com/1471-2105/14/185)\n\n[2\\. 4 Limitations of WebGL for Publishing Real-Time 3D - PureWeb](https://www.pureweb.com/news-updates/4-limitations-of-webgl-for-publishing-real-time-3d/#:~:text=WebGL%20is%20a%20low-level,run%20them%20in%20a%20browser.)\n\n[3\\. WebGL (three.js) Standardisation de la 3D sur le web](https://nguyen.univ-tln.fr/share/Infographie3D/trans_webgl.pdf)\n\n[4\\. Web based 3D analysis and visualization using HTML5 and WebGL](http://essay.utwente.nl/84311/1/chaturvedi.pdf)\n\n[5\\. Web-based visualization of 3D cadastre](https://gdmc.nl/publications/2018/MSC_THESIS_BARBARA_CEMELLINI.pdf)\n\n[6\\. Visualization/dissemination of 3D Cadastre](https://research.tudelft.nl/files/51543834/TS05C_cemellini_rod_et_al_9591.pdf)\n\n[7\\. Streaming Unreal Engine content to multiple platforms](https://cdn2.unrealengine.com/Unreal+Engine/PixelStreamingWhitepaper/PixelStreamingWhitepaper-V1.9C-ec1985a0f32c53c6650b2d02c11a67c32fa4e176.pdf)\n\n[8\\. 借助WebGL三维可视化技术检索3D动态图像](https://zhuanlan.zhihu.com/p/34168105)\n\n[9\\. HTML5 3D Visualisations](https://courses.isds.tugraz.at/ivis/surveys/ss2011/g2-survey-web-ivis-3d.pdf)\n\n[10\\. WEB XR TECHNICAL WHITEPAPER](https://f.hubspotusercontent-eu1.net/hubfs/26083943/web%20xr%20whitepaper.pdf)\n\n[11\\. Web3D, VR, AR: A-frame, three.js, WebGL, OpenGL, WebXR + ChatGPT AI](https://blog.dragansr.com/2023/05/web3d-vr-ar-frame-threejs-webgl-opengl.html)\n\n[12\\. Research and Application of Web3D Exhibition Based on WebGL and Html5](https://www.atlantis-press.com/article/22432.pdf)\n\n[13\\. R. Baruah. “Getting Started.” AR and VR Using the WebXR API](https://doi.org/10.1007/978-1-4842-6318-1_1)\n\n[14\\. A. Julin, Kaisa Jaalama et al. “Automated Multi-Sensor 3D Reconstruction for the Web.” ISPRS Int. J. Geo Inf.](https://doi.org/10.3390/IJGI8050221)\n\n[15\\. 3D Visualizations in Web Based Environment](https://repositorio-aberto.up.pt/bitstream/10216/135828/2/489863.pdf)\n\n[16\\. A Framework for Web-Based Immersive Analytics](https://chesterrep.openrepository.com/bitstream/handle/10034/623604/PWSB_Thesis.pdf?sequence=1&isAllowed=y)\n\n[17\\. A Domain-Specific Visual Modeling Language for Augmented Reality Applications Using WebXR](https://www.unifr.ch/inf/digits/en/assets/public/files/research/papers/Muff_Fill_2023_Augmented_Reality_Modeling_Language.pdf)\n\n[18\\. T. Marrinan, Ethan Honzik et al. “Image Synthesis from a Collection of Depth Enhanced Panoramas: Creating Interactive Extended Reality Experiences from Static Images.” Proceedings of the 2024 ACM International Conference on Interactive Media Experiences](https://doi.org/10.1145/3639701.3656312)\n\n[19\\. IT Innovationen Band 32 Januar 2024](https://www.hs-esslingen.de/fileadmin/media/Fakultaeten/it/SERVICE/IT-Innovationen/IT-Innovationen-Band32_WS2324.pdf)\n\n[20\\. T. Moraes, P. Amorim et al. “Out-of-Core Progressive Web-Based Rendering of Triangle Meshes.”](https://doi.org/10.1007/978-3-319-68195-5_50)\n\n[21\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[22\\. Microformat and Microdata Schemas for Interactive 3D Web Content](https://annals-csis.org/Volume_1/pliks/231.pdf)\n\n[23\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[24\\. 3DGCQA: A Quality Assssment Database for 3D AI-Generat...](http://arxiv.org/html/2409.07236v1)\n\n[25\\. A. Ramesh, Prafulla Dhariwal et al. “Hierarchical Text-Conditional Image Generation with CLIP Latents.” ArXiv](https://doi.org/10.48550/arXiv.2204.06125)\n\n[26\\. Chitwan Saharia, William Chan et al. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” ArXiv](https://doi.org/10.48550/arXiv.2205.11487)\n\n[27\\. \\[2503.11195\\] Provenance Detection for AI-Generated Images](https://www.arxiv.org/abs/2503.11195)\n\n[28\\. Prafulla Dhariwal, Alex Nichol. “Diffusion Models Beat GANs on Image Synthesis.” ArXiv](https://arxiv.org/abs/2105.05233)\n\n[29\\. 3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows](https://www.research.autodesk.com/app/uploads/2023/08/3DALL-E_DIS23-v2-compressed.pdf)\n\n[30\\. J. Flotyński, K. Walczak. “Microformat and Microdata schemas for interactive 3D web content.” 2013 Federated Conference on Computer Science and Information Systems](https://www.semanticscholar.org/paper/8d9f9a0dff8e4dcac0ea862cbd069a7f15ca205a)\n\n[31\\. Response to USCO Inquiry on Artificial Intelligence and Copyright](https://downloads.regulations.gov/COLC-2023-0006-9044/attachment_1.pdf)\n\n[32\\. Decentralized Content Rights in the Age of Generative AI](https://arxiv.org/html/2503.14519v1)\n\n[33\\. D. Pitzalis, F. Niccolucci et al. “Using LIDO to handle 3D cultural heritage documentation data provenance.” 2011 9th International Workshop on Content-Based Multimedia Indexing (CBMI)](https://doi.org/10.1109/CBMI.2011.5972517)\n\n[34\\. Andrea D'Andrea, K. Fernie. “CARARE 2.0: A metadata schema for 3D cultural objects.” 2013 Digital Heritage International Congress (DigitalHeritage)](https://doi.org/10.1109/DigitalHeritage.2013.6744745)\n\n[35\\. NIST Trustworthy and Responsible AI NIST AI 600-1 Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile](https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=958388)\n\n[36\\. J. Ratican, James Hutson et al. “A Proposed Meta-Reality Immersive Development Pipeline: Generative AI Models and Extended Reality (XR) Content for the Metaverse.” Journal of Intelligent Learning Systems and Applications](https://doi.org/10.4236/jilsa.2023.151002)\n\n[37\\. Timo Homburg, A. Cramer et al. “Metadata schema and ontology for capturing and processing of 3D cultural heritage objects.” Heritage Science](https://doi.org/10.1186/s40494-021-00561-w)\n\n[41\\. Neural Rendering Explained: How AI Powers Real-Time Visuals](https://aufaittechnologies.com/blog/neural-rendering-web-development-ai-real-time-visuals/)\n\n[42\\. Architectures for Ubiquitous 3D on Heterogeneous Computing Platforms](https://publikationen.sulb.uni-saarland.de/bitstream/20.500.11880/27054/2/thesis_print.pdf)\n\n[43\\. Emerging web and game engine tech for 3D cities](https://www.gim-international.com/content/article/emerging-web-and-game-engine-tech-for-3d-cities)\n\n[44\\. 3DRepo4Unity: Dynamic Loading of Version Controlled 3D Assets into the Unity Game Engine](https://3drepo.com/wp-content/uploads/2017/06/2017-06-06-3DRepo4Unity-Dynamic-Loading-of-Version-Controller-3D-Assets-into-the-Unity-Game-Engine.pdf)\n\n[45\\. T. Funkhouser, C. Séquin. “Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments.” Proceedings of the 20th annual conference on Computer graphics and interactive techniques](https://doi.org/10.1145/166117.166149)\n\n[46\\. Crafting Worlds: 3d Animation](https://ijsret.com/wp-content/uploads/2025/03/IJSRET_V11_issue2_725.pdf)\n\n[47\\. GIM INTERNATIONAL](https://www.gim-international.com/files/2ace4e865ac765157ebf2c8ce2c16472.pdf)\n\n[48\\. Web3 and Artificial Intelligence: The State of Play](https://tensquared.com/research/research-24-02/10SQ-Web3+AI-Report-Feb'24.pdf)\n\n[49\\. A Distributed Platform for Archiving and Viewing Cultural Artifacts in 3D Using WebGL](https://ijimt.org/papers/329-CM321.pdf)\n\n[50\\. The Role of Web Platforms in Balancing Sustainable Conservation and Development in Large Archaeological Site: the Naxos case study](https://isprs-archives.copernicus.org/articles/XLVIII-2-W8-2024/303/2024/isprs-archives-XLVIII-2-W8-2024-303-2024.pdf)\n\n[51\\. Metallica - A Review](https://www.ijprems.com/uploadedfiles/paper/volume_4/issue_11_november_2024/36934/1731778851.docx)\n\n[52\\. Max Limper, Maik Thöner et al. “SRC - a streamable format for generalized web-based 3D data transmission.” International Conference on 3D Technologies for the World Wide Web](https://doi.org/10.1145/2628588.2628589)\n\n[53\\. Integrating open source distributed rendering solutions in public and closed networking environments](https://www.theseus.fi/bitstream/10024/7903/1/Opinnaytetyo_Seppala_Suomalainen.pdf)\n\n[54\\. A survey of real-time rendering on Web3D application](https://dds.sciengine.com/cfs/files/pdfs/view/2096-5796/B60034C9D0314D8FBC4C8CD549A6EA3A.pdf)\n\n[55\\. Development of a web-based distributed interactive simulation (DIS) environment using javascript](https://calhoun.nps.edu/server/api/core/bitstreams/7cb06d61-d814-417a-b450-2dae4893859b/content)\n\n[56\\. Web3 Innovations: The Rise of Distributed Compute for AI – Not a Fib](https://notafib.com/2023/05/28/web3-innovations-the-rise-of-distributed-compute-for-ai/)\n\n[57\\. The convergence of AI and immersive environments: Shaping the future of digital realities](https://www.capgemini.com/wp-content/uploads/2024/08/The-convergence-of-AI-and-immersive-environments.pdf)\n\n[58\\. AI-Enhanced Real-time Algorithm for Streamlined Rendering in a 3D Modeling workflow (AIR STREaM)](https://cdn.prod.website-files.com/665ce57def7361e24f1dcbbe/665f113960a91936292e3757_AI-Enhanced%20Realtime%20Algorithm%20for%20Streamlined%20Rendering%20in%20a%203D%20Modeling%20workflow%20%28AIR%20STREaM%29.pdf)\n\n[61\\. gpuweb/proposals/compatibility-mode.md at 4229efd2433b...](https://github.com/gpuweb/gpuweb/blob/4229efd2433b131455a807e3c97654a41616a5b9/proposals/compatibility-mode.md)\n\n[62\\. J. Behr, Peter Eschler et al. “X3DOM: a DOM-based HTML5/X3D integration model.” International Conference on 3D Technologies for the World Wide Web](https://doi.org/10.1145/1559764.1559784)\n\n[63\\. Early access to the new WebGPU backend - Unity Engine](https://discussions.unity.com/t/early-access-to-the-new-webgpu-backend/933493)\n\n[64\\. T. H. Kolbe. “Representing and Exchanging 3D City Models with CityGML.”](https://doi.org/10.1007/978-3-540-87395-2_2)\n\n[65\\. A. Evans, M. Romeo et al. “3D graphics on the web: A survey.” Comput. Graph.](https://doi.org/10.1016/j.cag.2014.02.002)\n\n[66\\. Building Secure Systems Across All Layers](https://www.escholarship.org/content/qt7jg4t3c7/qt7jg4t3c7.pdf)\n\n[67\\. A list WebGL/WebGPU/WebXR of libraries and frameworks.](https://github.com/trusktr/WebGL-WebGPU-frameworks-libraries)\n\n[68\\. J. Ratican, James Hutson et al. “A Proposed Meta-Reality Immersive Development Pipeline: Generative AI Models and Extended Reality (XR) Content for the Metaverse.” Journal of Intelligent Learning Systems and Applications](https://doi.org/10.4236/jilsa.2023.151002)\n\n[69\\. Web3D, VR, AR: A-frame, three.js, WebGL, OpenGL, WebXR + ChatGPT AI](https://blog.dragansr.com/2023/05/web3d-vr-ar-frame-threejs-webgl-opengl.html)\n\n[70\\. Rendering Volumetric Data in WebGL with WebXR](https://users.soe.ucsc.edu/~pang/261/f21/projects/maxim/kuznetsov-volume-rendering.pdf)\n\n[71\\. Decentralized Content Rights in the Age of Generative AI](https://arxiv.org/html/2503.14519v1)\n\n[72\\. Dan Hendrycks, Thomas G. Dietterich. “Benchmarking Neural Network Robustness to Common Corruptions and Perturbations.” ArXiv](https://arxiv.org/abs/1903.12261)\n\n[73\\. Timnit Gebru, Jamie Morgenstern et al. “Datasheets for datasets.” Communications of the ACM](https://doi.org/10.1145/3458723)\n\n[74\\. The Future of Detecting AI-Generated Content: Watermarking & Beyond](https://www.unitary.ai/articles/genai-watermarking-a-trust-safety-primer)\n\n[75\\. Content Credentials C2PA Technical Specification](https://c2pa.org/specifications/specifications/2.1/specs/_attachments/C2PA_Specification.pdf)\n\n[76\\. C2PA Implementation Guidance :: C2PA Specifications](https://c2pa.org/specifications/specifications/1.0/guidance/Guidance.html)\n\n[77\\. Josh A. Goldstein, Girish Sastry et al. “Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations.” ArXiv](https://doi.org/10.48550/arXiv.2301.04246)\n\n[78\\. The Provenance Principle: How C2PA Combats Media Manipulation to Shape AI’s Future](https://www.dalet.com/blog/provenance-principle-c2pa-media-manipulation-ai-future/)\n\n[79\\. C2PA Technical Specification](https://c2pa.org/specifications/specifications/1.0/specs/_attachments/C2PA_Specification.pdf)\n\n[80\\. OpenAI宣布DALL·E 3将采用C2PA标准进行图像元数据标记](https://www.maginative.com/article/openai-says-dall-e-3-to-adopt-c2pa-standard-for-image-metadata/)\n\n[81\\. What is the Coalition for Content Provenance and ...](https://www.techtarget.com/whatis/definition/Coalition-for-Content-Provenance-and-Authenticity-C2PA)\n\n[82\\. THE DEVELOPMENT OF GENERATIVE ARTIFICIAL INTELLIGENCE FROM A COPYRIGHT PERSPECTIVE](https://www.europarl.europa.eu/meetdocs/2024_2029/plmrep/COMMITTEES/JURI/DV/2025/05-12/2025.05.12_item6_Study_GenAIfromacopyrightperspective_EN.pdf)\n\n[83\\. OpenAI Submission to the Select Committee on Adopting Artificial Intelligence (AI) of the Australian Senate](https://www.aph.gov.au/DocumentStore.ashx?id=7ba594f3-c536-47fd-9d4a-e6e4671ccca4&subId=756229)\n\n[84\\. Guidance for Artificial Intelligence and Machine Learning](https://c2pa.org/specifications/specifications/2.2/ai-ml/ai_ml.html)\n\n[85\\. S. Shoker, A. Reddie et al. “Confidence-Building Measures for Artificial Intelligence: Workshop Proceedings.” ArXiv](https://doi.org/10.48550/arXiv.2308.00862)\n\n[91\\. Anthony Dewayne Hunt. “Bitcoin: A Peer-to-Peer Electronic Cash System.”](https://doi.org/10.2139/ssrn.3440802)\n\n[92\\. Render Network Whitepaper](https://framerusercontent.com/assets/rIeLSxNt8ws08bPfc1Mp034Cko.pdf)\n\n[93\\. Elli Androulaki, Artem Barger et al. “Hyperledger fabric: a distributed operating system for permissioned blockchains.” Proceedings of the Thirteenth EuroSys Conference](https://doi.org/10.1145/3190508.3190538)\n\n[94\\. Crypto & Artificial Intelligence](https://assets.ctfassets.net/4ua9vnmkuhzj/3EaTwUbcb93rGLbcmektxL/0d5cc00f956f380550cb6b4efddaab31/Caleb___Brown_Research_-_Crypto_x_AI.pdf)\n\n[95\\. Render Network](https://rendernetwork.com/)\n\n[96\\. Render (RNDR)：区块链驱动的去中心化渲染网络](https://www.tectack.com/2023/04/render-rndr.html)\n\n[97\\. Maurice Herlihy. “Atomic Cross-Chain Swaps.” Proceedings of the 2018 ACM Symposium on Principles of Distributed Computing](https://doi.org/10.1145/3212734.3212736)\n\n[98\\. Toward an Interoperability Architecture for Blockchain Autonomous Systems.IEEE Transactions on Engineering Management](https://doi.org/10.1109/TEM.2019.2920154)\n\n[99\\. WisdomTree Issuer X Limited](https://www.wisdomtree.eu/-/media/eu-media-files/key-documents/prospectus/etf-securities/prospectus---wisdomtree-issuer-x-limited.pdf)\n\n[100\\. Render Network：基于区块链的去中心化GPU计算平台](https://www.coingabbar.com/en/crypto-research-papers/render-token)\n\n[101\\. STATE OF DEPIN](https://mpost.io/Report-State-of-DePin.pdf)\n\n[102\\. Render Network 中文白皮书](https://zhuanlan.zhihu.com/p/665229514)\n\n[103\\. Fundamentals of Decentralized AI](https://public.bnbstatic.com/static/files/research/fundamentals-of-decentralized-ai.pdf)\n\n[104\\. Render Network：链接全球 GPU 能源引领 3D 渲染新革命](https://www.588btc.com/news/doc/3236.docx)\n\n[105\\. Render(RNDR)：基于区块链的GPU渲染网络平台](https://www.18btc.com/46712.html)\n\n[106\\. T. Koens, E. Poll. “Assessing interoperability solutions for distributed ledgers.” Pervasive Mob. Comput.](https://doi.org/10.1016/J.PMCJ.2019.101079)\n\n[111\\. Jean-Francis Balaguer, E. Gobbetti. “i3D: a high-speed 3D Web browser.” Virtual Reality Modeling Language Symposium](https://doi.org/10.1145/217306.217316)\n\n[112\\. SpiderGL: A JavaScript 3D Graphics Library for Next-Generation WWW](https://vcgdata.isti.cnr.it/Publications/2010/DPGS10/spidergl.pdf)\n\n[113\\. M. Benedetto, M. Corsini et al. “SPIDERGL: A GRAPHICS LIBRARY FOR 3D WEB APPLICATIONS.” ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences](https://doi.org/10.5194/ISPRSARCHIVES-XXXVIII-5-W16-467-2011)\n\n[114\\. Using WebGL and AI for Intelligent 3D Graphics on the Web](https://blog.pixelfreestudio.com/using-webgl-and-ai-for-intelligent-3d-graphics-on-the-web/)\n\n[115\\. WebGL 3D Model Viewer Using three.js | manu.ninja](https://manu.ninja/webgl-3d-model-viewer-using-three-js/)\n\n[116\\. WebGL Position vertex inside vertex shader? - Stack Overflow](https://stackoverflow.com/questions/78216275/webgl-position-vertex-inside-vertex-shader)\n\n[117\\. 基于眼动测试的三维设计软件界面优化研究](https://pdf.hanspub.org/Design20230200000_52604762.pdf)\n\n[118\\. Projectverse: A web-based Virtual Reality Platform for Visualizing Research Projects](https://repositum.tuwien.at/bitstream/20.500.12708/192965/1/Lautenbach%20Tom%20-%202023%20-%20Projectverse%20A%20web-based%20Virtual%20Reality%20Platform%20for...pdf)\n\n[119\\. Development of the Browser-based 3D Visualisation Approach for the ATLAS Outreach Applications](https://cds.cern.ch/record/2911677/files/ATL-OREACH-PROC-2024-007.pdf)\n\n[120\\. Virtual Texturing with WebGL](https://publications.lib.chalmers.se/records/fulltext/155126.pdf)\n\n[121\\. Sphere-based Information Visualization: Challenges and Benefits](https://uncharted.software/assets/sphere-based-information-visualization.pdf)\n\n[122\\. 3D Visualizations in Web Based Environment](https://repositorio-aberto.up.pt/bitstream/10216/135828/2/489863.pdf)\n\n[123\\. Web3D, VR, AR: A-frame, three.js, WebGL, OpenGL, WebXR + ChatGPT AI](https://blog.dragansr.com/2023/05/web3d-vr-ar-frame-threejs-webgl-opengl.html)\n\n[124\\. Working in VR: a Web-based approach combining 2D and 3D Interfaces](https://collab.dvb.bayern/download/attachments/77832894/WorkingInVR.pdf?version=1&modificationDate=1600786711240&api=v2)\n\n[125\\. Building 3D WebGL Applications](https://researchportal.tuni.fi/files/2145126/anttonen_salminen_building_3d_webgl_applications.pdf)\n\n[126\\. Web-based Exploration of and Interaction with Large and Deeply Structured Semantic 3D City Models using HTML5 and WebGL](https://www.dgpf.de/src/tagung/jt2015/proceedings/papers/34_DGPF2015_Chaturvedi_et_al.pdf)\n\n[127\\. Build Complex 3D models with WebGL](https://egghead.io/courses/build-complex-3d-models-with-webgl)\n\n[131\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[132\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[133\\. Decentralized Content Rights in the Age of Generative AI](https://arxiv.org/html/2503.14519v1)\n\n[134\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[135\\. Detecting deepfakes and generative AI: Report on standards for AI watermarking and multimedia authenticity workshop](https://biometriq.pl/wp-content/uploads/2024/09/Raport-AI-detekcja.pdf)\n\n[136\\. A. Ramesh, Prafulla Dhariwal et al. “Hierarchical Text-Conditional Image Generation with CLIP Latents.” ArXiv](https://doi.org/10.48550/arXiv.2204.06125)\n\n[137\\. Chitwan Saharia, William Chan et al. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” ArXiv](https://doi.org/10.48550/arXiv.2205.11487)\n\n[138\\. The Future of Detecting AI-Generated Content: Watermarking & Beyond](https://www.unitary.ai/articles/genai-watermarking-a-trust-safety-primer)\n\n[139\\. Content Credentials C2PA Technical Specification](https://c2pa.org/specifications/specifications/2.1/specs/_attachments/C2PA_Specification.pdf)\n\n[140\\. How to identify AI-generated images | Mashable](https://mashable.com/article/how-to-identify-ai-generated-images)\n\n[141\\. C2PA Technical Specification](https://c2pa.org/specifications/specifications/1.3/specs/_attachments/C2PA_Specification.pdf)\n\n[142\\. Content Credentials: Strengthening Multimedia Integrity in the Generative AI Era](https://media.defense.gov/2025/Jan/29/2003634788/-1/-1/0/CSI-CONTENT-CREDENTIALS.PDF)\n\n[143\\. The Provenance Principle: How C2PA Combats Media Manipulation to Shape AI’s Future](https://www.dalet.com/blog/provenance-principle-c2pa-media-manipulation-ai-future/)\n\n[144\\. BSA RESPONSE TO PUBLIC CONSULTATION ON AI AND COPYRIGHT](https://www.bsa.org/files/policy-filings/en12062024kraicopyright.pdf)\n\n[145\\. C2PA and Content Credentials Explainer](https://c2pa.org/specifications/specifications/2.2/explainer/Explainer.html)\n\n[146\\. Open Problems in Technical AI Governance](https://openreview.net/pdf?id=1nO4qFMiS0)\n\n[147\\. Google推出AI图像识别功能](https://www.cloverinfotech.com/blog/google-is-making-it-easier-for-you-to-identify-ai-images-heres-how/)\n\n[148\\. OpenAI Submission to the Select Committee on Adopting Artificial Intelligence (AI) of the Australian Senate](https://www.aph.gov.au/DocumentStore.ashx?id=7ba594f3-c536-47fd-9d4a-e6e4671ccca4&subId=756229)\n\n[151\\. Render Network Whitepaper](https://framerusercontent.com/modules/assets/Wa2lC0JcjksdQEKLA1PAvTHXe3Y~o---EuM8cIAdZuUrwHjJw7JcrKvj9afAV9nk7wmVsbo.pdf)\n\n[152\\. Render Network Review: Decentralized Computing Revolution](https://www.coinbureau.com/review/render-network-review/)\n\n[153\\. Render Network 中文白皮书](https://zhuanlan.zhihu.com/p/665229514)\n\n[154\\. RNDR Network User Manual](https://rendernetwork.com/assets/files/RNDR_User_Manual.pdf)\n\n[155\\. Strategic Research and Innovation Agenda (SRIA) of the European Open Science Cloud (EOSC)](https://eosc.eu/wp-content/uploads/2024/12/20241031_SRIA_1.3_final_Annex.pdf)\n\n[156\\. Render Network Knowledge Base](https://know.rendertoken.com/basics/what-is-an-orbx)\n\n[157\\. THUBA 研报｜render 去中心化算力——兼谈depin 赛道](https://foresightnews.pro/article/detail/53218)\n\n[158\\. W3C Technologies: a Key for Interoperability](https://www.w3c.it/papers/SignoreForCMG2003.pdf)\n\n[159\\. Standards Are Only Open If They Protect Security and Interoperability](https://www.eff.org/es/node/90759)\n\n[161\\. LearnOpenGL - Model](https://learnopengl.com/Model)\n\n[162\\. Using WebGL and AI for Intelligent 3D Graphics on the Web](https://blog.pixelfreestudio.com/using-webgl-and-ai-for-intelligent-3d-graphics-on-the-web/)\n\n[163\\. AI-powered 3D Mesh Optimizer](https://assets-global.website-files.com/63711bc40ce0bd4e874070a3/653b114f2e921340fc2a1bfb_AI%20Optimizer_compressed.pdf)\n\n[164\\. 3D Modeling using Artificial Intelligence](https://mdh.diva-portal.org/smash/get/diva2:1938159/FULLTEXT01.pdf)\n\n[165\\. Michael Garland, Paul S. Heckbert. “Surface simplification using quadric error metrics.” Proceedings of the 24th annual conference on Computer graphics and interactive techniques](https://doi.org/10.1145/258734.258849)\n\n[166\\. Hugues Hoppe. “Progressive meshes.” Proceedings of the 23rd annual conference on Computer graphics and interactive techniques](https://doi.org/10.1145/237170.237216)\n\n[167\\. Advanced 3D Game Programming with DirectX 9.0](https://research-solution.com/uplode/books/book-59196.pdf)\n\n[168\\. How to Optimize WebGL for High-Performance 3D Graphics](https://blog.pixelfreestudio.com/how-to-optimize-webgl-for-high-performance-3d-graphics/)\n\n[169\\. From Meshes to Splat: Exploring Dynamic Gaussian Splatting for Human Avatars in Volumetric Capture Workflows](https://webthesis.biblio.polito.it/35339/1/tesi.pdf)\n\n[170\\. Optimizing graphics performance](https://docs.unity3d.com/560/Documentation/Manual/OptimizingGraphicsPerformance.html)\n\n[171\\. Hugues Hoppe. “View-dependent refinement of progressive meshes.” Proceedings of the 24th annual conference on Computer graphics and interactive techniques](https://doi.org/10.1145/258734.258843)\n\n[172\\. SpiderGL: A JavaScript 3D Graphics Library for Next-Generation WWW](https://vcgdata.isti.cnr.it/Publications/2010/DPGS10/spidergl.pdf)\n\n[173\\. Level of Detail for 3D Graphics](https://theswissbay.ch/pdf/Gentoomen%20Library/Game%20Development/Designing/Level%20of%20Detail%20for%203D%20Graphics.pdf)\n\n[174\\. WebGL based 3D Game Engine](https://blog.nobel-joergensen.com/wp-content/uploads/2012/03/webgl-based-game-engine1.pdf)\n\n[175\\. M. Deering. “Geometry compression.” Proceedings of the 22nd annual conference on Computer graphics and interactive techniques](https://doi.org/10.1145/218380.218391)\n\n[176\\. M. Chow. “Optimized geometry compression for real-time rendering.” Proceedings. Visualization '97 (Cat. No. 97CB36155)](https://doi.org/10.1109/VISUAL.1997.663902)\n\n[177\\. WebGL Programming Guide: Interactive 3D Graphics Programming with WebGL](https://www.pearson.de/media/muster/ext/9780133364941.pdf)\n\n[178\\. MeshGS: Adaptive Mesh-Aligned Gaussian Splatting for High-Quality Rendering](https://openaccess.thecvf.com/content/ACCV2024/papers/Choi_MeshGS_Adaptive_Mesh-Aligned_Gaussian_Splatting_for_High-Quality_Rendering_ACCV_2024_paper.pdf)\n\n[181\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[182\\. Best Practices for Sharing and Embedding USDz and GLB ...](https://manual.keyshot.com/manual/models-tab/export/export-formats/best-practices-for-sharing-and-embedding-usdz-and-glb-files/)\n\n[183\\. 3D 建模中的 GLTF、USDZ 和 GLB 3D 文件格式](https://zhuanlan.zhihu.com/p/676618792)\n\n[184\\. Convert GLB to USDZ online for FREE - File Format Docs](https://docs.fileformat.com/3d/glb-to-usdz/)\n\n[185\\. 通过 Java 将 GLB 转换为 USDZ | products.aspose.com](https://products.aspose.com/3d/zh/java/conversion/glb-to-usdz/)\n\n[186\\. OpenAI Submission to the Select Committee on Adopting Artificial Intelligence (AI) of the Australian Senate](https://www.aph.gov.au/DocumentStore.ashx?id=7ba594f3-c536-47fd-9d4a-e6e4671ccca4&subId=756229)\n\n[187\\. Paul Groth, L. Moreau. “PROV-Overview. An Overview of the PROV Family of Documents.”](https://www.semanticscholar.org/paper/f20b317aacc729a512563b4bfffeac7f9754d8bb)\n\n[188\\. Kyle N. Plunkett. “A Simple and Practical Method for Incorporating Augmented Reality into the Classroom and Laboratory.” Journal of Chemical Education](https://doi.org/10.26434/chemrxiv.7137827)\n\n[189\\. GLB Export](https://help.marvinxr.com/books/marvin-xr-help-manual/page/glb-export/export/pdf)\n\n[190\\. I. Altintas, Oscar Barney et al. “Provenance Collection Support in the Kepler Scientific Workflow System.” International Provenance and Annotation Workshop](https://doi.org/10.1007/11890850_14)\n\n[191\\. Convert USDZ to GLB File - Online for Free](https://blog.aspose.com/3d/convert-usdz-to-glb-online/)\n\n[192\\. Content Credentials C2PA Technical Specification](https://c2pa.org/specifications/specifications/2.1/specs/_attachments/C2PA_Specification.pdf)\n\n[193\\. Best Practices for Sharing and Embedding USDz and GLB Files](https://manual.keyshot.com/keyshot11/manual/models-tab/export/export-formats/best-practices-for-sharing-and-embedding-usdz-and-glb-files/#:~:text=KeyShot%20allows%20you%20to%20export,iOS%20and%20Android%20mobile%20devices.)\n\n[194\\. Responsible AI Transparency Report](https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/responsible-aI-transparency-report-2024.pdf)\n\n[195\\. J. Aythora, R. Burke-Agüero et al. “MULTI-STAKEHOLDER MEDIA PROVENANCE MANAGEMENT TO COUNTER SYNTHETIC MEDIA RISKS IN NEWS PUBLISHING.”](https://www.semanticscholar.org/paper/4dbfb45e5b89ffd704e7bab61b9dba26710ed929)\n\n[196\\. Product 3D Model Viewer Extension User Guide](https://www.magebees.com/media/attachment/user_guide_magebees_prod3dview_for_magento_2_extension_v1.0.0.pdf?srsltid=AfmBOoqvzCVABxbfZSoUiBEkCOy7Ow5oPWc0mlAy8fnZagzvBvdDI_FH)\n\n[197\\. Convert Sketchup to GLB | CAD Exchanger](https://cadexchanger.com/sketchup-to-glb/)\n\n[198\\. USDZ/GLB格式在线转换](https://zhuanlan.zhihu.com/p/671116666)\n\n[199\\. USDZ 转 GLB - 3D模型在线转换 | 3Dconvert](https://3dconvert.nsdt.cloud/usdz/to/glb)\n\n[201\\. Everything you need to know about the 3D web of the future: WebGPUs](https://animech.com/en/visualization/everything-you-need-to-know-about-the-3d-web-of-the-future-webgpus/)\n\n[202\\. WebGPU And WGSL Modern 3D on the web; Spec; Adapters/Devices, Buffers, Textures, Layouts, Commands, Pipelines](https://clipcode.net/assets/academy/live/us6.pdf)\n\n[203\\. THUBA 研报 | render 去中心化算力——兼谈 depin 赛道](https://www.588btc.com/news/doc/1615.docx)\n\n[204\\. 国际新闻](https://www.chinaw3c.org/category/world-news/)\n\n[205\\. WebGPU 概述及建筑工程行业应用分析](http://tmjzgcxxjs.manuscripts.cn/cn/article/pdf/preview/10.16670/j.cnki.cn11-5823/tu.2024.03.10.pdf)\n\n[206\\. WebGPU frameworks for 2D graphics with BabylonJS](https://forum.babylonjs.com/t/webgpu-frameworks-for-2d-graphics-with-babylonjs/42307)\n\n[207\\. Render Network Knowledge Base](https://know.rendertoken.com/basics/what-is-an-orbx)\n\n[208\\. Reese Levine, Tianhao Guo et al. “MC Mutants: Evaluating and Improving Testing for Memory Consistency Specifications.” Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2](https://doi.org/10.1145/3575693.3575750)\n\n[209\\. GitHub - DavidPeicho/wgpu: Native WebGPU implementation based...](https://github.com/DavidPeicho/wgpu)\n\n[211\\. LearnOpenGL - Model](https://learnopengl.com/Model)\n\n[212\\. W. Lorensen, H. Cline. “Marching cubes: A high resolution 3D surface construction algorithm.” Proceedings of the 14th annual conference on Computer graphics and interactive techniques](https://doi.org/10.1145/37401.37422)\n\n[213\\. AI-powered 3D Mesh Optimizer](https://assets-global.website-files.com/63711bc40ce0bd4e874070a3/653b114f2e921340fc2a1bfb_AI%20Optimizer_compressed.pdf)\n\n[214\\. Michael Garland, Paul S. Heckbert. “Surface simplification using quadric error metrics.” Proceedings of the 24th annual conference on Computer graphics and interactive techniques](https://doi.org/10.1145/258734.258849)\n\n[215\\. Using WebGL and AI for Intelligent 3D Graphics on the Web](https://blog.pixelfreestudio.com/using-webgl-and-ai-for-intelligent-3d-graphics-on-the-web/)\n\n[216\\. Hugues Hoppe. “Progressive meshes.” Proceedings of the 23rd annual conference on Computer graphics and interactive techniques](https://doi.org/10.1145/237170.237216)\n\n[217\\. Multiresolution and fast decompression for optimal web-based rendering](https://iris.cnr.it/retrieve/3f0fce61-1bdc-4630-9eca-f299eb22defd/prod_359239-doc_126692.pdf)\n\n[218\\. Hugues Hoppe, T. DeRose et al. “Mesh optimization.” Proceedings of the 20th annual conference on Computer graphics and interactive techniques](https://doi.org/10.1145/166117.166119)\n\n[219\\. 基于距离场的三维模型网格重建](https://www.ecice06.com/CN/article/downloadArticleFile.do?attachType=PDF&id=18504)\n\n[220\\. 结合全局信息和局部信息的三维网格分割框架](https://www.zjujournals.com/eng/CN/article/downloadArticleFile.do?attachType=PDF&id=46816)\n\n[221\\. MeshGS: Adaptive Mesh-Aligned Gaussian Splatting for High-Quality Rendering](https://openaccess.thecvf.com/content/ACCV2024/papers/Choi_MeshGS_Adaptive_Mesh-Aligned_Gaussian_Splatting_for_High-Quality_Rendering_ACCV_2024_paper.pdf)\n\n[222\\. New Algorithm Unlocked: 3D Gaussian Splatting (3DGS) - GeoAI](https://geoai.au/new-algorithm-unlocked-3d-gaussian-splatting-3dgs/)\n\n[223\\. Yue Liu, Ningning Xie et al. “Matrix Optimization Algorithm for WebGL-Based 3D Visualization Construction Models.” 2024 10th International Conference on Computer and Communications (ICCC)](https://doi.org/10.1109/ICCC62609.2024.10942003)\n\n[224\\. Advanced 3D Game Programming with DirectX 9.0](https://research-solution.com/uplode/books/book-59196.pdf)\n\n[225\\. Rendering SVG Paths In WebGL](https://css-tricks.com/rendering-svg-paths-in-webgl/)\n\n[226\\. Computer-Aided Design and Intelligent Optimization in the Inheritance of Intangible Cultural Heritage](https://www.cad-journal.net/files/vol_22/CAD_22%28S7%29_2025_41-54.pdf)\n\n[231\\. Content Credentials C2PA Technical Specification](https://c2pa.org/specifications/specifications/2.1/specs/_attachments/C2PA_Specification.pdf)\n\n[232\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[233\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[234\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[235\\. C2PA Technical Specification](https://c2pa.org/specifications/specifications/1.0/specs/_attachments/C2PA_Specification.pdf)\n\n[236\\. A. Ramesh, Prafulla Dhariwal et al. “Hierarchical Text-Conditional Image Generation with CLIP Latents.” ArXiv](https://doi.org/10.48550/arXiv.2204.06125)\n\n[237\\. Prafulla Dhariwal, Alex Nichol. “Diffusion Models Beat GANs on Image Synthesis.” ArXiv](https://arxiv.org/abs/2105.05233)\n\n[238\\. Guidance for Artificial Intelligence and Machine Learning](https://c2pa.org/specifications/specifications/2.2/ai-ml/ai_ml.html)\n\n[239\\. Best Practices for Sharing and Embedding USDz and GLB ...](https://manual.keyshot.com/manual/models-tab/export/export-formats/best-practices-for-sharing-and-embedding-usdz-and-glb-files/)\n\n[240\\. Google to roll out features for detecting AI-generated images across ...](https://siliconangle.com/2024/09/17/google-roll-features-detecting-ai-generated-images-across-several-services/)\n\n[241\\. 3D 建模中的 GLTF、USDZ 和 GLB 3D 文件格式](https://www.mvrlink.com/gltf-usdz-and-glb-3d-file-formats-in-3d-modeling/)\n\n[242\\. 人工智能生成合成内容标识的国际经验与制度挑战【AI战略洞...](https://new.qq.com/rain/a/20250318A08QRN00)\n\n[243\\. Convert Sketchup to GLB](https://cadexchanger.com/sketchup-to-glb/)\n\n[251\\. Ethan Rublee, V. Rabaud et al. “ORB: An efficient alternative to SIFT or SURF.” 2011 International Conference on Computer Vision](https://doi.org/10.1109/ICCV.2011.6126544)\n\n[252\\. Render Network 中文白皮书](https://zhuanlan.zhihu.com/p/665229514)\n\n[253\\. THUBA 研报 | render 去中心化算力——兼谈 depin 赛道](https://www.588btc.com/news/doc/1615.docx)\n\n[254\\. WebGPU](https://www.orillusion.com/zh/webgpu.html)\n\n[255\\. Everything you need to know about the 3D web of the future: WebGPUs](https://animech.com/en/visualization/everything-you-need-to-know-about-the-3d-web-of-the-future-webgpus/)\n\n[256\\. Orbix Administrator's Guide](https://www.microfocus.com/documentation/orbix/orbix6310/administrators_guide_6310.pdf)\n\n[257\\. WebGPU Evaluation: A Comprehensive Analysis of WebGPU as a Compute Platform](https://ulb-dok.uibk.ac.at/ulbtirolhs/download/pdf/11304364)\n\n[258\\. FusionRender: Harnessing WebGPU’s Power for Enhanced Graphics Performance on Web Browsers](https://openreview.net/pdf/9279e47ad0e3300efad55781c6b88e63619cb896.pdf)\n\n[259\\. Octane for Modo - ORBX Proxies](https://docs.otoy.com/ModoH/Modo/ORBXProxies.htm)\n\n[260\\. OTOY • OTOY unveils OctaneRender 3: Massive upgrade ... - OTOY • Home](https://home.otoy.com/otoy-unveils-octanerender-3-worlds-best-gpu-renderer/)\n\n[261\\. W. Lorensen, H. Cline. “Marching cubes: A high resolution 3D surface construction algorithm.” Proceedings of the 14th annual conference on Computer graphics and interactive techniques](https://doi.org/10.1145/37401.37422)\n\n[262\\. INFORMATION TECHNOLOGIES AND AUTOMATION– 2024](https://otfk.od.ua/conference/files/17_01_2025_4.pdf)\n\n[263\\. B. Curless, M. Levoy. “A volumetric method for building complex models from range images.” Proceedings of the 23rd annual conference on Computer graphics and interactive techniques](https://doi.org/10.1145/237170.237269)\n\n[264\\. PLOD: Point cloud level of detail for polygon mesh](http://wscg.zcu.cz/WSCG2024/CSRN-2024/B29-2024.pdf)\n\n[265\\. Hugues Hoppe. “Progressive meshes.” Proceedings of the 23rd annual conference on Computer graphics and interactive techniques](https://doi.org/10.1145/237170.237216)\n\n[266\\. AI-powered 3D Mesh Optimizer](https://assets-global.website-files.com/63711bc40ce0bd4e874070a3/653b114f2e921340fc2a1bfb_AI%20Optimizer_compressed.pdf)\n\n[267\\. Multiresolution and fast decompression for optimal web-based rendering](https://iris.cnr.it/retrieve/3f0fce61-1bdc-4630-9eca-f299eb22defd/prod_359239-doc_126692.pdf)\n\n[268\\. GPU-Driven LOD Rendering in Vulkan](https://dspace.cvut.cz/bitstream/handle/10467/123421/F3-DP-2025-Smely-Richard-DiplomaThesis.pdf?sequence=-1&isAllowed=y)\n\n[269\\. Using WebGL and AI for Intelligent 3D Graphics on the Web](https://blog.pixelfreestudio.com/using-webgl-and-ai-for-intelligent-3d-graphics-on-the-web/)\n\n[270\\. Optimizing scenes for better WebGL performance — Soft8Soft](https://www.soft8soft.com/docs/manual/en/introduction/Optimizing-WebGL-performance.html)\n\n[271\\. Enhancing the Industrial Design Process with Generative AI](https://www.theseus.fi/bitstream/handle/10024/886323/Fattah_Saleh_Helan.pdf?sequence=2&isAllowed=y)\n\n[272\\. How to Optimize WebGL for High-Performance 3D Graphics](https://blog.pixelfreestudio.com/how-to-optimize-webgl-for-high-performance-3d-graphics/)\n\n[273\\. Image Precision Silhouette Edges](https://web.media.mit.edu/~raskar/RaskarPapers/99silhouetteI3D.pdf)\n\n[274\\. Rendering SVG Paths In WebGL](https://css-tricks.com/rendering-svg-paths-in-webgl/)\n\n[275\\. T. Funkhouser, C. Séquin. “Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments.” Proceedings of the 20th annual conference on Computer graphics and interactive techniques](https://doi.org/10.1145/166117.166149)\n\n[276\\. J. Cohen, A. Varshney et al. “Simplification envelopes.” Proceedings of the 23rd annual conference on Computer graphics and interactive techniques](https://doi.org/10.1145/237170.237220)\n\n[277\\. Volumetric Terrain Rendering with WebGL](https://raw.githubusercontent.com/vanruesc/rabbit-hole/main/thesis-volumetric-terrain-rendering-with-webgl.pdf)\n\n[278\\. Yue Liu, Ningning Xie et al. “Matrix Optimization Algorithm for WebGL-Based 3D Visualization Construction Models.” 2024 10th International Conference on Computer and Communications (ICCC)](https://doi.org/10.1109/ICCC62609.2024.10942003)\n\n[281\\. Content Credentials C2PA Technical Specification](https://c2pa.org/specifications/specifications/2.1/specs/_attachments/C2PA_Specification.pdf)\n\n[282\\. C2PA Technical Specification](https://c2pa.org/specifications/specifications/1.0/specs/_attachments/C2PA_Specification.pdf)\n\n[283\\. J. Beeler, Vicky DiProva. “Family adjustment following disclosure of homosexuality by a member: themes discerned in narrative accounts..” Journal of marital and family therapy](https://doi.org/10.1111/J.1752-0606.1999.TB00261.X)\n\n[284\\. Convert Sketchup to GLB | CAD Exchanger](https://cadexchanger.com/sketchup-to-glb/)\n\n[285\\. Understanding manifests - CAI open source SDK](https://opensource.contentauthenticity.org/docs/manifest/understanding-manifest/)\n\n[286\\. 无需安装！点击快速优化GLB模型- 袋鼠云数栈](https://www.cnblogs.com/DTinsight/p/18944056)\n\n[287\\. What is a GLB file? Learn How to use GLB Files Formats | Visao](https://visao.ca/what-is-glb-file/)\n\n[288\\. 通過Java 將3MF 轉換為GLB](https://products.aspose.com/3d/zh-hant/java/conversion/3mf-to-glb/)\n\n[289\\. GLB 文件格式详解](https://docs.fileformat.com/3d/glb/)\n\n[299\\. Render Network Whitepaper](https://framerusercontent.com/modules/assets/Wa2lC0JcjksdQEKLA1PAvTHXe3Y~o---EuM8cIAdZuUrwHjJw7JcrKvj9afAV9nk7wmVsbo.pdf)\n\n[300\\. WebGPU Compatibility Mode](https://webgpufundamentals.org/webgpu/lessons/webgpu-compatibility-mode.html)\n\n[301\\. The Multidimensional Visual Analyser (MVA)](https://ftp.isds.tugraz.at/pub/theses/ogolob-2024-msc.pdf)\n\n[302\\. A WebGPU backend for Futhark](https://futhark-lang.org/student-projects/sebastian-msc-thesis.pdf)\n\n[303\\. WebGPU](https://www.orillusion.com/zh/webgpu.html)\n\n[304\\. WebGPU Evaluation: A Comprehensive Analysis of WebGPU as a Compute Platform](https://ulb-dok.uibk.ac.at/ulbtirolhs/download/pdf/11304364)\n\n[305\\. Everything you need to know about the 3D web of the future: WebGPUs](https://animech.com/en/visualization/everything-you-need-to-know-about-the-3d-web-of-the-future-webgpus/)\n\n[306\\. WebGL & WebGPU BOF](https://www.khronos.org/assets/uploads/developers/presentations/WebGL__WebGPU_BOF_2024-07-31.pdf)\n\n[309\\. G. Karypis, Vipin Kumar. “A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs.” SIAM J. Sci. Comput.](https://doi.org/10.1137/S1064827595287997)\n\n[310\\. \"Efficient GPU rendering of subdivision surfaces using adaptive ...](https://history.siggraph.org/learning/efficient-gpu-rendering-of-subdivision-surfaces-using-adaptive-quadtrees/)\n\n[311\\. A high-performance software graphics pipeline architecture for the GPU | ACM Transactions on Graphics](https://dl.acm.org/doi/abs/10.1145/3197517.3201374)\n\n[312\\. J. Krüger, R. Westermann. “Acceleration techniques for GPU-based volume rendering.” IEEE Visualization, 2003. VIS 2003.](https://doi.org/10.1109/VIS.2003.10001)\n\n[313\\. Memory-efficient Adaptive Subdivision for Software Rendering on the GPU](https://www.marshallplan.at/images/All-Papers/MP-2014/Weber.pdf)\n\n[314\\. AI/ML for Network Engineers](https://www.ciscolive.com/c/dam/r/ciscolive/apjc/docs/2024/pdf/BRKENT-2209.pdf)\n\n[315\\. 基于 GPU 并行粒子群优化的超声弹性实时成像算法](http://give.zju.edu.cn/select/2.pdf)\n\n[316\\. WebGPU Evaluation: A Comprehensive Analysis of WebGPU as a Compute Platform](https://ulb-dok.uibk.ac.at/ulbtirolhs/download/pdf/11304364)\n\n[317\\. GPU Coroutines for Flexible Splitting and Scheduling o...](https://dl.acm.org/doi/10.1145/3687766)\n\n[318\\. Markus Hadwiger, P. Ljung et al. “Advanced illumination techniques for GPU-based volume raycasting.” International Conference on Computer Graphics and Interactive Techniques](https://doi.org/10.1145/1667239.1667241)\n\n[319\\. 平台和负载特征感知的在线图分割算法](http://cjc.ict.ac.cn/online/onlinepaper/lul-20207694907.pdf)\n\n[320\\. Rust Mesh Optimizer | Graham Wihlidal](https://www.wihlidal.com/blog/pipeline/2018-10-20-rust-mesh-optimizer/)\n\n[321\\. FastFCN: 基于扩展卷积的主干语义分割方法研究](https://b23.tv/BV1qqKJznEnM?t=222)\n\n[322\\. Potree in WebGPU](https://www.khronos.org/assets/uploads/developers/library/2021-webgl-meetup-may/webgpu-potree_May21.pdf)\n\n[323\\. Liming Ye, Gang Liu et al. “3D Model Occlusion Culling Optimization Method Based on WebGPU Computing Pipeline.” Comput. Syst. Sci. Eng.](https://doi.org/10.32604/csse.2023.041488)\n\n[324\\. Optimizing Graphics Pipelines with Meshlets: A Guide to ...](https://www.packtpub.com/en-us/learning/how-to-tutorials/optimizing-graphics-pipelines-with-meshlets-a-guide-to-efficient-geometry-processing?srsltid=AfmBOopNDO-yX-OTsqKEJ_oLIjHhrTcqL7VFYk1CAB7qt_yxxfJ3h_r3)\n\n[325\\. Optimized watershed delineation library for server-side and client-side web applications](https://opengeospatialdata.springeropen.com/articles/10.1186/s40965-019-0068-9)\n\n[326\\. Zengsheng Liu, Shizhao Chen et al. “ISpliter: an intelligent and automatic surface mesh generator using neural networks and splitting lines.” Advances in Aerodynamics](https://doi.org/10.1186/s42774-023-00150-4)\n\n[327\\. 博士論文 へテロジニアス計算環境における深層学習基盤の研究](https://repository.dl.itc.u-tokyo.ac.jp/record/2004464/files/A36569.pdf)\n\n[329\\. C2PA Technical Specification](https://c2pa.org/specifications/specifications/1.3/specs/_attachments/C2PA_Specification.pdf)\n\n[330\\. GitHub - contentauth/c2pa-rs: Rust SDK for the core C2PA (Coalition for ...](https://github.com/contentauth/c2pa-rs)\n\n[331\\. C2PA Technical Specification :: C2PA Specifications](https://c2pa.org/specifications/specifications/1.3/specs/C2PA_Specification.html)\n\n[332\\. What is the Coalition for Content Provenance and ...](https://www.techtarget.com/whatis/definition/Coalition-for-Content-Provenance-and-Authenticity-C2PA)\n\n[333\\. Geopipe/gltf2glb - GitHub](https://github.com/Geopipe/gltf2glb)\n\n[334\\. Understanding manifests - CAI open source SDK](https://opensource.contentauthenticity.org/docs/manifest/understanding-manifest/)\n\n[335\\. Karthika Balan, Alexander Black et al. “DECORAIT - DECentralized Opt-in/out Registry for AI Training.” Proceedings of the 20th ACM SIGGRAPH European Conference on Visual Media Production](https://doi.org/10.1145/3626495.3626506)\n\n[339\\. WebGPU](https://www.orillusion.com/zh/webgpu.html)\n\n[340\\. The Multidimensional Visual Analyser (MVA)](https://ftp.isds.tugraz.at/pub/theses/ogolob-2024-msc.pdf)\n\n[341\\. WebGPU Compatibility Mode](https://webgpufundamentals.org/webgpu/lessons/webgpu-compatibility-mode.html)\n\n[342\\. Nova Documentation](https://cdn.prod.website-files.com/66735931639446f0f4e81fb3/6680d9884b915ad8e127a5b7_Nova%20Documentation.pdf)\n\n[343\\. Detecting WebGPU compatability/Good idea in the first place?](https://forum.babylonjs.com/t/detecting-webgpu-compatability-good-idea-in-the-first-place/29918)\n\n[344\\. GitHub - gpuweb/gpuweb: Where the GPU for the Web work happens!](https://github.com/gpuweb/gpuweb)\n\n[345\\. GitHub - DavidPeicho/wgpu: Native WebGPU implementation based...](https://github.com/DavidPeicho/wgpu)\n\n[346\\. Generic and Automated Drive-by GPU Cache Attacks from the Browser](https://hal.science/hal-04613708v1/document)\n\n[347\\. 国际新闻](https://www.chinaw3c.org/category/world-news/)\n\n[348\\. W3C最新标准进展WebGPU着色语言草案公布](https://www.w3cschool.cn/article/42171944.html)"
  },
  {
    "id": 35,
    "question": "How do you reconcile the potential for AI agents to expand productivity and labor capabilities with concerns about companies exploiting this technology to ruthlessly cut workforces?",
    "response": "**Research Report: The AI Productivity Paradox – Reconciling Workforce Augmentation and Automation-Driven Reductions**\n\n**Date:** July 26, 2025\n\n**Author:** Expert Researcher\n\n**1.0 Executive Summary**\n\nAs of mid-2025, the integration of sophisticated Artificial Intelligence (AI) agents into the global economy represents a pivotal moment in the history of labor and productivity. This report synthesizes recent data and analysis to address the central tension of our time: how to reconcile the immense potential for AI agents to expand productivity and augment human capabilities with the pervasive concern that corporations will exploit this same technology for ruthless workforce reductions.\n\nAnalysis of industry data from 2024 and early 2025 reveals a clear and significant productivity boom in sectors most exposed to AI \\[24\\]\\[72\\]. Simultaneously, executive surveys and economic forecasts point to a tangible risk of job displacement \\[23\\]\\[71\\]. This report argues that reconciliation is not a passive outcome but a result of deliberate choices across three key domains: technological design, corporate strategy, and public policy.\n\nThe technical architecture of modern AI agent systems, including multi-agent collaboration and standardized protocols like Google's Agent-to-Agent (A2A), contains mechanisms explicitly designed for human-AI partnership \\[7\\]\\[69\\]. However, corporate strategy remains the primary determinant of whether AI is deployed as a collaborative tool or a replacement technology. While some firms are investing in upskilling and redesigning work, others are clearly focused on cost-cutting through automation. Finally, government interventions, such as the now-rescinded 2023 Biden AI Executive Order, represent attempts to guide AI's trajectory toward more equitable outcomes, though their effectiveness remains a subject of debate due to political volatility and a lack of direct, empirical impact data \\[94\\]\\[159\\]. The path forward requires a multi-stakeholder approach to ensure the benefits of the AI revolution are broadly shared.\n\n**2.0 The Dual Impact of AI on Productivity and Labor**\n\nThe discourse surrounding AI in 2025 is dominated by two powerful, and often conflicting, narratives. On one hand, AI is hailed as the engine of a new productivity revolution. On the other, it is feared as the harbinger of mass technological unemployment. The evidence suggests both narratives hold a degree of truth.\n\n**2.1 The Productivity Promise: AI as an Augmentation Engine**\n\nData from 2024 and 2025 unequivocally demonstrates that AI is having a profound positive impact on labor productivity. Industries with the highest exposure to AI are experiencing labor productivity growth that is 4.8 times higher than in less exposed sectors \\[24\\]\\[25\\]\\[72\\]. This trend has seen productivity growth in these key industries nearly quadruple since 2022, rising from 7% to 27% \\[75\\]. Deloitte has estimated that AI and machine learning could boost overall labor productivity by as much as 37% by the end of 2025 by automating manual processes and freeing human workers for more complex, higher-value activities \\[33\\]\\[76\\].\n\nThis productivity surge is reflected in key financial metrics for publicly traded companies. An analysis by BofA Global Research suggests enterprise AI implementations could expand S&P 500 operating margins by 200 basis points over the next five years, unlocking approximately $55 billion in annual cost savings \\[136\\]. Accenture's modeling further projects that if S&P 500 companies invest in human-machine collaboration at the rate of top performers, they could collectively boost revenues by 38% \\[133\\]. In 2024, surveys indicated that nearly all firms implementing AI reported revenue increases of 10-20% and comparable cost reductions \\[195\\].\n\nAt the individual worker level, the gains are equally impressive. Microsoft reported in 2024 that users of its Copilot tool were saving over 10 hours per month, and predicted that 2025 would be the year these individual gains are translated into broader business process transformation through the deployment of AI agents \\[113\\]. A landmark study by researchers from Stanford and MIT involving a Fortune 500 software company found that access to a generative AI tool increased the productivity of customer service agents by an average of 14%, with less-experienced workers seeing their performance improve by as much as 35% \\[123\\]. These findings support the \"augmentation\" thesis: AI acts as a co-pilot, enhancing the skills and efficiency of the existing workforce \\[81\\]\\[83\\].\n\n**2.2 The Specter of Displacement: AI as an Automation Tool**\n\nDespite the clear evidence of productivity gains through augmentation, fears of workforce reduction are not unfounded. The same technologies that can assist a worker can also fully automate their tasks. A 2023 survey revealed that 41% of senior executives expect their workforces to be smaller in the future due to AI, with 44% predicting AI-driven layoffs in 2024 \\[23\\]. Goldman Sachs has gone as far as to predict a potential 24% reduction in the overall workforce over a 10-year period due to automation \\[71\\].\n\nCertain job functions are particularly vulnerable. A 2023 survey identified service operations (54%), supply chain management (45%), and HR (41%) as functions most likely to see employment decrease due to generative AI \\[37\\]\\[148\\]. This points to a strategic choice being made by some companies to pursue \"talent rightsizing\" through automation, hiring freezes, and layoffs to boost operational efficiency \\[141\\].\n\nHowever, the picture is complex. While some companies are reducing headcount, others report that AI is creating new roles, leading to a transformation of work rather than a net loss of jobs \\[23\\]\\[26\\]. Data from the Linux Foundation shows that in 2023-2024, only 5% of surveyed organizations felt AI had reduced their overall headcount, and more organizations were hiring due to AI than were laying off staff \\[31\\]. This suggests that while displacement in specific roles is real, there may be a counterbalancing effect from the creation of new tasks and roles that complement AI systems \\[34\\]. The reconciliation of these opposing forces depends heavily on the specific design of AI systems and the strategic goals of the companies deploying them.\n\n**3.0 Bridging the Divide: Mechanisms for Human-AI Collaboration**\n\nThe reconciliation of productivity and employment rests significantly on whether AI systems are architected for collaboration or simple replacement. The technology itself is not inherently one or the other; its design reflects the intentions of its creators and deployers. As of 2025, there are clear technical mechanisms that favor a collaborative, augmentation-focused approach.\n\n**3.1 Technical Architecture for Augmentation**\n\nModern AI agent systems are increasingly built around the concept of **multi-agent collaboration**, where specialized agents work together, and with humans, to achieve complex goals \\[7\\]\\[15\\]. This collaborative framework is enabled by several key technical features:\n\n**Distributed Task Decomposition:** AI systems can break down complex human requests into smaller subtasks, intelligently assigning them to the most appropriate agent—whether AI or human—based on its capabilities \\[7\\]. This allows AI to handle rote, repetitive, or data-intensive subtasks, freeing up human workers to focus on strategic oversight, creative problem-solving, and nuanced judgment calls \\[2\\]\\[4\\].\n\n**Dynamic Coordination Protocols:** Protocols like Google's **Agent-to-Agent (A2A) protocol**, introduced in April 2025, are crucial for enabling seamless communication and coordination between different AI agents, even across organizational boundaries \\[7\\]\\[69\\]. The A2A protocol is designed to support long-running workflows that explicitly involve points of human input and approval, cementing the human's role within the automated process \\[69\\].\n\n**Human-Centered Design and Meaningful Control:** A growing movement in AI development emphasizes **meaningful human control**, which ensures that a human remains responsible for critical decisions and that the system's actions can be traced back to human oversight \\[9\\]\\[16\\]. This philosophy is embedded in the design of collaborative AI, where the system is a tool to inform and empower the human decision-maker, not replace them.\n\nThese technical designs frame AI agents as powerful assistants that operate in the background to enhance existing business processes, improving efficiency and accuracy without necessitating the removal of the human worker \\[4\\].\n\n**3.2 Case Study Evidence: A Mixed and Ambiguous Picture**\n\nDespite the technical potential for augmentation, finding clear, quantitative case studies from major corporations that demonstrate productivity gains _without any_ workforce reduction remains challenging. The data is often ambiguous or incomplete.\n\nFor instance, an enterprise case study of the A2A protocol implementation highlighted remarkable efficiency improvements: a 300% increase in customer service efficiency, an 80% reduction in operations response time, and a 60% reduction in operations cost \\[62\\]\\[65\\]. Crucially, it also cited a \"50% labor cost saved\" \\[65\\]. This figure is open to interpretation: it could mean that the company achieved the same output with half the staff, or that it doubled its output with the same number of now more productive staff. This ambiguity is common in corporate reporting.\n\nSimilarly, the widely cited Stanford/MIT study of a Fortune 500 company showed a 14% productivity increase from generative AI \\[123\\]. While it demonstrated that AI can enhance worker performance, particularly for the less experienced, the study did not include data on the company's overall workforce size before and after the implementation, leaving the question of net job impact unanswered.\n\nThe lack of definitive, public \"win-win\" case studies from Fortune 500 companies \\[119\\]\\[124\\]suggests one of three possibilities: 1) These ideal outcomes are rare; 2) Companies achieving them are not publicizing the detailed workforce data for competitive reasons; or 3) The transition is still too early, and most companies are still navigating the complex process of reorganizing work around AI, with the final impact on headcount yet to be determined.\n\n**4.0 The Role of Strategy and Policy in Shaping Outcomes**\n\nThe impact of AI is not technologically determined; it is shaped by human choices. Corporate strategies and government policies are the primary levers for guiding AI's deployment toward either a collaborative or a displacement-focused future.\n\n**4.1 Corporate Strategy: Augmentation vs. Reduction as a Business Choice**\n\nCompanies are at a strategic crossroads. The decision to use AI to augment or reduce the workforce is influenced by their time horizon, their view of human capital, and how they measure success.\n\nThe predominant narrative in S&P 500 earnings calls, where mentions of AI have soared, is one of efficiency and productivity \\[116\\]\\[200\\]. However, how this efficiency is achieved is a critical strategic choice. A narrow focus on short-term ROI on labor spend may lead companies to cut headcount as soon as a task can be automated \\[89\\]. In fact, one in three executives notes that AI is prompting them to rethink how productivity itself is measured, indicating a potential shift away from simplistic metrics like Full-Time Equivalents (FTEs) \\[36\\]\\[89\\].\n\nCompanies pursuing a long-term augmentation strategy are more likely to focus on upskilling their workforce and redesigning jobs to create new value. The data shows that jobs are indeed growing in every industry analyzed by PwC, with augmented jobs often growing faster than non-augmented ones \\[75\\]. This supports the idea that many companies are choosing to reinvest productivity gains into creating new, higher-value roles that leverage human skills in partnership with AI. These companies are betting that human-AI collaboration will ultimately create more value and a more resilient business than a purely automated one.\n\n**4.2 Government Intervention: The Attempt to Steer AI Development**\n\nRecognizing the profound societal implications of AI, governments have begun to intervene. The most significant U.S. effort was President Joe Biden's Executive Order of October 30, 2023, on \"Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\" \\[43\\]\\[44\\]\\[161\\].\n\nThe order's directives were squarely aimed at the central conflict this report addresses. It instructed federal agencies to develop principles and best practices to mitigate job displacement, protect labor standards, and prevent AI-driven wage and hour violations \\[97\\]\\[105\\]. A major emphasis was placed on workforce development, directing the National Science Foundation to prioritize AI-related education and upskilling to minimize job losses \\[97\\]\\[104\\]\\[165\\]. The order represented a clear attempt by the government to nudge the development and deployment of AI in a pro-worker direction.\n\nHowever, the search for empirical data directly attributing changes in workforce size or productivity at Fortune 500 companies to this executive order yields no results \\[166\\]\\[208\\]. There are several reasons for this. First, the order was primarily a set of directives to federal agencies to create future guidelines, not an immediate set of legally binding requirements on private corporations \\[94\\]. Second, its time in effect was short. The subsequent Trump administration rescinded the Biden AI Executive Order in early 2025, highlighting the political fragility of such policy interventions \\[100\\]\\[159\\]\\[214\\]. This political volatility makes it difficult for corporations to plan around long-term regulatory frameworks, potentially leading them to prioritize more immediate, market-driven goals like cost reduction.\n\nWhile the Biden EO's direct impact is unmeasurable, it was part of a broader international movement, including the G7's Hiroshima Process and the OECD's AI Principles, to establish norms for trustworthy and human-centric AI \\[46\\]\\[42\\].\n\n**5.0 Conclusion: Reconciling the Future Through Deliberate Action**\n\nThe tension between AI-driven productivity and workforce stability is the defining economic challenge of 2025. This report concludes that reconciliation is possible, but it is not inevitable. It requires conscious and concerted effort from technologists, corporate leaders, and policymakers.\n\n1.  **Technological Design:** The architecture of modern AI agent systems, with its emphasis on multi-agent collaboration and human-in-the-loop workflows, provides a technical foundation for augmentation over replacement. The continued development of protocols like A2A that standardize human-AI interaction will be critical.\n2.  **Corporate Strategy:** The ultimate impact of AI on jobs will be decided in the boardrooms of the world's largest companies. The evidence shows a split between firms pursuing short-term efficiency gains through automation-driven layoffs and those investing in a long-term strategy of augmentation, upskilling, and work redesign. The most successful and resilient companies of the next decade will likely be those that master the art of human-AI collaboration, leveraging technology to empower, not eliminate, their workforce. A critical evolution will be the shift from measuring FTEs to valuing future skills.\n3.  **Public Policy:** Government has a crucial role to play in setting the \"rules of the road\" to incentivize augmentation and mitigate the negative consequences of displacement. The short life of the 2023 Biden AI Executive Order demonstrates the difficulty of creating stable, long-term policy in a volatile political environment. Nevertheless, continued efforts to establish standards for transparency, fairness, and worker protection are essential to ensure a just transition and maintain social cohesion.\n\nReconciling the AI productivity paradox is not a one-time solution but an ongoing process of adaptation. The path forward will involve a period of significant job transformation. The challenge for society is to manage this transition by fostering a culture of lifelong learning, investing in new skills, and building a policy framework that ensures the unprecedented wealth generated by AI leads to shared prosperity rather than deepening inequality.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. 2025 AI驱动组织人才发展白皮书](https://www.phbs.pku.edu.cn/__local/8/EC/E3/6C1E297AC3083019D4032443696_E778CD5B_32332F7.pdf)\n\n[2\\. 2025年技术趋势](https://pdf.dfcfw.com/pdf/H3_AP202501091641873262_1.pdf?1736450041000.pdf)\n\n[3\\. AI Industry Landscape Report 2025](https://repository.ceibs.edu/files/59116885/AI_Industry_landscape_report_2025.pdf)\n\n[4\\. Agent助力AI从工具到伙伴](https://mp.weixin.qq.com/s?__biz=MjM5ODI0MzkxMg%3D%3D&mid=2654774475&idx=1&sn=ab23bbcc5419bfb56842182bac7a3697&chksm=bc960ba19760a9b2ef8c75bca7518c41f08716a10b9d14cbcef61030b2742b7517a326ff97a5&scene=27)\n\n[5\\. Gary Klein, David D. Woods et al. “Ten Challenges for Making Automation a \"Team Player\" in Joint Human-Agent Activity.” IEEE Intell. Syst.](https://doi.org/10.1109/MIS.2004.74)\n\n[6\\. T. O’Neill, Nathan J. Mcneese et al. “Human–Autonomy Teaming: A Review and Analysis of the Empirical Literature.” Human Factors](https://doi.org/10.1177/0018720820960865)\n\n[7\\. Prashik Buddhaghosh Bansod. “Distinguishing Autonomous AI Agents from Collaborative Agentic Systems: A Comprehensive Framework for Understanding Modern Intelligent Architectures.”](https://arxiv.org/abs/2506.01438)\n\n[8\\. 开放环境下的协作多智能体强化学习进展综述](https://www.lamda.nju.edu.cn/zhangzq/zzq_index_files/openmarl.pdf)\n\n[9\\. F. S. D. Sio, J. Hoven. “Meaningful Human Control over Autonomous Systems: A Philosophical Account.” Frontiers Robotics AI](https://doi.org/10.3389/frobt.2018.00015)\n\n[10\\. 电子行业2025年度策略：AI云侧端侧共振，复苏加持国产替代](https://pdf.dfcfw.com/pdf/H3_AP202412161641310944_1.pdf?1734359895000.pdf)\n\n[11\\. Wei Xu, Zaifeng Gao et al. “An HCAI Methodological Framework: Putting It Into Action to Enable Human-Centered AI.” ArXiv](https://doi.org/10.48550/arXiv.2311.16027)\n\n[12\\. Nolan Bard, Jakob N. Foerster et al. “The Hanabi Challenge: A New Frontier for AI Research.” ArXiv](https://doi.org/10.1016/j.artint.2019.103216)\n\n[13\\. Dominik Dellermann, P. Ebel et al. “Hybrid Intelligence.” Business & Information Systems Engineering](https://doi.org/10.1007/s12599-019-00595-2)\n\n[14\\. 原点与探赜](https://openedu.sou.edu.cn/upload/qikanfile/202503280953456477.pdf)\n\n[15\\. 2025 AI Agent迷局：谁在玩真的，谁在演戏？](https://www.myzaker.com/article/6780e6248e9f09753c57e6cb)\n\n[16\\. AI普及化背景下的价值提升机制与未来研究方向——基于人机持续互信视角](https://www.rdfybk.com/qw/DownPdf?id=882830)\n\n[17\\. Khanh-Tung Tran, Dung Dao et al. “Multi-Agent Collaboration Mechanisms: A Survey of LLMs.”](https://arxiv.org/abs/2501.06322)\n\n[18\\. 大家早上好!在2025年,AI将不再只是一个工具,而将成为一个...](https://caifuhao.eastmoney.com/news/20250101075551300084140)\n\n[19\\. Yijia Shao, Vinay Samuel et al. “Collaborative Gym: A Framework for Enabling and Evaluating Human-Agent Collaboration.”](https://arxiv.org/abs/2412.15701)\n\n[21\\. PwC Releases 2025 Global AI Jobs Barometer](https://www.hubbis.com/news/pwc-releases-2025-global-ai-jobs-barometer)\n\n[22\\. PwC 2025 Global AI Jobs Barometer](https://www.pwc.com/gx/en/news-room/press-releases/2025/ai-linked-to-a-fourfold-increase-in-productivity-growth.html)\n\n[23\\. In focus: AI statistics, insights and trends | Definition](https://www.thisisdefinition.com/resources/ai-statistics)\n\n[24\\. PwC's 2024 AI Jobs Barometer](https://www.lexology.com/library/detail.aspx?g=bb24a363-96e5-4a18-8232-0213a14f56f9)\n\n[25\\. PwC 2024 Global AI Jobs Barometer](https://www.pwc.com/bm/en/press-releases/pwc-2024-global-ai-jobs-barometer.html)\n\n[26\\. AI Jobs Barometer: Hur påverkar AI: Jobben, kompetens, löner och produktivitet? Sverige och globalt 2024](https://mb.cision.com/Public/365/3988234/bccbd7b1bb69377c.pdf)\n\n[27\\. AI-Driven Productivity Scenarios](https://www.cigionline.org/documents/3425/AI-scenarios_final_updated.pdf)\n\n[28\\. AI linked to a fourfold increase in productivity growth and ...](https://www.ryt9.com/en/prg/12728056)\n\n[29\\. Barómetro de la IA en el empleo 2024](https://www.pwc.com/cl/es/publicaciones/barometro-de-la-ia-en-el-empleo/Barometro-de-IA-en-empleos-2024.pdf)\n\n[30\\. What is the Future of Work in the Generative AI Era? A Marxist and Ricardian Analysis](https://www.triple-c.at/index.php/tripleC/article/view/1536/1633)\n\n[31\\. The Economic and Workforce Impacts of Open Source AI](https://www.linuxfoundation.org/hubfs/LF%20Research/lfr_market_impact_052025a.pdf?hsLang=en)\n\n[32\\. Artificial Intelligence Index Report 2025: Chapter 4 Economy](https://hai-production.s3.amazonaws.com/files/hai_ai-index-report-2025_chapter4_final.pdf)\n\n[33\\. 2024 global insurance outlook](https://www2.deloitte.com/content/dam/insights/articles/us176247_cfs_outlook-insurance/DI_2024-Global-Insurance-Outlook.pdf)\n\n[34\\. Displacement or Augmentation? The Effects of AI Innovation on Workforce Dynamics and Firm Value](https://www.aeaweb.org/conference/2025/program/paper/BBsK4Zkd)\n\n[35\\. Daron Acemoglu, P. Restrepo. “Robots and Jobs: Evidence from US Labor Markets.” Journal of Political Economy](https://doi.org/10.1086/705716)\n\n[36\\. Rethinking productivity in the age of AI](https://www.mercer.com/insights/people-strategy/future-of-work/rethinking-productivity-in-the-age-of-ai/)\n\n[37\\. Artificial Intelligence Index Report 2024](https://www.hkdca.com/wp-content/uploads/2024/06/ai-index-report-2024-hai.pdf)\n\n[41\\. Policy approaches to reduce inequalities while boosting productivity growth](https://g20.gov.br/pt-br/trilhas/trilha-de-financas/economia-global/4-oecd-report-policy-approaches-to-reduce-inequalities-while-boosting.pdf/@@download/file)\n\n[42\\. OECD Employment Outlook 2023: Artificial Intelligence and the Labour Market](https://read.oecd.org/10.1787/079c6ebb-de?format=html,read)\n\n[43\\. Algorithmic Accountability: The Next Frontier in ...](https://www.employerslawyersblog.com/2024/08/algorithmic-accountability-the-next-frontier-in-employment-law.html)\n\n[44\\. Generative AI Will Stay Top of Mind in 2024](https://www.clearygottlieb.com/news-and-insights/publication-listing/generative-ai-will-stay-top-of-mind-in-2024)\n\n[45\\. THE 2023 WORLD MANUFACTURING REPORT: NEW BUSINESS MODELS FOR THE MANUFACTURING OF THE FUTURE](https://worldmanufacturing.org/wp-content/uploads/27/6-WMF-Report-2023_E-Book.pdf)\n\n[46\\. Artificial Intelligence and Tourism](https://www.ministeroturismo.gov.it/wp-content/uploads/2024/11/policy-paper.pdf)\n\n[47\\. Futures of Global AI Governance: Co-Creating an Approach for Transforming Economies and Societies](https://www.oecd.org/content/dam/oecd/en/about/programmes/strategic-foresight/GSG%20Background%20Note_GSG%282024%291en.pdf/_jcr_content/renditions/original./GSG%20Background%20Note_GSG%282024%291en.pdf)\n\n[48\\. Balancing Ai and Workforce: 9 Pronged Strategy to Reduce Job Loss](https://www.techthirsty.com/balancing-ai-and-workforce-9-pronged-strategy-to-reduce-job-loss/)\n\n[49\\. 2023 Proposed Policy Resolutions](https://chamber.ca/wp-content/uploads/2023/09/2023_Proposed_Policy_Resolutions.pdf)\n\n[50\\. Highlights of the 2023 Executive Order on Artificial Intelligence for Congress](https://www.congress.gov/118/meeting/house/116793/documents/HHRG-118-FD00-20240206-SD003.pdf)\n\n[51\\. ECS Strategic Research and Innovation Agenda 2024](https://ecssria.eu/ECS-SRIA-2024.pdf)\n\n[52\\. The State of AI in 2023: Generative AI's Breakout Year | Pico](https://sg.pico.com/en/insights/editor-s-pick/the-state-of-ai-in-2023-generative-ai-s-breakout-year)\n\n[53\\. Technology Trends for 2024](https://www.bbvaspark.com/contenido/en/news/tech-trends-2024-bbva-spark/)\n\n[54\\. Generative AI Applications: Overview](https://assets.ctfassets.net/f1df9zr7wr1a/2EnDgQVfYIHzDIEc0yeVGQ/d43aea03476e44705524567126441aa1/163._Generative_AI_Applications__Overview.docx)\n\n[55\\. Regulatory Alert: First 100 Days: Upcoming Regulatory Signals for AI](https://kpmg.com/kpmg-us/content/dam/kpmg/pdf/2025/first-100-days-upcoming-regulatory-signals-ai-reg-alert.pdf)\n\n[56\\. 2023-2024 CISA Roadmap for Artificial Intelligence](https://www.cisa.gov/sites/default/files/2023-11/2023-2024_CISA-Roadmap-for-AI_508c.pdf)\n\n[57\\. C. Coglianese, Colton R. Crum. “Taking Training Seriously: Human Guidance and Management-Based Regulation of Artificial Intelligence.” ArXiv](https://doi.org/10.48550/arXiv.2402.08466)\n\n[58\\. DeepSeek 驱动行业智变提速，腾讯云汇聚大咖共话进阶之路](https://www.geekpark.net/news/347813)\n\n[61\\. A2A Protocol: Boost Employee Efficiency with AI ...](https://www.byteplus.com/en/topic/551142)\n\n[62\\. A2A Protocol Enterprise Practice Case Study](https://a2acn.com/en/blog/implementing-a2a-in-enterprise/)\n\n[63\\. A2A Protocol Blog](https://a2acn.com/en/blog/)\n\n[64\\. Proposal for Improving Google A2A Protocol: Safeguarding ...](https://arxiv.org/html/2505.12490v1)\n\n[65\\. A2A Protocol企业实践案例](https://a2acn.com/blog/implementing-a2a-in-enterprise/)\n\n[66\\. 2024 global insurance outlook](https://www2.deloitte.com/content/dam/insights/articles/us176247_cfs_outlook-insurance/DI_2024-Global-Insurance-Outlook.pdf)\n\n[67\\. Policy approaches to reduce inequalities while boosting productivity growth](https://g20.gov.br/pt-br/trilhas/trilha-de-financas/economia-global/4-oecd-report-policy-approaches-to-reduce-inequalities-while-boosting.pdf/@@download/file)\n\n[68\\. AI Progress in 2025: What's Happened and What's Next](https://www.digitalbricks.ai/blog-posts/ai-progress-in-2025-whats-happened-and-whats-next)\n\n[69\\. Understanding A2A — The Protocol for Agent Collaboration](https://www.googlecloudcommunity.com/gc/Community-Blogs/Understanding-A2A-The-Protocol-for-Agent-Collaboration/ba-p/906323#:~:text=The%20A2A%20protocol%20structures%20interactions,exchange%20information%20in%20various%20formats.)\n\n[70\\. Workforce 2.0: Unlocking human potential in a machine-augmented world](https://www.talencore.com/files/2024%20Mercer%20-%20Global%20Talent%20Trends.pdf?srsltid=AfmBOooVj1k3BdCF18NJC-2vNgpL1lykWulmo_8Fo36igTPuJAT3fOna)\n\n[71\\. A GUIDE TO ARTIFICIAL INTELLIGENCE. 2024](https://www.iab.org.pl/wp-content/uploads/2024/09/The-Guide-to-AI_IAB-POLAND.pdf)\n\n[72\\. PwC 2024 Global AI Jobs Barometer](https://www.pwc.com/bm/en/press-releases/pwc-2024-global-ai-jobs-barometer.html)\n\n[73\\. PwC's 2024 AI Jobs Barometer](https://www.lexology.com/library/detail.aspx?g=bb24a363-96e5-4a18-8232-0213a14f56f9)\n\n[74\\. TREND REPORT 2024](https://www.robotspaceship.com/fileadmin/user_upload/240131-trend-report_EN.pdf)\n\n[75\\. AI linked to a fourfold increase in productivity growth and ...](https://www.pwc.com/th/en/press-room/press-release/2025/press-release-09-07-25-en.html)\n\n[76\\. AI in the Workplace: Key Use Cases, Benefits, and Challenges](https://www.itransition.com/ai/workplace)\n\n[77\\. AI Jobs Barometer: Hur påverkar AI: Jobben, kompetens, löner och produktivitet? Sverige och globalt 2024](https://mb.cision.com/Public/365/3988234/bccbd7b1bb69377c.pdf)\n\n[78\\. The Global Impact of AI: Mind the Gap](https://www.elibrary.imf.org/view/journals/001/2025/076/article-A001-en.pdf)\n\n[79\\. 2025 Outlook Report](https://privatebank.jpmorgan.com/content/dam/jpm-pb-aem/global/en/documents/2025-Outlook_Building_on_Strength.pdf?page=43)\n\n[80\\. CHARTING DISRUPTION: OUTLOOK FOR 2024 AND BEYOND](https://www.globalxetfs.com/content/files/Charting-Disruption-2024-Paradigm-Shifting-Technologies.pdf)\n\n[81\\. Futures of Global AI Governance: Co-Creating an Approach for Transforming Economies and Societies](https://www.oecd.org/content/dam/oecd/en/about/programmes/strategic-foresight/GSG%20Background%20Note_GSG%282024%291en.pdf/_jcr_content/renditions/original./GSG%20Background%20Note_GSG%282024%291en.pdf)\n\n[82\\. Daron Acemoglu, P. Restrepo. “Robots and Jobs: Evidence from US Labor Markets.” Journal of Political Economy](https://doi.org/10.1086/705716)\n\n[83\\. 2024 global insurance outlook](https://www2.deloitte.com/content/dam/insights/articles/us176247_cfs_outlook-insurance/DI_2024-Global-Insurance-Outlook.pdf)\n\n[84\\. Artificial Intelligence Index Report 2024](https://www.bollettinoadapt.it/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_.pdf)\n\n[85\\. Top 140 Artificial Intelligence Stats for 2025](https://thunderbit.com/blog/top-artificial-intelligence-stats)\n\n[86\\. Barómetro de la IA en el empleo 2024](https://www.pwc.com/cl/es/publicaciones/barometro-de-la-ia-en-el-empleo/Barometro-de-IA-en-empleos-2024.pdf)\n\n[87\\. C. Frey, Michael A. Osborne. “The future of employment: How susceptible are jobs to computerisation?.” Technological Forecasting and Social Change](https://doi.org/10.1016/J.TECHFORE.2016.08.019)\n\n[88\\. AI and the Future of Productivity at Work: 2024 Statistics](https://tech.co/news/productivity-work-statistics)\n\n[89\\. Rethinking productivity in the age of AI](https://www.mercer.com/insights/people-strategy/future-of-work/rethinking-productivity-in-the-age-of-ai/)\n\n[91\\. Daron Acemoglu, P. Restrepo. “Robots and Jobs: Evidence from US Labor Markets.” Journal of Political Economy](https://doi.org/10.1086/705716)\n\n[92\\. C. Frey, Michael A. Osborne. “The future of employment: How susceptible are jobs to computerisation?.” Technological Forecasting and Social Change](https://doi.org/10.1016/J.TECHFORE.2016.08.019)\n\n[93\\. D. Acemoglu, P. Restrepo. “Automation and New Tasks: How Technology Displaces and Reinstates Labor.” NBER Working Paper Series](https://doi.org/10.1257/JEP.33.2.3)\n\n[94\\. The Biden Executive Order on AI: Key Takeaways](https://shlegal-technology.com/insight/biden-executive-order-ai-key-takeaways)\n\n[95\\. Artificial Intelligence Executive Order: Workplace Implications - Mintz](https://www.mintz.com/insights-center/viewpoints/2226/2025-02-12-artificial-intelligence-executive-order-workplace)\n\n[96\\. Biden’s Executive Order on “Safe, Secure, and Trustworthy” AI](http://editions.lib.umn.edu/mjlst/category/regulatory/)\n\n[97\\. President Biden Issues Landmark Artificial Intelligence Executive Order](https://www.littler.com/es/node/614016)\n\n[98\\. Comprehensive Guide to President Biden's Artificial Intelligences Executive Order: AI Security and Safety](https://www.aiornot.com/blog/comprehensive-guide-to-president-bidens-artificial-intelligences-executive-order-ai-security-and-safety)\n\n[99\\. United States approach to artificial intelligence](https://www.europarl.europa.eu/RegData/etudes/ATAG/2024/757605/EPRS_ATA%282024%29757605_EN.pdf)\n\n[100\\. Trump Issues Executive Order to Boost AI](https://www.shrm.org/mena/topics-tools/news/technology/trump-issues-executive-order-to-boost-ai)\n\n[101\\. D. Acemoglu, P. Restrepo. “Artificial Intelligence, Automation and Work.” Alfred P. Sloan Foundation Economic Research Paper Series](https://doi.org/10.2139/ssrn.3098384)\n\n[102\\. Biden's Big AI Executive Order: Implications for the Future of Tech](https://www.pluralsight.com/resources/blog/ai-and-data/biden-executive-order-artificial-intelligence)\n\n[103\\. Georg Graetz, Guy Michaels. “The Review of Economics and Statistics.”](https://www.semanticscholar.org/paper/f2d582c675ac69cc5da0b54d9b1fe53329f01820)\n\n[104\\. Biden’s AI Order and the Implications for Employers](https://www.huntonprivacyblog.com/2023/11/29/bidens-ai-order-and-the-implications-for-employers/)\n\n[105\\. Biden's AI Executive Order: What it says, and what it means for security teams](https://www.wiz.io/blog/bidens-ai-executive-order-what-it-means-for-security-teams)\n\n[106\\. CURRICULUM VITAE](https://faculty.washington.edu/ebender/EmilyMBender_CV.pdf)\n\n[107\\. Highlights of the 2023 Executive Order on Artificial Intelligence for Congress](https://www.congress.gov/118/meeting/house/116793/documents/HHRG-118-FD00-20240206-SD003.pdf)\n\n[108\\. President Biden’s AI Executive Order and Its Impact o...](https://hrdailyadvisor.blr.com/2023/11/28/preseident-bidens-ai-executive-order-and-its-impact-on-employers/)\n\n[109\\. President Biden’s Executive Order on AI: Harnessing the Expertise of Agencies to Lasso the Bullish Artificial Intelligence Industry](https://moritzlaw.osu.edu/sites/default/files/2023-12/BarcyBlog%201.pdf)\n\n[110\\. Executive Order on Artificial Intelligence](https://www.shrm.org/topics-tools/tools/express-requests/executive-order-on-artificial-intelligence)\n\n[111\\. D. Acemoglu, David Autor. “Skills, Tasks and Technologies: Implications for Employment and Earnings.” IRPN: Innovation & Stratification (Topic)](https://doi.org/10.3386/W16082)\n\n[112\\. Daron Acemoglu, P. Restrepo. “Robots and Jobs: Evidence from US Labor Markets.” Journal of Political Economy](https://doi.org/10.1086/705716)\n\n[113\\. AI at Work: In 2025, AI will Help Us Navigate Uncertainty](https://www.microsoft.com/en-us/worklab/ai-at-work-in-2025-ai-will-help-us-navigate-uncertainty)\n\n[114\\. C. Syverson. “What Determines Productivity?.” ERN: Productivity (Topic)](https://doi.org/10.1257/JEL.49.2.326)\n\n[115\\. Georg Graetz, Guy Michaels. “The Review of Economics and Statistics.”](https://www.semanticscholar.org/paper/f2d582c675ac69cc5da0b54d9b1fe53329f01820)\n\n[116\\. 李飞飞团队关于2024年人工智能发展报告总结（Artificial ...](http://www.mfbz.cn/a/621632.html)\n\n[117\\. R. Vinuesa, Hossein Azizpour et al. “The role of artificial intelligence in achieving the Sustainable Development Goals.” Nature Communications](https://doi.org/10.1038/s41467-019-14108-y)\n\n[118\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2024.” ArXiv](https://doi.org/10.48550/arXiv.2405.19522)\n\n[119\\. Zarina Tasheva, Vitali Karpovich. “SUPERCHARGE HUMAN POTENTIAL THROUGH AI TO INCREASE PRODUCTIVITY THE WORKFORCE IN THE COMPANIES.” American Journal of Applied Science and Technology](https://doi.org/10.37547/ajast/volume04issue02-05)\n\n[120\\. Jigar Patel, Harish Kumar S. Purohit. “LEVERAGING AI FOR ESG GOALS: A CASE STUDY OF SELECT FORTUNE 500 COMPANIES.” INTERNATIONAL JOURNAL OF MANAGEMENT](https://doi.org/10.34218/ijm_16_02_001)\n\n[121\\. Work Change Report: AI Is Coming to Work](https://economicgraph.linkedin.com/content/dam/me/economicgraph/en-us/PDF/Work-Change-Report.pdf)\n\n[122\\. Erik Brynjolfsson, Danielle Li et al. “Generative AI at Work.” SSRN Electronic Journal](https://doi.org/10.3386/w31161)\n\n[123\\. 研究发现生成AI帮助提高14%生产率 对新手帮助最大](https://www.bilibili.com/video/av227898135)\n\n[124\\. 关注信创国产化和AI商业化两大主线——计算机行业2025年度投资策略](https://pdf.dfcfw.com/pdf/H3_AP202502201643304336_1.pdf?1740048032000.pdf)\n\n[125\\. AI 驱动下的全球产业变革与投资机遇——基于美国近两年五倍股的启示](https://pdf.dfcfw.com/pdf/H3_AP202503251647191342_1.pdf?1742896031000.pdf)\n\n[129\\. D. Acemoglu, David Autor. “Skills, Tasks and Technologies: Implications for Employment and Earnings.” IRPN: Innovation & Stratification (Topic)](https://doi.org/10.3386/W16082)\n\n[130\\. Digital Disruption Survey 2024: How Business and Technology Leaders Collaborate to Create Value](https://www.alixpartners.com/media/eooijwxt/alixpartners-2024-digital-disruption-survey.pdf)\n\n[131\\. C. Frey, Michael A. Osborne. “The future of employment: How susceptible are jobs to computerisation?.” Technological Forecasting and Social Change](https://doi.org/10.1016/J.TECHFORE.2016.08.019)\n\n[132\\. D. Acemoglu, P. Restrepo. “The Race between Man and Machine: Implications of Technology for Growth, Factor Shares, and Employment.” American Economic Review](https://doi.org/10.1257/AER.20160696)\n\n[133\\. REWORKING THE REVOLUTION](https://insuranceblog.accenture.com/wp-content/uploads/2018/02/Accenture-Reworking-the-Revolution-Jan-2018.pdf)\n\n[134\\. 2024: The Year That Changes Business Forever (Podcast)](https://joshbersin.com/2023/12/2024-the-year-that-changes-business-forever-podcast/)\n\n[135\\. Opportunity Knocks: Mid-Year Outlook for 2024](https://www.kkr.com/content/dam/kkr/insights/pdf/2024-mid-year-outlook.pdf)\n\n[136\\. AI: From evolution to revolution?](https://institute.bankofamerica.com/content/dam/transformation/ai-evolution-to-revolution.pdf)\n\n[137\\. 2024 A strong economy in a fragile world](https://privatebank.jpmorgan.com/content/dam/jpm-pb-aem/global/en/documents/mid-year-outlook-2024.pdf)\n\n[138\\. Which Fortune 500 Companies are the Most Productive?](https://dofollow.com/blog/fortune500-profit-per-employee-2024)\n\n[139\\. AI Jobs Statistics That Will Shock You in 2024](https://www.greataiprompts.com/guide/ai-jobs-statistics/)\n\n[140\\. Harnessing the value of generative AI: 2nd edition Top use cases across sectors](https://www.capgemini.com/us-en/wp-content/uploads/sites/30/2024/11/Generative-AI-in-Organizations-Refresh_25112024.pdf)\n\n[141\\. The Innovation Economy Outlook](https://www.jpmorgan.com/content/dam/jpmorgan/documents/cb/insights/outlook/jpm-ie-outlook-midyear-report-h2-2024-final-ada.pdf)\n\n[142\\. Unveiling the Key Statistics on HR Trends 2024](https://rethinkhr.co.in/unveiling-the-key-statistics-on-hr-trends/)\n\n[143\\. Generative AI in 2024: A potential lifeline amid workplace turbulence](https://www.cypherlearning.com/hubfs/docs/atd/ATD-CYPHER-Learning-2023-Research-on-Generative-AI.pdf)\n\n[144\\. Niladri B. Syam, Arun Sharma. “Waiting for a sales renaissance in the fourth industrial revolution: Machine learning and artificial intelligence in sales research and practice.” Industrial Marketing Management](https://doi.org/10.1016/J.INDMARMAN.2017.12.019)\n\n[145\\. The S&P 500 Index in 2024: A Market Driven Once Again by the Mag 7](https://www.ftportfolios.com/Commentary/EconomicResearch/2025/1/8/the-sp-500-index-in-2024-a-market-driven-once-again-by-the-mag-7)\n\n[146\\. Economic Review: November 2024 - Silvercrest](https://www.silvercrestgroup.com/economic-review-november-2024/)\n\n[147\\. HR trends highlight for businesses in 2024](https://peopleforce.io/blog/hr-trends-highlight-for-businesses-in-2024)\n\n[148\\. Artificial Intelligence Index Report 2024](https://www.hkdca.com/wp-content/uploads/2024/06/ai-index-report-2024-hai.pdf)\n\n[149\\. D. Acemoglu, P. Restrepo. “The Race between Man and Machine: Implications of Technology for Growth, Factor Shares, and Employment.” American Economic Review](https://doi.org/10.1257/AER.20160696)\n\n[150\\. Daron Acemoglu, P. Restrepo. “Robots and Jobs: Evidence from US Labor Markets.” Journal of Political Economy](https://doi.org/10.1086/705716)\n\n[151\\. Artificial Intelligence Index Report 2024](https://www.bollettinoadapt.it/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_.pdf)\n\n[152\\. D. Acemoglu, P. Restrepo. “Artificial Intelligence, Automation and Work.” Alfred P. Sloan Foundation Economic Research Paper Series](https://doi.org/10.2139/ssrn.3098384)\n\n[153\\. David Autor. “Why Are There Still So Many Jobs? The History and Future of Workplace.”](https://doi.org/10.1257/JEP.29.3.3)\n\n[154\\. Work, workforce, workers: Reinvented in the age of generative AI](https://www.accenture.com/content/dam/accenture/final/accenture-com/document-2/Accenture-Work-Can-Become-Era-Generative-AI.pdf)\n\n[155\\. Georg Graetz, Guy Michaels. “The Review of Economics and Statistics.”](https://www.semanticscholar.org/paper/f2d582c675ac69cc5da0b54d9b1fe53329f01820)\n\n[156\\. Automation Now & Next 2023: State of Intelligent Automation Report](https://www.automationanywhere.com/sites/default/files/internal-assets/now-and-next-report_en.pdf)\n\n[157\\. OECD Employment Outlook 2023: Artificial Intelligence and the Labour Market](https://read.oecd.org/10.1787/079c6ebb-de?format=html,read)\n\n[158\\. ACCELERATING AI SKILLS: PREPARING THE WORKFORCE FOR JOBS OF THE FUTURE](https://assets.aboutamazon.com/84/4b/c039993d4111b54af1afcb6bfbfb/aws-accelerating-ai-skills-us-report.pdf)\n\n[159\\. Key Insights on President Trump’s New AI Executive Order and Policy & Regulatory Implications](https://www.squirepattonboggs.com/-/media/files/insights/publications/2025/02/key-insights-on-president-trumps-new-ai-executive-order-and-policy--regulatory-implications/president_trumps_new_ai_eo.pdf?rev=e146494e56c5479fbd1bc5af27ad671e&sc_lang=en&hash=65F06034A2720C1FC1C3DF9DD6BCB6AF)\n\n[160\\. The Impact of Artificial Intelligence Applications on Corporate Labor Productivity](https://www.ewadirect.com/journals/jaeps/article/view/21036/pdf)\n\n[161\\. President Biden’s Executive Order on AI: Harnessing the Expertise of Agencies to Lasso the Bullish Artificial Intelligence Industry](https://moritzlaw.osu.edu/sites/default/files/2023-12/BarcyBlog%201.pdf)\n\n[162\\. The Adoption of Artificial Intelligence in Firms: New Evidence for Policymaking](https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/05/the-adoption-of-artificial-intelligence-in-firms_8fab986b/f9ef33c3-en.pdf)\n\n[163\\. President Biden’s Executive Order on Artificial Intelligence — Initial Analysis of Private Sector Implications](https://www.lw.com/admin/upload/SiteAttachments/President-Bidens-Executive-Order-on-AI-Initial-Analysis-of-Private-Sector-Implications.pdf)\n\n[164\\. FORM 10-Q](https://ir.neumoratx.com/static-files/f2463cf9-52d8-4c61-8c72-44ee9766bd4e)\n\n[165\\. Biden's Big AI Executive Order: Implications for the Future of Tech](https://www.pluralsight.com/resources/blog/ai-and-data/biden-executive-order-artificial-intelligence)\n\n[166\\. Governor's Task Force on Workforce and Artificial Intelligence Advisory Action Plan](https://dwd.wisconsin.gov/ai-taskforce/pdf/ai-advisory-action-plan.pdf)\n\n[167\\. Boosting U.S. worker power and voice in the AI-enabled ...](https://equitablegrowth.org/boosting-u-s-worker-power-and-voice-in-the-ai-enabled-workplace/)\n\n[168\\. Biden’s AI Order and the Implications for Employers](https://www.huntonprivacyblog.com/2023/11/29/bidens-ai-order-and-the-implications-for-employers/)\n\n[169\\. Daron Acemoglu, P. Restrepo. “Robots and Jobs: Evidence from US Labor Markets.” Journal of Political Economy](https://doi.org/10.1086/705716)\n\n[170\\. C. Syverson. “What Determines Productivity?.” ERN: Productivity (Topic)](https://doi.org/10.1257/JEL.49.2.326)\n\n[171\\. D. Acemoglu, P. Restrepo. “Artificial Intelligence, Automation and Work.” Alfred P. Sloan Foundation Economic Research Paper Series](https://doi.org/10.2139/ssrn.3098384)\n\n[172\\. Philippe Aghion, Benjamin F. Jones et al. “Artificial Intelligence and Economic Growth.” Kauffman: Conferences & Seminars (Topic)](https://doi.org/10.3386/W23928)\n\n[173\\. Georg Graetz, Guy Michaels. “The Review of Economics and Statistics.”](https://www.semanticscholar.org/paper/f2d582c675ac69cc5da0b54d9b1fe53329f01820)\n\n[174\\. The Ultimate AI/ML Workflow Tool for Prototype to Production](https://www.xugj520.cn/en/archives/metaflow-ai-ml-workflow.html)\n\n[175\\. Generative AI and Security Operations Center Productivity: Evidence from Live Operations](https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/Generative-AI-and-Security-Operations-Center-Productivity-Evidence-from-Live-Operations_v2.5-FINAL.pdf)\n\n[176\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2024.” ArXiv](https://doi.org/10.48550/arXiv.2405.19522)\n\n[177\\. Erik Brynjolfsson, Danielle Li et al. “Generative AI at Work.” SSRN Electronic Journal](https://doi.org/10.3386/w31161)\n\n[178\\. 2024年AI实际应用报告](http://www.sccio.cn/uploads/20241205/2e8ef69f777a086fd1db2b6e6fa4e47f.pdf)\n\n[179\\. かけはし テクノロジーでつながる、よりよい未来への懸け橋 February 2025, 2 社会や働き方を支える デジタル技術の活用](https://www.microsoft.com/ja-jp/industry/blog/wp-content/uploads/sites/12/2025/02/kakehashi_2025_Feb_edition.pdf)\n\n[180\\. AI use cases for nonprofit leaders: Realize value with AI](https://www.microsoft.com/content/dam/microsoft/msc/documents/presentations/nonprofits/pdfs/AI-use-cases-for-nonprofit-leaders.pdf)\n\n[183\\. E. Fama, M. C. Jensen. “Separation of Ownership and Control.” The Journal of Law and Economics](https://doi.org/10.1086/467037)\n\n[184\\. VERSES AI INC. Management's Discussion and Analysis As of July 2, 2024](https://21624003.fs1.hubspotusercontent-na1.net/hubfs/21624003/Investor%20Material/2024-03-31%20Verses%20AI%20Inc.%20Management%20Discussion%20and%20Analysis.pdf)\n\n[185\\. APX: Strategic Transformation to Generative AI Drives Emerging Turnaround FY2024 Results Analysis](https://www.alphainsights.com.au/content/files/2025/04/APX_FY24_February2025.pdf)\n\n[186\\. W. Schulze, M. Lubatkin et al. “Agency Relationships in Family Firms: Theory and Evidence.” Organization Science](https://doi.org/10.1287/ORSC.12.2.99.10114)\n\n[187\\. AI for Everyone, Everywhere](https://files-scs.pstatic.net/2024/10/14/SZwHTMNVk0/%EC%98%A4%ED%94%88%EC%97%A3%EC%A7%80%ED%85%8C%ED%81%AC%EB%86%80%EB%A1%9C%EC%A7%80%2024%EB%85%8410%EC%9B%94%20IR%EC%9E%90%EB%A3%8C.pdf)\n\n[188\\. LSTM versus Transformers: A Practical Comparison of Deep Learning Models for Trading Financial Instruments](https://www.scitepress.org/Papers/2024/129811/129811.pdf)\n\n[189\\. Andrei Shleifer. “Do Demand Curves for Stocks Slope Down.” Journal of Finance](https://doi.org/10.1111/J.1540-6261.1986.TB04518.X)\n\n[190\\. SenseTime (20 HK) Strengthening Gen AI competitive edges](https://testtoo1.oss-cn-hangzhou.aliyuncs.com/eastmoney_pdf/AP202408281639506653.pdf)\n\n[191\\. EARNINGS INSIGHT](https://advantage.factset.com/hubfs/Website/Resources%20Section/Research%20Desk/Earnings%20Insight/EarningsInsight_052424.pdf)\n\n[192\\. Icons of Artificial Intelligence: The Top AI Stocks for 2025](https://www.gilderreport.com/wp-content/uploads/GTR-Icons-of-Artificial-Intelligence-The-Top-AI-Stocks-for-2025.pdf)\n\n[193\\. SECURITIES AND EXCHANGE COMMISSION FORM DEF 14A Definitive proxy statements](http://pdf.secdatabase.com/161/0001193125-25-044112.pdf)\n\n[194\\. M. Beneish, R. Whaley. “An Anatomy of the “S&P Game”: The Effects of Changing the Rules.” Journal of Finance](https://doi.org/10.1111/J.1540-6261.1996.TB05231.X)\n\n[195\\. Revenue and cost impact of AI in finance 2024](https://www.statista.com/statistics/1254724/revenue-impact-of-ai-financial-services/)\n\n[196\\. FY24 Annual Report: Asking More of AI](https://s23.q4cdn.com/574569502/files/doc_financials/2024/ar/salesforce-fy24-annual-report.pdf)\n\n[197\\. Earnings Insight](https://advantage.factset.com/hubfs/Website/Resources%20Section/Research%20Desk/Earnings%20Insight/EarningsInsight_072624.pdf)\n\n[198\\. The combined revenues of leading AI companies grew by ...](https://epoch.ai/data-insights/ai-companies-revenue)\n\n[199\\. Trends – Artificial Intelligence (AI)](https://www.mrbaogao.com/storage/attachments/2025/06/03/TJX0rgflGCJtvJM5o6ziHtrMrE6Yg9BpRiZSu2I0.pdf)\n\n[200\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2024.” ArXiv](https://doi.org/10.48550/arXiv.2405.19522)\n\n[203\\. Daron Acemoglu, P. Restrepo. “Robots and Jobs: Evidence from US Labor Markets.” Journal of Political Economy](https://doi.org/10.1086/705716)\n\n[204\\. D. Acemoglu, P. Restrepo. “The Race between Man and Machine: Implications of Technology for Growth, Factor Shares, and Employment.” American Economic Review](https://doi.org/10.1257/AER.20160696)\n\n[205\\. David Autor. “Why Are There Still So Many Jobs? The History and Future of Workplace.”](https://doi.org/10.1257/JEP.29.3.3)\n\n[206\\. Biden's Big AI Executive Order: Implications for the Future of Tech](https://www.pluralsight.com/resources/blog/ai-and-data/biden-executive-order-artificial-intelligence)\n\n[207\\. President Biden’s Executive Order on AI: Harnessing the Expertise of Agencies to Lasso the Bullish Artificial Intelligence Industry](https://moritzlaw.osu.edu/sites/default/files/2023-12/BarcyBlog%201.pdf)\n\n[208\\. Biden Signs AI Executive Order: Find Out the Meaning Here!](https://mandynews.com/biden-signs-ai-executive-order/)\n\n[209\\. ACCELERATING AI SKILLS: PREPARING THE WORKFORCE FOR JOBS OF THE FUTURE](https://assets.aboutamazon.com/84/4b/c039993d4111b54af1afcb6bfbfb/aws-accelerating-ai-skills-us-report.pdf)\n\n[210\\. D. Czarnitzki, Gastón P. Fernández et al. “Artificial Intelligence and Firm-Level Productivity.” SSRN Electronic Journal](https://doi.org/10.2139/ssrn.4049824)\n\n[211\\. What You Need to Know About Biden's AI Executive Order](https://blog.hubspot.com/ai/ai-executive-order)\n\n[212\\. OECD Employment Outlook 2023: Artificial Intelligence and the Labour Market](https://read.oecd.org/10.1787/079c6ebb-de?format=html,read)\n\n[213\\. Biden’s AI Order and the Implications for Employers](https://www.huntonprivacyblog.com/2023/11/29/bidens-ai-order-and-the-implications-for-employers/)\n\n[214\\. AI Action Plan](https://americansforprosperity.org/wp-content/uploads/2025/04/AI-Action-Plan-Comments-PC.docx)\n\n[215\\. Biden's AI Executive Order: What it says, and what it means for security teams](https://www.wiz.io/blog/bidens-ai-executive-order-what-it-means-for-security-teams)\n\n[216\\. SYNTHESIS, THEMES, COUNTRY CONTEXTS AND PROVOCATIONS](https://5025575.fs1.hubspotusercontent-na1.net/hubfs/5025575/Rhetta_Synthesis_Themes_Provocations%202024-09-20%2004_43_59.docx)\n\n[217\\. President Trump Revokes Biden Administration’s AI EO:...](https://www.wiley.law/alert-President-Trump-Revokes-Biden-Administrations-AI-EO-What-To-Know)\n\n[218\\. Foreshadowing Biden’s AI Executive Order](https://www.mintz.com/insights-center/viewpoints/2191/2023-10-20-foreshadowing-bidens-ai-executive-order-ai-washington)\n\n[219\\. Energy Sector Takeaways From Biden's AI Executive Order](https://www.foley.com/insights/publications/2023/12/energy-sector-biden-ai-executive-order/)\n\n[220\\. Emerging AI: The White House’s Executive Order and Its Impact on the Workplace](https://www.bassford.com/news/the-work-week-with-bassford-remele-emerging-ai-12-18-23)\n\n[221\\. Center for American Entrepreneurship Applauds President Biden’s Executive Order on Artificial Intelligence](https://startupsusa.org/press-releases/center-for-american-entrepreneurship-applauds-president-bidens-executive-order-on-artificial-intelligence/)\n\n[222\\. DeepSeek V3 API Call Does Not Stop Automatically in ...](https://github.com/langchain-ai/langgraph/issues/3097)"
  },
  {
    "id": 36,
    "question": "What fundamental architectural differences between Salesforce's agent approach and large language model wrappers like Co-Pilot ensure both security and actionable business value?",
    "response": "**Research Report**\n\n**To:** Interested Parties\n\n**From:** Expert Researcher\n\n**Date:** July 27, 2025\n\n**Subject:** Analysis of Fundamental Architectural Differences Between Salesforce AI Agents and LLM Wrappers for Enterprise Security and Business Value\n\n**Executive Summary**\n\nThis report provides a comprehensive analysis of the fundamental architectural differences between Salesforce's AI agent approach, primarily embodied by the Einstein 1 Platform and Agentforce, and the Large Language Model (LLM) wrapper or \"co-pilot\" model, exemplified by Microsoft's Co-Pilot ecosystem. The core of this investigation centers on how these distinct architectures deliver on two critical enterprise requirements: robust data security and actionable business value within CRM workflows.\n\nOur research, based on the provided data, reveals a key divergence in strategy. Salesforce has engineered a dedicated, multi-layered security and trust architecture—the **Einstein Trust Layer**—that acts as a secure gateway, proactively masking data and enforcing zero-retention policies _before_ any interaction with an LLM, whether internal or third-party \\[94\\]\\[138\\]. This architecture is deeply integrated into an agent-centric platform designed for autonomous task completion \\[76\\].\n\nConversely, Microsoft Co-Pilot's architecture emphasizes **stateless processing** and deep integration into its existing enterprise ecosystem \\[33\\]\\[202\\]. Its security model relies heavily on leveraging the robust, pre-existing access control frameworks of the underlying CRM (like Dynamics 365 or a connected Salesforce instance) and Microsoft Entra ID, ensuring that data is accessed in real-time and not copied or retained for model training \\[95\\]\\[170\\].\n\nIn terms of business value, both platforms report significant productivity gains. Salesforce cites a deep data advantage from its vast proprietary CRM data pool, leading to claimed improvements in sales efficiency and cycle time \\[65\\]. Microsoft highlights its ability to unify disparate applications (Outlook, Teams, CRM) and automate clerical tasks, resulting in documented increases in seller revenue and faster case resolution \\[163\\]\\[228\\].\n\nHowever, a critical finding is the current state of multi-turn task reliability. Independent benchmarks like CRMArena-Pro indicate that while single-step workflows are becoming tractable for advanced agents (over 83% success), performance in complex, multi-turn business scenarios plummets to approximately 35% \\[118\\]\\[226\\]. This suggests that realizing the full potential of autonomous agents remains a significant challenge for the entire industry, including both Salesforce and Microsoft.\n\n**1\\. Core Architectural Paradigms: AI Agent vs. LLM Wrapper**\n\nThe functional differences between Salesforce's agent-centric platform and Microsoft's Co-Pilot stem from a fundamental divergence in their architectural philosophy. One is built for autonomy, while the other is primarily designed for assistance.\n\n**1.1. The AI Agent Framework Architecture**\n\nA true AI agent framework, as distinct from a simple LLM wrapper, is architecturally defined by its capacity for autonomous operation. It is not merely a conversational interface but a system designed to reason, plan, and act. Key architectural components include:\n\n**Foundation Model Core:** An LLM serves as the central \"brain,\" providing reasoning, language comprehension, and multimodal interpretation capabilities \\[1\\]\\[15\\].\n\n**Planning and Reasoning Engine:** This is a critical differentiator. The engine allows the agent to decompose a high-level goal into a sequence of executable steps, often using techniques like chain-of-thought prompting to improve logic and planning \\[3\\]\\[5\\]\\[92\\]. This planning ability is what separates an agent from a more passive co-pilot that requires explicit, step-by-step instructions \\[92\\].\n\n**Tool Use and Action Module:** Agents are architected to interact with and control external systems and APIs. This module allows the agent to move beyond text generation to perform concrete actions like retrieving data from a database, updating a CRM record, or sending an email \\[4\\]\\[9\\]\\[10\\].\n\n**Memory Systems:** To maintain context across complex, multi-turn interactions, agents employ both short-term memory (for the current conversation) and long-term memory (to store knowledge and past interactions for future use) \\[1\\]\\[9\\]\\[10\\].\n\n**Perception Module:** This component enables the agent to receive and interpret observations from its environment, providing the necessary feedback to adjust its plan \\[19\\].\n\nSalesforce is explicitly pursuing this agent-centric model with **Agentforce**, which integrates these autonomous capabilities directly into its core CRM, Data Cloud, and various \"Builder\" tools, aiming to create an \"intelligent hub\" for business operations \\[76\\].\n\n**1.2. The LLM Wrapper / Co-Pilot Architecture**\n\nThe term \"LLM wrapper\" or \"co-pilot\" typically describes a less autonomous system. While technologically sophisticated, its architecture is centered on assisting a human user rather than replacing a workflow.\n\n**Human-in-the-Loop:** Co-Pilots are designed to augment human capabilities by providing suggestions, generating content, and automating parts of a process. The final decision and action often remain with the human user \\[27\\]\\[29\\]\\[86\\].\n\n**Prompt-Dependent Interaction:** Unlike agents that can self-plan, co-pilots generally rely on more direct, continuous, and explicit instructions or prompts from the user to perform tasks \\[88\\]\\[92\\].\n\n**Integrated Assistance:** Microsoft's Co-Pilot architecture excels at integrating with a wide array of applications (e.g., Microsoft 365, Teams) and external systems (e.g., Salesforce, ServiceNow) to provide unified assistance within a user's existing workflow \\[69\\]\\[163\\]. Its strength lies in this breadth of integration rather than the depth of autonomy for a single complex task.\n\nWhile Microsoft is also developing more autonomous agent capabilities, its flagship Co-Pilot product architecturally aligns more with the assistive, human-in-the-loop paradigm \\[76\\]\\[293\\].\n\n**2\\. Security and Data Governance Architectures**\n\nThe most significant architectural divergence between the two platforms lies in their approach to data security and governance, particularly when handling sensitive CRM data.\n\n**2.1. Salesforce's Proactive Security: The Einstein Trust Layer**\n\nSalesforce has architected a dedicated, multi-layered security framework, the **Einstein Trust Layer**, which sits between the user/CRM data and the LLM. This layer is not an afterthought but a core component of the Einstein 1 Platform, designed to build trust by default \\[171\\]\\[172\\]. Its protocol-level implementation includes several critical enforcement points:\n\n**Secure Data Retrieval and Grounding:** The architecture first ensures that any data retrieved to \"ground\" the LLM prompt is done so securely, respecting the user's existing permissions within the Salesforce platform \\[181\\]\\[261\\].\n\n**Pattern-Based Data Masking and Tokenization:** Before a prompt is sent to an LLM, the Trust Layer's **Data Masking Layer** intervenes. It uses pattern-matching and machine learning techniques to automatically identify and mask sensitive data types, such as PII (names, phone numbers, etc.) and PCI information \\[180\\]\\[185\\]\\[244\\]. This sensitive information is replaced with generic, reversible tokens \\[182\\]\\[195\\]. This means the LLM never sees the raw, sensitive customer data, yet can still process a structurally similar prompt to generate a relevant response \\[154\\]. Administrators can also configure which specific CRM fields should be subject to masking, providing granular control \\[150\\]\\[190\\].\n\n**Zero-Retention Policy:** A cornerstone of the Trust Layer's architecture is its strict zero-data retention policy. Prompts and responses are not stored by third-party LLM providers (like OpenAI) or used to train their models \\[138\\]\\[151\\]. The data is deleted as soon as the AI has finished processing it, drastically minimizing the risk of data leakage and aiding compliance with regulations like GDPR \\[138\\]\\[184\\].\n\n**Toxicity Detection and Auditing:** The architecture also includes a \"Toxicity Layer\" to scan AI-generated responses for harmful or inappropriate content before it reaches the user, and a comprehensive **Audit Trail Layer** that logs all interactions for accountability and compliance purposes \\[181\\]\\[191\\]\\[253\\].\n\nThis architecture demonstrates a \"trust by design\" philosophy, where security controls are programmatically enforced within a dedicated layer before data ever leaves the secure Salesforce environment.\n\n**2.2. Microsoft's Reactive Security: Stateless Processing and Ecosystem Integration**\n\nMicrosoft's security architecture for Co-Pilot is fundamentally different, relying on statelessness and the integrity of its broader enterprise ecosystem rather than a dedicated pre-processing trust layer like Salesforce's.\n\n**Stateless Architecture:** A key security feature of Microsoft 365 Co-Pilot is that it is designed to be stateless \\[33\\]. This means it does not retain information about the prompts submitted, the customer data used for grounding, or the responses provided \\[208\\]. Crucially, Microsoft explicitly states that customer data is not used to train the underlying AI models unless a customer provides explicit permission \\[95\\]\\[97\\].\n\n**Real-Time Integration, Not Data Replication:** The Co-Pilot architecture avoids security risks associated with data duplication. It does not copy CRM data to a separate system. Instead, it accesses CRM data (from Dynamics 365 or Salesforce) in real-time through secure integrations \\[202\\]\\[217\\]. This ensures that the CRM remains the single source of truth and that its existing data retention and compliance policies are respected \\[202\\].\n\n**Leveraging Existing Access Control:** Co-Pilot's access control is not governed by a new, separate mechanism. It is architecturally bound to the user's existing identity and permissions. When a user interacts with Co-Pilot, the system authenticates them via Microsoft Entra ID (Azure Active Directory) and all data access operations are performed in the context of that user's established permissions within the connected CRM system \\[170\\]\\[204\\]. This ensures that an AI agent cannot access any data that the user themselves is not authorized to see.\n\n**Standard Security Protocols:** The architecture is further secured by standard enterprise-grade protocols, including Transport Layer Security (TLS) for all data in transit and robust encryption for any data at rest within the Microsoft cloud ecosystem \\[97\\]\\[217\\].\n\nThis approach can be seen as \"trust through integration,\" where security is derived from the proven, mature access control and data handling policies of the surrounding enterprise environment.\n\n**3\\. Actionable Business Value and Performance**\n\nBoth companies position their AI offerings as drivers of significant business value, but the nature of that value and the evidence supporting it differ based on their architectural strengths.\n\n**3.1. Quantifiable Business Outcomes**\n\n**Salesforce:** Leaning on its architecture's deep integration with its own vast data stores (reportedly 200-300 PB of customer data), Salesforce claims its agent-centric approach provides highly relevant and accurate outputs that translate directly to sales performance \\[65\\]. Published claims include a **40% increase in sales efficiency** and a **30% reduction in sales cycle time** \\[118\\]. The value proposition is centered on leveraging an organization's own data to create hyper-contextualized and autonomous AI agents.\n\n**Microsoft:** Microsoft's value proposition is centered on cross-application productivity and task automation. By integrating Co-Pilot across its suite, it streamlines clerical work like writing emails and summarizing meetings \\[121\\]. Internal case studies and performance data report significant gains: a **15% improvement in service rep productivity**, a **20% reduction in average handling time**, and in some sales teams, **9.4% higher revenue per seller** and **20% more deals closed** \\[166\\]\\[228\\]. The value here is derived from reducing administrative overhead and unifying fragmented workflows.\n\n**3.2. The Reality of Multi-Turn Task Completion**\n\nDespite vendor claims, independent analysis suggests that the true litmus test for AI agents—reliably completing complex, multi-turn business workflows—remains a significant hurdle. No direct, independent, third-party performance benchmark comparing Salesforce AI agents and Microsoft Co-Pilot was found in the provided research.\n\nHowever, general benchmarks like **CRMArena-Pro** provide a sober assessment of the current state-of-the-art for LLM agents in realistic business scenarios. These benchmarks show that while agents are becoming proficient at single-turn tasks (with success rates over 83% for workflow execution), performance degrades dramatically in multi-turn settings, **dropping to approximately 35% success** \\[226\\]\\[289\\]. This indicates that maintaining context, adapting to feedback, and following complex rules over multiple steps is an unsolved problem. This limitation would apply to both Salesforce's and Microsoft's current offerings, suggesting that their advertised business value is likely concentrated in more constrained, single-step automation and content generation tasks rather than fully autonomous end-to-end process execution.\n\n**4\\. Conclusion**\n\nThe architectural strategies of Salesforce and Microsoft in the enterprise AI space represent two distinct paths toward delivering secure and valuable AI.\n\n1.  **Architectural Divergence:** Salesforce has invested in a **proactive, agent-centric architecture** featuring a dedicated **Einstein Trust Layer**. This design prioritizes data security through pre-emptive masking and zero-retention policies, creating a fortified sandbox for AI operations. Microsoft has opted for a **reactive, ecosystem-integrated architecture** that leverages **stateless processing** and the existing security fabric of its enterprise cloud. This design prioritizes seamless integration and relies on the strength of established access control models.\n2.  **Security Implications:** Salesforce’s approach offers a more explicit and auditable security guarantee _within_ the AI platform itself, which may appeal to highly regulated industries. Microsoft’s approach offers strong security guarantees by tightly binding the AI’s capabilities to the user's existing permissions, ensuring the principle of least privilege is maintained by default.\n3.  **Business Value and Future Outlook:** Both platforms demonstrably provide value by automating tasks and improving productivity. However, the promise of fully autonomous agents capable of navigating complex, multi-turn CRM workflows is still on the horizon. The low success rates reported in independent benchmarks for such tasks indicate that this is a frontier of active research and development. The ultimate winner in this space will not only be the one with the most impressive headline features but the one whose architecture can first solve the critical challenge of maintaining reliability, security, and context across long-running, dynamic business processes.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. A Survey of AI Agent Protocols](https://media.licdn.com/dms/document/media/v2/D4E1FAQGEX5OgViSaHg/feedshare-document-pdf-analyzed/B4EZaXRu.lHYAY-/0/1746294740993?e=1747267200&v=beta&t=kZ3AjxGVZLywF378HjkBea5Tae_okm6C3LCe6jh_jWA)\n\n[2\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[3\\. AI Agent，重塑人机交互方式](http://irunvc.com/home/6/b/rbgcyt/resource/2024/03/21/65fbe99c93bfd.pdf)\n\n[4\\. 基于大语言模型与智能体的智能座舱研究与实践](http://ictp.caict.ac.cn/CN/article/downloadArticleFile.do?attachType=PDF&id=1338)\n\n[5\\. Shunyu Yao, Jeffrey Zhao et al. “ReAct: Synergizing Reasoning and Acting in Language Models.” ArXiv](https://arxiv.org/abs/2210.03629)\n\n[6\\. J. Park, Joseph C. O'Brien et al. “Generative Agents: Interactive Simulacra of Human Behavior.” Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology](https://doi.org/10.1145/3586183.3606763)\n\n[7\\. LLM-based Agent在B端商业化的技术探索与实践](https://s3.51cto.com/oss/202406/26/56ec6a8391eb8bf51c2fc45c110e95fb.pdf)\n\n[8\\. Noah Shinn, Federico Cassano et al. “Reflexion: language agents with verbal reinforcement learning.” Neural Information Processing Systems](https://arxiv.org/abs/2303.11366)\n\n[9\\. AI麦可打造外贸企业专属数智员工，打开全新收费天花板](https://pdf.dfcfw.com/pdf/H3_AP202311071609233530_1.pdf?1699364051000.pdf)\n\n[10\\. 全球大模型将往何处去？](http://www.ecconsortium.org/Uploads/file/20241010/1728556243241730.pdf)\n\n[11\\. Lei Wang, Chengbang Ma et al. “A Survey on Large Language Model based Autonomous Agents.” ArXiv](https://doi.org/10.1007/s11704-024-40231-1)\n\n[12\\. Vineeth Sai Narajala, Om Narayan. “Securing Agentic AI: A Comprehensive Threat Model and Mitigation Framework for Generative AI Agents.”](https://arxiv.org/abs/2504.19956)\n\n[13\\. Robert E. Wray, James R. Kirk et al. “Architectural Precedents for General Agents using Large Language Models.”](https://arxiv.org/abs/2505.07087)\n\n[14\\. Shaina Raza, Ranjan Sapkota et al. “TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems.”](https://arxiv.org/abs/2506.04133)\n\n[15\\. AI Agent：基于大模型的自主智能体，在探索AGI的道路上前进](https://bigdata-s3.wmcloud.com/researchreport/2023-08/6a60dd73e0e1ba35100343d73ecfedd8.pdf)\n\n[16\\. Amine B. Hassouna, Hana Chaari et al. “LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents.”](https://arxiv.org/abs/2409.11393)\n\n[17\\. 传媒行业 2024 年度策略](https://pdf.dfcfw.com/pdf/H3_AP202401181617679567_1.pdf?1705579952000.pdf)\n\n[18\\. AI+软件研发数字峰会](https://aidd.vip/resources/upload/a7844a45d8ab55e/file/%E7%8E%8B%E6%98%8A%E5%A5%8B-ALM%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E8%8C%83%E5%BC%8F-%E5%B7%B2%E4%BF%AE%E6%94%B9.pdf)\n\n[19\\. Yapeng Mi, Zhi Gao et al. “Building LLM Agents by Incorporating Insights from Computer Systems.”](https://arxiv.org/abs/2504.04485)\n\n[21\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[22\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[23\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[24\\. A Survey of AI Agent Protocols](https://media.licdn.com/dms/document/media/v2/D4E1FAQGEX5OgViSaHg/feedshare-document-pdf-analyzed/B4EZaXRu.lHYAY-/0/1746294740993?e=1747267200&v=beta&t=kZ3AjxGVZLywF378HjkBea5Tae_okm6C3LCe6jh_jWA)\n\n[25\\. Shunyu Yao, Jeffrey Zhao et al. “ReAct: Synergizing Reasoning and Acting in Language Models.” ArXiv](https://arxiv.org/abs/2210.03629)\n\n[26\\. Nicholas Carlini, Florian Tramèr et al. “Extracting Training Data from Large Language Models.” USENIX Security Symposium](https://arxiv.org/abs/2012.07805)\n\n[27\\. o3 deep research: LLM 驱动的Agent 综述- 李维的博文](https://blog.sciencenet.cn/blog-362400-1476565.html)\n\n[28\\. Yifeng He, Ethan Wang et al. “Security of AI Agents.” ArXiv](https://doi.org/10.48550/arXiv.2406.08689)\n\n[29\\. AIPC赛道风起，产业链创新云涌——AI产业系列深度报告（二）](https://pdf.dfcfw.com/pdf/H3_AP202411151640922196_1.pdf?1731705235000.pdf)\n\n[30\\. o3 deep research: LLM 驱动的Agent 综述](https://cloud.tencent.com.cn/developer/article/2503168)\n\n[31\\. o3 deep research: LLM 驱动的 Agent 综述-腾讯云开发者社...](https://cloud.tencent.com/developer/article/2503168?policyId=1004)\n\n[32\\. LLM-based Agent在B端商业化的技术探索与实践](https://s3.51cto.com/oss/202406/26/56ec6a8391eb8bf51c2fc45c110e95fb.pdf)\n\n[33\\. GDPR & Generative AI: A Guide for the Public Sector](https://wwps.microsoft.com/wp-content/uploads/2024/04/GDPR-and-Generative-AI-A-Guide-for-the-Public-Sector-FINAL.pdf)\n\n[34\\. Hao Li, Xiaogeng Liu et al. “DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents.”](https://arxiv.org/abs/2506.12104)\n\n[36\\. Shunyu Yao, Jeffrey Zhao et al. “ReAct: Synergizing Reasoning and Acting in Language Models.” ArXiv](https://arxiv.org/abs/2210.03629)\n\n[37\\. Shishir G. Patil, Tianjun Zhang et al. “Gorilla: Large Language Model Connected with Massive APIs.” ArXiv](https://doi.org/10.48550/arXiv.2305.15334)\n\n[38\\. Lei Wang, Chengbang Ma et al. “A Survey on Large Language Model based Autonomous Agents.” ArXiv](https://doi.org/10.1007/s11704-024-40231-1)\n\n[39\\. 《Building effective agents》——agent笔记【未完待续】 - 日记](https://m.douban.com/note/869521253/?from=author)\n\n[40\\. Mandar Kulkarni. “Agent-S: LLM Agentic workflow to automate Standard Operating Procedures.”](https://arxiv.org/abs/2503.15520)\n\n[41\\. LLM Agent CRM AI 代理：选择微软、Salesforce 还是 Anthropic？](https://www.bilibili.com/video/av113442350571384?t=497)\n\n[42\\. Kung-Hsiang Huang, Akshara Prabhakar et al. “CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions.”](https://arxiv.org/abs/2505.18878)\n\n[43\\. Salesforce CEO Benioff 谈企业级AI：Agent 智能体重塑商业未来- 大 ...](https://masterdns.uweb.net.cn/news/zhinengyingxiao/2024110543917.html)\n\n[44\\. Salesforce（CRM）更新报告](https://pdf.dfcfw.com/pdf/H3_AP202412161641311797_1.pdf?1734363033000.pdf)\n\n[46\\. AI 工具多场景覆盖，经营利润持续释放](https://pdf.dfcfw.com/pdf/H3_AP202405131632948205_1.pdf?1724427652000.pdf)\n\n[47\\. CRM 领导者 Salesforce，大力探索生成式 AI 可能性——AIGC 行业跟踪报告（二十二）](https://bigdata-s3.wmcloud.com/researchreport/2023-09/2deee7b2521b7973779b40d5857eeef6.pdf)\n\n[48\\. 他山之石系列报告（一）: Salesforce的大模型ToB应用分析](https://pdf.dfcfw.com/pdf/H301_AP202307051592042069_1.pdf)\n\n[49\\. 海外大厂主导 AI 商业化浪潮，行业应用已至爆发前夕——人工智能行业深度报告](https://pdf.dfcfw.com/pdf/H3_AP202308301596824513_1.pdf)\n\n[50\\. SaaS 鼻祖 Salesforce 的千亿帝国——“云”时代系列研究之一](http://stock.tianyancha.com/ResearchReport/eastmoney/4f6bebc8b4488a336395b0b6e6141dbd.pdf)\n\n[51\\. Carlos Rios-Campos, Sandra Marcela Zapata Vega et al. “Artificial Intelligence and Business.” South Florida Journal of Development](https://doi.org/10.46932/sfjdv4n9-015)\n\n[52\\. 策略深度报告：AI应用的普罗米修斯时刻：科技闭环与产业跟踪](https://file.iyanbao.com/pdf/3d59b-3a17e7fa-35f3-43b0-8b59-22852698233a.pdf)\n\n[53\\. Venkat sumanth Guduru. “Exploring the Future of Salesforce with Einstein GPT.” INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT](https://doi.org/10.55041/ijsrem36940)\n\n[54\\. Salesforce（CRM）更新报告](https://pdf.dfcfw.com/pdf/H3_AP202412161641311797_1.pdf?1734363033000.pdf)\n\n[55\\. Salesforce Einstein | BangDan123目录](http://bangdan123.com/ai/salesforce_einstein)\n\n[56\\. 2023年6月 AI前沿 Salesforce Einstein GPT](https://verticalmind.oss-accelerate.aliyuncs.com/new/article/pdf/230611_%E5%BC%98%E5%88%99%E7%A7%91%E6%8A%80%E3%80%90AI%E5%89%8D%E6%B2%BF%E3%80%91Salesforce%20Einstein%20GPT.pdf)\n\n[57\\. salesforce:AI在客户关系管理中的创新应用。\\_月光AI博客](https://blog.moontak.com/id/17074/)\n\n[58\\. Salesforce's Generative AI-Powered EinsteinGPT](https://www.bilibili.com/video/av908368090)\n\n[59\\. 2019年中国CRM软件行业概览](http://qccdata.qichacha.com/ReportData/PDF/52dc65d17ae10204951a64c195865771.pdf)\n\n[60\\. Salesforce Einstein功能介绍，了解AI+CRM](https://zhuanlan.zhihu.com/p/85789861)\n\n[61\\. Кирило Калюта. “UTILIZING MACHINE LEARNING FOR DATA ANALYSIS IN SALESFORCE.” Наука і техніка сьогодні](https://doi.org/10.52058/2786-6025-2023-10%2824%29-371-383)\n\n[62\\. Salesforce Einstein Copilot - 融合企业数据的客户关系管理 ...](https://www.dongaigc.com/p/tools/salesforce-einstein-copilot)\n\n[63\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[64\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[65\\. ...会议上直接对比/现怼了 微软 的Copilot“因为 Agentfor...](https://xueqiu.com/1253294411/315537578)\n\n[66\\. AI 在客户体验领域的竞争：Salesforce Agentforce 与Microsoft ...](https://docs.feishu.cn/article/wiki/YimjwtAeBi2SeIkwBTBcppoInBh)\n\n[67\\. Kung-Hsiang Huang, Akshara Prabhakar et al. “CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions.”](https://arxiv.org/abs/2505.18878)\n\n[68\\. Microsoft Sales Copilot Architecture Overview](https://download.microsoft.com/download/c/d/a/cda6b140-a325-4e19-af3c-31eb2ec22320/Microsoft%20Sales%20Copilot%20-%20Architecture%20Overview.pdf)\n\n[69\\. Getting Started with AI: A Microsoft Copilot Guide for Service Leaders](https://adoption.microsoft.com/files/copilot-for-service/Microsoft-Copilot-for-Service_Get-started-with-AI.pdf)\n\n[70\\. Microsoft is named a Leader in the 2024 Gartner® Magic ...](https://www.microsoft.com/en-us/dynamics-365/blog/business-leader/2024/12/17/microsoft-is-named-a-leader-in-2024-gartner-magic-quadrant-for-crm-customer-engagement-center/)\n\n[71\\. Salesforce（CRM）更新报告](https://pdf.dfcfw.com/pdf/H3_AP202412161641311797_1.pdf?1734363033000.pdf)\n\n[72\\. AI应用”变天“：Copilot已死，Agent当道？](https://wallstreetcn.com/articles/3728670)\n\n[73\\. R. Sandhu, E. Coyne et al. “Role-Based Access Control Models.” Computer](https://doi.org/10.1109/2.485845)\n\n[74\\. Barton A. Weitz. “Effectiveness in sales interactions: A contingency framework..” Journal of Marketing](https://doi.org/10.2307/1251723)\n\n[75\\. AI Agent元年，企业服务有望最先落地【浙商计算机】](https://www.fupanwang.com/kplart_info/16015.html)\n\n[76\\. AI Agent 与端侧新入口共筑 AI 应用未来](https://pdf.dfcfw.com/pdf/H3_AP202412091641234275_1.pdf?1733780203000.pdf)\n\n[77\\. Douglas N. Behrman, W. D. Perreault. “Measuring the performance of industrial salespersons.” Journal of Business Research](https://doi.org/10.1016/0148-2963%2882%2990039-X)\n\n[78\\. Adrian B. Ryans, C. Weinberg. “Territory Sales Response.” Journal of Marketing Research](https://doi.org/10.1177/002224377901600402)\n\n[79\\. 以 IP 为桨，扬 AI 之帆——2025 年传媒行业投资策略报告](https://pdf.dfcfw.com/pdf/H3_AP202412271641451400_1.pdf?1735319200000.pdf)\n\n[80\\. 2024中国AI Agent行业研究报告：大模型时代的“APP”，探索新一代人机交互及协作范式](https://fuyuzhe-oss.oss-cn-hongkong.aliyuncs.com/remotejob/img/d4f18d3e6851af9445db1ea3176e844f6f94c462.pdf)\n\n[81\\. 深度解读大模型最火的智能体(Agent)](https://53ai.com/news/LargeLanguageModel/2024060213965.html)\n\n[82\\. 计算机行业深度研究报告- Agents，AI商业化落地第一站](https://www.sdyanbao.com/detail/854169)\n\n[83\\. 计算机行业研究 海外 VC AI 投资复盘：通用场景向 Agent 发展，垂类场景更贴近业务核心](https://bigdata-s3.wmcloud.com/researchreport/2023-12/91c5489143a77ad9edcbbc20058cd96b.pdf)\n\n[84\\. 盘点四家企业软件巨头的Gen AI应用进程- 大模型知识库](https://53ai.com/news/zhishiguanli/2024061016832.html)\n\n[85\\. 太平洋计算机2025年投资策略：AI应用和自主可控有望持续演绎](https://pdf.dfcfw.com/pdf/H301_AP202501211642398525_1.pdf)\n\n[86\\. 与知识库对话- agent和copilot的区别](https://www.waytoagi.com/zh/question/88172)\n\n[87\\. Alan J. Dubinsky, T. E. Barry. “A survey of sales management practices.” Industrial Marketing Management](https://doi.org/10.1016/0019-8501%2882%2990007-4)\n\n[88\\. 跟硅谷的核心AI公司聊完后，得到了这60 条关键洞察- 大模型 ...](https://www.53ai.com/news/LargeLanguageModel/2025012481453.html)\n\n[89\\. AIPC赛道风起，产业链创新云涌——AI产业系列深度报告（二）](https://pdf.dfcfw.com/pdf/H3_AP202411151640922196_1.pdf?1731705235000.pdf)\n\n[90\\. Salesforce CEO Benioff 谈企业级AI：Agent 智能体重塑商业 ...](https://www.813zy.com/news/zhinengyingxiao/2024110543917.html)\n\n[91\\. Microsoft Agent Builder](https://techcommunity.microsoft.com/t5/s/gxcuf89792/attachments/gxcuf89792/HealthcareAndLifeSciencesBlog/2432/1/AgentBuilder.pdf)\n\n[92\\. 2023·爱分析 大模型厂商全景报告](https://runwise.oss-accelerate.aliyuncs.com/sites/15/2024/03/2023%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%82%E5%95%86%E5%85%A8%E6%99%AF%E6%8A%A5%E5%91%8A_%E7%88%B1%E5%88%86%E6%9E%90_2024%E5%B9%B4.pdf)\n\n[93\\. Salesforce Einstein GPT vs Dynamics 365 AI: A Comparison](https://www.techforceservices.com/blog/salesforce-einstein-gpt-microsoft-dynamics365-ai/)\n\n[94\\. Salesforce推出Einstein Copilot：CRM领域的可信对话式AI助手](https://www.fexle.com/blogs/salesforce-brings-einstein-copilot-a-trusted-conversational-ai-assistant-for-crm/)\n\n[95\\. Salesforce Einstein vs. Microsoft Dynamics 365: Which Copilot Is for ...](https://www.salesforceben.com/salesforce-einstein-vs-microsoft-dynamics-365-which-copilot-is-for-you/)\n\n[96\\. Vantage Point - Microsoft Copilot vs Salesforce Copilot: Battle of the AI Business Assistants!](https://blog.vantagepoint.io/microsoft-copilot-vs-salesforce-copilot-the-battle-of-the-ai-business-assistants/)\n\n[97\\. Microsoft Sales Copilot Architecture Overview](https://download.microsoft.com/download/c/d/a/cda6b140-a325-4e19-af3c-31eb2ec22320/Microsoft%20Sales%20Copilot%20-%20Architecture%20Overview.pdf)\n\n[98\\. Salesforce readies AI assistant for CRM | InfoWorld](https://www.infoworld.com/article/2334877/salesforce-readies-ai-assistant-for-crm.html)\n\n[99\\. Salesforce数据云推动下一代AI驱动的企业应用](https://www.salesforce.com/uk/news/stories/data-cloud-ai-future/?bc=WA)\n\n[100\\. How Salesforce's Einstein Transforms Sales](https://www.eweek.com/artificial-intelligence/how-salesforce-drives-business-through-ai/)\n\n[101\\. Six Strategies to Prepare for Trusted Generative AI](https://www.salesforce.com/content/dam/web/en_au/www/documents/research/23191-01_sf_generativeai_midebook_anz_final.pdf)\n\n[102\\. Salesforce bringt Einstein 1 Studio auf den Markt - Sa...](https://www.salesforce.com/de/company/news-press/press-releases/2024/03/060324/)\n\n[103\\. As Salesforce unveiled Einstein Copilot, is it better than Microsoft's ...](https://mspoweruser.com/as-salesforce-unveiled-einstein-copilot-is-it-better-than-microsofts-dynamics-365-copilot/)\n\n[104\\. Microsoft Dynamics vs Salesforce Comparison Guide 2024](https://www.korcomptenz.com/assets/whitepaper/Microsoft-Dynamics-vs-Salesforce-CRM-Comparison-Guide-2024.pdf)\n\n[105\\. Elevate Your CRM App with Salesforce Einstein 1 Studio](https://crmtechzone.com/crm-news/2024/03/07/elevate-your-crm-app-with-salesforce-einstein-1-studio/)\n\n[106\\. AI 工具多场景覆盖，经营利润持续释放](https://pdf.dfcfw.com/pdf/H3_AP202405131632948205_1.pdf?1724427652000.pdf)\n\n[107\\. Einstein Copilot: el asistente de IA del mundo empresarial](https://www.portafolio.co/contenido-patrocinado/einstein-copilot-el-asistente-de-ia-del-mundo-empresarial-610995)\n\n[108\\. Einstein Copilot In-Depth: What It Is, How It Works, and What It Can Do](https://www.salesforce.com/au/news/stories/about-einstein-copilot/?bc=WA)\n\n[109\\. Exam Questions Salesforce-AI-Specialist](https://www.dumps-files.com/downloadfile.html?id=6508)\n\n[110\\. D. Kristol, L. Montulli. “HTTP State Management Mechanism.” RFC](https://doi.org/10.17487/RFC2109)\n\n[111\\. Salesforce Launches Einstein 1 Studio: Low-Code AI Tools ...](https://www.salesforce.com/au/news/press-releases/2024/03/06/einstein-1-studio-news/)\n\n[112\\. Generative AI in the Enterprise Software Sector: A Look At Adoption in 2024](https://www.alpha-sense.com/wp-content/uploads/2024/02/Generative-AI-in-the-Enterprise-Software-Sector_-A-Look-At-Adoption-in-2024-DoR.pdf)\n\n[113\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[114\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[115\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[116\\. Shunyu Yao, Jeffrey Zhao et al. “ReAct: Synergizing Reasoning and Acting in Language Models.” ArXiv](https://arxiv.org/abs/2210.03629)\n\n[117\\. Michael Ahn, Anthony Brohan et al. “Do As I Can, Not As I Say: Grounding Language in Robotic Affordances.” Conference on Robot Learning](https://arxiv.org/abs/2204.01691)\n\n[118\\. Kung-Hsiang Huang, Akshara Prabhakar et al. “CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions.”](https://arxiv.org/abs/2505.18878)\n\n[119\\. Sales scenario: Quicker customer response and CRM updates (Co...](https://enablement.microsoft.com/en-us/scenario-library/sales/quicker-customer-response-and-crm-updates/)\n\n[120\\. Tablou de bord robot | Microsoft Learn](https://learn.microsoft.com/ro-ro/dynamics365/customer-service/use/oc-bot-dashboard)\n\n[121\\. Copilot for Dynamics 365](https://learn.microsoft.com/en-us/microsoft-cloud/dev/copilot/copilot-for-dynamics365)\n\n[122\\. Microsoft Sales Copilot Architecture Overview](https://download.microsoft.com/download/c/d/a/cda6b140-a325-4e19-af3c-31eb2ec22320/Microsoft%20Sales%20Copilot%20-%20Architecture%20Overview.pdf)\n\n[123\\. Kung-Hsiang Huang, Akshara Prabhakar et al. “CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments.”](https://arxiv.org/abs/2411.02305)\n\n[124\\. Dynamics 365 AI | Microsoft AI](https://www.microsoft.com/zh-cn/dynamics-365/solutions/ai?msockid=0017b856927467c0240eacef9359664b)\n\n[125\\. R. Sandhu, E. Coyne et al. “Role-Based Access Control Models.” Computer](https://doi.org/10.1109/2.485845)\n\n[126\\. W. Ouchi. “A Conceptual Framework for the Design of Organizational Control Mechanisms.” Management Science](https://doi.org/10.1287/MNSC.25.9.833)\n\n[127\\. K. Eisenhardt. “Control: Organizational and Economic Approaches.” Management Science](https://doi.org/10.1287/MNSC.31.2.134)\n\n[128\\. R. Saxe, Barton A. Weitz. “The SOCO Scale: A measure of the customer orientation of salespeople..” Journal of Marketing Research](https://doi.org/10.2307/3151568)\n\n[129\\. R. Sandhu, P. Samarati. “Access control: principle and practice.” IEEE Communications Magazine](https://doi.org/10.1109/35.312842)\n\n[130\\. Microsoft Sales Copilot Architecture Overview](https://download.microsoft.com/download/c/d/a/cda6b140-a325-4e19-af3c-31eb2ec22320/Microsoft%20Sales%20Copilot%20-%20Architecture%20Overview.pdf)\n\n[131\\. AI 工具多场景覆盖，经营利润持续释放](https://pdf.dfcfw.com/pdf/H3_AP202405131632948205_1.pdf?1724427652000.pdf)\n\n[132\\. 微软敲定 Copilot 落地时间，AI 商业化进程持续加速——人工智能行业周报](https://pdf.dfcfw.com/pdf/H3_AP202309251599762506_1.pdf?1695654167000.pdf)\n\n[133\\. Einstein Copilot - Salesforce推出的CRM系统AI对话助手](https://ai-bot.cn/sites/8941.html)\n\n[134\\. Salesforce Einstein Copilot - 融合企业数据的客户关系管理 ...](https://www.dongaigc.com/p/tools/salesforce-einstein-copilot)\n\n[135\\. 海外 B 端 AI 应用商业化进展梳理：AI 应用加速落地](https://pdf.dfcfw.com/pdf/H3_AP202310101601129914_1.pdf)\n\n[136\\. Salesforce宣布公开测试AI助手Einstein Copilot - AIGC](https://www.skycaiji.com/aigc/ai10859.html)\n\n[137\\. Six Strategies to Prepare for Trusted Generative AI](https://www.salesforce.com/content/dam/web/en_au/www/documents/research/23191-01_sf_generativeai_midebook_anz_final.pdf)\n\n[138\\. Unveiling Salesforce's Einstein Trust Layer](https://inclusioncloud.com/insights/blog/unveiling-salesforce-einstein-trust-layer/)\n\n[139\\. Einstein Trust Layer: Secure AI Architecture](https://www.salesforce.com/ap/artificial-intelligence/trusted-ai/)\n\n[140\\. Einstein Trust Layer: Secure AI Architecture - Salesfo...](https://www.salesforce.com/mx/artificial-intelligence/trusted-ai/?d=pb)\n\n[141\\. AI in Tableau and the Einstein Trust Layer - Tableau](https://help.tableau.com/current/tableau/en-gb/tableau_gai_einstein_trust_layer.htm)\n\n[142\\. Salesforce数据云推动下一代AI驱动的企业应用](https://www.salesforce.com/uk/news/stories/data-cloud-ai-future/?bc=WA)\n\n[143\\. How Salesforce's Einstein Transforms Sales](https://www.eweek.com/artificial-intelligence/how-salesforce-drives-business-through-ai/)\n\n[144\\. 5 Things You Need to Know About Salesforce Einstein 1 in 2025](https://www.nexelero.com/post/5-things-you-need-to-know-about-salesforce-einstein-1-in-2025)\n\n[145\\. Einstein GPT - Exploring Salesforce’s Leap into Generative AI](https://blogs.emorphis.com/einstein-gpt-exploring-salesforce-leap-into-generative-ai/)\n\n[146\\. Salesforce推出Einstein Copilot：CRM领域的可信对话式AI助手](https://www.fexle.com/blogs/salesforce-brings-einstein-copilot-a-trusted-conversational-ai-assistant-for-crm/)\n\n[147\\. Einstein Trust Layer: Secure AI Architecture | Salesfo...](https://www.salesforce.com/eu/products/secure-ai/)\n\n[148\\. Salesforce Einstein Trust Layer: Enhancing AI Trust](https://thesalesforcefirst.com/salesforce-einstein-trust-layer/)\n\n[149\\. Elevate Your CRM App with Salesforce Einstein 1 Studio](https://crmtechzone.com/crm-news/2024/03/07/elevate-your-crm-app-with-salesforce-einstein-1-studio/)\n\n[150\\. Salesforce Einstein vs. Microsoft Dynamics 365: Which Copilot Is for ...](https://www.salesforceben.com/salesforce-einstein-vs-microsoft-dynamics-365-which-copilot-is-for-you/)\n\n[151\\. Vantage Point - Understanding Generative AI: GPT-3 and Data Security with Salesforce Einstein GPT Trust Layer](https://blog.vantagepoint.io/understanding-generative-ai-what-is-gpt-and-whos-the-sheriff/)\n\n[152\\. Einstein GPT Trust Layer | ETG Digital](https://www.etg.digital/etg-blogs/benefits-of-einstein-gpt-trust-layer/)\n\n[153\\. Why Data, Trust, and Contextual UX Are the Real AI Gol...](https://www.salesforce.com/news/stories/data-for-ai/)\n\n[154\\. Explore Your Data with Tableau Agent - Tableau](https://help.tableau.com/current/online/en-us/web_author_einstein.htm)\n\n[155\\. Einstein Copilot: el asistente de IA del mundo empresarial](https://www.portafolio.co/contenido-patrocinado/einstein-copilot-el-asistente-de-ia-del-mundo-empresarial-610995)\n\n[156\\. Salesforce bringt Einstein 1 Studio auf den Markt - Sa...](https://www.salesforce.com/de/company/news-press/press-releases/2024/03/060324/)\n\n[157\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[158\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[159\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[160\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[161\\. Kung-Hsiang Huang, Akshara Prabhakar et al. “CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions.”](https://arxiv.org/abs/2505.18878)\n\n[162\\. Shishir G. Patil, Tianjun Zhang et al. “Gorilla: Large Language Model Connected with Massive APIs.” ArXiv](https://doi.org/10.48550/arXiv.2305.15334)\n\n[163\\. Microsoft Copilot for Sales - Application experiences ...](https://learn.microsoft.com/zh-tw/dynamics365/release-plan/2024wave2/sales/microsoft-copilot-sales/cross-app-experiences)\n\n[164\\. Getting Started with AI: A Microsoft Copilot Guide for Service Leaders](https://adoption.microsoft.com/files/copilot-for-service/Microsoft-Copilot-for-Service_Get-started-with-AI.pdf)\n\n[165\\. AI 在客户体验领域的竞争：Salesforce Agentforce 与Microsoft ...](https://docs.feishu.cn/article/wiki/YimjwtAeBi2SeIkwBTBcppoInBh)\n\n[166\\. Microsoft is named a Leader in the 2024 Gartner® Magic ...](https://www.microsoft.com/en-us/dynamics-365/blog/business-leader/2024/12/17/microsoft-is-named-a-leader-in-2024-gartner-magic-quadrant-for-crm-customer-engagement-center/)\n\n[167\\. Damian Ciechan. “Comparative analysis of frameworks and automation tools in terms of functionality and performance on the Salesforce CRM Platform.” Journal of Computer Sciences Institute](https://doi.org/10.35784/jcsi.3560)\n\n[168\\. Dynamics 365 Sales - Copilot | Microsoft Learn](https://learn.microsoft.com/ja-jp/dynamics365/release-plan/2023wave2/sales/dynamics365-sales/copilot)\n\n[169\\. Microsoft Sales Copilot Architecture Overview](https://download.microsoft.com/download/c/d/a/cda6b140-a325-4e19-af3c-31eb2ec22320/Microsoft%20Sales%20Copilot%20-%20Architecture%20Overview.pdf)\n\n[170\\. Microsoft Sales Copilot Architecture Overview](https://download.microsoft.com/download/c/d/a/cda6b140-a325-4e19-af3c-31eb2ec22320/Microsoft%20Sales%20Copilot%20-%20Architecture%20Overview.pdf)\n\n[171\\. AI 工具多场景覆盖，经营利润持续释放](https://pdf.dfcfw.com/pdf/H3_AP202405131632948205_1.pdf?1724427652000.pdf)\n\n[172\\. Einstein Copilot - Salesforce推出的CRM系统AI对话助手](https://celou.haoshang123.com/show/einsteincopilot.html)\n\n[173\\. 微软“AI+操作系统”初见雏形，生态壁垒是AIGC核心竞争力——AIGC系列跟踪报告（二十七）](https://bigdata-s3.wmcloud.com/researchreport/2023-09/44f6c758d7d360d7b9b8490b3aefc697.pdf)\n\n[174\\. Microsoft Bing Search - Ratings and Reviews](https://apps.apple.com/us/app/bing-chat-with-ai-gpt-4/id345323231?l=zh-Hans-CN&see-all=reviews)\n\n[175\\. Venkat sumanth Guduru. “Exploring the Future of Salesforce with Einstein GPT.” INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT](https://doi.org/10.55041/ijsrem36940)\n\n[176\\. Microsoft Agent Builder](https://techcommunity.microsoft.com/t5/s/gxcuf89792/attachments/gxcuf89792/HealthcareAndLifeSciencesBlog/2432/1/AgentBuilder.pdf)\n\n[177\\. Dipanker Jyoti, James A. Hutcherson. “Salesforce Security Architecture.”](https://doi.org/10.1007/978-1-4842-6631-1_5)\n\n[178\\. Elizabeth Nathania Witanto, Y. Oktian et al. “Toward Data Integrity Architecture for Cloud-Based AI Systems.” Symmetry](https://doi.org/10.3390/sym14020273)\n\n[180\\. Explore Your Data with Tableau Agent - Tableau](https://help.tableau.com/current/online/en-us/web_author_einstein.htm)\n\n[181\\. Unveiling Salesforce's Einstein Trust Layer](https://inclusioncloud.com/insights/blog/unveiling-salesforce-einstein-trust-layer/)\n\n[182\\. Salesforce AI Specialist篇之 Einstein Trust Layer - 博客园](https://www.cnblogs.com/zero-zyq/p/18434084)\n\n[183\\. Trusted AI: The Einstein Trust Layer](https://www.salesforce.com/artificial-intelligence/trusted-ai/)\n\n[184\\. Einstein Trust Layer: arquitectura de IA segura - Sale...](https://www.salesforce.com/es/artificial-intelligence/trusted-ai/?bc=OTH)\n\n[185\\. Optimizing AI with Salesforce's Einstein Trust Layer - Trailhead](https://trailhead.salesforce.com/content/learn/modules/the-einstein-trust-layer/follow-the-prompt-journey)\n\n[186\\. Artificial Intelligence—AI Solutions | Salesforce US](https://www.salesforce.com/products/artificial-intelligence/)\n\n[187\\. Elevate Your CRM App with Salesforce Einstein 1 Studio](https://crmtechzone.com/crm-news/2024/03/07/elevate-your-crm-app-with-salesforce-einstein-1-studio/)\n\n[188\\. Einstein Trust Layer: Secure AI Architecture | Salesfo...](https://www.salesforce.com/eu/products/secure-ai/)\n\n[189\\. What's the Difference Between Agentforce and Einstein GPT?](https://acsgbl.com/blog/whats-the-difference-between-agentforce-and-einstein-gpt/)\n\n[190\\. Salesforce bringt Einstein 1 Studio auf den Markt - Sa...](https://www.salesforce.com/de/company/news-press/press-releases/2024/03/060324/)\n\n[191\\. Einstein Trust Layer: Secure AI Architecture - Salesfo...](https://www.salesforce.com/mx/artificial-intelligence/trusted-ai/?d=pb)\n\n[192\\. Einstein Trust Layer: 안전한 AI 아키텍처...](https://www.salesforce.com/kr/artificial-intelligence/trusted-ai/)\n\n[193\\. Einstein Trust Layer: veilige AI-architectuur - Salesf...](https://www.salesforce.com/nl/artificial-intelligence/trusted-ai/)\n\n[194\\. Einstein Trust Layer 安全なAIアーキテクチャ - セールス...](https://www.salesforce.com/jp/artificial-intelligence/trusted-ai/?/d=cta-body-promo-8&bc=HA)\n\n[195\\. How Can I Trust Tableau Agent?](https://www.tableau.com/blog/how-can-i-trust-tableau-agent)\n\n[196\\. E. Rahm, P. Bernstein. “A survey of approaches to automatic schema matching.” The VLDB Journal](https://doi.org/10.1007/s007780100057)\n\n[197\\. How To Find and protect sensitive data with the power of intelligent pattern matching.\\*](https://enterprise-guide.s3.amazonaws.com/resources/einsteindatadetect.pdf)\n\n[198\\. How Trusted Enterprise Data Gives Einstein Copilot the Edge](https://www.salesforce.com/uk/news/stories/einstein-copilot-explained/?bc=WA)\n\n[199\\. 他山之石系列报告（一）: Salesforce的大模型ToB应用分析](https://pdf.dfcfw.com/pdf/H301_AP202307051592042069_1.pdf)\n\n[200\\. Tim Berners-Lee, R. Fielding et al. “Hypertext Transfer Protocol - HTTP/1.1.” RFC](https://doi.org/10.17487/RFC2068)\n\n[201\\. Knowledge in Microsoft Copilot Studio - Microsoft Power...](https://www.microsoft.com/en-us/power-platform/blog/?post_type=it-pro&p=127929)\n\n[202\\. Microsoft Sales Copilot Architecture Overview](https://download.microsoft.com/download/c/d/a/cda6b140-a325-4e19-af3c-31eb2ec22320/Microsoft%20Sales%20Copilot%20-%20Architecture%20Overview.pdf)\n\n[203\\. Microsoft Dynamics 365 and Microsoft Copilot for business functions: 2024 release wave 2 plan](https://www.arbentia.com/wp-content/uploads/2024/08/Dynamics-365-Wave-2-2024.pdf)\n\n[204\\. Navigating AI Security: A Guide for Microsoft Copilot for Microsoft 365](https://covenant.global/wp-content/uploads/2024/07/Navigating-AI-Security-for-Copilot.pdf)\n\n[205\\. Diego Ongaro, Stephen M. Rumble et al. “Fast crash recovery in RAMCloud.” Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles](https://doi.org/10.1145/2043556.2043560)\n\n[206\\. D. Kristol, L. Montulli. “HTTP State Management Mechanism.” RFC](https://doi.org/10.17487/RFC2109)\n\n[207\\. AI-Powered Chatbots and Service Agents: Redefining Customer Support with Microsoft Co-Pilot](https://ijisae.org/index.php/IJISAE/article/download/7267/6235/12564)\n\n[208\\. Getting Started with AI: A Microsoft Copilot Guide for Service Leaders](https://adoption.microsoft.com/files/copilot-for-service/Microsoft-Copilot-for-Service_Get-started-with-AI.pdf)\n\n[209\\. Exploration of Microsoft Copilot Use Cases for Process Optimization in SMES](http://www.theseus.fi/bitstream/10024/867169/2/Adil_SwerJabeen.pdf)\n\n[210\\. Microsoft Copilot for Security and NIST 800-171: Acces...](https://techcommunity.microsoft.com/t5/public-sector-blog/microsoft-copilot-for-security-and-nist-800-171-access-control/ba-p/4075263)\n\n[211\\. Cybersecurity Assessment of Microsoft Co-Pilot](https://cyberalberta.ca/system/files/cybersecurity-assessment-microsoft-co-pilot.pdf)\n\n[212\\. State Saving Methods in Stateless Protocols](http://etutorials.org/Programming/PHP+MySQL+and+Apache+in+24+hours/Part+III+Getting+Involved+with+the+Code/Hour+16.+Working+with+User+Sessions/Summary/)\n\n[213\\. Copilot for Service 中的数据处理(预览) - Copilot for S...](https://learn.microsoft.com/zh-cn/microsoft-copilot-service/data-handling)\n\n[214\\. A. Liu, Jason M. Kovacs et al. “A secure cookie protocol.” Proceedings. 14th International Conference on Computer Communications and Networks, 2005. ICCCN 2005.](https://doi.org/10.1109/ICCCN.2005.1523880)\n\n[215\\. Approccio basato su linguaggio naturale per l'esplorazione dei dati del CRM](https://webthesis.biblio.polito.it/30830/1/tesi.pdf)\n\n[216\\. Copilot for Sales: 4-Week Implementation](https://appsource.microsoft.com/fi-fi/marketplace/consulting-services/rsmproductsalesllc1604685958273.vivasales4wkimplementation)\n\n[217\\. 销售用 Copilot 体系结构 - Training | Microsoft Learn](https://learn.microsoft.com/zh-cn/training/modules/sales-copilot-deploy-configure/architecture)\n\n[218\\. Microsoft 安全Copilot 中的隐私和数据安全](https://learn.microsoft.com/zh-cn/copilot/security/privacy-data-security)\n\n[220\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[221\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[222\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[223\\. J. Park, Joseph C. O'Brien et al. “Generative Agents: Interactive Simulacra of Human Behavior.” Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology](https://doi.org/10.1145/3586183.3606763)\n\n[224\\. Shunyu Yao, Howard Chen et al. “WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents.” ArXiv](https://doi.org/10.48550/arXiv.2207.01206)\n\n[225\\. Getting Started with AI: A Microsoft Copilot Guide for Service Leaders](https://adoption.microsoft.com/files/copilot-for-service/Microsoft-Copilot-for-Service_Get-started-with-AI.pdf)\n\n[226\\. Kung-Hsiang Huang, Akshara Prabhakar et al. “CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions.”](https://arxiv.org/abs/2505.18878)\n\n[227\\. Microsoft is named a Leader in the 2024 Gartner® Magic ...](https://www.microsoft.com/en-us/dynamics-365/blog/business-leader/2024/12/17/microsoft-is-named-a-leader-in-2024-gartner-magic-quadrant-for-crm-customer-engagement-center/)\n\n[228\\. New autonomous agents scale your team like never before](https://news.microsoft.com/en-hk/2024/10/22/new-autonomous-agents-scale-your-team-like-never-before/)\n\n[229\\. AI 在客户体验领域的竞争：Salesforce Agentforce 与Microsoft ...](https://docs.feishu.cn/article/wiki/YimjwtAeBi2SeIkwBTBcppoInBh)\n\n[230\\. Tăng cường nhân lực với AI agent – Trang...](https://news.microsoft.com/vi-vn/2024/10/22/tang-cuong-nhan-luc-voi-ai-agent/)\n\n[231\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2023.” ArXiv](https://doi.org/10.48550/arXiv.2310.03715)\n\n[232\\. Dynamics 365 AI | Microsoft AI](https://www.microsoft.com/zh-cn/dynamics-365/solutions/ai?msockid=0017b856927467c0240eacef9359664b)\n\n[233\\. R. Sandhu, E. Coyne et al. “Role-Based Access Control Models.” Computer](https://doi.org/10.1109/2.485845)\n\n[234\\. Ronald Fagin. “Reasoning about knowledge.”](https://doi.org/10.7551/mitpress/5803.001.0001)\n\n[235\\. W. Ouchi. “A Conceptual Framework for the Design of Organizational Control Mechanisms.” Management Science](https://doi.org/10.1287/MNSC.25.9.833)\n\n[236\\. K. Eisenhardt. “Control: Organizational and Economic Approaches.” Management Science](https://doi.org/10.1287/MNSC.31.2.134)\n\n[237\\. Mohammed Alshiekh, R. Bloem et al. “Safe Reinforcement Learning via Shielding.” ArXiv](https://doi.org/10.1609/aaai.v32i1.11797)\n\n[238\\. Ameni ben Fadhel, D. Bianculli et al. “Model-Driven Run-Time Enforcement of Complex Role-Based Access Control Policies.” 2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE)](https://doi.org/10.1145/3238147.3238167)\n\n[239\\. Bettina Könighofer, Roderick Bloem et al. “Correct-by-Construction Runtime Enforcement in AI - A Survey.” Principles of Systems Design](https://doi.org/10.48550/arXiv.2208.14426)\n\n[240\\. AI 工具多场景覆盖，经营利润持续释放](https://pdf.dfcfw.com/pdf/H3_AP202405131632948205_1.pdf?1724427652000.pdf)\n\n[241\\. ...像 Microsoft 、Oracle、 Salesforce 这样的公司正在竞...](https://xueqiu.com/1742281038/308711939)\n\n[242\\. Einstein Copilot - Salesforce推出的CRM系统AI对话助手](https://celou.haoshang123.com/show/einsteincopilot.html)\n\n[243\\. 微软敲定 Copilot 落地时间，AI 商业化进程持续加速——人工智能行业周报](https://pdf.dfcfw.com/pdf/H3_AP202309251599762506_1.pdf?1695654167000.pdf)\n\n[244\\. Explore Your Data with Tableau Agent - Tableau](https://help.tableau.com/current/online/en-us/web_author_einstein.htm)\n\n[245\\. ...with AI Customer Service Chatbot Solutions | Salesf...](https://www.salesforce.com/service/customer-service-chatbot/?bc=HA)\n\n[246\\. Trusted AI: The Einstein Trust Layer](https://www.salesforce.com/artificial-intelligence/trusted-ai/)\n\n[247\\. Unveiling Salesforce's Einstein Trust Layer](https://inclusioncloud.com/insights/blog/unveiling-salesforce-einstein-trust-layer/)\n\n[248\\. Salesforce bringt Einstein 1 Studio auf den Markt - Sa...](https://www.salesforce.com/de/company/news-press/press-releases/2024/03/060324/)\n\n[249\\. Einstein Trust Layer: arquitectura de IA segura - Sale...](https://www.salesforce.com/es/artificial-intelligence/trusted-ai/?bc=OTH)\n\n[250\\. Artificial Intelligence—AI Solutions | Salesforce US](https://www.salesforce.com/products/artificial-intelligence/)\n\n[251\\. Elevate Your CRM App with Salesforce Einstein 1 Studio](https://crmtechzone.com/crm-news/2024/03/07/elevate-your-crm-app-with-salesforce-einstein-1-studio/)\n\n[252\\. Optimizing AI with Salesforce's Einstein Trust Layer - Trailhead](https://trailhead.salesforce.com/content/learn/modules/the-einstein-trust-layer/follow-the-prompt-journey)\n\n[253\\. Salesforce AI Specialist Exam](http://pdf.passcert.com/Salesforce%20AI%20Specialist.pdf)\n\n[254\\. CRM + AI + Data + Trust = Future of Business](https://www.salesforce.com/content/dam/web/ko_kr/www/documents/pdf/Salesforce_branded_newspaper.pdf)\n\n[255\\. Einstein Trust Layer: veilige AI-architectuur - Salesf...](https://www.salesforce.com/nl/artificial-intelligence/trusted-ai/?bc=OTH)\n\n[256\\. Einstein Trust Layer: Secure AI Architecture | Salesfo...](https://www.salesforce.com/uk/artificial-intelligence/trusted-ai/?bc=HA)\n\n[257\\. Einstein Trust Layer: Secure AI Architecture - Salesfo...](https://www.salesforce.com/mx/artificial-intelligence/trusted-ai/?d=pb)\n\n[258\\. Einstein Trust Layer: 안전한 AI 아키텍처...](https://www.salesforce.com/kr/artificial-intelligence/trusted-ai/)\n\n[259\\. How Trusted Enterprise Data Gives Einstein Copilot the Edge](https://www.salesforce.com/uk/news/stories/einstein-copilot-explained/?bc=WA)\n\n[260\\. Ethical AI: What is Salesforce’s Einstein Trust Layer?](https://www.gerent.com.au/posts/ethical-ai-what-is-salesforces-einstein-trust-layer)\n\n[261\\. Salesforce数据云推动下一代AI驱动的企业应用](https://www.salesforce.com/uk/news/stories/data-cloud-ai-future/?bc=WA)\n\n[262\\. Salesforce Einstein Features, Examples, and Guidelines](https://www.itransition.com/crm/salesforce/einstein)\n\n[263\\. 10 Salesforce Einstein Implementation FAQs Answered](https://www.salesforceben.com/salesforce-einstein-implementation-faqs-answered/)\n\n[264\\. Microsoft Sales Copilot Architecture Overview](https://download.microsoft.com/download/c/d/a/cda6b140-a325-4e19-af3c-31eb2ec22320/Microsoft%20Sales%20Copilot%20-%20Architecture%20Overview.pdf)\n\n[265\\. Copilot for Sales architecture - Training | Microsoft Learn](https://learn.microsoft.com/en-us/training/modules/sales-copilot-deploy-configure/architecture)\n\n[266\\. 销售用 Copilot 体系结构 - Training | Microsoft Learn](https://learn.microsoft.com/zh-cn/training/modules/sales-copilot-deploy-configure/architecture)\n\n[267\\. Eric Jonas, Johann Schleier-Smith et al. “Cloud Programming Simplified: A Berkeley View on Serverless Computing.” ArXiv](https://arxiv.org/abs/1902.03383)\n\n[268\\. 雷托·克雷默, 安诺·R·兰根. “Network management system and method for a communication session.”](https://www.semanticscholar.org/paper/b272aad4fa99d87cffee843299b68634798f6485)\n\n[269\\. J. Hellerstein, Jose M. Faleiro et al. “Serverless Computing: One Step Forward, Two Steps Back.” ArXiv](https://arxiv.org/abs/1812.03651)\n\n[270\\. Eric Jonas, Qifan Pu et al. “Occupy the cloud: distributed computing for the 99%.” Proceedings of the 2017 Symposium on Cloud Computing](https://doi.org/10.1145/3127479.3128601)\n\n[271\\. Multi-agent orchestration and more: Copilot Studio ...](https://www.microsoft.com/en-us/microsoft-copilot/blog/copilot-studio/multi-agent-orchestration-maker-controls-and-more-microsoft-copilot-studio-announcements-at-microsoft-build-2025/)\n\n[272\\. Copilot for Service 中的数据处理(预览) - Copilot for S...](https://learn.microsoft.com/zh-cn/microsoft-copilot-service/data-handling)\n\n[273\\. Shannon Joyner, M. MacCoss et al. “Ripple: A Practical Declarative Programming Framework for Serverless Compute.” ArXiv](https://arxiv.org/abs/2001.00222)\n\n[274\\. Microsoft Sales Copilot Top 10 Architecture Questions](https://download.microsoft.com/download/2/9/9/299c4def-2e5c-49c6-98c9-f9c4e3c23c62/Microsoft%20Sales%20Copilot%20-%20Top%2010%20Architecture%20Questions.pdf)\n\n[275\\. Josep Sampé, G. Vernik et al. “Serverless Data Analytics in the IBM Cloud.” Proceedings of the 19th International Middleware Conference Industry](https://doi.org/10.1145/3284028.3284029)\n\n[276\\. レト クラマー, アノー エル ランゲン. “System and method for managing a communication session in a network.”](https://www.semanticscholar.org/paper/a231b16b285899d80c5611997f4373a7ff027c8e)\n\n[277\\. Microsoft Dynamics 365 Copilot: Tailored 4-Week Implementation](https://appsource.microsoft.com/sl-si/marketplace/consulting-services/confizlimited1591780705931.d365-copilot-implementation-consulting-offer)\n\n[278\\. Z. Trabelsi, Safaa Zeidan. “Enhanced Session Table Architecture for Stateful Firewalls.” 2018 IEEE International Conference on Communications (ICC)](https://doi.org/10.1109/ICC.2018.8422079)\n\n[284\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[285\\. AI 在客户体验领域的竞争：Salesforce Agentforce 与Microsoft ...](https://docs.feishu.cn/article/wiki/YimjwtAeBi2SeIkwBTBcppoInBh)\n\n[286\\. New autonomous agents scale your team like never before](https://news.microsoft.com/en-hk/2024/10/22/new-autonomous-agents-scale-your-team-like-never-before/)\n\n[287\\. Tăng cường nhân lực với AI agent – Trang...](https://news.microsoft.com/vi-vn/2024/10/22/tang-cuong-nhan-luc-voi-ai-agent/)\n\n[288\\. Getting Started with AI: A Microsoft Copilot Guide for Service Leaders](https://adoption.microsoft.com/files/copilot-for-service/Microsoft-Copilot-for-Service_Get-started-with-AI.pdf)\n\n[289\\. Kung-Hsiang Huang, Akshara Prabhakar et al. “CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions.”](https://arxiv.org/abs/2505.18878)\n\n[290\\. Microsoft (MSFT US) AI and Copilot unlocking growth potential](https://hk-official.cmbi.info/upload/09643377-ce2f-415f-be97-731cc1bd1107.pdf)\n\n[291\\. Damian Ciechan. “Comparative analysis of frameworks and automation tools in terms of functionality and performance on the Salesforce CRM Platform.” Journal of Computer Sciences Institute](https://doi.org/10.35784/jcsi.3560)\n\n[292\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2023.” ArXiv](https://doi.org/10.48550/arXiv.2310.03715)\n\n[293\\. Microsoft Cloud & Hosting Partner Online Meeting](https://techcommunity.microsoft.com/t5/s/gxcuf89792/attachments/gxcuf89792/ANZPartnergereral/46/1/250325%20Partner%20Update%20e163.pdf)\n\n[294\\. R. Sandhu, E. Coyne et al. “Role-Based Access Control Models.” Computer](https://doi.org/10.1109/2.485845)\n\n[295\\. Ameni ben Fadhel, D. Bianculli et al. “Model-Driven Run-Time Enforcement of Complex Role-Based Access Control Policies.” 2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE)](https://doi.org/10.1145/3238147.3238167)\n\n[296\\. Jay Ligatti, Lujo Bauer et al. “Run-Time Enforcement of Nonsafety Policies.” ACM Trans. Inf. Syst. Secur.](https://doi.org/10.1145/1455526.1455532)\n\n[297\\. F. Schneider. “Enforceable security policies.” Foundations of Intrusion Tolerant Systems, 2003 \\[Organically Assured and Survivable Information Systems\\]](https://doi.org/10.1109/FITS.2003.1264930)\n\n[298\\. Vincent C. Hu, David F. Ferraiolo et al. “Guide to Attribute Based Access Control (ABAC) Definition and Considerations.”](https://doi.org/10.6028/NIST.SP.800-162)\n\n[299\\. Fahad Shaon, Sazzadur Rahaman et al. “The Queen's Guard: A Secure Enforcement of Fine-grained Access Control In Distributed Data Analytics Platforms.” Proceedings of the 39th Annual Computer Security Applications Conference](https://doi.org/10.1145/501978.501980)\n\n[300\\. N. Damianou, Naranker Dulay et al. “The Ponder Policy Specification Language.” IEEE International Symposium on Policies for Distributed Systems and Networks](https://doi.org/10.1007/3-540-44569-2_2)\n\n[301\\. Ú. Erlingsson, F. Schneider. “SASI enforcement of security policies: a retrospective.” Proceedings DARPA Information Survivability Conference and Exposition. DISCEX'00](https://doi.org/10.1109/DISCEX.2000.821527)\n\n[302\\. Sagar Tamang, Dibya Jyoti Bora. “Enforcement Agents: Enhancing Accountability and Resilience in Multi-Agent AI Frameworks.”](https://arxiv.org/abs/2504.04070)\n\n[303\\. T. Dimitrakos, I. Djordjevic et al. “Policy-driven access control over a distributed firewall architecture.” Proceedings Third International Workshop on Policies for Distributed Systems and Networks](https://doi.org/10.1109/POLICY.2002.1011314)\n\n[304\\. 信息安全技术 术语](https://www.tc260.org.cn/file/2018-12-26/d0c3a98a-9759-480d-863a-d5978392d116.doc)\n\n[305\\. AI 工具多场景覆盖，经营利润持续释放](https://pdf.dfcfw.com/pdf/H3_AP202405131632948205_1.pdf?1724427652000.pdf)"
  },
  {
    "id": 37,
    "question": "Can AI models continue to scale when you add more compute, data, and power? Are we seeing diminishing returns?",
    "response": "**Research Report: The Future of AI Scaling**\n\n**Date:** July 27, 2025\n\n**Prepared by:** Expert AI Researcher\n\n**Executive Summary**\n\nThe field of artificial intelligence has been dominated for the last decade by the paradigm of scaling: the hypothesis, largely proven correct, that bigger models trained on more data with more computational power yield better performance. This has led to an exponential increase in the size and capabilities of AI systems, particularly large language models (LLMs). However, as of 2025, the AI community is confronting a critical inflection point. The brute-force approach of scaling is exhibiting clear and quantifiable signs of diminishing returns. Performance gains are slowing, the limits of available high-quality data are being reached, and the economic and energy costs of computation are becoming unsustainable.\n\nThis report analyzes the current state of AI scaling, drawing on recent experimental results and research trends. It confirms that traditional scaling of dense models is facing significant headwinds. However, this is not the end of progress. Instead, the field is transitioning from an era of \"brute-force scaling\" to one of \"smart scaling.\" New frontiers are opening up through architectural innovations like Mixture-of-Experts (MoE), new conceptual paradigms like test-time compute, and the development of radically different, energy-efficient hardware such as neuromorphic systems. The central question is no longer _if_ AI can continue to improve, but _how_ it can scale in a more efficient, sustainable, and intelligent manner.\n\n**1\\. The Wall of Diminishing Returns: Limits of Traditional Scaling**\n\nThe foundation of the modern AI revolution rests on \"neural scaling laws,\" empirical principles demonstrating that model performance improves predictably as a power-law function of increases in model size, dataset size, and compute \\[128\\]\\[131\\]\\[198\\]. This principle fueled the development of ever-larger models, culminating in systems that exhibit remarkable emergent abilities \\[121\\]\\[181\\]. However, recent evidence from 2024 and 2025 indicates that this exponential ride is slowing down, encountering fundamental limits across data, hardware, and performance.\n\n**1.1. Evidence of Performance Plateaus**\n\nThe most direct evidence of diminishing returns comes from the slowing pace of capability improvements. Major AI labs are reporting that year-over-year gains are becoming harder to achieve, even with massive additional investment \\[4\\]\\[6\\]. This is observable in established benchmarks; for example, performance on tasks like object detection on the COCO dataset has begun to plateau, where significant increases in resources now yield only minimal improvements in accuracy \\[10\\].\n\nFurther quantitative analysis reveals the nature of this slowdown. While model size generally correlates with better performance on out-of-distribution tasks, this relationship follows a power law that results in diminishing returns, with one study noting a significant tapering of gains after models exceed 300 million parameters \\[68\\]. This suggests that simply making a model bigger does not guarantee a proportional increase in its learning effectiveness.\n\n**1.2. The Data Bottleneck**\n\nA critical ingredient for scaling is high-quality training data, and the world's supply is not infinite. Researchers have identified that the availability of high-quality data is becoming a natural constraint on further progress \\[6\\]\\[12\\]. While efforts to generate synthetic data are underway, the effectiveness and diversity of this data compared to genuine human-generated text and images remain an open research question. The pursuit of more data at any cost risks a decline in data quality, which can negatively impact model performance and introduce biases.\n\n**1.3. The Hardware and Energy Wall**\n\nThe computational requirements for training state-of-the-art models are staggering. Training a single large model can consume over 1,000 MWh of electricity \\[102\\]\\[109\\]. This voracious energy consumption, which one estimate places at 7% of the world's total electricity usage for the AI sector, presents a significant environmental and economic challenge \\[41\\].\n\nBeyond the raw energy cost, the physical architecture of large-scale training clusters introduces its own diminishing returns.\n\n**Communication Overheads:** As training is parallelized across thousands of GPUs, the cost of communication between them begins to dominate the computation itself \\[1\\]\\[5\\]\\[2\\]. This \"communication boundedness\" means that adding more GPUs does not lead to a linear increase in processing speed. Instead, hardware utilization drops, and overall efficiency decreases \\[2\\]\\[2\\]. One 2024 study demonstrated that scaling a training job from 128 to 2,048 GPUs resulted in a 37.22% decrease in throughput and a significant drop in power efficiency \\[2\\].\n\n**Power Efficiency Decay:** At the system level, power efficiency itself exhibits diminishing returns. As the number of devices in a cluster scales up, the per-device computational throughput is reduced, leading to lower overall energy efficiency for the entire system \\[65\\].\n\n**1.4. Quantifying the Slowdown: The Power Law Exponents**\n\nScaling laws are often expressed with a power-law exponent, which describes the rate of improvement. A smaller exponent indicates a faster rate of diminishing returns. While the research community actively investigates these exponents across different domains, a consolidated, peer-reviewed comparative analysis from 2025 remains elusive \\[195\\]\\[307\\]\\[313\\]. However, domain-specific studies are beginning to provide quantitative figures. For instance, a 2025 study of scaling laws in robotics found an average power law exponent for model size scaling of -0.246, providing a concrete measure of diminishing returns in that field \\[312\\]. Research into scaling for mathematical reasoning and vision tasks is also highly active in 2025, but specific exponent values are not yet widely published \\[186\\]\\[189\\]\\[245\\]. This lack of a unified quantitative picture underscores the complexity of the issue, as scaling dynamics appear to vary by task and data modality \\[257\\].\n\n**2\\. New Frontiers: Pathways to \"Smart Scaling\"**\n\nThe challenges of traditional scaling have catalyzed a wave of innovation aimed at achieving greater capabilities with greater efficiency. The focus is shifting from \"more is better\" to a more nuanced approach that leverages architectural novelties, new training paradigms, and algorithmic optimizations.\n\n**2.1. Architectural Innovation: Mixture-of-Experts (MoE)**\n\nThe most prominent architectural shift to combat scaling costs is the Mixture-of-Experts (MoE) model. Unlike traditional \"dense\" models where every parameter is used for every input token, MoE models consist of numerous \"expert\" sub-networks, and a routing mechanism activates only a small subset of these experts for any given input \\[93\\]\\[209\\].\n\nThis approach effectively decouples the total parameter count from the computational cost (FLOPs) of training and inference, enabling the creation of models with hundreds of billions or even trillions of parameters while maintaining manageable computational budgets \\[92\\]\\[151\\]\\[338\\].\n\n**Quantitative Efficiency Gains:**\n\n**Training Efficiency:** The computational savings are dramatic. 2024 studies show that a compute-optimal MoE model can achieve the same performance as a much larger dense model using 20x to 40x less compute \\[205\\]\\[264\\]\\[385\\]. Other benchmarks report that MoE models can match a dense model's performance with approximately 3x to 4x fewer training FLOPs \\[203\\]\\[381\\]or reduce overall training costs by as much as 78% \\[141\\].\n\n**Inference Efficiency:** The benefits extend to inference, where a 67.1 billion parameter MoE model was reported to have a 20-50 times lower inference cost than a comparable dense model \\[82\\]\\[143\\]. This efficiency is what makes models like Databricks' DBRX (132B total parameters, 36B active) and Mixtral (46.7B total, 12.9B active) practical to deploy \\[89\\]. The 2024-2025 trend has shifted towards architectures with a high number of smaller experts, such as DeepSeek-V2, to further optimize load balancing and efficiency \\[28\\].\n\n**Challenges and Trade-offs:** MoE is not a panacea. The increased memory footprint to store all experts can be substantial, and the dynamic routing can introduce latency, sometimes making MoE models slower in terms of raw throughput despite using fewer FLOPs \\[263\\]\\[384\\]. They may also require more total parameters than a dense model to achieve the same level of performance \\[146\\].\n\n**2.2. A New Paradigm: Scaling Test-Time Compute**\n\nA conceptually new approach to scaling is emerging, focused not on pre-training but on inference. \"Test-time compute\" involves giving a model more computational resources and time to \"think\" before providing an answer \\[4\\]\\[11\\]. This can be implemented through techniques like multi-step reasoning, chain-of-thought prompting \\[193\\], or iteratively re-prompting the model to break down a complex problem \\[4\\].\n\nResearch from 2025 suggests that optimally scaling test-time compute can be a more effective path to improved performance, particularly for complex reasoning tasks, than simply scaling the number of model parameters \\[133\\]\\[184\\]. This represents a fundamental shift, treating intelligence not just as a function of a model's stored weights but as an active computational process.\n\n**3\\. The Power Imperative: Reimagining the Hardware Foundation**\n\nThe immense energy consumption of AI is an existential threat to continued scaling. This has spurred urgent research into alternative, low-power computing hardware.\n\n**3.1. The Promise of Neuromorphic Computing**\n\nNeuromorphic computing draws inspiration directly from the human brain, which performs an estimated 10^16 operations per second using only about 20 watts of power—making it orders of magnitude more efficient than a top-tier GPU consuming hundreds of watts \\[170\\]. Neuromorphic hardware aims to replicate this efficiency by building chips with neuron-like and synapse-like components that operate in an event-driven, sparse manner.\n\n**Demonstrated Energy Efficiency:**\n\nExperimental results have validated this promise. Neuromorphic systems have demonstrated:\n\nEnergy savings exceeding 60% compared to conventional deep learning hardware \\[43\\].\n\nUp to an 85% improvement in power efficiency over traditional computing \\[51\\].\n\nA memristor-based system with 100x lower energy consumption than a GPU for the same task \\[50\\].\n\nIn a direct comparison for CNN training, neuromorphic chips required less than 1/35th the energy of a GPU-accelerated system \\[166\\]\\[166\\].\n\nIntel's Loihi 2 chip, in preliminary 2024 experiments, showed up to 1,000x higher energy efficiency than an NVIDIA Jetson Orin Nano for certain tasks \\[351\\].\n\n**3.2. Current Limitations and the Path Forward**\n\nDespite its immense potential, neuromorphic computing is not yet ready to replace GPUs for large-scale AI training in 2025. Major challenges remain:\n\n**Scalability:** Today's chips like Intel's Loihi 2 support around one million neurons, a tiny fraction of the 86 billion in the human brain \\[49\\].\n\n**Training Algorithms:** End-to-end training of massive models like LLMs on neuromorphic architectures is an unsolved problem. The search results contained no benchmarks for training LLMs or even standard vision models like ResNet-50 on Loihi 2 (Queries 6, 11, 13).\n\n**Standardization:** The field lacks standardized benchmarks for accurately measuring and comparing energy consumption, making direct, reliable comparisons difficult \\[42\\]\\[43\\]\\[54\\].\n\n**Ecosystem and Tooling:** The software and algorithmic ecosystem for neuromorphic computing is far less mature than the CUDA-based ecosystem for GPUs.\n\n**Conclusion**\n\nThe era of effortless gains from scaling AI models with more compute, data, and power is drawing to a close. The industry is now grappling with tangible diminishing returns across performance, data availability, and hardware efficiency. The evidence strongly suggests that simply building ever-larger dense models is an unsustainable and increasingly inefficient path forward.\n\nHowever, this does not signify the end of AI progress. Rather, it marks a pivotal transition towards a more sophisticated and multi-faceted strategy of \"smart scaling.\" The future of AI advancement will be driven by a confluence of innovations:\n\n1.  **Efficient Architectures:** Mixture-of-Experts and other sparse models will become standard, allowing parameter counts to grow into the trillions without a corresponding explosion in computational cost.\n2.  **Novel Scaling Paradigms:** Scaling test-time compute will unlock new capabilities in reasoning and complex problem-solving, shifting focus from static knowledge to dynamic computation.\n3.  **Algorithmic and Data Optimization:** Continued improvements in training algorithms, data pruning, and data synthesis will be crucial for maximizing efficiency.\n4.  **Next-Generation Hardware:** In the long term, the development and maturation of energy-efficient hardware like neuromorphic systems will be essential to break through the power wall and enable the next generation of truly large-scale, sustainable AI.\n\nIn conclusion, AI models can continue to scale, but not in the way they have for the past decade. The path forward is not a straight line of bigger models, but a complex and exciting landscape of architectural ingenuity, conceptual innovation, and hardware revolution.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. NVIDIA DGX SUPERPOD DELIVERING RECORD-BREAKING SUPERCOMPUTING TO EVERY ENTERPRISE](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/dgx-pod/nvidia-dgx-superpod-datasheet.pdf)\n\n[2\\. Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training](https://arxiv.org/pdf/2411.13055)\n\n[3\\. Beyond Compute: A Weighted Framework for AI Capability Governance](http://www.scitepress.org/Papers/2025/131288/131288.pdf)\n\n[4\\. Current AI Scaling Laws are Showing Diminishing Returns, Forcing AI Labs to Change Course](https://www.aitechnobyte.com/current-ai-scaling-laws-are-showing-diminishing-returns-forcing-ai-labs-to-change-course/)\n\n[5\\. NVIDIA DGX SUPERPOD INSTANT INFRASTRUCTURE FOR AI LEADERSHIP](https://www.nvidia.cn/content/dam/en-zz/solutions/data-center/documents/dgx-superpod-solution-brief.pdf)\n\n[6\\. Navigating the Boom: Confronting Generative AI’s Most Pressing Questions](https://www.williamblair.com/-/media/downloads/eqr/2025/williamblair_navigating-the-boom-confronting-generative-ais-most-pressing-questions.pdf)\n\n[7\\. Neural Scaling Laws in Robotics](https://arxiv.org/pdf/2405.14005)\n\n[8\\. Diminishing Returns — Model Railway Exhibition](https://www.last.fm/zh/music/Model%20Railway%20Exhibition/_/Diminishing%20Returns)\n\n[9\\. Governing AI Beyond the Pretraining Frontier](https://www.arxiv.org/pdf/2502.15719)\n\n[10\\. Common Sense Is All You Need](https://arxiv.org/pdf/2501.06642)\n\n[11\\. Test-Time Compute: The Next Frontier in AI Scaling](https://www.ikangai.com/test-time-compute-the-next-frontier-in-ai-scaling/)\n\n[12\\. 智能的路径在Agent 爆发的前夜](https://juejin.cn/post/7513361556890681396)\n\n[13\\. Research News](https://english.cas.cn/newsroom/research_news/)\n\n[14\\. AI Agent Performance: Success Rates & ROI in 2025](https://research.aimultiple.com/ai-agent-performance/)\n\n[21\\. DYNAMIC MIXTURE OF EXPERTS: AN AUTO-TUNING APPROACH FOR EFFICIENT TRANSFORMER MODELS](https://openreview.net/pdf?id=T26f9z2rEe)\n\n[22\\. MoC-System: Efficient Fault Tolerance for Sparse Mixture-of-Experts Model Training](http://www.arxiv.org/pdf/2408.04307v3)\n\n[23\\. A Survey on Mixture of Experts in Large Language Models](https://arxiv.org/pdf/2407.06204)\n\n[24\\. MoNTA: Accelerating Mixture-of-Experts Training with N...](http://arxiv.org/html/2411.00662v1)\n\n[25\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[26\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[27\\. Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks](https://arxiv.org/pdf/2401.02731)\n\n[28\\. DeepSeek Theory.md](https://github.com/junfanz1/AI-ML-CS-Quant-Readings/blob/main/DeepSeek/DeepSeek%20Theory.md)\n\n[29\\. MoE++: ACCELERATING MIXTURE-OF-EXPERTS METHODS WITH ZERO-COMPUTATION EXPERTS](https://openreview.net/pdf?id=t7P5BUKcYv)\n\n[30\\. Dynamic Mixture of Experts for Adaptive Computation in ...](https://www.mdpi.com/2078-2489/16/6/483)\n\n[31\\. 大模型时代的混合专家系统优化综述](https://crad.ict.ac.cn/cn/article/pdf/preview/10.7544/issn1000-1239.202440016.pdf)\n\n[32\\. Mixture of Experts with Mixture of Precisions for Tuni...](http://arxiv.org/html/2407.14417v1)\n\n[33\\. Nonparametric High-dimensional Models: Sparsity, Efficiency, Interpretability](https://dspace.mit.edu/bitstream/handle/1721.1/156296/ibrahim-shibal-phd-eecs-2024-thesis.pdf;jsessionid=B35F09571A04EE98605D9F9686272EE9?sequence=1)\n\n[34\\. MoE-INFINITY: Efficient MoE Inference on Personal Machines with Sparsity-Aware Expert Cache](https://arxiv.org/pdf/2401.14361)\n\n[41\\. THE 11TH INTERNATIONAL WORKSHOP ON ADVANCED MATERIALS SCIENCE AND NANOTECHNOLOGY](https://iwamsn.ac.vn/storage/program-and-abstract-book-1609.pdf)\n\n[42\\. Enrique Barba Roque, Luis Cruz. “Energy Aware Development of Neuromorphic Implantables: From Metrics to Action.”](https://arxiv.org/abs/2506.09599)\n\n[43\\. Comparative Analysis on Developed Optimization Techniques for Reducing Energy Consumption in AI Training and Inference](https://jisem-journal.com/index.php/journal/article/download/2166/844/3477)\n\n[44\\. INSIDE Magazine 8](https://intranet.inside-association.eu/publication/download/inside-magazine-8.pdf)\n\n[45\\. Consciousness in Machines: A Critical Exploration](https://www.ijmra.in/v7i12/Doc/18.pdf)\n\n[46\\. Neuromorphic computing for sustainable AI: Energy ...](https://innspub.net/neuromorphic-computing-for-sustainable-ai-energy-efficient-architectures-for-resource-constrained-environment/)\n\n[47\\. Neuromorphic Computing: Implications for AI and ...](https://www.siberoloji.com/neuromorphic-computing-implications-for-ai-and-cybersecurity/)\n\n[48\\. Special Section Call for Papers“Advances in Heterogeneous Integration for Neuromorphic Computing”](https://eps.ieee.org/images/files/CFP_TCPMT_Special_Section_HIBIC_042124.pdf)\n\n[49\\. How Neuromorphic Chips Could Redefine Edge AI Devices](https://www.embedur.ai/how-neuromorphic-chips-could-redefine-edge-ai-devices/)\n\n[50\\. Watt Matters in AI: Hardware-based views on energy efficiency](https://ioplus.nl/en/posts/watt-matters-in-ai-hardware-based-views-on-energy-efficiency)\n\n[51\\. THE EVOLUTION AND ARCHITECTURE OF ARTIFICIAL INTELLIGENCE: FROM STATISTICAL FOUNDATIONS TO DEEP LEARNING](https://www.irjmets.com/uploadedfiles/paper/issue_2_february_2025/68226/final/fin_irjmets1740679491.pdf)\n\n[52\\. Zhimu Guo, A. Aadhi et al. “Fully analog end-to-end online training with real-time adaptibility on integrated photonic platform.”](https://arxiv.org/abs/2506.18041)\n\n[53\\. Training of Physical Neural Networks](https://hal.science/hal-04802998v1/file/2024%20-%20Training%20of%20Physical%20Neural%20Networks.pdf)\n\n[54\\. The neurobench framework for benchmarking neuromorphic computing algorithms and systems](https://physiologie.unibe.ch/PublicationPDF/Yik2025NeuroBench.pdf)\n\n[55\\. NIH BRAIN NeuroAI Workshop 2024 Program Book](https://n4solutionsllc.com/wp-content/uploads/2024/11/NIH_BRAIN_NeuroAI_Workshop_Program_Book_508c.pdf)\n\n[56\\. Perfecting Imperfect Physical Neural Networks with Transferable Robustness using Sharpness-Aware Training](https://arxiv.org/pdf/2411.12352)\n\n[61\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[62\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[63\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[64\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[65\\. Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training](https://arxiv.org/pdf/2411.13055)\n\n[66\\. PROGRESSIVE TOKEN LENGTH SCALING IN TRANSFORMER ENCODERS FOR EFFICIENT UNIVERSAL SEGMENTATION](https://openreview.net/pdf/eba55a2067277ffc3c5546358c13a29b30e626cf.pdf)\n\n[67\\. Distributional Scaling Laws for Emergent Capabilities](https://fetcher.alphaxiv.org/v2/pdf/2502.17356v2)\n\n[68\\. Life as a Function: Why Transformer Architectures Struggle to Gain Genome-Level Foundational Capabilities.](https://www.biorxiv.org/content/biorxiv/early/2025/03/25/2025.01.13.632745.full.pdf)\n\n[69\\. J. Kaplan, Sam McCandlish et al. “Scaling Laws for Neural Language Models.” ArXiv](https://arxiv.org/abs/2001.08361)\n\n[70\\. FROM SCALING LAW TO SUB-SCALING LAW: UNDERSTANDING THE DIMINISHING RETURNS OF LARGER MODELS](https://openreview.net/pdf?id=LJ1zlaGdPm)\n\n[71\\. Scaling Laws for Downstream Task Performance of Large Language Models](https://openreview.net/pdf?id=PXwdsgZjnX)\n\n[72\\. Scaling and Distilling Transformer Models for sEMG](https://openreview.net/pdf/73c292cad75a078ce8dbc0b11a66196805b2c5b0.pdf)\n\n[73\\. Transformer潜力不断！来看24年在各领域的应用创新：用梯度下降法学习因果结构](https://www.bilibili.com/video/av1951216079)\n\n[74\\. Transformer 用于量化投资论文推荐](https://bigdata-s3.wmcloud.com/researchreport/2023-04/8e791690431a995031700401ad68ba4e.pdf)\n\n[75\\. LOT 2: Distribution and power transformers Tasks 1 – 7](https://transformers.vito.be/sites/transformers.vito.be/files/attachments/EuP_TransformersTask_1_7_V60.pdf)\n\n[76\\. WHY ARE SMALL TRANSFORMERS NOT BETTER?](https://openreview.net/pdf/ed180133511287d7ecadba1f05ae04440d5d96b3.pdf)\n\n[77\\. TokenFormer: Rethinking Transformer Scaling with Tokenized Model ...](https://openreview.net/forum?id=oQ4igHyh3N)\n\n[78\\. ANNUAL BUDGET 2024 - 2025](https://www.newtown-ct.gov/sites/g/files/vyhlif12216/f/uploads/budget_document_2024_2025_0.pdf)\n\n[79\\. Haiyang-W/TokenFormer - GitHub](https://github.com/Haiyang-W/TokenFormer)\n\n[81\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[82\\. Quarterly Market Insight Q4 2024](https://vmsam.com/wp-content/uploads/2025/02/4Q2024-PE-Quarterly-Insights_ENG_vf.pdf)\n\n[83\\. MoC-System: Efficient Fault Tolerance for Sparse Mixture-of-Experts Model Training](http://www.arxiv.org/pdf/2408.04307v3)\n\n[84\\. Scaling Laws for Fine-Grained Mixture of Experts](https://openreview.net/pdf?id=Iizr8qwH7J)\n\n[85\\. Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models](https://arxiv.org/pdf/2501.11873)\n\n[86\\. A Survey of Mixture of Experts Models: Architectures and Applications in Business and Finance](https://www.preprints.org/manuscript/202505.1603/v1/download)\n\n[87\\. Scaling to Billion Parameters for Time Series Foundation Models with Mixture of Experts](https://openreview.net/pdf?id=tPFxnyNYUP)\n\n[88\\. ...Meets Instruction Tuning: A Winning Combination for...](https://api.semanticscholar.org/arXiv:2305.14705)\n\n[89\\. How to use Mixture of Experts in Your Next AI Project?](https://www.projectpro.io/article/mixture-of-experts/1137)\n\n[90\\. MoE-Pruner: Pruning Mixture-of-Experts Large Language Model using the Hints from Its Router](https://openreview.net/pdf/fb7a34c100500cc28b4490c43b47c70d75497960.pdf)\n\n[91\\. Scattered Mixture-of-Experts Implementation](http://arxiv.org/html/2403.08245v2)\n\n[92\\. MoE-CAP: Benchmarking Cost, Accuracy and Performance o...](http://arxiv.org/html/2412.07067v3)\n\n[93\\. A Survey of Mixture of Experts Models: Architectures and ...](https://www.preprints.org/manuscript/202505.1603/v1)\n\n[94\\. R. Jacobs, Michael I. Jordan et al. “Adaptive Mixtures of Local Experts.” Neural Computation](https://doi.org/10.1162/neco.1991.3.1.79)\n\n[95\\. MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems](https://arxiv.org/pdf/2412.07067)\n\n[96\\. OLMoE: OPEN MIXTURE-OF-EXPERTS LANGUAGE MODELS](https://openreview.net/pdf/de8bda951e013f5ec54c4273b79414f3930bdda9.pdf)\n\n[97\\. Cheems: Wonderful Matrices More Efficient and More Effective Architecture](https://arxiv.org/pdf/2407.16958v1)\n\n[98\\. Mixture of insightTful Experts (MoTE): The Synergy of Reasoning Chains and Expert Mixtures in Self-Alignment](https://arxiv.org/pdf/2405.00557)\n\n[99\\. Alexandre Muzio, Alex Sun et al. “SEER-MoE: Sparse Expert Efficiency through Regularization for Mixture-of-Experts.” ArXiv](https://doi.org/10.48550/arXiv.2404.05089)\n\n[101\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[102\\. Large Language Models: What You Need to Know in 2025](https://hatchworks.com/blog/gen-ai/large-language-models-guide/#:~:text=In-context%20learning%20refers%20to,a%20broad%20range%20of%20applications.)\n\n[103\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[104\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[105\\. BUILDING A LARGE LANGUAGE MODEL POWERED CHATBOT WITH SEAMLESS INTERNAL DOCUMENT RETRIEVAL](https://upcommons.upc.edu/bitstream/handle/2117/408081/185192.pdf;jsessionid=B1D6E0DDFB31F61DE03E49E06AE90186?sequence=2)\n\n[106\\. A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods](https://www.arxiv.org/pdf/2501.13947)\n\n[107\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[108\\. Large Language Models In 2025: Your Guide To Next-Gen AI](https://acecloud.ai/blog/large-language-models/#:~:text=By%202025,%20advancements%20in%20hardware,becoming%20more%20efficient%20and%20accessible.)\n\n[109\\. LLM statistics 2025: Adoption, trends, and market insights](https://www.hostinger.com/tutorials/llm-statistics#:~:text=As%20of%202025,%2067%25%20of,the%20quality%20of%20their%20work.)\n\n[110\\. A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf?fbclid=IwAR3GYBQ2P9Cww2HVM3oUbML9i5i3DMDBVv5_FvYWfEi-vdZqZoSM78jE2-s)\n\n[111\\. Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models](https://arxiv.org/pdf/2502.05610)\n\n[112\\. Large Language Models in 2025: How Much Understanding and Intelligence?](https://nlp.stanford.edu/~manning/talks/WWK-Understanding-and-Intelligence-2025.pdf)\n\n[113\\. Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training](https://arxiv.org/pdf/2411.13055)\n\n[114\\. Software Engineering and its Automation with Large Language Models](https://www.mn.uio.no/ifi/forskning/aktuelt/arrangementer/disputaser/2024/grishina---software-engineering-and-its-automation-with-large-language-models---phd-thesis-2024-published.pdf)\n\n[115\\. ...and performance of large language models on...](https://link.springer.com/article/10.1186/s13054-025-05302-0)\n\n[116\\. The Top 10 AI Imperatives for 2025: How CXOs Can Transform Enterprise AI and Business Strategy](https://www.infosys.com/iki/documents/top10-ai-imperatives-2025.pdf)\n\n[117\\. 2025年《高科技、媒体及电信产业趋势预测》](https://www2.deloitte.com/content/dam/Deloitte/tw/Documents/technology-media-telecommunications/2025-tmt-prediction.pdf)\n\n[118\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai-index-report-2025_chapter1_final.pdf)\n\n[119\\. Training Large Language Models (LLMs)](https://www.epfl.ch/labs/lions/wp-content/uploads/2025/04/ee-628-Lecture_01_Archtectures.pdf)\n\n[121\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[122\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[123\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[124\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[125\\. Scaling Exponents Across Parameterizations and Optimizers](https://openreview.net/pdf?id=0ksNeD1SJT)\n\n[126\\. Distributional Scaling Laws for Emergent Capabilities](https://fetcher.alphaxiv.org/v2/pdf/2502.17356v2)\n\n[127\\. INFERENCE SCALING LAWS: AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR LLM PROBLEM-SOLVING](https://openreview.net/pdf?id=VNckp7JEHn)\n\n[128\\. A Solvable Attention for Neural Scaling Laws](https://openreview.net/pdf?id=wYxOMEzpkl)\n\n[129\\. CS 715: Seminar on Solving Complex Tasks using Large ...](https://www.uni-mannheim.de/dws/teaching/course-details/courses-for-master-candidates/cs-715-seminar-on-solving-complex-tasks-using-large-language-models/)\n\n[130\\. Qwen3 Technical Report](https://www.arxiv.org/pdf/2505.09388)\n\n[131\\. \\[2024 LLM 스터디\\] Scaling Laws for Neural Language Models (2020)](https://velog.io/@zvezda/2024-LLM-%EC%8A%A4%ED%84%B0%EB%94%94-Scaling-Laws-for-Neural-Language-Models-2020)\n\n[132\\. A Multi-Power Law for Loss Curve Prediction Across Learning Rate Schedules](https://openreview.net/attachment?id=DFeRFTv368&name=pdf)\n\n[133\\. Unifying Two Types of Scaling Laws from the Perspectiv...](http://arxiv.org/html/2501.06802v2)\n\n[134\\. Navigating Scaling Laws: Accelerating Vision Transformer's Training via Adaptive Strategies](https://openreview.net/pdf/3aa98ffc12b80d4cec6d191be9f3b74b78e77404.pdf)\n\n[135\\. Compute Optimal Scaling of Skills: Knowledge vs Reasoning](https://fetcher.alphaxiv.org/v2/pdf/2503.10061v2)\n\n[136\\. Scaling Laws for Post Training Quantized Large Language Models](https://raw.githubusercontent.com/mlresearch/v262/main/assets/xu24a/xu24a.pdf)\n\n[137\\. Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge](https://arxiv.org/pdf/2502.12501)\n\n[138\\. A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems](https://openreview.net/pdf/c2dfb1fd027ff814ec8f28d05764af37b7696f64.pdf)\n\n[141\\. Scaling to Billion Parameters for Time Series Foundation Models with Mixture of Experts](https://openreview.net/pdf?id=tPFxnyNYUP)\n\n[142\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[143\\. Quarterly Market Insight Q4 2024](https://vmsam.com/wp-content/uploads/2025/02/4Q2024-PE-Quarterly-Insights_ENG_vf.pdf)\n\n[144\\. MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems](https://arxiv.org/pdf/2505.11415)\n\n[145\\. MoE-CAP: Benchmarking Cost, Accuracy and Performance ...](https://arxiv.org/html/2505.11415v1)\n\n[146\\. \\[2404.05567\\] Dense Training, Sparse Inference](https://arxiv.org/abs/2404.05567)\n\n[147\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[148\\. MoE-CAP: Benchmarking Cost, Accuracy and Performance o...](http://arxiv.org/html/2412.07067v3)\n\n[149\\. Load balancing and memory optimizations for expert parallel training of large language models](https://dspace.mit.edu/bitstream/handle/1721.1/153897/wisdom-dwisdom-meng-eecs-2024-thesis.pdf?sequence=1&isAllowed=y)\n\n[150\\. Mixture of Experts - DeepSpeed](https://www.deepspeed.ai/tutorials/mixture-of-experts/)\n\n[151\\. Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models](https://arxiv.org/pdf/2501.11873)\n\n[152\\. Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks](https://arxiv.org/pdf/2401.02731)\n\n[153\\. SiDA-MoE: Sparsity-Inspired Data-Aware Serving for Efficient and ...](https://www.aimodels.fyi/papers/arxiv/sida-moe-sparsity-inspired-data-aware-serving)\n\n[154\\. Scaling Laws for Fine-Grained Mixture of Experts](https://openreview.net/pdf?id=Iizr8qwH7J)\n\n[155\\. Designing Large Foundation Models for Efficient Training and Inference: A Survey](https://arxiv.org/pdf/2409.01990)\n\n[156\\. Binary-Integer-Programming Based Algorithm for Expert Load Balancing in Mixture-of-Experts Models](http://www.arxiv.org/pdf/2502.15451)\n\n[157\\. MIXTURE OF EXPERTS WITH MIXTURE OF PRECISIONS FOR TUNING QUALITY OF SERVICE](https://arxiv.org/pdf/2407.14417)\n\n[158\\. Lancet: Accelerating Mixture-of-Experts Training via Whole Graph Computation-Communication Overlapping](https://proceedings.mlsys.org/paper_files/paper/2024/file/339caf45a6fa281cae8adc6465343464-Paper-Conference.pdf)\n\n[159\\. 2024年大模型LLM还有哪些可研究的方向？](https://www.zhihu.com/question/637595961/answer/3394282253)\n\n[161\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[162\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[163\\. Yann LeCun, Yoshua Bengio et al. “Deep Learning.”](https://www.semanticscholar.org/paper/2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb)\n\n[164\\. Focus issue on energy-efficient neuromorphic devices, systems and algorithms](https://research.tudelft.nl/files/168282738/Mehonic_2023_Neuromorph._Comput._Eng._3_040201.pdf)\n\n[165\\. Integrated Optical Neural Network Processor](https://backend.orbit.dtu.dk/ws/files/332399205/PhD_thesis_178_sider_116_farvede.pdf)\n\n[166\\. Minseon Kang, Yongseok Lee et al. “Energy Efficiency of Machine Learning in Embedded Systems Using Neuromorphic Hardware.” Electronics](https://doi.org/10.3390/electronics9071069)\n\n[167\\. Energy Efficiency of Convolutional Neural Network Inference on FPGAs and Accelerated GPUs](https://odr.chalmers.se/bitstreams/703926f1-06e7-40e9-9cb0-4cdd0449eaa4/download)\n\n[168\\. Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning](https://m.riunet.upv.es/bitstream/handle/10251/204447/DesislavovMartinez-PlumedHernandez-Orallo%20-%20Trends%20in%20AI%20inference%20energy%20consumption%20Beyond%20the%20....pdf?sequence=1&isAllowed=y)\n\n[169\\. Characterizing neural dynamics using highly comparative time-series analysis](https://ocns.memberclicks.net/assets/CNS_Meetings/CNS2020/CNS2020-all.pdf)\n\n[170\\. Top Neuromorphic Computing Stocks for 2025](https://exoswan.com/neuromorphic-computing-stocks)\n\n[171\\. A walk on the frontier of energy electronics with power ultra-wide bandgap oxides and ultra-thin neuromorphic 2D materials](https://www.gallia-project.fr/wp-content/uploads/sites/15/2022/09/116871Y.pdf)\n\n[172\\. Neuromorphic Machine Learning Framework based on GTD](https://www.ijeiat.com/images/sliders/82019naveena%20paper.docx)\n\n[173\\. Neuromorphic Computing - an Edgier, Greener AI](https://neural.vision/blog/neuroai/Neuromorphic-Computing-Greener-Edgier-AI/)\n\n[181\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[182\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[183\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[184\\. INFERENCE SCALING LAWS: AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR LLM PROBLEM-SOLVING](https://openreview.net/pdf?id=VNckp7JEHn)\n\n[185\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[186\\. Exploring the Mystery of Influential Data for Mathematical Reasoning](https://openreview.net/pdf/e1507aa725a2285623b73ab44805d8ef793b9054.pdf)\n\n[187\\. Transformers Beyond NLP: Expanding Horizons in Machine Learning](https://www.irejournals.com/formatedpaper/1706957.pdf)\n\n[188\\. Neural Scaling Laws in Robotics](https://arxiv.org/pdf/2405.14005)\n\n[189\\. Qwen3 Technical Report](https://www.arxiv.org/pdf/2505.09388)\n\n[190\\. Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge](https://arxiv.org/pdf/2502.12501)\n\n[191\\. Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models](https://arxiv.org/pdf/2502.05610)\n\n[192\\. \\[2024 LLM 스터디\\] Scaling Laws for Neural Language Models (2020)](https://velog.io/@zvezda/2024-LLM-%EC%8A%A4%ED%84%B0%EB%94%94-Scaling-Laws-for-Neural-Language-Models-2020)\n\n[193\\. System 2 Reasoning Capabilities Are Nigh](https://arxiv.org/html/2410.03662v2)\n\n[194\\. CLLMs: Consistency Large Language Models](https://arxiv.org/html/2403.00835v1)\n\n[195\\. Compute Optimal Scaling of Skills: Knowledge vs Reasoning](https://fetcher.alphaxiv.org/v2/pdf/2503.10061v2)\n\n[196\\. MASTERMINDEval: A Simple But Scalable Reasoning Benchmark](https://openreview.net/attachment?id=H4donosutm&name=pdf)\n\n[197\\. Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling](https://openreview.net/pdf/cb8c559106c3bf5682e2e13302e65e6d1819ebf0.pdf)\n\n[198\\. A Solvable Attention for Neural Scaling Laws](https://openreview.net/pdf?id=wYxOMEzpkl)\n\n[199\\. Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving](https://openreview.net/pdf?id=j7DZWSc8qu)\n\n[200\\. Navigating Scaling Laws: Accelerating Vision Transformer's Training via Adaptive Strategies](https://openreview.net/pdf/3aa98ffc12b80d4cec6d191be9f3b74b78e77404.pdf)\n\n[201\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[202\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[203\\. OLMoE: OPEN MIXTURE-OF-EXPERTS LANGUAGE MODELS](https://openreview.net/pdf/de8bda951e013f5ec54c4273b79414f3930bdda9.pdf)\n\n[204\\. MOE-Pruner: Pruning Mixture-of-Experts Large Language Model Using the Hints from Its Router](https://arxiv.org/pdf/2410.12013)\n\n[205\\. Scaling Laws for Fine-Grained Mixture of Experts](https://openreview.net/pdf?id=Iizr8qwH7J)\n\n[206\\. Toward Efficient Inference for Mixture of Experts](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf)\n\n[207\\. Training Compute-Optimal Protein Language Models](https://papers.nips.cc/paper_files/paper/2024/file/8066ae1446b2bbccb5159587cc3b3bcc-Paper-Conference.pdf)\n\n[208\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[209\\. Mixture of Parrots: Mixtures of experts improve memorization more than reasoning](https://openreview.net/pdf?id=8YBLMm1yzz)\n\n[210\\. SEER-MoE: Sparse Expert Efficiency through Regularization for Mixture-of-Experts](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/final-projects/AlexMuzioAlexSunChuranHe.pdf)\n\n[211\\. Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark](https://openreview.net/pdf/2f52a3f2e1fb1e9a3ce45fb942d6529399d0d086.pdf)\n\n[212\\. MoE-CAP: Benchmarking Cost, Accuracy and Performance ...](https://arxiv.org/html/2505.11415v1)\n\n[213\\. EXAMINING POST-TRAINING QUANTIZATION FOR MIXTURE-OF-EXPERTS: A BENCHMARK](https://openreview.net/pdf?id=sMwYn2lZjO)\n\n[214\\. Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning](https://openreview.net/pdf/af78cd8aba64f9aa502655e6144612e50aee4f0d.pdf)\n\n[215\\. Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models](https://arxiv.org/pdf/2501.11873)\n\n[216\\. MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts](https://arxiv.org/pdf/2407.21770)\n\n[217\\. Mixture of Experts Pattern for Transformer Models](https://tinkerd.net/blog/machine-learning/mixture-of-experts/)\n\n[218\\. Mixture of A Million Experts](http://arxiv.org/html/2407.04153v1)\n\n[221\\. Adam Paszke, Sam Gross et al. “PyTorch: An Imperative Style, High-Performance Deep Learning Library.” ArXiv](https://arxiv.org/abs/1912.01703)\n\n[222\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[223\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[224\\. K. Simonyan, Andrew Zisserman. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” CoRR](https://arxiv.org/abs/1409.1556)\n\n[225\\. NVIDIA H100 NVL (SXM5) vs. NVIDIA RTX 4090 - Bizon Tech](https://bizon-tech.com/gpu-benchmarks/NVIDIA-H100-NVL-%28SXM5%29-vs-NVIDIA-RTX-4090/633vs637?srsltid=AfmBOopumuHxG9wy8IkBJrq44-Y3UZI6AMcMd19DSFcuh1rDtFfsjSeI)\n\n[226\\. Vinod Nair, Geoffrey E. Hinton. “Rectified Linear Units Improve Restricted Boltzmann Machines.” International Conference on Machine Learning](https://www.semanticscholar.org/paper/a538b05ebb01a40323997629e171c91aa28b8e2f)\n\n[227\\. Lecture 11: Large-Scale Distributed Training](https://cs231n.stanford.edu/slides/2025/lecture_11.pdf)\n\n[228\\. Notable AI Models Documentation](https://epoch.ai/data/notable-ai-models-documentation)\n\n[229\\. Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights](https://assets.caip.org/caip/CAIP%20NTIA%20Comment%20on%20Open%20Weights.pdf)\n\n[230\\. Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning](https://m.riunet.upv.es/bitstream/handle/10251/204447/DesislavovMartinez-PlumedHernandez-Orallo%20-%20Trends%20in%20AI%20inference%20energy%20consumption%20Beyond%20the%20....pdf?sequence=1&isAllowed=y)\n\n[231\\. A walk on the frontier of energy electronics with power ultra-wide bandgap oxides and ultra-thin neuromorphic 2D materials](https://www.gallia-project.fr/wp-content/uploads/sites/15/2022/09/116871Y.pdf)\n\n[232\\. Top Neuromorphic Computing Stocks for 2025](https://exoswan.com/neuromorphic-computing-stocks)\n\n[233\\. 筑波大学 計算科学研究センター 令和五年度 年次報告書](https://www.ccs.tsukuba.ac.jp/wp-content/uploads/sites/14/2024/10/R5_hpc.pdf)\n\n[234\\. 复苏之风起，AI赋能迎变而上——传媒互联网2023年中期策略](https://bigdata-s3.wmcloud.com/researchreport/2023-07/08b4ca2c74ac51adc08cb422eb518b2f.pdf)\n\n[235\\. NVIDIA® GPUs: Performance overview](https://assets.nebius.ai/download/gpu-performance-review.pdf)\n\n[236\\. Nvidia's H100 GPUs will consume more power than some countries — each GPU consumes 700W of power, 3.5 million are expected to be sold in the coming year | Tom's Hardware](https://www.tomshardware.com/tech-industry/nvidias-h100-gpus-will-consume-more-power-than-some-countries-each-gpu-consumes-700w-of-power-35-million-are-expected-to-be-sold-in-the-coming-year)\n\n[237\\. A Hybrid Iterative Neural Solver Based on Spectral Analysis for Parametric PDEs](https://arxiv.org/pdf/2408.08540)\n\n[238\\. NVIDIA GPUs: H100 vs. A100 | a detailed comparison - Gcore](https://gcore.com/blog/nvidia-h100-a100#:~:text=The%20H100%20offers%20undisputed%20improvements,deliver%20better%20performance%20and%20ROI.)\n\n[239\\. LOLA: LLM-Assisted Online Learning Algorithm for Content Experiments](https://thearf-org-unified-admin.s3.amazonaws.com/MSI_Report_24-150.pdf)\n\n[240\\. The Ultimate Guide to GPUs for Machine Learning in 2025](https://blog.spheron.network/the-ultimate-guide-to-gpus-for-machine-learning-in-2025)\n\n[241\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[242\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[243\\. Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models](https://arxiv.org/pdf/2502.05610)\n\n[244\\. Distributional Scaling Laws for Emergent Capabilities](https://fetcher.alphaxiv.org/v2/pdf/2502.17356v2)\n\n[245\\. Compute Optimal Scaling of Skills: Knowledge vs Reasoning](https://fetcher.alphaxiv.org/v2/pdf/2503.10061v2)\n\n[246\\. J. Kaplan, Sam McCandlish et al. “Scaling Laws for Neural Language Models.” ArXiv](https://arxiv.org/abs/2001.08361)\n\n[247\\. A Solvable Attention for Neural Scaling Laws](https://openreview.net/pdf?id=wYxOMEzpkl)\n\n[248\\. Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling](https://openreview.net/pdf/cb8c559106c3bf5682e2e13302e65e6d1819ebf0.pdf)\n\n[249\\. TOWARDS NEURAL SCALING LAWS FOR TIME SERIES FOUNDATION MODELS](https://openreview.net/pdf/3c5704b08e6b5bb6e7a0691a67af9576f04b4c04.pdf)\n\n[250\\. Exploring the Mystery of Influential Data for Mathematical Reasoning](https://openreview.net/pdf/e1507aa725a2285623b73ab44805d8ef793b9054.pdf)\n\n[251\\. Neural Scaling Laws in Robotics](https://arxiv.org/pdf/2405.14005)\n\n[252\\. \\[2024 LLM 스터디\\] Scaling Laws for Neural Language Models (2020)](https://velog.io/@zvezda/2024-LLM-%EC%8A%A4%ED%84%B0%EB%94%94-Scaling-Laws-for-Neural-Language-Models-2020)\n\n[253\\. Scaling Law for Recommendation Models: Towards General-Purpose User Representations](https://ojs.aaai.org/index.php/AAAI/article/view/25582/25354)\n\n[254\\. Joel Hestness, Sharan Narang et al. “Deep Learning Scaling is Predictable, Empirically.” ArXiv](https://arxiv.org/abs/1712.00409)\n\n[255\\. T. Henighan, J. Kaplan et al. “Scaling Laws for Autoregressive Generative Modeling.” ArXiv](https://arxiv.org/abs/2010.14701)\n\n[256\\. CAN 1B LLM SURPASS 405B LLM? RETHINKING COMPUTE-OPTIMAL TEST-TIME SCALING](https://openreview.net/pdf?id=CvjX9Lhpze)\n\n[257\\. Maor Ivgi, Y. Carmon et al. “Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/2022.findings-emnlp.544)\n\n[258\\. MuPT: A Generative Symbolic Music Pre-trained Transformer](https://openreview.net/pdf/5b85f273e65fef4e6e7ff8adaecfe1fbf248f367.pdf)\n\n[259\\. Enhancing LLM Reasoning by Scaling Multi-round Test ...](https://arxiv.org/html/2503.19855v1)\n\n[260\\. 盘一盘,2017年Transformer之后,LLM领域的重要论文](http://h5.ifeng.com/c/vivoArticle/v002f5fUxu1QFnWPHqli2F9rQM8kET-_Z9zdhWOOD3mwuktA__)\n\n[261\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[262\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[263\\. Toward Efficient Inference for Mixture of Experts](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf)\n\n[264\\. Scaling Laws for Fine-Grained Mixture of Experts](https://openreview.net/pdf?id=Iizr8qwH7J)\n\n[265\\. OLMoE: OPEN MIXTURE-OF-EXPERTS LANGUAGE MODELS](https://openreview.net/pdf/de8bda951e013f5ec54c4273b79414f3930bdda9.pdf)\n\n[266\\. DYNAMIC MIXTURE OF EXPERTS: AN AUTO-TUNING APPROACH FOR EFFICIENT TRANSFORMER MODELS](https://openreview.net/pdf?id=T26f9z2rEe)\n\n[267\\. Mixture of Parrots: Mixtures of experts improve memorization more than reasoning](https://openreview.net/pdf?id=8YBLMm1yzz)\n\n[268\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[269\\. Appendix](https://papers.nips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Supplemental-Conference.pdf)\n\n[270\\. Exploiting Activation Sparsity with Dense to Dynamic-k Mixture-of-Experts Conversion](https://proceedings.neurips.cc/paper_files/paper/2024/file/4c2092ec0b1370cce3fb5965ab255fae-Paper-Conference.pdf)\n\n[271\\. NO NEED TO TALK: ASYNCHRONOUS MIXTURE OF LANGUAGE MODELS](https://david.grangier.info/papers/2025/filippova_katharopoulos_grangier_collobert_iclr25.pdf)\n\n[272\\. Mixture of Parrots 🦜🦜🦜: Experts Improve Memorization More ...](https://kempnerinstitute.harvard.edu/research/deeper-learning/mixture-of-parrots-experts-improve-memorization-more-than-reasoning/)\n\n[273\\. Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models](https://arxiv.org/pdf/2501.11873)\n\n[274\\. Mixture of Experts Pattern for Transformer Models](https://tinkerd.net/blog/machine-learning/mixture-of-experts/)\n\n[275\\. Revisiting SMoE Language Models by Evaluating Inefficiencies with Task Specific Expert Pruning](https://assets.amazon.science/0b/fa/4e8cde264f3b94780d10c371bcdc/revisiting-smoe-language-models-by-evaluating-inefficiencies-with-task-specific-expert-pruning.pdf)\n\n[276\\. Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning](https://openreview.net/pdf/af78cd8aba64f9aa502655e6144612e50aee4f0d.pdf)\n\n[277\\. MoE-CAP: Benchmarking Cost, Accuracy and Performance ...](https://arxiv.org/html/2505.11415v1)\n\n[278\\. Mixture of Tokens: Continuous MoE through Cross ...](https://arxiv.org/html/2310.15961v2)\n\n[281\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[282\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[283\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[284\\. Olga Russakovsky, Jia Deng et al. “ImageNet Large Scale Visual Recognition Challenge.” International Journal of Computer Vision](https://doi.org/10.1007/s11263-015-0816-y)\n\n[285\\. Informatik in der Land-, Forst- und Ernährungswirtschaft](https://www.openagrar.de/servlets/MCRFileNodeServlet/openagrar_derivate_00064982/GIL_2025.pdf)\n\n[286\\. 2025 \"人工智能+\" 行业发展蓝皮书](https://www.acem.sjtu.edu.cn/ueditor/jsp/upload/file/20250427/1745731689854071357.pdf)\n\n[287\\. Priya Goyal, Piotr Dollár et al. “Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.” ArXiv](https://arxiv.org/abs/1706.02677)\n\n[288\\. Forecasting GPU Performance for Deep Learning Training and Inference](https://arxiv.org/pdf/2407.13853)\n\n[289\\. Advancing Neural Networks: Innovations and Impacts on Energy Consumption](https://iris.cnr.it/retrieve/4f01ebc4-3e5a-45c4-9734-57e59454ce9b/Advancing%20Neural%20Networks%3A%20Innovations%20and%20Impacts%20on%20Energy%20Consumption.pdf)\n\n[290\\. Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning](https://m.riunet.upv.es/bitstream/handle/10251/204447/DesislavovMartinez-PlumedHernandez-Orallo%20-%20Trends%20in%20AI%20inference%20energy%20consumption%20Beyond%20the%20....pdf?sequence=1&isAllowed=y)\n\n[291\\. Empirical Measurements of AI Training Power Demand on ...](https://arxiv.org/html/2412.08602v2)\n\n[292\\. Benchmark GPU - PyTorch, ResNet50](https://pavlokhmel.com/benchmark-gpu-pytorch-resnet50.html)\n\n[293\\. Analyzing GPU Energy Consumption in Data Movement and Storage](https://hal.umontpellier.fr/hal-04604802v1/document)\n\n[294\\. 英特尔裁员求生存,砍业务,在中国推定制版AI芯片-icspec](https://www.icspec.com/news/article-details/2207091)\n\n[295\\. Case Study: Empirical Measurements of AI Training Power Demand on a GPU-Accelerated Node](https://indico.cern.ch/event/1450798/contributions/6207689/attachments/2963185/5212326/HEPiX-%2011-7-24.pdf)\n\n[296\\. Advanced Profiling Techniques For Evaluating GPU Computing Efficiency Executing ML Applications](https://theses.hal.science/tel-04742193/file/DELESTRAC_2024_archivage.pdf)\n\n[297\\. Region Similarity Representation Learning](https://zhuanlan.zhihu.com/p/675271859)\n\n[298\\. NVIDIA H100 vs NVIDIA A100](https://www.amax.com/nvidia-h100-vs-nvidia-a100/)\n\n[299\\. NVIDIA GPUs: H100 vs. A100 | a detailed comparison - Gcore](https://gcore.com/blog/nvidia-h100-a100#:~:text=According%20to%20benchmarks%20by%20NVIDIA,half%20the%20time%20to%20complete.)\n\n[300\\. 性能实测](https://backup.autodl.com/docs/gpu_perf/)\n\n[301\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[302\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[303\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[304\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[305\\. Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models](https://arxiv.org/pdf/2502.05610)\n\n[306\\. J. Kaplan, Sam McCandlish et al. “Scaling Laws for Neural Language Models.” ArXiv](https://arxiv.org/abs/2001.08361)\n\n[307\\. Distributional Scaling Laws for Emergent Capabilities](https://fetcher.alphaxiv.org/v2/pdf/2502.17356v2)\n\n[308\\. TOWARDS NEURAL SCALING LAWS FOR TIME SERIES FOUNDATION MODELS](https://openreview.net/pdf/3c5704b08e6b5bb6e7a0691a67af9576f04b4c04.pdf)\n\n[309\\. Exploring the Mystery of Influential Data for Mathematical Reasoning](https://openreview.net/pdf/e1507aa725a2285623b73ab44805d8ef793b9054.pdf)\n\n[310\\. A Solvable Attention for Neural Scaling Laws](https://openreview.net/pdf?id=wYxOMEzpkl)\n\n[311\\. \\[2024 LLM 스터디\\] Scaling Laws for Neural Language Models (2020)](https://velog.io/@zvezda/2024-LLM-%EC%8A%A4%ED%84%B0%EB%94%94-Scaling-Laws-for-Neural-Language-Models-2020)\n\n[312\\. Neural Scaling Laws in Robotics](https://arxiv.org/pdf/2405.14005)\n\n[313\\. Compute Optimal Scaling of Skills: Knowledge vs Reasoning](https://fetcher.alphaxiv.org/v2/pdf/2503.10061v2)\n\n[314\\. Neural Scaling Laws on Graphs](https://arxiv.org/html/2402.02054v1)\n\n[315\\. Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling](https://openreview.net/pdf/cb8c559106c3bf5682e2e13302e65e6d1819ebf0.pdf)\n\n[316\\. INFERENCE SCALING LAWS: AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR LLM PROBLEM-SOLVING](https://openreview.net/pdf?id=VNckp7JEHn)\n\n[317\\. Scaling Law for Recommendation Models: Towards General-Purpose User Representations](https://ojs.aaai.org/index.php/AAAI/article/view/25582/25354)\n\n[318\\. 盘一盘,2017年Transformer之后,LLM领域的重要论文](http://h5.ifeng.com/c/vivoArticle/v002f5fUxu1QFnWPHqli2F9rQM8kET-_Z9zdhWOOD3mwuktA__)\n\n[319\\. CAN 1B LLM SURPASS 405B LLM? RETHINKING COMPUTE-OPTIMAL TEST-TIME SCALING](https://openreview.net/pdf?id=CvjX9Lhpze)\n\n[320\\. Enhancing LLM Reasoning by Scaling Multi-round Test ...](https://arxiv.org/html/2503.19855v1)\n\n[321\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[322\\. OLMoE: OPEN MIXTURE-OF-EXPERTS LANGUAGE MODELS](https://openreview.net/pdf/de8bda951e013f5ec54c4273b79414f3930bdda9.pdf)\n\n[323\\. Scaling Laws for Fine-Grained Mixture of Experts](https://openreview.net/pdf?id=Iizr8qwH7J)\n\n[324\\. DYNAMIC MIXTURE OF EXPERTS: AN AUTO-TUNING APPROACH FOR EFFICIENT TRANSFORMER MODELS](https://openreview.net/pdf?id=T26f9z2rEe)\n\n[325\\. Toward Efficient Inference for Mixture of Experts](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf)\n\n[326\\. MOE-Pruner: Pruning Mixture-of-Experts Large Language Model Using the Hints from Its Router](https://arxiv.org/pdf/2410.12013)\n\n[327\\. Mixture of Parrots: Mixtures of experts improve memorization more than reasoning](https://openreview.net/pdf?id=8YBLMm1yzz)\n\n[328\\. Lancet: Accelerating Mixture-of-Experts Training via Whole Graph Computation-Communication Overlapping](https://proceedings.mlsys.org/paper_files/paper/2024/file/339caf45a6fa281cae8adc6465343464-Paper-Conference.pdf)\n\n[329\\. Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models](https://arxiv.org/pdf/2501.11873)\n\n[330\\. MIXTURE OF EXPERTS WITH MIXTURE OF PRECISIONS FOR TUNING QUALITY OF SERVICE](https://arxiv.org/pdf/2407.14417)\n\n[331\\. A Survey on Mixture of Experts in Large Language Models](https://arxiv.org/pdf/2407.06204)\n\n[332\\. Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks](https://arxiv.org/pdf/2401.02731)\n\n[333\\. MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts](https://arxiv.org/pdf/2407.21770)\n\n[334\\. Mixture of Experts Pattern for Transformer Models](https://tinkerd.net/blog/machine-learning/mixture-of-experts/)\n\n[335\\. MoE-CAP: Benchmarking Cost, Accuracy and Performance ...](https://arxiv.org/html/2505.11415v1)\n\n[336\\. Mixture of Tokens: Continuous MoE through Cross ...](https://arxiv.org/html/2310.15961v2)\n\n[337\\. Samira Abnar, Harshay Shah et al. “Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models.”](https://arxiv.org/abs/2501.12370)\n\n[338\\. Mixture of A Million Experts](http://arxiv.org/html/2407.04153v1)\n\n[341\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[342\\. Sergey Ioffe, Christian Szegedy. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” ArXiv](https://arxiv.org/abs/1502.03167)\n\n[343\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[344\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[345\\. 2025 \"人工智能+\" 行业发展蓝皮书](https://www.acem.sjtu.edu.cn/ueditor/jsp/upload/file/20250427/1745731689854071357.pdf)\n\n[346\\. Priya Goyal, Piotr Dollár et al. “Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.” ArXiv](https://arxiv.org/abs/1706.02677)\n\n[347\\. ResNet-50 v1.5 for MXNet](https://github.com/NVIDIA/DeepLearningExamples/blob/master/MxNet/Classification/RN50v1.5/README.md)\n\n[348\\. NVIDIA H100 vs A100: Detailed GPU Comparison for 2024](https://docs.jarvislabs.ai/blog/h100vsa100#:~:text=A:%20The%20A100%20does%20not,and%20efficiency%20with%20FP8%20operations.)\n\n[349\\. Forecasting GPU Performance for Deep Learning Training and Inference](https://arxiv.org/pdf/2407.13853)\n\n[350\\. Neuromorphic Principles for Efficient Large Language Models on Intel Loihi 2](https://openreview.net/pdf?id=qaDM1R2nlm)\n\n[351\\. What's new in neuromorphic computing? (July 2025)](https://quickmarketpitch.com/blogs/news/neuromorphic-computing-news)\n\n[352\\. NVIDIA H100 NVL (SXM5) vs. NVIDIA RTX 4090 - Bizon Tech](https://bizon-tech.com/gpu-benchmarks/NVIDIA-H100-NVL-%28SXM5%29-vs-NVIDIA-RTX-4090/633vs637?srsltid=AfmBOopumuHxG9wy8IkBJrq44-Y3UZI6AMcMd19DSFcuh1rDtFfsjSeI)\n\n[353\\. NVIDIA H100 GPU: Uncovering the Engine Behind Next-Generation HPC](https://community.fs.com/article/nvidia-h100-gpu-uncovering-the-engine-behind-nextgeneration-ai-and-hpc.html)\n\n[354\\. Volta Tensor Core GPU Achieves New AI Performance Milestones](https://developer.nvidia.com/blog/tensor-core-ai-performance-milestones/)\n\n[355\\. Global Neuron Shape Reasoning with Point Affinity Transformers](https://www.biorxiv.org/content/10.1101/2024.11.24.625067v1.full.pdf)\n\n[356\\. NVIDIA GPUs H100 vs. A100 - Architecture, Performance ...](https://www.trgdatacenters.com/resource/h100-vs-a100/)\n\n[357\\. Richard Liaw, Romil Bhardwaj et al. “HyperSched: Dynamic Resource Reallocation for Model Development on a Deadline.” Proceedings of the ACM Symposium on Cloud Computing](https://doi.org/10.1145/3357223.3362719)\n\n[358\\. NVIDIA GPUs: H100 vs. A100 | A Detailed Comparison](https://gcore.com/blog/nvidia-h100-a100/)\n\n[359\\. Momentum contrast for Unsupervised representation learning (MoCo)](https://cmp.felk.cvut.cz/~toliageo/rg/papers/slides/moco.pdf)\n\n[360\\. Breaking MLPerf Training Records with NVIDIA H100 GPUs](https://databloom.com/2023/06/27/breaking-mlperf-training-records-with-nvidia-h100-gpus/)\n\n[361\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[362\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[363\\. Exploring the Mystery of Influential Data for Mathematical Reasoning](https://openreview.net/pdf/e1507aa725a2285623b73ab44805d8ef793b9054.pdf)\n\n[364\\. INFERENCE SCALING LAWS: AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR LLM PROBLEM-SOLVING](https://openreview.net/pdf?id=VNckp7JEHn)\n\n[365\\. Neural Scaling Laws on Graphs](https://arxiv.org/html/2402.02054v1)\n\n[366\\. Qwen3 Technical Report](https://www.arxiv.org/pdf/2505.09388)\n\n[367\\. Neural Scaling Laws in Robotics](https://arxiv.org/pdf/2405.14005)\n\n[368\\. TOWARDS NEURAL SCALING LAWS FOR TIME SERIES FOUNDATION MODELS](https://openreview.net/pdf/3c5704b08e6b5bb6e7a0691a67af9576f04b4c04.pdf)\n\n[369\\. Scaling Laws For Precision](https://openreview.net/pdf?id=wg1PCg3CUP)\n\n[370\\. Compute Optimal Scaling of Skills: Knowledge vs Reasoning](https://fetcher.alphaxiv.org/v2/pdf/2503.10061v2)\n\n[371\\. Beyond neural scaling laws: beating power law scaling via data pruning](https://proceedings.neurips.cc/paper_files/paper/2022/file/7b75da9b61eda40fa35453ee5d077df6-Paper-Conference.pdf)\n\n[372\\. A Solvable Attention for Neural Scaling Laws](https://openreview.net/pdf?id=wYxOMEzpkl)\n\n[373\\. Scaling Laws for Downstream Task Performance in Machine Translation](https://arxiv.org/pdf/2402.04177)\n\n[374\\. 盘一盘,2017年Transformer之后,LLM领域的重要论文](http://h5.ifeng.com/c/vivoArticle/v002f5fUxu1QFnWPHqli2F9rQM8kET-_Z9zdhWOOD3mwuktA__)\n\n[375\\. Wenxuan Yang, Qingqu Wei et al. “Scaling Laws for Data-Efficient Visual Transfer Learning.”](https://arxiv.org/abs/2504.13219)\n\n[376\\. Distributional Scaling Laws for Emergent Capabilities](https://fetcher.alphaxiv.org/v2/pdf/2502.17356v2)\n\n[377\\. Reconciling Kaplan and Chinchilla Scaling Laws](https://openreview.net/pdf/f91518ed8d3298ef5e2625a7c2b5c611cfb94f60.pdf)\n\n[381\\. OLMoE: OPEN MIXTURE-OF-EXPERTS LANGUAGE MODELS](https://openreview.net/pdf/de8bda951e013f5ec54c4273b79414f3930bdda9.pdf)\n\n[382\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[383\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[384\\. Toward Efficient Inference for Mixture of Experts](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf)\n\n[385\\. Scaling Laws for Fine-Grained Mixture of Experts](https://openreview.net/pdf?id=Iizr8qwH7J)\n\n[386\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[387\\. DYNAMIC MIXTURE OF EXPERTS: AN AUTO-TUNING APPROACH FOR EFFICIENT TRANSFORMER MODELS](https://openreview.net/pdf?id=T26f9z2rEe)\n\n[388\\. Mixture of Parrots: Mixtures of experts improve memorization more than reasoning](https://openreview.net/pdf?id=8YBLMm1yzz)\n\n[389\\. MoE-CAP: Benchmarking Cost, Accuracy and Performance ...](https://arxiv.org/html/2505.11415v1)\n\n[390\\. Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models](https://arxiv.org/pdf/2501.11873)\n\n[391\\. Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks](https://arxiv.org/pdf/2401.02731)\n\n[392\\. MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts](https://arxiv.org/pdf/2407.21770)\n\n[393\\. Mixture of insightTful Experts (MoTE): The Synergy of Reasoning Chains and Expert Mixtures in Self-Alignment](https://arxiv.org/pdf/2405.00557)\n\n[394\\. MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems](https://arxiv.org/pdf/2412.07067)\n\n[395\\. MoEUT: Mixture-of-Experts Universal Transformers](https://arxiv.org/pdf/2405.16039)\n\n[396\\. Mixture of Experts Pattern for Transformer Models](https://tinkerd.net/blog/machine-learning/mixture-of-experts/)\n\n[397\\. Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning](https://openreview.net/pdf/af78cd8aba64f9aa502655e6144612e50aee4f0d.pdf)\n\n[398\\. Mixture of A Million Experts](http://arxiv.org/html/2407.04153v1)\n\n[399\\. Samira Abnar, Harshay Shah et al. “Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models.”](https://arxiv.org/abs/2501.12370)\n\n[401\\. P. Merolla, J. Arthur et al. “A million spiking-neuron integrated circuit with a scalable communication network and interface.” Science](https://doi.org/10.1126/science.1254642)\n\n[402\\. E. Neftci, H. Mostafa et al. “Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-based optimization to spiking neural networks.” IEEE Signal Processing Magazine](https://doi.org/10.1109/MSP.2019.2931595)\n\n[403\\. Neuromorphic Principles for Efficient Large Language Models on Intel Loihi 2](https://openreview.net/pdf?id=qaDM1R2nlm)\n\n[404\\. The neurobench framework for benchmarking neuromorphic computing algorithms and systems](https://physiologie.unibe.ch/PublicationPDF/Yik2025NeuroBench.pdf)\n\n[405\\. B. Benjamin, P. Gao et al. “Neurogrid: A Mixed-Analog-Digital Multichip System for Large-Scale Neural Simulations.” Proceedings of the IEEE](https://doi.org/10.1109/JPROC.2014.2313565)\n\n[406\\. S. Furber, F. Galluppi et al. “The SpiNNaker Project.” Proceedings of the IEEE](https://doi.org/10.1109/JPROC.2014.2304638)\n\n[407\\. The 2025 Edge AI Technology Report](https://www.hkdca.com/wp-content/uploads/2025/06/edge-ai-technology-report-2025.pdf)\n\n[408\\. Top 15 Neuromorphic Computing Examples in 2025](https://www.weetechsolution.com/blog/neuromorphic-computing-examples)\n\n[409\\. S. Shrestha, G. Orchard. “SLAYER: Spike Layer Error Reassignment in Time.” Neural Information Processing Systems](https://arxiv.org/abs/1810.08646)\n\n[410\\. Taking Neuromorphic Computing to the Next Level with Loihi 2](https://download.intel.com/newsroom/2021/new-technologies/neuromorphic-computing-loihi-2-brief.pdf)\n\n[411\\. Intel Reveals 4nm Loihi 2 Neuromorphic Research Chip](https://www.tomshardware.com/news/intel-reveals-loihi-2-neuromorphic-research-chip)\n\n[412\\. 2025 \"人工智能+\" 行业发展蓝皮书](http://www.sccio.cn/uploads/20250522/8696496143e2c9e662e2e45890c9c1b4.pdf)\n\n[413\\. Applied and Computational Engineering: Proceedings of the 2023 International Conference on Machine Learning and Automation](https://www.ewadirect.com/media/var/media/upload/vol_pdf/ace/41.pdf)\n\n[414\\. Neuromorphic neuromodulation: Towards the next generation of closed-loop neurostimulation](https://academic.oup.com/pnasnexus/article-pdf/3/11/pgae488/60680820/pgae488.pdf)"
  },
  {
    "id": 38,
    "question": "Does AI's ability to generate physically coherent videos indicate progress in understanding the physical world, or is it just pattern matching?",
    "response": "**Research Report**\n\n**To:** Interested Parties\n\n**From:** Expert Researcher\n\n**Date:** July 27, 2025\n\n**Subject:** Analysis of Physical Understanding in AI Video Generation: Genuine World Modeling or Advanced Pattern Matching?\n\n**1.0 Introduction: The Dawn of AI-Powered Cinematography**\n\nAs of mid-2025, the field of artificial intelligence has reached a remarkable milestone: the generation of high-fidelity, physically coherent, and often narratively complex videos from simple text prompts. Models developed by leading research labs, such as OpenAI's _Sora_, Google DeepMind's _Veo 3_, ByteDance's _Seedance_, Tencent's _HunyuanVideo_, and Kuaishou's _Kling_ series, are capable of producing video content at resolutions up to 4K with a level of realism in motion, lighting, and texture that begins to rival traditional computer-generated imagery (CGI) \\[4\\]\\[6\\]\\[5\\].\n\nThese systems are predominantly built upon a powerful combination of **diffusion models** and **transformer architectures** \\[16\\]\\[9\\]\\[17\\]. By training on vast datasets of visual and motion data, these models learn to generate new video sequences that are statistically similar to the data they have seen \\[6\\]. This technological leap has ignited a profound debate within the AI community. Does the stunning ability of these models to depict seemingly plausible physical interactions—a wave crashing, a ball bouncing, an object shattering—signify genuine progress in building AIs that \"understand\" the physical world? Or are we witnessing a grand illusion, an exceptionally sophisticated form of pattern matching that mimics the physics of our world without grasping its underlying principles?\n\nThis report synthesizes evidence from technical analyses, benchmark evaluations, and documented failure modes to critically examine this question. It argues that while current AI systems demonstrate emergent capabilities suggestive of nascent world models, their performance is primarily attributable to advanced pattern matching. Rigorous quantitative analysis reveals a significant chasm between visual realism and true physical reasoning, and the architectural foundations of these models explain their characteristic and predictable failures in simulating the laws of physics.\n\n**2.0 The Case for Sophisticated Pattern Matching: Unmasking the Illusion**\n\nThe most compelling argument that current video generation models are pattern matchers rather than physicists lies in a direct examination of their architecture, their learning process, and their documented failures. While the output can be visually stunning, a closer look reveals a brittle and superficial grasp of reality.\n\n**2.1 Architectural Foundations: Learning Correlations, Not Causation**\n\nModern text-to-video systems like _Sora_, _Veo 3_, and _Seedance_ are built on **diffusion-transformer architectures** \\[16\\]\\[9\\]\\[6\\]. The diffusion process learns to reverse a process of gradually adding noise to video data, effectively learning a probability distribution of realistic video sequences. The transformer architecture allows the model to handle long-range dependencies across video frames and integrate conditioning information, such as text prompts \\[4\\].\n\nThis process is fundamentally statistical. The model learns intricate correlations between textual descriptions and sequences of pixels. For example, it learns that the text \"a glass falls and shatters\" is associated with a sequence of images depicting a glass object descending and then transforming into many small, sharp fragments. It does not, however, learn the concepts of gravity, force, stress, or fracture mechanics. Its \"knowledge\" is a high-dimensional mapping of patterns, not a causal, generative model of the world \\[93\\]\\[153\\]. The integration of Large Language Models (LLMs) and multimodal training, as seen with CLIP, enhances the model's ability to understand prompts by learning from massive datasets of (image, text) pairs, but this remains a process of learning associations, not first principles \\[3\\]\\[10\\].\n\n**2.2 Catalog of Failures: When the Simulation Breaks Down**\n\nThe most direct evidence against genuine physical understanding comes from the common failure modes of even state-of-the-art models. These are not random errors but systematic breakdowns that occur precisely when a situation requires reasoning beyond learned statistical regularities.\n\n**Inaccurate Simulation of Basic Physics:** _Sora_ has been widely documented to struggle with accurately modeling fundamental physical interactions. A frequently cited example is its inability to correctly simulate glass breaking; generated videos may show glass bizarrely growing or deforming rather than shattering upon impact \\[83\\]\\[85\\]\\[86\\]. Similar failures are observed in fluid dynamics, complex object collisions, and the correct interaction of light and shadow \\[94\\]\\[140\\].\n\n**Incorrect Object State Changes:** The models lack a persistent understanding of object properties. A classic failure case involves a person eating a cookie, where the generated video fails to show bite marks on the cookie after a bite is taken \\[83\\]\\[148\\]\\[149\\]. The model generates a plausible-looking scene of \"eating\" but fails to correctly update the state of the objects involved, betraying a lack of object permanence and causality.\n\n**Violations of Spatiotemporal Consistency:** In longer video samples, the models' simulated world often frays at the seams. Objects may spontaneously appear or disappear, character identities can morph, and the laws of physics can become inconsistent over time \\[83\\]\\[84\\]\\[148\\]. This points to an inability to maintain a coherent, persistent world model beyond a short temporal window.\n\n**Failure of Causal Reasoning:** The models exhibit a profound difficulty with cause and effect. They can generate a sequence of events that is visually plausible but logically backward \\[140\\]\\[140\\]. This limitation in simulating dynamic interactions and predicting how actions alter outcomes reveals a core deficit in causal reasoning, a cornerstone of physical understanding \\[94\\]\\[146\\]\\[149\\].\n\n**2.3 The Architectural Roots of Failure**\n\nThese failures are not arbitrary but are direct consequences of the models' architectures. The reliance on **latent diffusion models (LDMs)** and **U-Net architectures** introduces specific vulnerabilities.\n\nLDMs operate on a compressed, latent representation of the video data to save computational costs. However, the autoencoder that creates this latent space may not perfectly preserve all the fine-grained physical information necessary for accurate simulation. If crucial details about object boundaries, mass, or velocity are lost in compression, the model cannot reconstruct them accurately, leading to physical impossibilities \\[195\\]\\[201\\]\\[206\\].\n\nThe **U-Net architecture**, a staple in diffusion models, uses a series of downsampling and upsampling blocks \\[199\\]\\[202\\]\\[252\\]. While effective for image synthesis, the downsampling process inherently involves information loss \\[209\\]\\[310\\]. This loss of high-frequency spatial detail at the deepest layers of the U-Net can contribute to the model's inability to simulate rigid structures accurately or maintain object integrity throughout a video sequence \\[205\\]. The models are \"painting it well\" based on learned visual textures, but are not \"painting it right\" based on an underlying structural model \\[142\\].\n\n**3.0 The Case for Emerging World Models: Glimmers of Understanding**\n\nDespite these significant limitations, it would be a mistake to dismiss the progress entirely. The sheer quality and consistency of the generated videos suggest that something more than simple 2D frame-by-frame mimicry is occurring. These models exhibit emergent capabilities that hint at the formation of nascent, implicit world models.\n\n**3.1 Emergent 3D Consistency and Object Permanence**\n\nA key advancement in models like _Sora_ is a marked improvement in temporal and 3D consistency. When generating a video with a moving camera, the model can often maintain the identity and appearance of objects as they are occluded and then reappear. It demonstrates an understanding that objects continue to exist even when out of view and that they should look consistent from different angles \\[7\\]. This capability, along with the efforts of other models like Alibaba's _Wan2.1_ to achieve 3D consistency, suggests the model is not just generating a flat sequence of images but is operating on an internal representation that has some properties of a 3D scene \\[7\\]\\[7\\]. Models like _Runway ML Gen 3_ and _Kling 2.1_ are also specifically noted for their strong temporal and scene consistency, which is a prerequisite for physical plausibility \\[5\\]\\[6\\].\n\n**3.2 Measuring What Matters: The Rise of Physical Reasoning Benchmarks**\n\nRecognizing the limitations of traditional metrics like Fréchet Video Distance (FVD), which measure visual quality but not physical plausibility, the research community has developed a new class of benchmarks designed specifically to probe physical understanding \\[26\\]\\[41\\].\n\n**Physics-IQ:** This benchmark is a leading example, designed to evaluate whether a model can predict the continuation of real-world videos that require deep physical reasoning. It deliberately uses out-of-distribution scenarios where simple pattern reproduction is insufficient \\[26\\]\\[27\\]. It uses specialized metrics like **Spatial IoU** (Intersection over Union) and **Spatiotemporal IoU** to quantify whether the \"action\" in the generated video happens in the right place and at the right time compared to reality \\[26\\]\\[119\\].\n\n**PhyCoBench:** This benchmark focuses on text-to-video models, providing prompts that describe seven fundamental physical phenomena, such as gravity, collision, and fluid dynamics. It includes an automated evaluation tool, **PhyCoPredictor**, to assess the physical coherence of the generated output \\[44\\]\\[52\\]\\[56\\].\n\n**PisaBench and VMBench:** Other notable efforts include PisaBench, which challenges models with falling and colliding objects \\[44\\], and VMBench, which introduces metrics aligned with human perception of motion, such as Temporal Coherence, Motion Smoothness, and Object Integrity \\[30\\]\\[31\\]\\[43\\].\n\nThe very existence of these sophisticated benchmarks signifies a maturation of the field, moving beyond visual appeal to demand genuine physical reasoning.\n\n**3.3 A Quantitative Reality Check: The Gap Between Realism and Reality**\n\nThe results from these new benchmarks provide the most sobering and conclusive evidence. While models like _Sora_ and Google's _VideoPoet_ achieve high scores on visual realism, their performance on physical reasoning tasks is abysmal.\n\nOn the **Physics-IQ benchmark**, where a score of 100.0% represents natural physical variance, the best-performing pure neural network model, _VideoPoet_, achieved a score of only **24.1%** \\[26\\]\\[119\\]. _Sora_ (in image-to-video mode) scored even lower at **10.0%** \\[175\\]\\[238\\]. These scores starkly quantify the massive gap between creating a video that _looks_ real and one that _is_ physically sound \\[26\\]. The high visual quality of these models masks a severe deficiency in their understanding of the world, a deficiency that is laid bare by targeted, quantitative evaluation.\n\n**4.0 Bridging the Gap: The Next Frontier of Physics-Integrated AI**\n\nThe limitations of pure data-driven approaches have spurred a new line of research focused on explicitly integrating physical laws into the AI models themselves. This represents a shift from hoping physical understanding will _emerge_ from data to ensuring it is present by _design_.\n\nModels like **PhysGen** exemplify this hybrid approach. _PhysGen_ combines traditional, model-based physics engines with data-driven deep learning to generate videos that are not only visually realistic but also grounded in and constrained by physical simulation \\[1\\]. This allows for plausible image-space dynamics and precise control over the generated content. Similarly, the **PhyT2V** approach uses an LLM to extract physical rules from a prompt and refine the video generation process to improve physical realism \\[27\\].\n\nHowever, a critical gap in the current research landscape, as of this report's date, is the lack of direct, peer-reviewed, quantitative comparisons of these physics-integrated models against pure neural architectures on standardized benchmarks. While _PhysGen_ is described as producing physically-grounded videos, its specific score on the Physics-IQ benchmark, and how it compares to _Sora's_ 10.0% score, is not available in the provided search results \\[177\\], Query: \"Report quantitative performance of PhysGen versus Sora...\"). This head-to-head comparison will be a crucial next step in validating the efficacy of physics-integrated approaches.\n\n**5.0 The Unproven Hypothesis: From Physical Coherence to General Reasoning**\n\nA tantalizing, long-term question is whether building AIs with a robust understanding of the physical world could be a stepping stone toward more general forms of artificial intelligence. The hypothesis is that a rich \"world model\" could provide a foundation for abstract reasoning, planning, and problem-solving in non-physical domains.\n\nTo investigate this, this report's research sought evidence of transfer learning—specifically, whether improvements in physical coherence have been shown to boost performance on unrelated abstract reasoning tasks, such as **Raven's Progressive Matrices (RPM)**, a classic test of fluid intelligence.\n\nThe findings are unequivocal: as of July 2025, there is no peer-reviewed, empirical evidence to support this hypothesis. Searches for studies demonstrating that training AI systems with \"physical coherence metrics\" or \"physical coherence loss functions\" leads to performance gains on RPM or other general reasoning tasks came up empty \\[263\\]\\[276\\]Query: \"Has training AI systems with physical coherence loss functions...\", \\[316\\]\\[333\\]Query: \"Have peer-reviewed studies demonstrated Raven's Progressive Matrices performance...\"). The link between developing a physical world model and achieving a more general reasoning capability remains purely speculative. The research into transfer learning from cognitive training in humans shows mixed and narrow results, providing no strong parallel to support the idea of broad transfer in AI systems \\[157\\]\\[162\\]\\[164\\].\n\n**6.0 Conclusion: Sophisticated Parrots, Not Nascent Physicists**\n\nThe ability of modern AI to generate physically coherent video is a monumental achievement in engineering, but it is not yet a revolution in scientific understanding. The evidence overwhelmingly supports the conclusion that these systems are, for now, masters of **sophisticated pattern matching** rather than possessors of genuine physical insight.\n\n1.  **Dominance of Pattern Matching:** The models' architectural foundations and their characteristic, predictable failures in simulating basic physics demonstrate that they are learning statistical correlations from data, not causal laws of nature.\n2.  **Quantifiable Deficits:** The emergence of rigorous benchmarks like Physics-IQ has moved the debate from subjective impression to objective measurement. The starkly low scores of leading models like _Sora_ (10.0%) and _VideoPoet_ (24.1%) provide quantitative proof of their shallow physical understanding, despite their high visual fidelity.\n3.  **A Path Forward:** The most promising path toward true physical understanding appears to be the development of hybrid, physics-integrated models like _PhysGen_, which combine the strengths of data-driven synthesis with the constraints of explicit physical simulators. However, their superiority remains to be demonstrated on standardized benchmarks.\n4.  **The Unproven Leap to General AI:** The broader dream that a robust physical world model will unlock general abstract reasoning remains, at present, an article of faith. There is currently no causal evidence linking improvements in physical coherence to better performance on unrelated reasoning tasks.\n\nIn essence, today's video generation models are akin to \"sophisticated parrots\" that have watched millions of hours of video. They can flawlessly mimic the sights and sounds of the physical world but have no conception of the script they are reciting. They are not yet the \"nascent physicists\" that many hope for—AIs that can reason about the world from first principles. The journey from mimicry to true understanding has just begun.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. 人工智能生成内容在电影创制中的革新：ECCV 2024 AIGC 技术综述](https://www.crifst.ac.cn/uploadfile/ueditor/file/20250213/1739434457577468.pdf)\n\n[2\\. I. Goodfellow, Jean Pouget-Abadie et al. “Generative adversarial networks.” Communications of the ACM](https://doi.org/10.1145/3422622)\n\n[3\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[4\\. AI Video Generation Advancements: Transforming ...](https://insights.made-in-china.com/AI-Video-Generation-Advancements-Transforming-Creativity-in-2025_BGHaNMJvbEij.html)\n\n[5\\. The Best AI Video Generators: Complete Benchmark 2025](https://nugg.ad/en/generate-ai-videos-the-comparative-guide-to-the-different-tools/)\n\n[6\\. Best AI Video Generation APIs in 2025](https://www.edenai.co/post/best-ai-video-generation-apis-in-2025)\n\n[7\\. 3D Reconstruction From AI Generated Videos](https://deepresearchpdf.com/download-pdf/7f59a580-b445-4482-81b2-c344f4d5a671)\n\n[8\\. Top 10 AI Video Generation Tools for 2025](https://www.techwrix.com/top-10-ai-video-generation-tools-for-2025/)\n\n[9\\. The Future of Generative AI: Trends to Watch in 2025 and Beyond - EIMT](https://www.eimt.edu.eu/the-future-of-generative-ai-trends-to-watch-in-2025-and-beyond#:~:text=Personalisation%20is%20at%20the%20heart,specifically%20designed%20images%20and%20copy.)\n\n[10\\. Create AI Videos with 5 Best ComfyUI Text-to- ...](https://www.mimicpc.com/learn/creating-ai-videos-with-comfyui-text-to-video-workflows)\n\n[11\\. Latte: Latent Diffusion Transformer for Video Generation](https://openreview.net/pdf/fba4b5babe86bcf41cca50d98375bb43328210d1.pdf)\n\n[12\\. Inteligencias artificiales generativas a 2023](https://proyectodescartes.org/iCartesiLibri/PDF/IA.pdf)\n\n[13\\. NTIRE 2025 XGC Quality Assessment Challenge: Methods and Results](https://arxiv.org/pdf/2506.02875)\n\n[14\\. Advancements and Applications of AI-Driven Text-to-Image, GIF, and Video Generation](https://zenodo.org/records/15158326/files/4-1-13-Riya%20Sharma-Samrudhi%20Chaudhari-Mohit%20Gawande-Anurag%20Digrase-Neha%20Barley.pdf?download=1)\n\n[15\\. The Ultimate Guide to AI Video Creation in 2025](https://focalml.com/blog/the-ultimate-guide-to-ai-video-creation-tools-in-2025/)\n\n[16\\. Automatic Video Generator](https://www.ijisrt.com/assets/upload/files/IJISRT24DEC207.pdf)\n\n[17\\. Exploring the Top Veo 3 Alternatives for AI Video in 2025](https://reccloud.com/veo-3-alternative.html)\n\n[18\\. A Survey of Generative Artificial Intelligence Techniques](https://mesopotamian.press/journals/index.php/BJAI/article/download/201/192/426)\n\n[19\\. Best AI Video Generation Tools in 2025](https://www.aitoolsexplained.com/categories/video-generation)\n\n[21\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[22\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[23\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[24\\. Sepp Hochreiter, J. Schmidhuber. “Long Short-Term Memory.” Neural Computation](https://doi.org/10.1162/neco.1997.9.8.1735)\n\n[25\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[26\\. \\\\titlefontDo generative video models learn physical...](http://arxiv.org/html/2501.09038v1)\n\n[27\\. Generative Physical AI in Vision: A Survey](https://arxiv.org/pdf/2501.10928)\n\n[28\\. AI视频生成研究报告](https://s.eiix.top/5J4eJI)\n\n[29\\. Physion++: Evaluating Physical Scene Understanding that Requires Online Inference of Different Physical Properties](https://www.mit.edu/~k2smith/pdf/Tung_Ding_et_al_PhysionPP_2023.pdf)\n\n[30\\. AI视频是否符合物理规律,量化基准来了,实现人类感知对齐 | 阿里...](http://k.sina.com.cn/article_6105753431_16bee6757019019jpy.html)\n\n[31\\. AI视频是否符合物理规律，量化基准来了，实现人类感知对齐](https://i.ifeng.com/c/8hskPX3QgNS)\n\n[32\\. TOWARDS WORLD SIMULATOR: CRAFTING PHYSICAL COMMONSENSE-BASED BENCHMARK FOR VIDEO GENERATION](https://openreview.net/pdf/1814f0c3473ab9a04aca4edcd8aab3e678055bdd.pdf)\n\n[33\\. MAGI-1: Autoregressive Video Generation at Scale](https://static.magi.world/static/files/MAGI_1.pdf)\n\n[34\\. 每日论文](https://www.chatpaper.ai/zh/dashboard/papers/2025-02-18)\n\n[35\\. AI视频运动生成：迈向物理规律与人类感知的和谐](https://www.showapi.com/news/article/67dcd1454ddd791c0e00b7e9)\n\n[36\\. Wilson Yan, Yunzhi Zhang et al. “VideoGPT: Video Generation using VQ-VAE and Transformers.” ArXiv](https://arxiv.org/abs/2104.10157)\n\n[37\\. VideoAlign: A Comprehensive Model for Evaluating Alignment Between Text and Generated Videos](https://openreview.net/pdf?id=WlGZfWjKk6)\n\n[38\\. AI-Generated Video Content Detection Using Vision Language Models](https://www.crcv.ucf.edu/wp-content/uploads/2018/11/Keerthi-REU_Report-3.pdf)\n\n[41\\. A Physical Coherence Benchmark for Evaluating Video...](http://arxiv.org/html/2502.05503v1)\n\n[42\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[43\\. AI视频是否符合物理规律，量化基准来了，实现人类感知对齐](https://i.ifeng.com/c/8hskPX3QgNS)\n\n[44\\. Generative Physical AI in Vision: A Survey](https://arxiv.org/pdf/2501.10928)\n\n[45\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[46\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[47\\. GAIA: Rethinking Action Quality Assessment for AI-Generated Videos](https://proceedings.neurips.cc/paper_files/paper/2024/file/46b5405a720a99db4c758cff43c8b4d3-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[48\\. Benchmarking Multi-dimensional AIGC Video Quality Assessment: A Dataset and Unified Model](https://arxiv.org/pdf/2407.21408)\n\n[49\\. Face Consistency Benchmark for GenAI Video](https://arxiv.org/pdf/2505.11425)\n\n[50\\. Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated Videos](https://arxiv.org/pdf/2502.07327)\n\n[51\\. Beyond Deepfake Images: Detecting AI-Generated Videos](https://openaccess.thecvf.com/content/CVPR2024W/WMF/papers/Vahdati_Beyond_Deepfake_Images_Detecting_AI-Generated_Videos_CVPRW_2024_paper.pdf)\n\n[52\\. Yongfan Chen, Xiuwen Zhu et al. “A Physical Coherence Benchmark for Evaluating Video Generation Models via Optical Flow-guided Frame Prediction.”](https://arxiv.org/abs/2502.05503)\n\n[53\\. ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation](https://proceedings.neurips.cc/paper_files/paper/2024/file/25b9960c8a5bd887eb5476c951260403-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[54\\. DreamMesh4D: Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh Hybrid Representation](https://proceedings.neurips.cc/paper_files/paper/2024/file/25f7be9694d7b32d5cc670927b8091e1-Paper-Conference.pdf)\n\n[55\\. Advancements and Applications of AI-Driven Text-to-Image, GIF, and Video Generation](https://zenodo.org/records/15158326/files/4-1-13-Riya%20Sharma-Samrudhi%20Chaudhari-Mohit%20Gawande-Anurag%20Digrase-Neha%20Barley.pdf?download=1)\n\n[56\\. Yongfan Chen, Xiuwen Zhu et al. “A Physical Coherence Benchmark for Evaluating Video Generation Models via Optical Flow-guided Frame Prediction.”](https://arxiv.org/abs/2502.05503)\n\n[57\\. Fanda Fan, Chunjie Luo et al. “AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI.” ArXiv](https://doi.org/10.48550/arXiv.2401.01651)\n\n[58\\. Benchmarking AIGC Video Quality Assessment: A Dataset and Unified Model](https://arxiv.org/pdf/2407.21408v1)\n\n[61\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[62\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[63\\. TOWARDS WORLD SIMULATOR: CRAFTING PHYSICAL COMMONSENSE-BASED BENCHMARK FOR VIDEO GENERATION](https://openreview.net/pdf/1814f0c3473ab9a04aca4edcd8aab3e678055bdd.pdf)\n\n[64\\. PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation](https://shaoweiliu.web.illinois.edu/eccv2024_physgen_arxiv.pdf)\n\n[65\\. \\\\titlefontDo generative video models learn physical...](http://arxiv.org/html/2501.09038v1)\n\n[66\\. Generative Physical AI in Vision: A Survey](https://arxiv.org/pdf/2501.10928)\n\n[67\\. M. Raissi, P. Perdikaris et al. “Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.” J. Comput. Phys.](https://doi.org/10.1016/j.jcp.2018.10.045)\n\n[68\\. Heygen Streaming Avatar - AI Tools - AIbase](https://app.aibase.com/en/details/27923)\n\n[69\\. MAGI-1: Autoregressive Video Generation at Scale](https://static.magi.world/static/files/MAGI_1.pdf)\n\n[70\\. PhysGen：刚体物理基础的图像到视频生成](https://www.chatpaper.ai/zh/dashboard/paper/706b27bf-0e77-4218-aaed-7abc5c3c2dcd)\n\n[71\\. GitHub - google-deepmind/physics-IQ-benchmark: Benchmarking...](https://github.com/google-deepmind/physics-IQ-benchmark)\n\n[81\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[82\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[83\\. Sora:开启构建物理世界的通用模型](https://mp.weixin.qq.com/s?__biz=MzUyMzY3NDA5Nw%3D%3D&mid=2247496033&idx=1&sn=2c4982b240c3a976c948e602d287f903&chksm=fa3bb9ffcd4c30e9db555e1b70f0ba2dd5d4c952174cac0589b15b98e8b6cecfaa8a246d0e95&scene=27)\n\n[84\\. 通俗易懂地解释OpenAI Sora视频生成的特点及其与Runway Gen2、Pika的对比](https://wallstreetcn.com/articles/3708465)\n\n[85\\. OpenAI Sora模型发布，视频生成技术迎来突破性升级](https://pdf.dfcfw.com/pdf/H3_AP202403121626455315_1.pdf?1710265751000.pdf)\n\n[86\\. OpenAI Sora视频生成模型技术报告中英全文 总结 影响分析](https://www.360doc.cn/article/47115229_1114377336.html)\n\n[87\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[88\\. OpenAI Sora视频生成的特点有哪些？它与此前的Runway Gen2、Pika有什么区别？缺点是什么？](https://finance.sina.com.cn/roll/2024-02-18/doc-inaimfyq9464958.shtml?r=0&tr=164)\n\n[89\\. OPEN AI SORA 技术报告原文+译文+ 报告总结](https://www.datatn.com/uploads/20240229/7151675d205710c2081bd5e471e39f46.pdf)\n\n[90\\. \\[SORA\\] video-generation-models-as-world-simulators - ...](https://www.bilibili.com/read/mobile?id=31356518)\n\n[91\\. OpenAI 发布最强文生视频模型 Sora，该模型有哪些功能？](https://www.zhihu.com/question/644513788/answer/3408358006)\n\n[92\\. \\\\titlefontDo generative video models learn physical...](http://arxiv.org/html/2501.09038v1)\n\n[93\\. What is Lacking in Sora and V-JEPA’s World Models? -A Philosophical Analysis of Video AIs Through the Theory of Productive Imagination](https://philsci-archive.pitt.edu/23434/1/SoraVJEPA.pdf)\n\n[94\\. Understanding World or Predicting Future? A Comprehensive Survey of World Models](https://fi.ee.tsinghua.edu.cn/~dingjingtao/papers/WorldModel.pdf)\n\n[95\\. Jonathan Ho, William Chan et al. “Imagen Video: High Definition Video Generation with Diffusion Models.” ArXiv](https://doi.org/10.48550/arXiv.2210.02303)\n\n[96\\. Understanding Hallucinations in Diffusion Models through Mode Interpolation](https://proceedings.neurips.cc/paper_files/paper/2024/file/f29369d192b13184b65c6d2515474d78-Paper-Conference.pdf)\n\n[97\\. TOWARDS WORLD SIMULATOR: CRAFTING PHYSICAL COMMONSENSE-BASED BENCHMARK FOR VIDEO GENERATION](https://openreview.net/pdf/1814f0c3473ab9a04aca4edcd8aab3e678055bdd.pdf)\n\n[98\\. Sora文生视频模型深度剖析：全网独家指南，洞悉98%关键信息](https://www.nowcoder.com/discuss/589491214567612416)\n\n[99\\. Sora学习笔记](https://zhuanlan.zhihu.com/p/690073500)\n\n[100\\. A. Blattmann, Tim Dockhorn et al. “Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets.” ArXiv](https://doi.org/10.48550/arXiv.2311.15127)\n\n[101\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[102\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[103\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[104\\. P. Battaglia, Razvan Pascanu et al. “Interaction Networks for Learning about Objects, Relations and Physics.” Neural Information Processing Systems](https://arxiv.org/abs/1612.00222)\n\n[105\\. A. Bakhtin, L. Maaten et al. “PHYRE: A New Benchmark for Physical Reasoning.” Neural Information Processing Systems](https://arxiv.org/abs/1908.05656)\n\n[106\\. World Models in Artificial Intelligence: Sensing, Learning ...](https://arxiv.org/html/2503.15168)\n\n[107\\. PHYSBENCH: BENCHMARKING AND ENHANCING VISION-LANGUAGE MODELS FOR PHYSICAL WORLD UNDERSTANDING](https://openreview.net/pdf/03d740bded99ee328106bfc84811201a1dae6c32.pdf)\n\n[108\\. PhysWM - Learning from Causal Physical World Models](https://robotik.dfki-bremen.de/en/research/projects/physwm)\n\n[109\\. Minne Li, Mengyue Yang et al. “Causal World Models by Unsupervised Deconfounding of Physical Dynamics.” ArXiv](https://arxiv.org/abs/2012.14228)\n\n[110\\. Juan L. Gamella, Jonas Peters et al. “The Causal Chambers: Real Physical Systems as a Testbed for AI Methodology.” ArXiv](https://doi.org/10.48550/arXiv.2404.11341)\n\n[111\\. 怒斥 Sora 后，LeCun 放出视觉世界模型论文，揭示 AI 学习物理的关键](https://zhuanlan.zhihu.com/p/685439925)\n\n[112\\. Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment](http://proceedings.mlr.press/v139/ball21a/ball21a.pdf)\n\n[113\\. MERE 2023: Researching the role of AI tools in the research practice](https://repositori-api.upf.edu/api/core/bitstreams/5aa907d6-3521-4e28-811f-48622bbe705d/content)\n\n[117\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[118\\. VLIPP: Towards Physically Plausible Video Generation with Vision and Language Informed Physical Prior](https://arxiv.org/pdf/2503.23368)\n\n[119\\. \\\\titlefontDo generative video models learn physical...](http://arxiv.org/html/2501.09038v1)\n\n[120\\. 测试生成性视频模型在理解物理原则方面的能力 - 思空，简观](https://nullthought.net/?p=5569)\n\n[121\\. T. Chen, Yulia Rubanova et al. “Neural Ordinary Differential Equations.” Neural Information Processing Systems](https://arxiv.org/abs/1806.07366)\n\n[122\\. GitHub - google-deepmind/physics-IQ-benchmark: Benchmarking...](https://github.com/google-deepmind/physics-IQ-benchmark)\n\n[123\\. M. Raissi, P. Perdikaris et al. “Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.” J. Comput. Phys.](https://doi.org/10.1016/j.jcp.2018.10.045)\n\n[124\\. Learning Strategies for Physics-Informed Neural Networks](https://dr.ntu.edu.sg/bitstream/10356/182950/2/PhD_Thesis_NTU_Singapore_WONG_JIAN_CHENG-final.pdf)\n\n[125\\. Generative Physical AI in Vision: A Survey](https://arxiv.org/pdf/2501.10928)\n\n[126\\. Lu Lu, Xuhui Meng et al. “DeepXDE: A Deep Learning Library for Solving Differential Equations.” ArXiv](https://doi.org/10.1137/19M1274067)\n\n[127\\. PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation](https://shaoweiliu.web.illinois.edu/eccv2024_physgen_arxiv.pdf)\n\n[128\\. MAGI-1: Autoregressive Video Generation at Scale](https://static.magi.world/static/files/MAGI_1.pdf)\n\n[129\\. G. Karniadakis, I. Kevrekidis et al. “Physics-informed machine learning.” Nature Reviews Physics](https://doi.org/10.1038/s42254-021-00314-5)\n\n[130\\. TOWARDS WORLD SIMULATOR: CRAFTING PHYSICAL COMMONSENSE-BASED BENCHMARK FOR VIDEO GENERATION](https://openreview.net/pdf/1814f0c3473ab9a04aca4edcd8aab3e678055bdd.pdf)\n\n[131\\. Quantifying Uncertainty in Physics-Informed Neural Networks](https://openreview.net/pdf/6f403a22db3baf1dbc5ed31be762e68aa4195ba1.pdf)\n\n[132\\. 物理视频真实生成！大连理工&莫纳什大学团队提出 ...](https://www.aizws.net/news/detail/3291)\n\n[133\\. Analysing the role of physics-informed neural networks in ...](https://link.springer.com/article/10.1007/s12008-025-02364-w)\n\n[134\\. Physion++: Evaluating Physical Scene Understanding that Requires Online Inference of Different Physical Properties](https://www.mit.edu/~k2smith/pdf/Tung_Ding_et_al_PhysionPP_2023.pdf)\n\n[135\\. Solving Neutron Diffusion Equations Using Physics-Informed Neural Networks: Performance Analysis Against Traditional FEM](https://www.kns.org/files/pre_paper/53/25S-659-%EC%9D%B4%EC%9A%94%ED%95%9C-%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C.pdf)\n\n[136\\. Integrating Physics-Based Modeling with Machine Learning for Lithium-Ion Batteries](https://www.merl.com/publications/docs/TR2022-155.pdf)\n\n[137\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[138\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[139\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[140\\. What is Lacking in Sora and V-JEPA’s World Models? -A Philosophical Analysis of Video AIs Through the Theory of Productive Imagination](https://philsci-archive.pitt.edu/23434/1/SoraVJEPA.pdf)\n\n[141\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[142\\. Sora文生视频模型深度剖析：洞悉98%关键信息，纯干货](https://zhuanlan.zhihu.com/p/683028184)\n\n[143\\. Sora模型的进一步技术解读](https://zhuanlan.zhihu.com/p/685585361)\n\n[144\\. Understanding World or Predicting Future? A Comprehensive Survey of World Models](https://fi.ee.tsinghua.edu.cn/~dingjingtao/papers/WorldModel.pdf)\n\n[145\\. Romain Lopez, Pierre Boyeau et al. “AUTO-ENCODING VARIATIONAL BAYES.”](https://www.semanticscholar.org/paper/ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f)\n\n[146\\. TO EXPLORE AI \"SORA\" POSSIBLEMULTIDIMENSIONALINFLUENCESON THE FUTUREEDUCATION INDUSTRY, SHORT-VIDEO INDUSTRY, FILM INDUSTRY AND ROBOTICS INDUSTRY](https://www.journalijar.com/uploads/6769495af3f9b_IJAR-49532.pdf)\n\n[147\\. 为什么Sora不能成为世界模型?\\_ZAKER新闻](http://app.myzaker.com/news/article.php?pk=67c89b4d8e9f09686259a74a)\n\n[148\\. OPEN AI SORA 技术报告原文+译文+ 报告总结](https://www.datatn.com/uploads/20240229/7151675d205710c2081bd5e471e39f46.pdf)\n\n[149\\. Sora：大型视觉模型的背景、技术、局限性和机遇综述](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_2_28/2402.17177.pdf)\n\n[150\\. Unraveling Sora's Architecture and Working Intuitively!](https://towardsai.net/p/artificial-intelligence/sora-ai-unraveling-soras-architecture-and-working-intuitively)\n\n[151\\. OpenAI Sora模型发布，视频生成技术迎来突破性升级](https://www.hulianhutongshequ.cn/upload/tank/report/2024/202402/1/4145467bff9e419aa7b23bfb2a539fc8.pdf)\n\n[152\\. What is OpenAI's Sora? Use Cases, Alternatives & How it Works](https://www.strivemindz.com/blog/open-ai-sora/)\n\n[153\\. 字节联合清华研究：Sora等这类AI视频模型无法理解基本物理 ...](https://www.aibase.com/zh/news/www.aibase.com/zh/news/13286)\n\n[154\\. 字节联合清华研究：Sora等这类AI视频模型无法理解基本物理规律](https://www.aibase.com/zh/news/13286)\n\n[157\\. M. Melby-Lervåg, C. Hulme. “Is working memory training effective? A meta-analytic review..” Developmental psychology](https://doi.org/10.1037/a0028228)\n\n[158\\. T. Klingberg, E. Fernell et al. “Computerized training of working memory in children with ADHD--a randomized, controlled trial..” Journal of the American Academy of Child and Adolescent Psychiatry](https://doi.org/10.1097/00004583-200502000-00010)\n\n[159\\. Erika Dahlin, A. S. Neely et al. “Transfer of Learning After Updating Training Mediated by the Striatum.” Science](https://doi.org/10.1126/science.1155466)\n\n[160\\. Susanne M. Jaeggi, Martin Buschkuehl et al. “Short- and long-term benefits of cognitive training.” Proceedings of the National Academy of Sciences](https://doi.org/10.1073/pnas.1103228108)\n\n[161\\. J. Au, E. Sheehan et al. “Improving fluid intelligence with training on working memory: a meta-analysis.” Psychonomic Bulletin & Review](https://doi.org/10.3758/s13423-014-0699-x)\n\n[162\\. Evaluating the Effectiveness of Commercial Brain Game Training with Working-Memory Tasks](https://www.uni-wuerzburg.de/fileadmin/06020330/Methoden/Publikationen/Strobach_Huestegge_2018.pdf)\n\n[163\\. Enhanced Learning through Multimodal Training: Evidence from a Comprehensive Cognitive, Physical Fitness, and Neuroscience Intervention](https://www.nature.com/articles/s41598-017-06237-5?error=cookies_not_supported&code=97de6201-5684-42e7-ab15-cf18df56b5f4)\n\n[164\\. Activities and Programs That Improve Children’s Executive Functions](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=00d21266f5fc4b20c0457f2c33bb7904ac57e8c6)\n\n[165\\. An evidence-based approach to working-memory based training in secondary education to improve reasoning test achievements](https://cris.maastrichtuniversity.nl/files/25622770/c5999.pdf)\n\n[166\\. Gains in fluid intelligence after training non-verbal reasoning in 4-year-old children: a controlled, randomized study](https://www.klingberglab.se/pub/BergmanNutley_fluid_intelligence_2011.pdf)\n\n[167\\. Cognitive and Working Memory Training: Perspectives From Psychology, Neuroscience, and Human Development](https://www.devcogneuro.com/Publications/Diamond_Ling_2020_efforts_to_improve_EFs_whole_chapter.pdf)\n\n[168\\. Brain training: Memory games](https://www.nature.com/articles/531S10a?error=cookies_not_supported&code=1cdeaa18-1171-4403-b6ca-4acd9b8b14d4)\n\n[169\\. A New Media Approach For Improved Sense-Making Through Physiological Coherence](https://articlefeed.org/a-new-media-approach-for-improved-sense-making-through-physiological-coherence/)\n\n[170\\. Similarity-Based Reasoning, Raven’s Matrices, and General Intelligence](https://www.ijcai.org/proceedings/2018/0218.pdf)\n\n[173\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[174\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[175\\. GitHub - google-deepmind/physics-IQ-benchmark: Benchmarking...](https://github.com/google-deepmind/physics-IQ-benchmark)\n\n[176\\. \\\\titlefontDo generative video models learn physical...](http://arxiv.org/html/2501.09038v1)\n\n[177\\. PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10827.pdf)\n\n[178\\. Yonatan Bisk, Rowan Zellers et al. “PIQA: Reasoning about Physical Commonsense in Natural Language.” AAAI Conference on Artificial Intelligence](https://doi.org/10.1609/AAAI.V34I05.6239)\n\n[179\\. PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models](https://arxiv.org/pdf/2504.16074)\n\n[180\\. Phy-Q as a measure for physical reasoning intelligence...](https://ui.adsabs.harvard.edu/abs/2021arXiv210813696X/)\n\n[181\\. Kexin Yi, Chuang Gan et al. “CLEVRER: CoLlision Events for Video REpresentation and Reasoning.” ArXiv](https://arxiv.org/abs/1910.01442)\n\n[182\\. PHYSBENCH: BENCHMARKING AND ENHANCING VISION-LANGUAGE MODELS FOR PHYSICAL WORLD UNDERSTANDING](https://openreview.net/pdf/03d740bded99ee328106bfc84811201a1dae6c32.pdf)\n\n[183\\. 6 Papers Accepted at ACL 2025](https://www.a-star.edu.sg/cfar/news/news/features/6-papers-accepted-at-acl-2025)\n\n[184\\. A. Bakhtin, L. Maaten et al. “PHYRE: A New Benchmark for Physical Reasoning.” Neural Information Processing Systems](https://arxiv.org/abs/1908.05656)\n\n[185\\. TOWARDS WORLD SIMULATOR: CRAFTING PHYSICAL COMMONSENSE-BASED BENCHMARK FOR VIDEO GENERATION](https://openreview.net/pdf/1814f0c3473ab9a04aca4edcd8aab3e678055bdd.pdf)\n\n[186\\. Andrew Melnik, Robin Schiewer et al. “Benchmarks for Physical Reasoning AI.” ArXiv](https://doi.org/10.48550/arXiv.2312.10728)\n\n[187\\. Sand AI Releases MAGI-1: Autoregressive Video ...](https://comfyui-wiki.com/en/news/2025-04-23-magi-1-autoregressive-video-generation-model-released)\n\n[188\\. Arxiv Daily](http://mistariano.icu/)\n\n[189\\. Generative Physical AI in Vision: A Survey](https://arxiv.org/pdf/2501.10928)\n\n[193\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[194\\. ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_ImageNet-D_Benchmarking_Neural_Network_Robustness_on_Diffusion_Synthetic_Object_CVPR_2024_paper.pdf)\n\n[195\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[196\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[197\\. Denoising with a Joint-Embedding Predictive Architecture](https://openreview.net/pdf/bf91cb42318dd7bfa52fd55870c57556b1820fea.pdf)\n\n[198\\. A. Ramesh, Prafulla Dhariwal et al. “Hierarchical Text-Conditional Image Generation with CLIP Latents.” ArXiv](https://doi.org/10.48550/arXiv.2204.06125)\n\n[199\\. SSM MEETS VIDEO DIFFUSION MODELS: EFFICIENT VIDEO GENERATION WITH STRUCTURED STATE SPACES](https://openreview.net/pdf?id=jzbeme6FdW)\n\n[200\\. VIDEO SHIELD: REGULATING DIFFUSION-BASED VIDEO GENERATION MODELS VIA WATERMARKING](https://openreview.net/pdf/afeb3427ab6c64ea23bc0f4974619c49be26d350.pdf)\n\n[201\\. Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k](https://arxiv.org/pdf/2503.09642)\n\n[202\\. Talkin' 'Bout AI Generation: COPYRIGHT AND THE GENERATIVE-AI SUPPLY CHAIN](https://afedercooper.info/talkin.pdf)\n\n[203\\. Jiaming Song, Chenlin Meng et al. “Denoising Diffusion Implicit Models.” ArXiv](https://arxiv.org/abs/2010.02502)\n\n[204\\. PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation](https://arxiv.org/pdf/2412.00596)\n\n[205\\. A Survey on Video Diffusion Models](https://arxiv.org/pdf/2310.10647)\n\n[206\\. Automatická tvorba animovaného videa na základě textového příběhu](https://theses.cz/id/n17jgs/xkucha28_bp.pdf)\n\n[207\\. Articulated Kinematics Distillation from Video Diffusion Models](https://research.nvidia.com/labs/dir/akd/pdf/AKD_paper.pdf)\n\n[208\\. Open-Sora 2.0: Training a Commercial-Level Video ...](https://arxiv.org/html/2503.09642v1)\n\n[209\\. 视频生成发展模型进展—Sora](https://zhuanlan.zhihu.com/p/682988174)\n\n[210\\. ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation](https://openreview.net/pdf/75830ab5f66cec57a4bb0cf4e3d72ae24a6d9fff.pdf)\n\n[211\\. DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving](https://arxiv.org/pdf/2408.16647)\n\n[212\\. NeuroVidX: Text-To-Video Diffusion Models with an Expert Transformer](https://www.atlantis-press.com/article/126010074.pdf)\n\n[213\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[214\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[215\\. Aishwarya Agrawal, Jiasen Lu et al. “VQA: Visual Question Answering.” International Journal of Computer Vision](https://doi.org/10.1007/s11263-016-0966-6)\n\n[216\\. REVISITING THE SUPERFICIAL ALIGNMENT HYPOTHESIS](https://openreview.net/pdf/5e38d009942abec86d00fef13f398721bbab55f1.pdf)\n\n[217\\. M. Melby-Lervåg, C. Hulme. “Is working memory training effective? A meta-analytic review..” Developmental psychology](https://doi.org/10.1037/a0028228)\n\n[218\\. State of AI Reasoning for Theoretical Physics - Insights from the TPBench Project](https://pdf.pirsa.org/files/25040061.pdf)\n\n[219\\. AI Makes You Smarter, But None The Wiser: The Disconnect Between Performance and Metacognition](https://posthci.com/Papers/dunning.pdf)\n\n[220\\. Benchmarks for Physical Reasoning AI](https://openreview.net/forum?id=cHroS8VIyN)\n\n[221\\. Our scientific research on coherence and stress](https://heartmathbenelux.com/en/info/our-scientific-research-on-coherence-and-stress)\n\n[222\\. J. Au, E. Sheehan et al. “Improving fluid intelligence with training on working memory: a meta-analysis.” Psychonomic Bulletin & Review](https://doi.org/10.3758/s13423-014-0699-x)\n\n[223\\. Cognitive Forcing for Better Decision-Making: Reducing Overreliance on AI Systems Through Partial Explanations](https://nielsvanberkel.com/files/publications/cscw2025a.pdf)\n\n[224\\. Development and evaluation of AI-based personalization algorithms for attention training](https://theses.hal.science/tel-04884647/file/ADOLPHE_MAXIME_2024.pdf)\n\n[225\\. AI Machines Have Beaten Moore's Law Over The Last Decade, Say Computer Scientists](https://www.discovermagazine.com/technology/ai-machines-have-beaten-moores-law-over-the-last-decade-say-computer)\n\n[226\\. Enhancing Human-AI Collaboration Through Logic-Guided Reasoning](https://iclr.cc/media/iclr-2024/Slides/18568.pdf)\n\n[227\\. Artificial Intelligence Index Report 2024](https://files-scs.pstatic.net/2024/04/15/faSHrjTL79/HAI_AI-Index-Report-2024_Master.pdf)\n\n[228\\. Zheyuan Zhang, Shane Storks et al. “From Heuristic to Analytic: Cognitively Motivated Strategies for Coherent Physical Commonsense Reasoning.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.48550/arXiv.2310.18364)\n\n[229\\. Examining the Effects of Explainable Hints in AI-Driven Training](https://chienjuho.com/pub/CI24-ExplainableHint-Poster.pdf)\n\n[230\\. Logic Extraction: Enhancing AI Generalization in Abstraction and Reasoning Corpus Tasks](https://openreview.net/pdf?id=mMjzOoMKcs)\n\n[231\\. Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning](https://arxiv.org/pdf/2310.09430.pdf)\n\n[233\\. Joseph Redmon, S. Divvala et al. “You Only Look Once: Unified, Real-Time Object Detection.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2016.91)\n\n[234\\. Shaoqing Ren, Kaiming He et al. “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1109/TPAMI.2016.2577031)\n\n[235\\. Ross B. Girshick. “Fast R-CNN.”](https://arxiv.org/abs/1504.08083)\n\n[236\\. W. Liu, Dragomir Anguelov et al. “SSD: Single Shot MultiBox Detector.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-46448-0_2)\n\n[237\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[238\\. GitHub - google-deepmind/physics-IQ-benchmark: Benchmarking...](https://github.com/google-deepmind/physics-IQ-benchmark)\n\n[239\\. \\\\titlefontDo generative video models learn physical...](http://arxiv.org/html/2501.09038v1)\n\n[240\\. PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10827.pdf)\n\n[241\\. How to get IoU in validation metrics? #7042 - GitHub](https://github.com/open-mmlab/mmdetection/issues/7042)\n\n[242\\. reduced-8 results](http://www.semantic3d.net/results/reduced-8/)\n\n[243\\. Working with Diffusion Models | Krasamo](https://www.krasamo.com/diffusion-models/)\n\n[244\\. O. Ronneberger, P. Fischer et al. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” ArXiv](https://doi.org/10.1007/978-3-319-24574-4_28)\n\n[245\\. U-Net - Diffusion Model Architecture](https://deeplizard.com/lesson/dic2rlzadi)\n\n[246\\. Text - Conditioned Image Generation using Diffusion Models](https://uokerbala.edu.iq/wp-content/uploads/2024/12/Rp-Text-Conditioned-Image-Generation-using-Diffusion-Models.pdf)\n\n[247\\. Yang Song, Jascha Narain Sohl-Dickstein et al. “Score-Based Generative Modeling through Stochastic Differential Equations.” ArXiv](https://arxiv.org/abs/2011.13456)\n\n[248\\. Jascha Narain Sohl-Dickstein, Eric A. Weiss et al. “Deep Unsupervised Learning using Nonequilibrium Thermodynamics.” ArXiv](https://arxiv.org/abs/1503.03585)\n\n[249\\. Jonathan Ho. “Classifier-Free Diffusion Guidance.” ArXiv](https://doi.org/10.48550/arXiv.2207.12598)\n\n[250\\. Yang Song, Stefano Ermon. “Generative Modeling by Estimating Gradients of the Data Distribution.” Neural Information Processing Systems](https://arxiv.org/abs/1907.05600)\n\n[251\\. How to Train a Stable Diffusion Model like DALL·E 2 with PyTorch and Diffusers](https://reiserx.com/reiserx/how-to-train-a-stable-diffusion-model-like-dalle-2-with-pytorch-and-diffusers/)\n\n[252\\. Components of Stable Diffusion](https://deeplizard.com/lesson/dia3zlaidr)\n\n[253\\. Text2Form Diffusion: Framework for learning curated architectural vocabulary](https://papers.cumincad.org/data/works/att/ecaade2023_197.pdf)\n\n[254\\. Esha Saha, Giang Tran. “Diffusion Random Feature Model.” ArXiv](https://doi.org/10.48550/arXiv.2310.04417)\n\n[255\\. What are Diffusion Models?](https://www2.cs.uh.edu/~ceick/ai/DF_Reading.pdf)\n\n[256\\. Variational Autoencoders and Diffusion Models](https://cs231n.stanford.edu/slides/2023/lecture_15.pdf)\n\n[257\\. The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation](https://proceedings.neurips.cc/paper_files/paper/2023/file/7c119415672ae2186e17d492e1d5da2f-Paper-Conference.pdf)\n\n[258\\. FreeU: Free Lunch in Diffusion U-Net](https://openaccess.thecvf.com/content/CVPR2024/papers/Si_FreeU_Free_Lunch_in_Diffusion_U-Net_CVPR_2024_paper.pdf)\n\n[259\\. What types of neural network architectures are commonly ...](https://milvus.io/ai-quick-reference/what-types-of-neural-network-architectures-are-commonly-used-in-diffusion-models)\n\n[260\\. HiDiffusion: Unlocking Higher-Resolution Creativity and Efficiency in Pretrained Diffusion Models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06764.pdf)\n\n[263\\. A Conceptual Chronicle of Solving Raven’s Progressive Matrices Computationally](https://ceur-ws.org/Vol-3400/paper5.pdf)\n\n[264\\. A neuro-vector-symbolic architecture for solving Raven’s progressive matrices](https://qspapers.com/deep-neural-networks-and-vector-symbolic-models-are-combined-in-this-design/)\n\n[265\\. Spaced Cognitive Training Promotes Training Transfer](https://www.frontiersin.org/articles/10.3389/fnhum.2014.00217/full)\n\n[266\\. T. Klingberg, E. Fernell et al. “Computerized training of working memory in children with ADHD--a randomized, controlled trial..” Journal of the American Academy of Child and Adolescent Psychiatry](https://doi.org/10.1097/00004583-200502000-00010)\n\n[267\\. Martin Buschkuehl, Susanne M. Jaeggi. “Improving intelligence: a literature review..” Swiss medical weekly](https://doi.org/10.4414/SMW.2010.12852)\n\n[268\\. Transferable Representation Learning in Vision-and-Language Navigation](https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Transferable_Representation_Learning_in_Vision-and-Language_Navigation_ICCV_2019_paper.pdf)\n\n[269\\. L. Thorell, Sofia Lindqvist et al. “Training and transfer effects of executive functions in preschool children..” Developmental science](https://doi.org/10.1111/j.1467-7687.2008.00745.x)\n\n[270\\. N. Denney, Susan M. Heidrich. “Training effects on Raven's progressive matrices in young, middle-aged, and elderly adults..” Psychology and aging](https://doi.org/10.1037/0882-7974.5.1.144)\n\n[271\\. Machine Teaching for Building Modular AI Agents based on Zero-shot Learners](https://aitopics.org/search?filters=authorsRaw%3AGoel%2C+Ashok)\n\n[272\\. J. Au, E. Sheehan et al. “Improving fluid intelligence with training on working memory: a meta-analysis.” Psychonomic Bulletin & Review](https://doi.org/10.3758/s13423-014-0699-x)\n\n[273\\. Training of working memory in children with ADHD](https://pubmed.ncbi.nlm.nih.gov/12424652/?dopt=Abstract)\n\n[274\\. Chi Zhang, Feng Gao et al. “RAVEN: A Dataset for Relational and Analogical Visual REasoNing.” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2019.00546)\n\n[275\\. Adam Santoro, Felix Hill et al. “Measuring abstract reasoning in neural networks.” ArXiv](https://arxiv.org/abs/1807.04225)\n\n[276\\. Tao Zhuo, Mohan S. Kankanhalli. “Solving Raven's Progressive Matrices with Neural Networks.” ArXiv](https://arxiv.org/abs/2002.01646)\n\n[277\\. Yuan Yang, Mathilee Kunda. “Computational Models of Solving Raven's Progressive Matrices: A Comprehensive Introduction.” ArXiv](https://doi.org/10.48550/arXiv.2302.04238)\n\n[278\\. Deep Learning based Clinical Decision Support through Strong Differentiable Domain Priors](https://mediatum.ub.tum.de/doc/1661677/3ey4smi5ejgo2dzifji94dsvo.Dissertation_Hendrik_Burwinkel.pdf)\n\n[279\\. Chunjie Wang. “A Review of the Effects of Abacus Training on Cognitive Functions and Neural Systems in Humans.” Frontiers in Neuroscience](https://doi.org/10.3389/fnins.2020.00913)\n\n[280\\. Working memory and insight in the nine-dot problem](https://cw.fel.cvut.cz/wiki/_media/courses/a6m33ksy/working_memory_nine_dot_problem.pdf)\n\n[281\\. Cognitive enhancement by means of TMS and video game training: synergistic effects](https://openaccess.uoc.edu/bitstream/10609/93946/1/Cognitive+enhancement+by+means+of+TMS+and+video+game+training+-+synergistic+effects.pdf)\n\n[282\\. Cognitive training using self-discovery methods](https://www.researchwithrutgers.com/en/publications/cognitive-training-using-self-discovery-methods)\n\n[283\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[284\\. Joseph Redmon, S. Divvala et al. “You Only Look Once: Unified, Real-Time Object Detection.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2016.91)\n\n[285\\. Shaoqing Ren, Kaiming He et al. “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1109/TPAMI.2016.2577031)\n\n[286\\. Ross B. Girshick, Jeff Donahue et al. “Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.” 2014 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2014.81)\n\n[287\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[288\\. 3D spatial perception for underwater robots using point cloud data from orthogonal multibeam sonars fusion](https://dr.ntu.edu.sg/bitstream/10356/172750/2/Nicholas_Sadjoli-2nd_Revision_PhD_Thesis-Final-Grammar_Correct_v0.pdf)\n\n[289\\. Peer-Reviewed Publications - 2025](https://www.beg.utexas.edu/publications)\n\n[290\\. \\\\titlefontDo generative video models learn physical...](http://arxiv.org/html/2501.09038v1)\n\n[291\\. Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges](https://arxiv.org/pdf/2505.11618)\n\n[292\\. SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.14381)\n\n[293\\. PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10827.pdf)\n\n[294\\. 具身智能的研究与应用](https://tis.hrbeu.edu.cn/oa/pdfdow.aspx?Sid=202406044)\n\n[295\\. Peer-Reviewed Publications](https://www2.whoi.edu/staff/pbarry/wp-content/uploads/sites/210/2022/05/Barry_just-peer-rev-pubs_April_13_2022.pdf)\n\n[296\\. O. Ronneberger, P. Fischer et al. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” ArXiv](https://doi.org/10.1007/978-3-319-24574-4_28)\n\n[297\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[298\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[299\\. Working with Diffusion Models | Krasamo](https://www.krasamo.com/diffusion-models/)\n\n[300\\. Prafulla Dhariwal, Alex Nichol. “Diffusion Models Beat GANs on Image Synthesis.” ArXiv](https://arxiv.org/abs/2105.05233)\n\n[301\\. Jascha Narain Sohl-Dickstein, Eric A. Weiss et al. “Deep Unsupervised Learning using Nonequilibrium Thermodynamics.” ArXiv](https://arxiv.org/abs/1503.03585)\n\n[302\\. U-DiTs: Downsample Tokens in U-Shaped Diffusion Transformers](https://proceedings.neurips.cc/paper_files/paper/2024/file/5d2e24df9cfaad3189833b819c40b392-Paper-Conference.pdf)\n\n[303\\. Frontiers | A Modular U-Net for Automated Segmentation of X-Ray ...](https://www.frontiersin.org/articles/10.3389/fmats.2021.761229/full)\n\n[304\\. Device simulations with A U-Net model predicting physical ... - Nature](https://www.nature.com/articles/s41598-023-27599-z)\n\n[305\\. UDPM: Upsampling Diffusion Probabilistic Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/30bdd694743e381f9ba679ec49dab866-Paper-Conference.pdf)\n\n[306\\. Zheng Dai, David K Gifford. “Ablation Based Counterfactuals.” ArXiv](https://doi.org/10.48550/arXiv.2406.07908)\n\n[307\\. Larry Jin, Hannah Lu et al. “Fast uncertainty quantification of reservoir simulation with variational U-Net.” arXiv: Optimization and Control](https://arxiv.org/abs/1907.00718)\n\n[308\\. Hybrid dilation and attention residual U-Net for medical image segmentation](https://sci-hub.se/downloads/2021-06-25/5755/wang2021.pdf)\n\n[309\\. 深入浅出Diffusion模型：从原理到实践的全方位教程](https://cloud.tencent.com/developer/article/2529971?policyId=1003)\n\n[310\\. DiffVector: Boosting Diffusion Framework for Building Vector Extraction from Remote Sensing Images](https://luojianet.whu.edu.cn/DiffVector--2025.pdf)\n\n[311\\. Frontiers | A Modular U-Net for Automated Segmentation...](https://www.frontiersin.org/journals/materials/articles/10.3389/fmats.2021.761229/full)\n\n[312\\. Unlocking Implicit Visual Grounding in Large Multimodal Models Without Explicit Grounding Supervision](https://yxw.cs.illinois.edu/files/GroundLMM_ICCV2025.pdf)\n\n[313\\. Ziyang Song, Zerong Wang et al. “DepthMaster: Taming Diffusion Models for Monocular Depth Estimation.”](https://arxiv.org/abs/2501.02576)\n\n[314\\. ReconFusion: 3D Reconstruction with Diffusion Priors](https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_ReconFusion_3D_Reconstruction_with_Diffusion_Priors_CVPR_2024_paper.pdf)\n\n[315\\. TextDiffuser: Diffusion Models as Text Painters](https://proceedings.neurips.cc/paper_files/paper/2023/file/1df4afb0b4ebf492a41218ce16b6d8df-Paper-Conference.pdf)\n\n[316\\. Marius Jahrens, T. Martinetz. “Solving Raven’s Progressive Matrices with Multi-Layer Relation Networks.” 2020 International Joint Conference on Neural Networks (IJCNN)](https://doi.org/10.1109/IJCNN48605.2020.9207319)\n\n[317\\. A Conceptual Chronicle of Solving Raven’s Progressive Matrices Computationally](https://ceur-ws.org/Vol-3400/paper5.pdf)\n\n[318\\. R. Cattell. “Theory of fluid and crystallized intelligence: A critical experiment..” Journal of Educational Psychology](https://doi.org/10.1037/H0046743)\n\n[319\\. Chi Zhang, Feng Gao et al. “RAVEN: A Dataset for Relational and Analogical Visual REasoNing.” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2019.00546)\n\n[320\\. J. Au, E. Sheehan et al. “Improving fluid intelligence with training on working memory: a meta-analysis.” Psychonomic Bulletin & Review](https://doi.org/10.3758/s13423-014-0699-x)\n\n[321\\. Adam Santoro, Felix Hill et al. “Measuring abstract reasoning in neural networks.” ArXiv](https://arxiv.org/abs/1807.04225)\n\n[322\\. T. Klingberg. “Training and plasticity of working memory.” Trends in Cognitive Sciences](https://doi.org/10.1016/j.tics.2010.05.002)\n\n[323\\. Raven's Progressive Matrices](http://www.tkpv.com/Raven%20Progressive%20Matrices.htm)\n\n[324\\. Spaced Cognitive Training Promotes Training Transfer](https://www.frontiersin.org/articles/10.3389/fnhum.2014.00217/full)\n\n[325\\. Plan Diffuser: Grounding LLM Planners with Diffusion Models for Robotic Manipulation](https://openreview.net/pdf?id=2a3sgm5YeX)\n\n[326\\. Raven's Progressive Matrices - an overview - ScienceDirect](https://www.sciencedirect.com/topics/psychology/ravens-progressive-matrices)\n\n[327\\. Martin Buschkuehl, Susanne M. Jaeggi. “Improving intelligence: a literature review..” Swiss medical weekly](https://doi.org/10.4414/SMW.2010.12852)\n\n[328\\. N. Denney, Susan M. Heidrich. “Training effects on Raven's progressive matrices in young, middle-aged, and elderly adults..” Psychology and aging](https://doi.org/10.1037/0882-7974.5.1.144)\n\n[329\\. Raven's Standard Progressive Matrices for Adolescents](https://www.mdpi.com/2079-3200/11/4/72)\n\n[330\\. Process differences as a function of test modifications: Construct validity of Raven’s advanced progressive matrices under standard, abbreviated and/or speeded conditions – A meta-analysis](https://gwern.net/doc/iq/2022-tatel.pdf)\n\n[331\\. Raven's Matrices Test](https://nerdyseal.com/ravens-matrices-test/)\n\n[332\\. Raven’s Progressive Matrices, manipulations of complexity and measures of accuracy, speed and confidence](https://ptam-journal.com/wp-content/uploads/2025/01/03_Stankov.pdf)\n\n[333\\. Solving Raven's Progressive Matrices with Neural Networks](https://paperswithcode.com/paper/solving-ravens-progressive-matrices-with)\n\n[334\\. Raven's Progressive Matrices Test (2024 Guide)](https://www.testhq.com/blog/ravens-progressive-matrices-test)\n\n[336\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[337\\. Shaoqing Ren, Kaiming He et al. “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1109/TPAMI.2016.2577031)\n\n[338\\. Ross B. Girshick. “Fast R-CNN.”](https://arxiv.org/abs/1504.08083)\n\n[339\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[340\\. W. Liu, Dragomir Anguelov et al. “SSD: Single Shot MultiBox Detector.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-46448-0_2)\n\n[341\\. Peer-Reviewed Publications - 2025](https://www.beg.utexas.edu/publications)\n\n[342\\. IWORLD 2025 Report of Contributions](https://indico.cern.ch/event/1428808/contributions/contributions.pdf)\n\n[343\\. Fanqing Meng, Wenqi Shao et al. “PhyBench: A Physical Commonsense Benchmark for Evaluating Text-to-Image Models.” ArXiv](https://doi.org/10.48550/arXiv.2406.11802)\n\n[344\\. PEER-REVIEWED PUBLICATIONS](https://www.unige.ch/fapse/aging/application/files/4416/8554/0605/Kliegel_Publications_Mai_2023.pdf)\n\n[345\\. Andrew Melnik, Robin Schiewer et al. “Benchmarks for Physical Reasoning AI.” ArXiv](https://doi.org/10.48550/arXiv.2312.10728)\n\n[346\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[347\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[348\\. O. Ronneberger, P. Fischer et al. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” ArXiv](https://doi.org/10.1007/978-3-319-24574-4_28)\n\n[349\\. Jonathan Ho, Ajay Jain et al. “Denoising Diffusion Probabilistic Models.” ArXiv](https://arxiv.org/abs/2006.11239)\n\n[350\\. Robin Rombach, A. Blattmann et al. “High-Resolution Image Synthesis with Latent Diffusion Models.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01042)\n\n[351\\. ReconFusion: 3D Reconstruction with Diffusion Priors](https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_ReconFusion_3D_Reconstruction_with_Diffusion_Priors_CVPR_2024_paper.pdf)\n\n[352\\. RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Kara_RAVE_Randomized_Noise_Shuffling_for_Fast_and_Consistent_Video_Editing_CVPR_2024_paper.pdf)\n\n[353\\. Multi-scale conv-attention U-Net for medical image segmentation](https://www.nature.com/articles/s41598-025-96101-8.pdf)\n\n[354\\. VIDEO DIFFUSION MODELS](https://openreview.net/pdf?id=BBelR2NdDZ5)\n\n[355\\. Group Behavior Analysis in Videos](https://dr.ntu.edu.sg/bitstream/10356/182244/2/HuBo_PhD_Thesis.pdf)\n\n[356\\. A Research on Learned Image/Video Restoration and Compression for Solving Real-World Degradation](https://hosei.ecats-library.jp/da/repository/00025267/21_thesis_ho.pdf)\n\n[357\\. Video Diffusion Models](https://papers.neurips.cc/paper_files/paper/2022/file/39235c56aef13fb05a6adc95eb9d8d66-Paper-Conference.pdf)\n\n[358\\. Multi-scale conv-attention U-Net for medical image ...](https://www.nature.com/articles/s41598-025-96101-8)\n\n[359\\. Diffusion Model扩散模型2022-2024顶会论文摘录40篇（21-30）](https://zhuanlan.zhihu.com/p/688735213)\n\n[360\\. Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Upscale-A-Video_Temporal-Consistent_Diffusion_Model_for_Real-World_Video_Super-Resolution_CVPR_2024_paper.pdf)\n\n[361\\. simple diffusion: End-to-end diffusion for high resolution images](https://openreview.net/pdf?id=6l9YG3wHA9)\n\n[362\\. Zheng Dai, David K Gifford. “Ablation Based Counterfactuals.” ArXiv](https://doi.org/10.48550/arXiv.2406.07908)\n\n[363\\. Minimum Latency Deep Online Video Stabilization](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Minimum_Latency_Deep_Online_Video_Stabilization_ICCV_2023_paper.pdf)\n\n[364\\. Hybrid dilation and attention residual U-Net for medical image segmentation](https://sci-hub.se/downloads/2021-06-25/5755/wang2021.pdf)\n\n[365\\. Recurrent Multi-Scale U-Nets for Improved Medical Image Segmentation: A Comprehensive Investigation](https://openaccess.wgtn.ac.nz/ndownloader/files/53008259)\n\n[366\\. M. Daneman, P. Carpenter. “Individual differences in working memory and reading.” Journal of Verbal Learning and Verbal Behavior](https://doi.org/10.1016/S0022-5371%2880%2990312-6)\n\n[367\\. Raven's Progressive Matrices Test (2025 Guide) - TestHQ](https://www.testhq.com/blog/ravens-progressive-matrices-test#:~:text=Scores%20on%20the%20Raven%20test%20are%20interpreted%20in%20percentile%20ranks,problem-solving%20and%20reasoning%20skills.)\n\n[368\\. T. Salthouse, R. L. Babcock. “Decomposing adult age differences in working memory..” Developmental Psychology](https://doi.org/10.1037/0012-1649.27.5.763)\n\n[369\\. R. Cattell. “Theory of fluid and crystallized intelligence: A critical experiment..” Journal of Educational Psychology](https://doi.org/10.1037/H0046743)\n\n[370\\. Raven's Progressive Matrices Test (2024 Guide)](https://www.testhq.com/blog/ravens-progressive-matrices-test)\n\n[371\\. E. S. Conklin, C. Spearman. “\"THE ABILITIES OF MAN\"..” Science](https://doi.org/10.2307/1414471)\n\n[372\\. Diffusion Model with Perceptual Loss](http://arxiv.org/html/2401.00110v6)\n\n[373\\. Raven's Progressive Matrices – Test Guide 2024](https://psychometric-success.com/aptitude-tests/test-types/ravens-progressive-matrices)\n\n[374\\. Raven's Progressive Matrices Test - Guide & Tips (2024)](https://www.practicereasoningtests.com/ravens-progressive-matrices-test)\n\n[375\\. PHME 2024](https://phm-europe.org/wp-content/uploads/2024/07/Full_proceedings_PHME24_LR.pdf)\n\n[376\\. Raven's Progressive Matrices Test 2025: Questions, Strategies & Online Practice](https://www.jobtestprep.com/raven-matrices-assessment-test)\n\n[377\\. J. Raven. “Advanced progressive matrices : sets I and II.”](https://www.semanticscholar.org/paper/31f5a18e7bae014e65c030bdf6c6674e6bf58ef1)\n\n[378\\. Process differences as a function of test modifications: Construct validity of Raven’s advanced progressive matrices under standard, abbreviated and/or speeded conditions – A meta-analysis](https://gwern.net/doc/iq/2022-tatel.pdf)\n\n[379\\. Raven's Progressive Matrices - an overview - ScienceDirect](https://www.sciencedirect.com/topics/psychology/ravens-progressive-matrices)\n\n[380\\. Raven’s Progressive Matrices, manipulations of complexity and measures of accuracy, speed and confidence](https://ptam-journal.com/wp-content/uploads/2025/01/03_Stankov.pdf)\n\n[381\\. InterPore2024](https://events.interpore.org/event/46/book-of-abstracts.pdf)"
  },
  {
    "id": 39,
    "question": "Could the self-play mechanisms that mastered games like Dota 2 and StarCraft be adapted to accelerate scientific discovery in fields like physics or biology?",
    "response": "**Research Report**\n\n**To:** Interested Parties\n\n**From:** Expert Researcher\n\n**Date:** July 27, 2025\n\n**Subject:** Feasibility of Adapting Game-Mastering Self-Play AI for Scientific Discovery in Physics and Biology\n\n**Executive Summary**\n\nThis report assesses the potential for adapting the self-play reinforcement learning (RL) mechanisms, famously used to achieve superhuman performance in complex games like Go, Dota 2, and StarCraft II, to accelerate scientific discovery. As of mid-2025, the application of these techniques presents a bifurcated landscape. In computational biology, particularly protein engineering, self-play has emerged as a validated and powerful tool, exemplified by frameworks like **EvoPlay** which has produced experimentally verified improvements in protein function \\[1\\]\\[179\\]\\[275\\]. This success is largely predicated on the availability of a high-fidelity, albeit computationally expensive, \"simulation oracle\" in the form of AlphaFold2.\n\nConversely, in fields like condensed matter physics and quantum materials, the application of multi-agent self-play for discovery remains nascent and largely aspirational. A thorough review of recent literature (2023-2025) reveals a conspicuous absence of peer-reviewed, experimentally validated breakthroughs directly attributable to this methodology \\[135\\]\\[147\\]\\[195\\].\n\nThe primary barriers to broader adoption are fundamental: the transition from closed-game environments with perfect simulators to open-ended scientific problems with ambiguous goals and no ground truth \\[24\\]; the immense challenge of designing reward functions that incentivize genuine discovery rather than specification gaming \\[105\\]; and the prohibitive computational cost of simulating complex physical or biological systems at the scale required for self-play \\[29\\]\\[36\\]. Future progress hinges on developing more sophisticated intrinsic reward mechanisms, such as curiosity-driven exploration, and creating more efficient and accurate scientific simulators.\n\n**I. The Self-Play Paradigm: From Superhuman Game AI to a Tool for Science**\n\nThe foundation of modern self-play was laid by DeepMind's AlphaZero, which mastered Go, chess, and shogi without human game data, learning purely by playing against itself \\[61\\]. This paradigm typically combines a deep neural network, which serves as a policy-value function to evaluate board states and suggest moves, with a Monte Carlo Tree Search (MCTS) algorithm that intelligently explores the vast tree of possible future moves \\[32\\]\\[179\\]. Its successor, MuZero, further generalized this by learning a model of the environment's dynamics, removing the need for a pre-programmed, perfect simulator of the game's rules \\[23\\]\\[35\\].\n\nThese principles were scaled to achieve superhuman performance in vastly more complex, real-time, and imperfect-information games like Dota 2 (OpenAI Five) and StarCraft II (AlphaStar) \\[51\\]\\[60\\]. These systems demonstrated that competitive self-play could lead to the emergence of novel, complex, and highly effective strategies \\[46\\]\\[52\\]. The core appeal for science is tantalizing: could an AI agent, by \"playing the game\" of protein folding or particle physics against itself, discover novel solutions and principles that have eluded human researchers?\n\n**II. A Breakthrough in Biology: Self-Play for Protein Engineering**\n\nThe most compelling evidence for the successful adaptation of self-play to science comes from computational biology. The challenge of designing novel protein sequences with desired functions represents a vast, combinatorial search space, analogous to the game trees of chess or Go.\n\n**EvoPlay: An AlphaZero for Proteins**\n\nThe **EvoPlay** framework, first detailed in 2023, is a direct and successful translation of the AlphaZero paradigm to protein engineering \\[1\\]\\[3\\]\\[5\\]. It functions as a self-play RL system where:\n\nThe \"game\" is to design an optimal protein sequence.\n\nAn \"action\" is a single-site amino acid mutation \\[269\\].\n\nA policy-value neural network guides the search, and MCTS explores the sequence space \\[179\\]\\[276\\].\n\nCrucially, it leverages a pre-trained structure prediction model, **AlphaFold2**, as a \"structural surrogate\" or simulation oracle. After a mutation, EvoPlay uses AlphaFold2 to predict the resulting 3D structure and its properties, which then informs the reward signal \\[1\\]\\[179\\]\\[222\\].\n\n**Quantifiable Successes and Performance Metrics**\n\nEvoPlay has produced concrete, experimentally validated results. In one notable study, it was used to prospectively engineer luciferase variants, achieving a **7.8-fold improvement in bioluminescence** over the wild type, a significant discovery confirmed in wet-lab experiments \\[179\\]\\[275\\]. It has also been shown to design peptide binders with high affinity and generate high-quality binders at a significantly higher hit rate than other computational methods \\[1\\]\\[249\\]\\[393\\].\n\nPerformance in this domain is evaluated using metrics that mirror the goals of the design process. The EvoPlay framework and associated benchmarks like ProteinInvBench utilize metrics such as:\n\n**Fitness:** A measure of the desired property, such as binding affinity or bioluminescence \\[61\\]\\[64\\].\n\n**Diversity:** The average difference between generated sequences, often measured using Levenshtein distance, ensuring the exploration of varied solutions \\[65\\]\\[330\\].\n\n**Structural Consistency:** The self-consistency TM score (sc-TM), which measures how well the designed sequence folds back into the target structure \\[65\\]\\[282\\].\n\n**The Unanswered Question: Benchmarking Against Traditional Methods**\n\nDespite EvoPlay's success, a significant gap exists in the literature: there are no direct, peer-reviewed, quantitative benchmark comparisons between self-play frameworks like EvoPlay and established, physics-based computational methods like **Rosetta@home** \\[116\\]\\[134\\]\\[178\\]. While Rosetta is known for its detailed energy functions and has been a cornerstone of protein modeling for years \\[186\\]\\[245\\]\\[351\\]it operates on a different principle (distributed computing of energy landscapes) than EvoPlay's goal-directed RL search. Numerous queries for direct comparisons on standard benchmarks like the CASP datasets using metrics like GDT-TS and wall-clock time yielded no results \\[288\\]\\[299\\]\\[346\\]. This makes it difficult to definitively quantify the efficiency and accuracy gains of self-play over traditional approaches in a head-to-head manner.\n\n**III. The Physics Frontier: A Realm of Unrealized Potential**\n\nIn stark contrast to biology, the application of multi-agent self-play to accelerate discovery in physics, particularly in areas like quantum materials and condensed matter, appears to be in a very early, pre-breakthrough stage. Extensive searches for peer-reviewed studies published between 2023 and July 2025 demonstrating such contributions yielded no direct case studies of experimentally validated discoveries \\[135\\]\\[147\\]\\[195\\].\n\nThe search results point to related but distinct areas of research:\n\n**Multi-Agent Collaboration:** Frameworks like **PriM** use multi-agent _collaboration_—leveraging different AI agents (e.g., planners, executors) with access to computational tools—for general material discovery. However, these do not appear to be based on the competitive or iterative self-play paradigm \\[144\\]\\[203\\]\\[203\\].\n\n**AI for Quantum Problems:** AI has been used to simplify complex quantum problems, for instance by reducing the number of equations needed for a simulation \\[382\\]. One 2023 paper described a quantum multi-agent neural network for coordinating robots in a factory, but this is an application in engineering, not fundamental physics discovery \\[323\\].\n\nThe lack of progress suggests that the fundamental challenges of adapting self-play are significantly more pronounced in physics than in the specific sub-domain of protein engineering.\n\n**IV. Bridging the Gap: Core Technical Challenges in Adapting Self-Play for Science**\n\nThe disparity in success between biology and physics highlights several core technical hurdles that must be overcome to generalize the self-play approach.\n\n**Challenge 1: The Open-Ended Problem Space**\n\nGames are closed systems with explicit rules, defined win/loss conditions, and a finite (though often vast) set of possible actions. Scientific discovery is the opposite: an open-ended problem with incompletely known rules, ambiguous goals, and a poorly defined search space \\[24\\]. The lack of clear boundaries makes it difficult for an RL agent to generalize and know when it has \"won\" or made a meaningful discovery.\n\n**Challenge 2: The Simulation Oracle**\n\nSelf-play requires an environment to \"play\" in. For AlphaZero, this was the game engine. For EvoPlay, AlphaFold2 acts as this oracle, providing fast, high-quality structural predictions that serve as the basis for reward \\[1\\]\\[179\\]. The absence of an equivalent fast and accurate simulator for many problems in quantum materials or fundamental physics is a primary bottleneck. Simulating these systems from first principles is often computationally intractable, making the millions of \"games\" required for self-play training an impossibility.\n\n**Challenge 3: The Art of Reward Function Design for \"Discovery\"**\n\nPerhaps the most profound challenge is defining a reward function for discovery. In a game, the reward is simple: +1 for a win, -1 for a loss. In science, what is the reward for a \"novel\" protein or a \"surprising\" physical phenomenon? A naive reward function can lead to **specification gaming**, where the agent finds loopholes to maximize its score without achieving the intended scientific objective \\[105\\].\n\nTo address this, researchers are exploring **intrinsic motivation**, where the agent generates its own reward signal. The most prominent approach is **curiosity-driven exploration**:\n\nThe agent is rewarded for visiting novel states or for making actions whose outcomes it cannot accurately predict \\[103\\]\\[98\\]\\[168\\].\n\nThis is often implemented by training an auxiliary neural network to predict the next state; the prediction error becomes the curiosity reward, incentivizing the agent to explore parts of the environment it doesn't understand \\[169\\].\n\nWhile concepts like \"Curiosity Driven Protein Sequence Generation\" exist \\[231\\], and the EvoPlay GitHub repository includes code to calculate novelty and diversity metrics \\[330\\]\\[385\\]the search results provide no published, documented implementation of a specific **neural intrinsic curiosity module** being integrated directly into the EvoPlay framework for de novo protein design \\[216\\]\\[231\\]\\[269\\]. This remains a critical area for future research.\n\n**V. Conclusion and Future Outlook (as of July 27, 2025)**\n\nThe adaptation of self-play RL from games to scientific discovery is a field of immense promise and significant challenge. It is not a universal acid that can dissolve any scientific problem, but rather a specialized tool whose effectiveness is contingent on the specific characteristics of the domain.\n\n**Current State of the Art:** The application in computational biology, particularly protein engineering with EvoPlay, stands as the flagship success. It has demonstrated that when a high-fidelity simulation oracle (like AlphaFold2) is available, self-play can be a powerful engine for directed evolution and functional design, yielding experimentally validated results.\n\n**Major Gaps:** The application in physics and materials science remains largely theoretical. The lack of fast, accurate simulators and the difficulty in defining a reward for fundamental discovery are the primary impediments. Furthermore, even in the successful domain of biology, rigorous, head-to-head benchmarking against traditional computational methods is needed to precisely quantify the advantages of the self-play approach.\n\n**Future Directions:** The path forward involves a multi-pronged research effort focused on:\n\n1.  **Building Better Oracles:** Developing faster and more accurate simulators for complex physical and biological systems.\n2.  **Sophisticated Reward Engineering:** Moving beyond simple fitness functions to systematically integrate intrinsic rewards like curiosity, surprise, and novelty to guide agents toward genuinely interesting and non-obvious solutions.\n3.  **Domain Expansion:** Identifying other scientific problems that share the characteristics that made protein design tractable—a vast combinatorial search space coupled with a reasonable simulation oracle—as the next targets for this technology.\n\nIn conclusion, self-play is successfully making the leap from gaming to science, but its beachhead is currently confined to a specific, well-suited problem in biology. Expanding that beachhead to other fields will require fundamental advances in both simulation and the AI techniques used to navigate open-ended discovery.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. A Survey on Self-play Methods in Reinforcement Learning](https://arxiv.org/pdf/2408.01072)\n\n[2\\. R. S. Sutton, A. Barto. “Reinforcement Learning: An Introduction.” IEEE Trans. Neural Networks](https://doi.org/10.1109/TNN.1998.712192)\n\n[3\\. Self-play reinforcement learning guides protein engine...](https://link.springer.com/article/10.1038/s42256-023-00691-9)\n\n[4\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[5\\. Self-play reinforcement learning guides protein engineering | Nature Machine Intelligence](https://www.nature.com/articles/s42256-023-00691-9)\n\n[6\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[7\\. HighPlay : Cyclic Peptide Sequence Design Based on Reinforcement Learning and Protein Structure Prediction](https://www.biorxiv.org/content/10.1101/2025.03.17.643626v1.full.pdf)\n\n[8\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[9\\. Yixian Zhang, Huaze Tang et al. “Bidirectional Soft Actor-Critic: Leveraging Forward and Reverse KL Divergence for Efficient Reinforcement Learning.”](https://arxiv.org/abs/2506.01639)\n\n[10\\. David Silver, Julian Schrittwieser et al. “Mastering the game of Go without human knowledge.” Nature](https://doi.org/10.1038/nature24270)\n\n[11\\. Yicheng Lin, Jiakang Ma et al. “Integrating Reinforcement Learning and Monte Carlo Tree Search for enhanced neoantigen vaccine design.” Briefings in Bioinformatics](https://doi.org/10.1093/bib/bbae247)\n\n[12\\. Reinforcement Learning for Science](https://ai4s.lab.westlake.edu.cn/assets/file/courseware/2025/05_2_Reinforcement%20learning%20for%20science.pdf)\n\n[13\\. Differentially Private Reinforcement Learning with Self-Play](https://proceedings.neurips.cc/paper_files/paper/2024/file/a0da690a47b2f52faa63f6fe054057b5-Paper-Conference.pdf)\n\n[14\\. Modeling protein motions through reinforcement learning](https://pmc.ncbi.nlm.nih.gov/articles/PMC11665888/)\n\n[21\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[22\\. David Silver, Julian Schrittwieser et al. “Mastering the game of Go without human knowledge.” Nature](https://doi.org/10.1038/nature24270)\n\n[23\\. ENHANCED MONTE CARLO TREE SEARCH IN GAME-PLAYING AI: EVALUATING DEEPMIND'S ALGORITHMS](https://espace.rmc-cmr.ca/jspui/bitstream/11264/1502/1/MASTER_THESIS_Karla_Aug_2023%20%281%29.pdf)\n\n[24\\. AlphaZero：利用自学能力彻底改变人工智能 - Julien Florkin](https://julienflorkin.com/zh-CN/technology/artificial-intelligence/alphazero/)\n\n[25\\. Veštačka inteligencija](http://www.matf.bg.ac.rs/~predrag.janicic/books/VI_B5.pdf)\n\n[26\\. C. Browne, E. Powley et al. “A Survey of Monte Carlo Tree Search Methods.” IEEE Transactions on Computational Intelligence and AI in Games](https://doi.org/10.1109/TCIAIG.2012.2186810)\n\n[27\\. ELF OpenGo: An Analysis and Open Reimplementation of AlphaZero](http://proceedings.mlr.press/v97/tian19a/tian19a-supp.pdf)\n\n[28\\. Exploiting and Generalizing Epistemic Uncertainty in Reinforcement Learning and Planning](https://amsdottorato.unibo.it/id/eprint/11445/1/likmeta_amarildo_tesi.pdf)\n\n[29\\. Mastering the Game of Abalone using Deep Reinforcement Learning and Self-Play](https://serwiss.bib.hs-hannover.de/frontdoor/deliver/index/docId/2278/file/claussen2022-abalone_deep_rl_self-play.pdf)\n\n[30\\. AlphaZero登上Science封面:从小白开始制霸多个游戏](http://www.36kr.com/p/5166128.html)\n\n[31\\. Warm-Start AlphaZero Self-play Search Enhancements](https://link.springer.com/chapter/10.1007/978-3-030-58115-2_37)\n\n[32\\. Miquel Llobet Sanchez. “Learning complex games through self play - Pokémon battles.”](https://www.semanticscholar.org/paper/8f6a059ecc86e02f98c973acc9cf6815291a1880)\n\n[33\\. Julian Schrittwieser, Ioannis Antonoglou et al. “Mastering Atari, Go, chess and shogi by planning with a learned model.” Nature](https://doi.org/10.1038/s41586-020-03051-4)\n\n[34\\. AlphaZero登上《科学》封面:一个算法“通杀”三大棋](https://m.tmtpost.com/3632324.html)\n\n[35\\. Reflection AI: The Race to Unlock Superintelligence](https://www.sequoiacap.com/article/reflection-ai-spotlight/)\n\n[36\\. Policy or Value? Loss Function and Playing Strength in AlphaZero-like Self-play](https://liacs.leidenuniv.nl/~plaata1/papers/CoG2019.pdf)\n\n[37\\. Near-Optimal Reinforcement Learning with Self-Play](https://proceedings.neurips.cc/paper/2020/file/172ef5a94b4dd0aa120c6878fc29f70c-Paper.pdf)\n\n[38\\. Y. Björnsson, Róbert Leó Þormar Jónsson et al. “Expediting Self-Play Learning in AlphaZero-Style Game-Playing Agents.” European Conference on Artificial Intelligence](https://doi.org/10.3233/FAIA230279)\n\n[41\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[42\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[43\\. David Silver, Julian Schrittwieser et al. “Mastering the game of Go without human knowledge.” Nature](https://doi.org/10.1038/nature24270)\n\n[44\\. Christopher Berner, Greg Brockman et al. “Dota 2 with Large Scale Deep Reinforcement Learning.” ArXiv](https://arxiv.org/abs/1912.06680)\n\n[45\\. O. Vinyals, Igor Babuschkin et al. “Grandmaster level in StarCraft II using multi-agent reinforcement learning.” Nature](https://doi.org/10.1038/s41586-019-1724-z)\n\n[46\\. Accelerate Multi-Agent Reinforcement Learning in Zero-Sum Games with Subgame Curriculum Learning](http://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/4ba7865b-63d1-4605-9a88-8fd4ef7135b3.pdf)\n\n[47\\. A Survey on Self-play Methods in Reinforcement Learning](https://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/db43f779-dd0e-4f2e-a51c-1caa107e21eb.pdf)\n\n[48\\. 加速科学发现一直是人工智能（AI）研究的长期目标，早期的项目如 1979 年的橡树岭应用人工智能项目探索了（Team, 1985; Emrich et al., 1988; Johnson and Schaffer, 1994）。随着基础模型的进步，更近期的探索提供了一个端到端论文生成过程的完全自动化流程的概念验证（Achiam et al., 2023; Anthropic, 2024; Team et al., 2024; Dubey et al., 2024）。未来，我们设想 AI 研究代理能够独立进行文献搜索、生成科学假设、设计实验、实施新方法、分析结果，通过撰写科学论文传播发现，并在产品中应用此研究，从而协助研究过程的所有部分。这些代理应该能够完全自主工作或在人类监督下进行工作，考虑到用户的反馈。这个愿景来自于认识到 AI 有能力处理庞大数据集并辨识复杂模式，可以通过识别有前景的药物候选者或预测新材料的属性来加速药物发现和材料科学领域的科学突破（Hessler and Baringhaus, 2018; Schneider et al., 2020; Guo et al., 2021）。与传统方法不同，AI 代理可以通过分析庞大的知识图谱揭示隐藏的跨学科关系，提出复杂挑战（如气候建模）的新见解和解决方案。通过自动化繁重的任务和探索非传统途径，AI 代理可以解放科学家们，使之专注于更高层次的认知活动，从而推动创新并拓展知识的前沿。机器学习（ML）的研究，重视仿真中的经验验证和系统实验，为探索和提高 LLMs 在推动科学研究中的效用提供了理想的试验场。](https://www.xueshuxiangzi.com/downloads/2025_2_21/2502.14499.pdf)\n\n[49\\. Giulia Chemi, Simone Brogi. “Breakthroughs in Computational Approaches for Drug Discovery.”](https://doi.org/10.16966/2470-1009.129)\n\n[50\\. Hamid Omidian. “AI-powered breakthroughs in material science and biomedical polymers.” Journal of Bioactive and Compatible Polymers](https://doi.org/10.1177/08839115241308202)\n\n[51\\. Peng Sun, Jiechao Xiong et al. “TLeague: A Framework for Competitive Self-Play based Distributed Multi-Agent Reinforcement Learning.” ArXiv](https://arxiv.org/abs/2011.12895)\n\n[52\\. Yichuan Tang. “Towards Learning Multi-Agent Negotiations via Self-Play.” 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)](https://doi.org/10.1109/ICCVW.2019.00297)\n\n[55\\. R. S. Sutton, A. Barto. “Reinforcement Learning: An Introduction.” IEEE Trans. Neural Networks](https://doi.org/10.1109/TNN.1998.712192)\n\n[56\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[57\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[58\\. ProteinInvBench: Benchmarking Protein Inverse Folding on Diverse Tasks, Models, and Metrics](https://openreview.net/pdf?id=bqXduvuW5E)\n\n[59\\. David Silver, Julian Schrittwieser et al. “Mastering the game of Go without human knowledge.” Nature](https://doi.org/10.1038/nature24270)\n\n[60\\. A Survey on Self-play Methods in Reinforcement Learning](https://arxiv.org/pdf/2408.01072)\n\n[61\\. Self-play reinforcement learning guides protein engineering](https://gwern.net/doc/ai/nn/transformer/alphafold/2023-wang-3.pdf)\n\n[62\\. Enhancing PiFold: A new Deep Learning method for Inverse Protein Folding](https://oa.upm.es/83713/1/TFM_JOAQUIN_ALGORTA_BOVE.pdf)\n\n[63\\. R. Brafman, Moshe Tennenholtz. “R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning.” J. Mach. Learn. Res.](https://doi.org/10.1162/153244303765208377)\n\n[64\\. Self-play reinforcement learning guides protein engineering | Nature Machine Intelligence](https://www.nature.com/articles/s42256-023-00691-9)\n\n[65\\. Reinforcement learning on structure-conditioned catego...](http://arxiv.org/html/2410.17173v1)\n\n[66\\. Self-Supervised Contrastive Learning of Protein...](https://www.biorxiv.org/content/10.1101/2020.09.04.283929v1.full)\n\n[75\\. A Survey on Self-play Methods in Reinforcement Learning](https://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/db43f779-dd0e-4f2e-a51c-1caa107e21eb.pdf)\n\n[76\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[77\\. David Silver, Julian Schrittwieser et al. “Mastering the game of Go without human knowledge.” Nature](https://doi.org/10.1038/nature24270)\n\n[78\\. Ryan Lowe, Yi Wu et al. “Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.” ArXiv](https://arxiv.org/abs/1706.02275)\n\n[79\\. 脑科学与人工智能Arxiv每日论文推送 2023.10.29](https://zhuanlan.zhihu.com/p/663984479)\n\n[80\\. O. Vinyals, Igor Babuschkin et al. “Grandmaster level in StarCraft II using multi-agent reinforcement learning.” Nature](https://doi.org/10.1038/s41586-019-1724-z)\n\n[81\\. Accelerate Multi-Agent Reinforcement Learning in Zero-Sum Games with Subgame Curriculum Learning](http://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/4ba7865b-63d1-4605-9a88-8fd4ef7135b3.pdf)\n\n[82\\. Marc Lanctot, V. Zambaldi et al. “A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning.” Neural Information Processing Systems](https://arxiv.org/abs/1711.00832)\n\n[83\\. OpenRL: A Unified Reinforcement Learning Framework](https://www.aidd.vip/resources/upload/a7844a45d8ab55e/file/%E9%BB%84%E4%B8%96%E5%AE%87-OpenRL%E6%94%AF%E6%8C%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%E4%BB%A3%E7%9A%84PluginStore-%E5%B7%B2%E4%BF%AE%E6%94%B9.pdf)\n\n[84\\. 基于多智能体强化学习的博弈综述](https://dds.sciengine.com/cfs/files/pdfs/0254-4156/7E0D52D058BD432D824BA1270B666A28.pdf)\n\n[85\\. 开放环境下的协作多智能体强化学习进展](http://scis.scichina.com/cn/2025/SSI-2023-0335.pdf)\n\n[86\\. \\[AAAI2024\\] Accelerate Multi-Agent Reinforcement Learning in Zero-Sum Games with](https://www.bilibili.com/video/av707629299?t=580)\n\n[87\\. Yisak Park, Sunwoo Lee et al. “Center of Gravity-Guided Focusing Influence Mechanism for Multi-Agent Reinforcement Learning.”](https://arxiv.org/abs/2506.19417)\n\n[88\\. Engineering Fronts 2023](http://www.engineering.org.cn/views/uploadfiles/Engineering%20Fronts%202023.pdf)\n\n[89\\. Fanqing Lin, Shiyu Huang et al. “TiZero: Mastering Multi-Agent Football with Curriculum Learning and Self-Play.” Adaptive Agents and Multi-Agent Systems](https://doi.org/10.48550/arXiv.2302.07515)\n\n[90\\. Dan Qiao, Yu-Xiang Wang. “Differentially Private Reinforcement Learning with Self-Play.” ArXiv](https://doi.org/10.48550/arXiv.2404.07559)\n\n[91\\. Jingxiao Chen, Weiji Xie et al. “Offline Fictitious Self-Play for Competitive Games.” ArXiv](https://doi.org/10.48550/arXiv.2403.00841)\n\n[92\\. ...Multi-Agent Negotiations via Self-Play - Apple Mach...](https://machinelearning.apple.com/research/towards-learning-multi-agent-negotiations-via-self-play)\n\n[95\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[96\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[97\\. David Silver, Julian Schrittwieser et al. “Mastering the game of Go without human knowledge.” Nature](https://doi.org/10.1038/nature24270)\n\n[98\\. Richard M. Bailey. “Continuously evolving rewards in an open-ended environment.” ArXiv](https://doi.org/10.48550/arXiv.2405.01261)\n\n[99\\. 从自我进化视角出发，全面解析LLM的推理能力技术演进路径](https://finance.sina.com.cn/tech/roll/2025-03-06/doc-inentpez7713896.shtml)\n\n[100\\. 基于深度强化学习的作战概念能力需求分析关键技术](https://www.zhkzyfz.cn/fileup/1673-3819/PDF/1717030774851-203807959.pdf)\n\n[101\\. Deep Learning for Reward Design to Improve Monte Carlo Tree Search in ATARI Games](https://web.eecs.umich.edu/~honglak/ijcai2016-rewardDesign.pdf)\n\n[102\\. Reward Systems in Human Computation Games | Proceeding...](https://dl.acm.org/doi/10.1145/2967934.2968083)\n\n[103\\. Deepak Pathak, Pulkit Agrawal et al. “Curiosity-Driven Exploration by Self-Supervised Prediction.” 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)](https://doi.org/10.1109/CVPRW.2017.70)\n\n[104\\. Robin Hunicke, M. Leblanc et al. “MDA : A Formal Approach to Game Design and Game Research.”](https://www.semanticscholar.org/paper/2b134e5c46eec50f69c702c0b4aa29687d5d8fba)\n\n[105\\. Q. Davis. “Carle's Game: An Open-Ended Challenge in Exploratory Machine Creativity.” 2021 IEEE Conference on Games (CoG)](https://doi.org/10.1109/CoG52621.2021.9619011)\n\n[106\\. Crafting Engaging Reward Systems](https://www.numberanalytics.com/blog/crafting-engaging-reward-systems)\n\n[107\\. Design and Approach to Digital Game-Based Learning](https://www.warchild.net/documents/102/180003517_-_WC_Game_Documentation_Report_Design_v10_interactive_1.pdf)\n\n[108\\. Open-Ended Learning Team, Adam Stooke et al. “Open-Ended Learning Leads to Generally Capable Agents.” ArXiv](https://arxiv.org/abs/2107.12808)\n\n[109\\. 以人为中心的奖励设计：改善智能系统行为的研究- AI资讯](https://www.xinfinite.net/t/topic/7556)\n\n[110\\. Artificial and Computational Intelligence in Games: Revolutions in Computational Game AI](https://drops.dagstuhl.de/storage/04dagstuhl-reports/volume09/issue12/19511/DagRep.9.12.67/DagRep.9.12.67.pdf)\n\n[111\\. Collaborative Problem Solving in an Open-Ended Scienti...](https://www.ncbi.nlm.nih.gov/pubmed/30198019)\n\n[112\\. 基于强化学习与规则库增强的双方群体博弈策略训练方法研究](https://www.rjdk.org.cn/rc-pub/front/front-article/download/82077197/lowqualitypdf/%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%A7%84%E5%88%99%E5%BA%93%E5%A2%9E%E5%BC%BA%E7%9A%84%E5%8F%8C%E6%96%B9%E7%BE%A4%E4%BD%93%E5%8D%9A%E5%BC%88%E7%AD%96%E7%95%A5%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6.pdf)\n\n[113\\. Robert Meier, Asier Mujika. “Open-Ended Reinforcement Learning with Neural Reward Functions.” ArXiv](https://arxiv.org/abs/2202.08266)\n\n[115\\. J. Jumper, Richard Evans et al. “Highly accurate protein structure prediction with AlphaFold.” Nature](https://doi.org/10.1038/s41586-021-03819-2)\n\n[116\\. Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design](http://proceedings.mlr.press/v139/cao21a/cao21a.pdf)\n\n[117\\. Efficient and accurate prediction of protein structure using RoseTTAFold2](https://www.biorxiv.org/content/10.1101/2023.05.24.542179v1.full.pdf)\n\n[118\\. ColabFold - Making protein folding accessible to all](https://www.biorxiv.org/content/10.1101/2021.08.15.456425v2.full.pdf)\n\n[119\\. SE(3)-Equivariant Energy-based Models for End-to-End Protein Folding](https://www.biorxiv.org/content/10.1101/2021.06.06.447297v1.full.pdf)\n\n[120\\. State-of-the-art estimation of protein model accuracy using AlphaFold: Supplemental Information](https://journals.aps.org/prl/supplemental/10.1103/PhysRevLett.129.238101/af2rank_revtex_25Oct2022_SI.pdf)\n\n[121\\. Algorithm discovery by protein folding game players](https://www.cs.cornell.edu/courses/cs6700/2013sp/readings/02-b-FoldIt-PNAS.pdf)\n\n[122\\. Sidhartha Chaudhury, Monica Berrondo et al. “Benchmarking and Analysis of Protein Docking Performance in Rosetta v3.2.” PLoS ONE](https://doi.org/10.1371/journal.pone.0022477)\n\n[123\\. The computational models of AlphaFold2 and RoseTTAfold carry protein foldability information](https://www.biorxiv.org/content/10.1101/2022.01.27.477978v1.full.pdf)\n\n[124\\. Accurate prediction of nucleic acid and protein-nucleic acid complexes using RoseTTAFoldNA](https://www.biorxiv.org/content/10.1101/2022.09.09.507333v1.full.pdf)\n\n[125\\. Rosetta@home：分布式计算项目在蛋白质结构预测中的应用](https://www.bionity.com/en/encyclopedia/Rosetta@home.html)\n\n[126\\. Solving protein folding - humans outperform computers](https://mindblog.dericbownds.net/2010/08/solving-protein-folding-humans.html)\n\n[127\\. DeepFoldit - A Deep Reinforcement Learning Neural Network Folding Proteins](https://pergamos.lib.uoa.gr/uoa/dl/object/2884734/file.pdf)\n\n[128\\. Rui Yin, Brandon Yushan Feng et al. “Benchmarking AlphaFold for protein complex modeling reveals accuracy determinants.” Protein Science : A Publication of the Protein Society](https://doi.org/10.1002/pro.4379)\n\n[129\\. Multistate and functional protein design using RoseTTAFold sequence space diffusion](https://www.nature.com/articles/s41587-024-02395-w.pdf)\n\n[130\\. A Survey on Self-play Methods in Reinforcement Learning](https://arxiv.org/pdf/2408.01072)\n\n[131\\. Rosetta@home](https://www.equn.com/wiki/Rosetta@home)\n\n[132\\. K. Simons, C. Kooperberg et al. “Assembly of protein tertiary structures from fragments with similar local sequences using simulated annealing and Bayesian scoring functions..” Journal of molecular biology](https://doi.org/10.1006/JMBI.1997.0959)\n\n[133\\. Gaming the System: Video Gamers Help Researchers Untangle Protein Folding Problem](http://www.scientificamerican.com/article/gaming-the-system-video-gamers-help-researchers-untangle-protein-folding-problem/)\n\n[134\\. Rebecca F. Alford, Andrew Leaver-Fay et al. “The Rosetta all-atom energy function for macromolecular modeling and design.” bioRxiv](https://doi.org/10.1101/106054)\n\n[135\\. A Survey on Self-play Methods in Reinforcement Learning](https://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/db43f779-dd0e-4f2e-a51c-1caa107e21eb.pdf)\n\n[136\\. Accelerate Multi-Agent Reinforcement Learning in Zero-Sum Games with Subgame Curriculum Learning](http://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/4ba7865b-63d1-4605-9a88-8fd4ef7135b3.pdf)\n\n[137\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[138\\. LTD lab-publications](http://hubs.phys.tsinghua.edu.cn/LTDlab/publications.html)\n\n[139\\. David Silver, Julian Schrittwieser et al. “Mastering the game of Go without human knowledge.” Nature](https://doi.org/10.1038/nature24270)\n\n[140\\. Cooperative and Competitive Multi-Agent Systems: From Optimization to Games](https://www.ieee-jas.net/article/doi/10.1109/JAS.2022.105506)\n\n[141\\. Ryan Lowe, Yi Wu et al. “Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.” ArXiv](https://arxiv.org/abs/1706.02275)\n\n[142\\. 深空探测器多智能体强化学习自主任务规划](https://www.sciengine.com/doi/pdfView/1357D1C455DE446D8F02CDFA2F368345)\n\n[143\\. 基于多智能体强化学习的博弈综述](http://www.aas.net.cn/cn/article/doi/10.16383/j.aas.c240478)\n\n[144\\. Zheyuan Lai, Yingming Pu. “PriM: Principle-Inspired Material Discovery through Multi-Agent Collaboration.”](https://arxiv.org/abs/2504.08810)\n\n[145\\. O. Vinyals, Igor Babuschkin et al. “Grandmaster level in StarCraft II using multi-agent reinforcement learning.” Nature](https://doi.org/10.1038/s41586-019-1724-z)\n\n[146\\. Jakob N. Foerster, Gregory Farquhar et al. “Counterfactual Multi-Agent Policy Gradients.” AAAI Conference on Artificial Intelligence](https://doi.org/10.1609/aaai.v32i1.11794)\n\n[147\\. Jeremy Fersula, Nicolas Bredeche et al. “Self-aligning active agents with inertia and active torque.” Physical Review E](https://doi.org/10.1103/physreve.110.014606)\n\n[148\\. Xinran Li, Zifan Liu et al. “Individual Contributions as Intrinsic Exploration Scaffolds for Multi-agent Reinforcement Learning.” ArXiv](https://doi.org/10.48550/arXiv.2405.18110)\n\n[149\\. S. McAleer, Jb Lanier et al. “Self-Play PSRO: Toward Optimal Populations in Two-Player Zero-Sum Games.” ArXiv](https://doi.org/10.48550/arXiv.2207.06541)\n\n[150\\. Shaowei Zhang, Jiahang Cao et al. “Self-Motivated Multi-Agent Exploration.” Adaptive Agents and Multi-Agent Systems](https://doi.org/10.48550/arXiv.2301.02083)\n\n[151\\. Zhitao He, Pengfei Cao et al. “LEGO: A Multi-agent Collaborative Framework with Role-playing and Iterative Feedback for Causality Explanation Generation.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/2023.findings-emnlp.613)\n\n[152\\. Tianyu Qiu, David Fridovich-Keil. “Inferring Occluded Agent Behavior in Dynamic Games from Noise Corrupted Observations.”](https://arxiv.org/abs/2303.09744)\n\n[155\\. J. Jumper, Richard Evans et al. “Highly accurate protein structure prediction with AlphaFold.” Nature](https://doi.org/10.1038/s41586-021-03819-2)\n\n[156\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[157\\. M. Chadi, H. Mousannif et al. “Curiosity as a Self-Supervised Method to Improve Exploration in De Novo Drug Design.” 2023 International Conference on Information Technology Research and Innovation (ICITRI)](https://doi.org/10.1109/ICITRI59340.2023.10249596)\n\n[158\\. Computational Theories of Curiosity-Driven Learning](https://osf.io/6vyrd/download)\n\n[159\\. De novo protein design – from new structures to programmable functions](https://escholarship.org/content/qt3bj156jx/qt3bj156jx_noSplash_84e29d046e5b8460b0a88a98e63e6ab5.pdf?t=srigri)\n\n[160\\. Curiosity Driven Multi-agent Reinforcement Learning fo...](http://arxiv.org/html/2502.14606v1)\n\n[161\\. Unity ML-Agents: A flexible platform for Deep RL research](https://www.microsoft.com/en-us/research/uploads/prod/2018/03/FinalAJulianiMSR_2018.pdf)\n\n[162\\. De novo protein design by citizen scientists | Nature](https://www.nature.com/articles/s41586-019-1274-4)\n\n[163\\. Curiosity-driven exploration: Diversity of mechanisms and functions](https://inria.hal.science/hal-03447896/document)\n\n[164\\. Jianyi Yang, I. Anishchenko et al. “Improved protein structure prediction using predicted interresidue orientations.” Proceedings of the National Academy of Sciences](https://doi.org/10.1073/pnas.1914677117)\n\n[165\\. Andrew D. McNaughton, Mridula Bontha et al. “De novo design of protein target specific scaffold-based Inhibitors via Reinforcement Learning.” ArXiv](https://doi.org/10.48550/arXiv.2205.10473)\n\n[166\\. Advances and challenges in learning from experience replay](https://link.springer.com/content/pdf/10.1007/s10462-024-11062-0.pdf)\n\n[167\\. Marc G. Bellemare, S. Srinivasan et al. “Unifying Count-Based Exploration and Intrinsic Motivation.” Neural Information Processing Systems](https://arxiv.org/abs/1606.01868)\n\n[168\\. Deepak Pathak, Pulkit Agrawal et al. “Curiosity-Driven Exploration by Self-Supervised Prediction.” 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)](https://doi.org/10.1109/CVPRW.2017.70)\n\n[169\\. Современные технологии в теории и практике программирования](https://hsse.spbstu.ru/userfiles/files/Sovremennie-tehnologii-v-teorii-i-praktike-programmirovaniya-2022-s-titulami.pdf)\n\n[170\\. De Novo Protein Design](https://www.nersc.gov/assets/Uploads/David-Baker-slides-NERSC-50.pdf)\n\n[175\\. J. Jumper, Richard Evans et al. “Highly accurate protein structure prediction with AlphaFold.” Nature](https://doi.org/10.1038/s41586-021-03819-2)\n\n[176\\. Jianyi Yang, I. Anishchenko et al. “Improved protein structure prediction using predicted interresidue orientations.” Proceedings of the National Academy of Sciences](https://doi.org/10.1073/pnas.1914677117)\n\n[177\\. K. Simons, C. Kooperberg et al. “Assembly of protein tertiary structures from fragments with similar local sequences using simulated annealing and Bayesian scoring functions..” Journal of molecular biology](https://doi.org/10.1006/JMBI.1997.0959)\n\n[178\\. Rebecca F. Alford, Andrew Leaver-Fay et al. “The Rosetta all-atom energy function for macromolecular modeling and design.” bioRxiv](https://doi.org/10.1101/106054)\n\n[179\\. Self-play reinforcement learning guides protein engine...](https://link.springer.com/article/10.1038/s42256-023-00691-9)\n\n[180\\. Efficient and accurate prediction of protein structure using RoseTTAFold2](https://www.biorxiv.org/content/10.1101/2023.05.24.542179v1.full.pdf)\n\n[181\\. Proceedings of RESEARCHWORLD INTERNATIONAL CONFERENCE Berlin, Germany](https://themahadevworld.com/wp-content/uploads/2023/03/Proceedings-File-Sachin-Raval-Title-1.pdf)\n\n[182\\. Gaming the System: Video Gamers Help Researchers Untangle Protein Folding Problem](https://www.scientificamerican.com/article/gaming-the-system-video-gamers-help-researchers-untangle-protein-folding-problem/)\n\n[183\\. Sidhartha Chaudhury, Monica Berrondo et al. “Benchmarking and Analysis of Protein Docking Performance in Rosetta v3.2.” PLoS ONE](https://doi.org/10.1371/journal.pone.0022477)\n\n[184\\. Xiaoqiang Huang, Robin Pearce et al. “EvoEF2: accurate and fast energy function for computational protein design.” Bioinformatics](https://doi.org/10.1093/bioinformatics/btz740)\n\n[185\\. Andrew Leaver-Fay, M. Tyka et al. “ROSETTA3: an object-oriented software suite for the simulation and design of macromolecules..” Methods in enzymology](https://doi.org/10.1016/B978-0-12-381270-4.00019-6)\n\n[186\\. Rosetta@home：分布式计算项目在蛋白质结构预测中的应用](https://www.bionity.com/en/encyclopedia/Rosetta@home.html)\n\n[187\\. Analysis of the Confidence in the Prediction of the Protein Folding by Artificial Intelligence](https://digital.csic.es/bitstream/10261/331465/1/TejeraNevado_PACBB_2023.docx)\n\n[188\\. A Survey on Self-play Methods in Reinforcement Learning](https://arxiv.org/pdf/2408.01072)\n\n[189\\. EvoDesign: de novo protein design based on structural and evolutionary profiles](https://zhanggroup.org/papers/2013_9.pdf)\n\n[190\\. SE(3)-Equivariant Energy-based Models for End-to-End Protein Folding](https://www.biorxiv.org/content/10.1101/2021.06.06.447297v1.full.pdf)\n\n[191\\. Robin Pearce, Xiaoqiang Huang et al. “De novo protein fold design through sequence-independent fragment assembly simulations.” Proceedings of the National Academy of Sciences of the United States of America](https://doi.org/10.1073/pnas.2208275120)\n\n[192\\. Comparison of Two Academic Software Packages For Protein Structure Prediction](https://gvpress.com/journals/IJBSBT/vol6_no3/6.pdf)\n\n[193\\. Current protein structure predictors do not produce meaningful folding pathways](https://www.biorxiv.org/content/10.1101/2021.09.20.461137v1.full.pdf)\n\n[195\\. 化学/材料](https://www.x-mol.com/paper/chem/tag/3)\n\n[196\\. Accelerate Multi-Agent Reinforcement Learning in Zero-Sum Games with Subgame Curriculum Learning](http://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/4ba7865b-63d1-4605-9a88-8fd4ef7135b3.pdf)\n\n[197\\. A Survey on Self-play Methods in Reinforcement Learning](https://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/db43f779-dd0e-4f2e-a51c-1caa107e21eb.pdf)\n\n[198\\. David Silver, Julian Schrittwieser et al. “Mastering the game of Go without human knowledge.” Nature](https://doi.org/10.1038/nature24270)\n\n[199\\. Ryan Lowe, Yi Wu et al. “Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.” ArXiv](https://arxiv.org/abs/1706.02275)\n\n[200\\. EMULATE：模拟人类行为验证原子主张真实性的多代理框架](https://www.xueshuxiangzi.com/downloads/2025_5_23/2505.16576.pdf)\n\n[201\\. Microsoft New Future of Work Report 2024](https://www.microsoft.com/en-us/research/uploads/prod/2024/12/NFWReport2024_External.pdf)\n\n[202\\. O. Vinyals, Igor Babuschkin et al. “Grandmaster level in StarCraft II using multi-agent reinforcement learning.” Nature](https://doi.org/10.1038/s41586-019-1724-z)\n\n[203\\. Zheyuan Lai, Yingming Pu. “PriM: Principle-Inspired Material Discovery through Multi-Agent Collaboration.”](https://arxiv.org/abs/2504.08810)\n\n[204\\. Kamer Ali Yuksel and Hassan Sawaf. “A Multi-AI Agent System for Autonomous Optimization of Agentic AI Solutions via Iterative Refinement and LLM-Driven Feedback Loops.”](https://arxiv.org/abs/2412.17149)\n\n[205\\. MMRole: A Comprehensive Framework for Developing and Evaluating Multimodal Role-Playing Agents](https://gsai.ruc.edu.cn/uploads/20250303/210eaf2d45df7016cd5a1b04be3def72.pdf)\n\n[206\\. S. McAleer, Jb Lanier et al. “Self-Play PSRO: Toward Optimal Populations in Two-Player Zero-Sum Games.” ArXiv](https://doi.org/10.48550/arXiv.2207.06541)\n\n[207\\. Marc Lanctot, V. Zambaldi et al. “A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning.” Neural Information Processing Systems](https://arxiv.org/abs/1711.00832)\n\n[208\\. Baixuan Xu, Chunyang Li et al. “Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study.”](https://arxiv.org/abs/2505.07313)\n\n[209\\. Zhitao He, Pengfei Cao et al. “LEGO: A Multi-agent Collaborative Framework with Role-playing and Iterative Feedback for Causality Explanation Generation.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/2023.findings-emnlp.613)\n\n[210\\. Zherui Li, Yan Mi et al. “Goal-Aware Identification and Rectification of Misinformation in Multi-Agent Systems.”](https://arxiv.org/abs/2506.00509)\n\n[211\\. Xinran Li, Zifan Liu et al. “Individual Contributions as Intrinsic Exploration Scaffolds for Multi-agent Reinforcement Learning.” ArXiv](https://doi.org/10.48550/arXiv.2405.18110)\n\n[212\\. Jiaqi Shao, Tianjun Yuan et al. “Cognitive Insights and Stable Coalition Matching for Fostering Multi-Agent Cooperation.” ArXiv](https://doi.org/10.48550/arXiv.2405.18044)\n\n[213\\. Shaowei Zhang, Jiahang Cao et al. “Self-Motivated Multi-Agent Exploration.” Adaptive Agents and Multi-Agent Systems](https://doi.org/10.48550/arXiv.2301.02083)\n\n[215\\. R. S. Sutton, A. Barto. “Reinforcement Learning: An Introduction.” IEEE Trans. Neural Networks](https://doi.org/10.1109/TNN.1998.712192)\n\n[216\\. Self-play reinforcement learning guides protein engine...](https://link.springer.com/article/10.1038/s42256-023-00691-9)\n\n[217\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[218\\. Curiosity-driven learning in artificial intelligence and its applications](https://dr.ntu.edu.sg/bitstream/10356/172831/2/main_thesis.pdf)\n\n[219\\. A Survey on Self-play Methods in Reinforcement Learning](https://arxiv.org/pdf/2408.01072)\n\n[220\\. Intrinsic motivation, curiosity, and learning: Theory and applications in educational technologies](http://www.pyoudeyer.com/oudeyerGottliebLopesPBR16.pdf)\n\n[221\\. Self-play reinforcement learning guides protein engineering | Nature Machine Intelligence](https://www.nature.com/articles/s42256-023-00691-9)\n\n[222\\. Self-play reinforcement learning guides protein engineering](https://gwern.net/doc/ai/nn/transformer/alphafold/2023-wang-3.pdf)\n\n[223\\. Simulation Of Diverse Human Play-Styles In Video Games: A Reinforcement Learning Approach](https://theses.hal.science/tel-04602188v1/file/LePelletierDeWoillemont_These_2023.pdf)\n\n[224\\. De Novo Protein Design](https://www.nersc.gov/assets/Uploads/David-Baker-slides-NERSC-50.pdf)\n\n[225\\. Intrinsic Motivation and Reinforcement Learning](https://people.cs.umass.edu/~barto/IMCleVer-chapter-totypeset2.pdf)\n\n[226\\. A Model-Centric Review of Deep Learning for Protein Design](http://arxiv.org/pdf/2502.19173)\n\n[227\\. Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning](https://jmlr.org/papers/volume23/21-0808/21-0808.pdf)\n\n[228\\. Deepak Pathak, Pulkit Agrawal et al. “Curiosity-Driven Exploration by Self-Supervised Prediction.” 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)](https://doi.org/10.1109/CVPRW.2017.70)\n\n[229\\. Curiosity-driven exploration: Diversity of mechanisms and functions](https://inria.hal.science/hal-03447896/file/Ten2022Curiosity-driven.pdf)\n\n[230\\. Spatial Assembly with Self-Play Reinforcement Learning](https://papers.cumincad.org/data/works/att/acadia20_382.pdf)\n\n[231\\. Curiosity Driven Protein Sequence Generation via ...](https://openreview.net/forum?id=tPjVRmHqCg)\n\n[232\\. Nicolas Bougie, R. Ichise. “Skill-based curiosity for intrinsically motivated reinforcement learning.” Machine Learning](https://doi.org/10.1007/s10994-019-05845-8)\n\n[235\\. J. Jumper, Richard Evans et al. “Highly accurate protein structure prediction with AlphaFold.” Nature](https://doi.org/10.1038/s41586-021-03819-2)\n\n[236\\. Comparison of Two Academic Software Packages For Protein Structure Prediction](https://gvpress.com/journals/IJBSBT/vol6_no3/6.pdf)\n\n[237\\. Efficient and accurate prediction of protein structure using RoseTTAFold2](https://www.biorxiv.org/content/10.1101/2023.05.24.542179v1.full.pdf)\n\n[238\\. Computational Methods for Protein Structure Prediction](https://www.ijert.org/computational-methods-for-protein-structure-prediction)\n\n[239\\. Distance-AF: Modifying Predicted Protein Structure Models by Alphafold2 with User-Specified Distance Constraints](https://www.biorxiv.org/content/10.1101/2023.12.01.569498v1.full.pdf)\n\n[240\\. K. Simons, C. Kooperberg et al. “Assembly of protein tertiary structures from fragments with similar local sequences using simulated annealing and Bayesian scoring functions..” Journal of molecular biology](https://doi.org/10.1006/JMBI.1997.0959)\n\n[241\\. P. Bradley, K. Misura et al. “Toward High-Resolution de Novo Structure Prediction for Small Proteins.” Science](https://doi.org/10.1126/SCIENCE.1113801)\n\n[242\\. A. Zemla. “LGA: a method for finding 3D similarities in protein structures.” Nucleic acids research](https://doi.org/10.1093/NAR/GKG571)\n\n[243\\. Systematic benchmarking of deep-learning methods for tertiary RNA structure prediction](https://dr.ntu.edu.sg/bitstream/10356/182891/2/RNA_manuscript_DR-NTU.pdf)\n\n[244\\. Advances in AI for Protein Structure Prediction: Implications for Cancer Drug Discovery and Development](https://escholarship.org/content/qt94p5s58p/qt94p5s58p.pdf)\n\n[245\\. FLab: Benchmarking deep learning methods for antibody fitness prediction](https://www.biorxiv.org/content/10.1101/2024.01.13.575504v1.full.pdf)\n\n[246\\. Protein structure prediction via deep learning: an in-depth review](https://www.frontiersin.org/journals/pharmacology/articles/10.3389/fphar.2025.1498662/pdf)\n\n[247\\. EVALUATING THE PERFORMANCE OF PROTEIN STRUCTURE PREDICTION IN DETECTING STRUCTURAL CHANGES OF PATHOGENIC NONSYNONYMOUS SINGLE NUCLEOTIDE VARIANTS](https://www.biorxiv.org/content/10.1101/2023.11.25.568523v1.full.pdf)\n\n[248\\. Berkeley Open Infrastructure for Network Computing - an open distributed computing system](https://lvee.org/uploads/image_upload/file/263/boinc.pdf)\n\n[249\\. Self-play reinforcement learning guides protein engineering](https://gwern.net/doc/ai/nn/transformer/alphafold/2023-wang-3.pdf)\n\n[250\\. C. Rohl, C. Strauss et al. “Protein Structure Prediction Using Rosetta.” Methods in enzymology](https://doi.org/10.1016/S0076-6879%2804%2983004-0)\n\n[251\\. ProteinInvBench: Benchmarking Protein Inverse Folding on Diverse Tasks, Models, and Metrics](https://openreview.net/pdf?id=bqXduvuW5E)\n\n[255\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[256\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[257\\. David Silver, Julian Schrittwieser et al. “Mastering the game of Go without human knowledge.” Nature](https://doi.org/10.1038/nature24270)\n\n[258\\. O. Vinyals, Igor Babuschkin et al. “Grandmaster level in StarCraft II using multi-agent reinforcement learning.” Nature](https://doi.org/10.1038/s41586-019-1724-z)\n\n[259\\. EMULATE：模拟人类行为验证原子主张真实性的多代理框架](https://www.xueshuxiangzi.com/downloads/2025_5_23/2505.16576.pdf)\n\n[260\\. Marc Lanctot, V. Zambaldi et al. “A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning.” Neural Information Processing Systems](https://arxiv.org/abs/1711.00832)\n\n[261\\. A Survey on Self-play Methods in Reinforcement Learning](https://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/db43f779-dd0e-4f2e-a51c-1caa107e21eb.pdf)\n\n[262\\. Accelerate Multi-Agent Reinforcement Learning in Zero-Sum Games with Subgame Curriculum Learning](http://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/4ba7865b-63d1-4605-9a88-8fd4ef7135b3.pdf)\n\n[263\\. Shiyu Huang (黄世宇)](http://tartrl.cn/download/local/?filename=CV.pdf)\n\n[264\\. MMRole: A Comprehensive Framework for Developing and Evaluating Multimodal Role-Playing Agents](https://gsai.ruc.edu.cn/uploads/20250303/210eaf2d45df7016cd5a1b04be3def72.pdf)\n\n[265\\. 多智能体强化学习博弈训练方式研究综述](http://www.xactad.net/oa/pdfdow.aspx?Sid=20230403)\n\n[267\\. R. S. Sutton, A. Barto. “Reinforcement Learning: An Introduction.” IEEE Trans. Neural Networks](https://doi.org/10.1109/TNN.1998.712192)\n\n[268\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[269\\. Self-play reinforcement learning guides protein engine...](https://link.springer.com/article/10.1038/s42256-023-00691-9)\n\n[270\\. Deepak Pathak, Pulkit Agrawal et al. “Curiosity-Driven Exploration by Self-Supervised Prediction.” 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)](https://doi.org/10.1109/CVPRW.2017.70)\n\n[271\\. A Survey on Self-play Methods in Reinforcement Learning](https://arxiv.org/pdf/2408.01072)\n\n[272\\. Yuri Burda, Harrison Edwards et al. “Exploration by Random Network Distillation.” ArXiv](https://arxiv.org/abs/1810.12894)\n\n[273\\. Marc G. Bellemare, S. Srinivasan et al. “Unifying Count-Based Exploration and Intrinsic Motivation.” Neural Information Processing Systems](https://arxiv.org/abs/1606.01868)\n\n[274\\. Intrinsic motivation, curiosity, and learning: Theory and applications in educational technologies](http://www.pyoudeyer.com/oudeyerGottliebLopesPBR16.pdf)\n\n[275\\. Self-play reinforcement learning guides protein engineering | Nature Machine Intelligence](https://www.nature.com/articles/s42256-023-00691-9)\n\n[276\\. Efficient Protein Optimization via Structure-Aware Hamiltonian Dynamics](https://openreview.net/pdf/9e1361662d70dc415d6298249933cfb69b0a3d84.pdf)\n\n[277\\. Capturing Player Enjoyment in Computer Games](https://www.um.edu.mt/library/oar/bitstream/123456789/22896/1/Capturing_Player_Enjoyment_in_Computer_Games.pdf)\n\n[278\\. 把经典的强化学习应用到了蛋白质设计框架，华大智造 ...](https://www.sznews.com/news/content/2023-07/21/content_30351390.htm)\n\n[279\\. Nicolas Bougie, R. Ichise. “Skill-based curiosity for intrinsically motivated reinforcement learning.” Machine Learning](https://doi.org/10.1007/s10994-019-05845-8)\n\n[280\\. EvoDesign: de novo protein design based on structural and evolutionary profiles](https://zhanggroup.org/papers/2013_9.pdf)\n\n[281\\. Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability](https://papers.nips.cc/paper_files/paper/2023/file/40386e4770bebd63fdf47cbc67341c0b-Paper-Conference.pdf)\n\n[282\\. ProteinInvBench: Benchmarking Protein Inverse Folding on Diverse Tasks, Models, and Metrics](https://openreview.net/pdf?id=bqXduvuW5E)\n\n[283\\. Self-play reinforcement learning guides protein engineering](https://gwern.net/doc/ai/nn/transformer/alphafold/2023-wang-3.pdf)\n\n[287\\. S. Altschul, Thomas L. Madden et al. “Gapped BLAST and PSI-BLAST: a new generation of protein database search programs..” Nucleic acids research](https://doi.org/10.1093/NAR/25.17.3389)\n\n[288\\. Computational Methods for Protein Structure Prediction](https://www.ijert.org/computational-methods-for-protein-structure-prediction)\n\n[289\\. Efficient and accurate prediction of protein structure using RoseTTAFold2](https://www.biorxiv.org/content/10.1101/2023.05.24.542179v1.full.pdf)\n\n[290\\. J. Jumper, Richard Evans et al. “Highly accurate protein structure prediction with AlphaFold.” Nature](https://doi.org/10.1038/s41586-021-03819-2)\n\n[291\\. CRITICAL ASSESSMENT OF TECHNIQUES FOR PROTEIN STRUCTURE PREDICTION](https://predictioncenter.org/casp14/doc/CASP14_Abstracts.pdf)\n\n[292\\. Comparison of Two Academic Software Packages For Protein Structure Prediction](https://gvpress.com/journals/IJBSBT/vol6_no3/6.pdf)\n\n[293\\. Structure prediction using sparse simulated NOE restraints with Rosetta in CASP11](https://www.bakerlab.org/wp-content/uploads/2016/05/Ovchinnikov_et_al-2016-Proteins__Structure_Function_and_Bioinformatics.pdf)\n\n[294\\. LEARNING FROM PROTEIN STRUCTURE WITH GEOMETRIC VECTOR PERCEPTRONS](https://drorlab.stanford.edu/images/learning_from_protein_structure.pdf)\n\n[295\\. Improved de novo Structure Prediction in CASP11 by Incorporating Co-evolution Information into Rosetta](https://www.bakerlab.org/wp-content/uploads/2015/12/Ovchinnikov_Proteins_2015.pdf)\n\n[296\\. Protein structure prediction via deep learning: an in-depth review](https://www.frontiersin.org/journals/pharmacology/articles/10.3389/fphar.2025.1498662/pdf)\n\n[297\\. Cerebra: a computationally efficient framework for accurate protein structure prediction](https://www.biorxiv.org/content/10.1101/2024.02.02.578551v1.full.pdf)\n\n[298\\. New method helps scientists to determine specific protein molecular structure](https://www.news-medical.net/news/20110502/New-method-helps-scientists-to-determine-specific-protein-molecular-structure.aspx)\n\n[299\\. Efficient and accurate prediction of protein structure using ...](https://www.biorxiv.org/content/10.1101/2023.05.24.542179.full)\n\n[300\\. MSA Generation with Seqs2Seqs Pretraining: Advancing Protein Structure Prediction](https://openreview.net/forum?id=bM6LUC2lec)\n\n[307\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[308\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[309\\. David Silver, Julian Schrittwieser et al. “Mastering the game of Go without human knowledge.” Nature](https://doi.org/10.1038/nature24270)\n\n[310\\. O. Vinyals, Igor Babuschkin et al. “Grandmaster level in StarCraft II using multi-agent reinforcement learning.” Nature](https://doi.org/10.1038/s41586-019-1724-z)\n\n[311\\. J. Park, Joseph C. O'Brien et al. “Generative Agents: Interactive Simulacra of Human Behavior.” Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology](https://doi.org/10.1145/3586183.3606763)\n\n[312\\. MMRole: A Comprehensive Framework for Developing and Evaluating Multimodal Role-Playing Agents](https://gsai.ruc.edu.cn/uploads/20250303/210eaf2d45df7016cd5a1b04be3def72.pdf)\n\n[313\\. Engineering Fronts 2023](http://www.engineering.org.cn/views/uploadfiles/Engineering%20Fronts%202023.pdf)\n\n[314\\. Anne Beyer, Kranti Chalamalasetti et al. “clembench-2024: A Challenging, Dynamic, Complementary, Multilingual Benchmark and Underlying Flexible Framework for LLMs as Multi-Action Agents.” ArXiv](https://doi.org/10.48550/arXiv.2405.20859)\n\n[315\\. Xinrun Xu, Yuxin Wang et al. “A Survey on Game Playing Agents and Large Models: Methods, Applications, and Challenges.” ArXiv](https://doi.org/10.48550/arXiv.2403.10249)\n\n[316\\. 多智能体强化学习博弈训练方式研究综述](http://www.xactad.net/oa/pdfdow.aspx?Sid=20230403)\n\n[317\\. Zhitao He, Pengfei Cao et al. “LEGO: A Multi-agent Collaborative Framework with Role-playing and Iterative Feedback for Causality Explanation Generation.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/2023.findings-emnlp.613)\n\n[318\\. TQC 2023 - Self-testing real projective measurements](https://www.bilibili.com/video/av274086608?t=272)\n\n[319\\. Chi-Min Chan, Weize Chen et al. “ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate.” ArXiv](https://doi.org/10.48550/arXiv.2308.07201)\n\n[320\\. Agent Q：自主AI 代理的高级推理和学习](https://yiyibooks.cn/__trs__/arxiv/2408.07199v1/index.html)\n\n[321\\. A Survey on Self-play Methods in Reinforcement Learning](https://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/db43f779-dd0e-4f2e-a51c-1caa107e21eb.pdf)\n\n[322\\. A Generalized Framework for Self-Play Training - Micro...](https://www.microsoft.com/en-us/research/publication/a-generalized-framework-for-self-play-training/?locale=zh-cn)\n\n[323\\. Mohamad A. Hady, Siyi Hu et al. “Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey.”](https://arxiv.org/abs/2504.21048)\n\n[324\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[325\\. GitHub - melobio/EvoPlay](https://github.com/melobio/EvoPlay)\n\n[326\\. EvoPlay/README.md at main · melobio/EvoPlay · GitHub](https://github.com/melobio/EvoPlay/blob/main/README.md)\n\n[327\\. Alexander Rives, Siddharth Goyal et al. “Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences.” Proceedings of the National Academy of Sciences of the United States of America](https://doi.org/10.1101/622803)\n\n[328\\. ProteinGym: Large-Scale Benchmarks for Protein Design and Fitness Prediction](https://www.biorxiv.org/content/10.1101/2023.12.07.570727v1.full.pdf)\n\n[329\\. An integrative approach to protein sequence design through multiobjective optimization](https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1011953&type=printable)\n\n[330\\. Self-play reinforcement learning guides protein engineering](https://gwern.net/doc/ai/nn/transformer/alphafold/2023-wang-3.pdf)\n\n[331\\. Kevin Kaichuang Yang, Zachary Wu et al. “Machine-learning-guided directed evolution for protein engineering.” Nature Methods](https://doi.org/10.1038/s41592-019-0496-6)\n\n[332\\. Adam J. Riesselman, John Ingraham et al. “Deep generative models of genetic variation capture the effects of mutations.” Nature Methods](https://doi.org/10.1038/s41592-018-0138-4)\n\n[333\\. Thomas A. Hopf, John Ingraham et al. “Mutation effects predicted from sequence co-variation.” Nature Biotechnology](https://doi.org/10.1038/nbt.3769)\n\n[334\\. Plug & play directed evolution of proteins with gradient-based discrete MCMC](https://www.nrel.gov/docs/fy23osti/84201.pdf)\n\n[335\\. ProteinNPT: Improving Protein Property Prediction and Design with Non-Parametric Transformers](https://proceedings.neurips.cc/paper_files/paper/2023/file/6a4d5d85f7a52f062d23d98d544a5578-Paper-Conference.pdf)\n\n[336\\. Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval](https://proceedings.mlr.press/v162/notin22a/notin22a.pdf)\n\n[337\\. Self-play reinforcement learning guides protein engine...](https://link.springer.com/article/10.1038/s42256-023-00691-9)\n\n[338\\. A Model-Centric Review of Deep Learning for Protein Design](http://arxiv.org/pdf/2502.19173)\n\n[339\\. HighPlay : Cyclic Peptide Sequence Design Based on Reinforcement Learning and Protein Structure Prediction](https://www.biorxiv.org/content/10.1101/2025.03.17.643626v1.full.pdf)\n\n[344\\. J. Jumper, Richard Evans et al. “Highly accurate protein structure prediction with AlphaFold.” Nature](https://doi.org/10.1038/s41586-021-03819-2)\n\n[345\\. Accurate prediction of protein–nucleic acid complexes using RoseTTAFoldNA | Nature Methods](https://www.nature.com/articles/s41592-023-02086-5)\n\n[346\\. Protein structure prediction via deep learning: an in-depth review](https://www.frontiersin.org/journals/pharmacology/articles/10.3389/fphar.2025.1498662/pdf)\n\n[347\\. Efficient and accurate prediction of protein structure using RoseTTAFold2](https://www.biorxiv.org/content/10.1101/2023.05.24.542179v1.full.pdf)\n\n[348\\. DeepFold: enhancing protein structure prediction through optimized loss functions, improved template features, and re-optimized energy function](https://scholarworks.bwise.kr/hanyang/bitstream/2021.sw.hanyang/194352/1/DeepFold%20enhancing%20protein%20structure%20prediction%20through%20optimized%20loss%20functions,%20improved%20template%20features,%20and%20re-optimized%20energy%20function.pdf)\n\n[349\\. High-accuracy refinement using Rosetta in CASP13 - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC6851472/)\n\n[350\\. xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein](https://www.biorxiv.org/content/biorxiv/early/2024/12/03/2023.07.05.547496.full.pdf)\n\n[351\\. FLab: Benchmarking deep learning methods for antibody fitness prediction](https://www.biorxiv.org/content/10.1101/2024.01.13.575504v1.full.pdf)\n\n[352\\. Rosetta Energy Analysis of AlphaFold2 models: Point Mutations and Conformational Ensembles](https://www.biorxiv.org/content/10.1101/2023.09.05.556364v2.full.pdf)\n\n[353\\. Improved de novo Structure Prediction in CASP11 by Incorporating Co-evolution Information into Rosetta](https://www.bakerlab.org/wp-content/uploads/2015/12/Ovchinnikov_Proteins_2015.pdf)\n\n[354\\. MSA Generation with Seqs2Seqs Pretraining: Advancing Protein Structure Prediction](https://openreview.net/forum?id=bM6LUC2lec)\n\n[355\\. Bernard Moussad, Rahmatullah Roche et al. “The transformative power of transformers in protein structure prediction.” Proceedings of the National Academy of Sciences of the United States of America](https://doi.org/10.1073/pnas.2303499120)\n\n[356\\. CRITICAL ASSESSMENT OF TECHNIQUES FOR PROTEIN STRUCTURE PREDICTION](https://predictioncenter.org/casp13/doc/CASP13_Abstracts.pdf)\n\n[357\\. Protein Structure Prediction Using Rosetta](https://wp0.vanderbilt.edu/youngscientistjournal/article/protein-structure-prediction-using-rosetta)\n\n[358\\. A. Iacoangeli, P. Marcatili et al. “Exploiting homology information in nontemplate based prediction of protein structures..” Journal of chemical theory and computation](https://doi.org/10.1021/acs.jctc.5b00371)\n\n[359\\. Yang Zhang, Jeffrey Skolnick. “Scoring function for automated assessment of protein structure template quality.” Proteins: Structure](https://doi.org/10.1002/prot.20264)\n\n[364\\. LAMDA RL Lab](https://lamda-rl.nju.edu.cn/papers.html)\n\n[365\\. Timo Schick, Jane Dwivedi-Yu et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” ArXiv](https://doi.org/10.48550/arXiv.2302.04761)\n\n[366\\. MMRole: A Comprehensive Framework for Developing and Evaluating Multimodal Role-Playing Agents](https://gsai.ruc.edu.cn/uploads/20250303/210eaf2d45df7016cd5a1b04be3def72.pdf)\n\n[367\\. J. Park, Joseph C. O'Brien et al. “Generative Agents: Interactive Simulacra of Human Behavior.” Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology](https://doi.org/10.1145/3586183.3606763)\n\n[368\\. Yilun Du, Shuang Li et al. “Improving Factuality and Reasoning in Language Models through Multiagent Debate.” ArXiv](https://doi.org/10.48550/arXiv.2305.14325)\n\n[369\\. Yuzhuang Xu, Shuo Wang et al. “Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf.” ArXiv](https://doi.org/10.48550/arXiv.2309.04658)\n\n[370\\. Qingyun Wu, Gagan Bansal et al. “AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework.” ArXiv](https://doi.org/10.48550/arXiv.2308.08155)\n\n[371\\. A. Kusne, A. McDannald. “Scalable multi-agent lab framework for lab optimization.” Matter](https://doi.org/10.1016/j.matt.2023.03.022)\n\n[372\\. Zheyuan Lai, Yingming Pu. “PriM: Principle-Inspired Material Discovery through Multi-Agent Collaboration.”](https://arxiv.org/abs/2504.08810)\n\n[373\\. A. Kusne, A. McDannald. “Scalable Multi-Agent Framework for Optimizing the Lab and Warehouse.” ArXiv](https://doi.org/10.48550/arXiv.2208.09099)\n\n[374\\. Siyuan Wang, Zhuohan Long et al. “Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation.” ArXiv](https://doi.org/10.48550/arXiv.2402.11443)\n\n[375\\. A Survey on Self-play Methods in Reinforcement Learning](https://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/db43f779-dd0e-4f2e-a51c-1caa107e21eb.pdf)\n\n[376\\. AgentVerse: 多智体协同和智体涌现行为](https://zhuanlan.zhihu.com/p/657018851)\n\n[377\\. Lei Shen, Xiaoyu Shen. “Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant.”](https://arxiv.org/abs/2504.18373)\n\n[378\\. Xinrun Xu, Yuxin Wang et al. “A Survey on Game Playing Agents and Large Models: Methods, Applications, and Challenges.” ArXiv](https://doi.org/10.48550/arXiv.2403.10249)\n\n[379\\. 加速科学发现一直是人工智能（AI）研究的长期目标，早期的项目如 1979 年的橡树岭应用人工智能项目探索了（Team, 1985; Emrich et al., 1988; Johnson and Schaffer, 1994）。随着基础模型的进步，更近期的探索提供了一个端到端论文生成过程的完全自动化流程的概念验证（Achiam et al., 2023; Anthropic, 2024; Team et al., 2024; Dubey et al., 2024）。未来，我们设想 AI 研究代理能够独立进行文献搜索、生成科学假设、设计实验、实施新方法、分析结果，通过撰写科学论文传播发现，并在产品中应用此研究，从而协助研究过程的所有部分。这些代理应该能够完全自主工作或在人类监督下进行工作，考虑到用户的反馈。这个愿景来自于认识到 AI 有能力处理庞大数据集并辨识复杂模式，可以通过识别有前景的药物候选者或预测新材料的属性来加速药物发现和材料科学领域的科学突破（Hessler and Baringhaus, 2018; Schneider et al., 2020; Guo et al., 2021）。与传统方法不同，AI 代理可以通过分析庞大的知识图谱揭示隐藏的跨学科关系，提出复杂挑战（如气候建模）的新见解和解决方案。通过自动化繁重的任务和探索非传统途径，AI 代理可以解放科学家们，使之专注于更高层次的认知活动，从而推动创新并拓展知识的前沿。机器学习（ML）的研究，重视仿真中的经验验证和系统实验，为探索和提高 LLMs 在推动科学研究中的效用提供了理想的试验场。](https://www.xueshuxiangzi.com/downloads/2025_2_21/2502.14499.pdf)\n\n[380\\. Chi-Min Chan, Weize Chen et al. “ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate.” ArXiv](https://doi.org/10.48550/arXiv.2308.07201)\n\n[381\\. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents](https://cz5waila03cyo0tux1owpyofgoryroob.itic-sci.com/9C/54/B4/9C54B47616795A320E36FCB1EA595C91.pdf)\n\n[382\\. 世界前沿技术发展报告2023](https://zhuanlan.zhihu.com/p/671325821)\n\n[384\\. EvoPlay/README.md at main · melobio/EvoPlay · GitHub](https://github.com/melobio/EvoPlay/blob/main/README.md)\n\n[385\\. GitHub - melobio/EvoPlay](https://github.com/melobio/EvoPlay)\n\n[386\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[387\\. J. Jumper, Richard Evans et al. “Highly accurate protein structure prediction with AlphaFold.” Nature](https://doi.org/10.1038/s41586-021-03819-2)\n\n[388\\. Highly accurate and robust protein sequence design with CarbonDesign](https://www.researchsquare.com/article/rs-3251939/latest.pdf)\n\n[389\\. Self-play reinforcement learning guides protein engine...](https://link.springer.com/article/10.1038/s42256-023-00691-9)\n\n[390\\. ProteinInvBench: Benchmarking Protein Inverse Folding on Diverse Tasks, Models, and Metrics](https://openreview.net/pdf?id=bqXduvuW5E)\n\n[391\\. J. Dauparas, I. Anishchenko et al. “Robust deep learning based protein sequence design using ProteinMPNN.” Science (New York, N.Y.)](https://doi.org/10.1126/science.add2187)\n\n[392\\. EvoDesign: de novo protein design based on structural and evolutionary profiles](https://zhanggroup.org/papers/2013_9.pdf)\n\n[393\\. Self-play reinforcement learning guides protein engineering](https://gwern.net/doc/ai/nn/transformer/alphafold/2023-wang-3.pdf)\n\n[394\\. John Ingraham, Vikas K. Garg et al. “Generative Models for Graph-Based Protein Design.” DGS@ICLR](https://www.semanticscholar.org/paper/a9ec03dbe702f6909acd1f1f14a3395d0141043b)\n\n[395\\. Po-Ssu Huang, S. Boyken et al. “The coming of age of de novo protein design.” Nature](https://doi.org/10.1038/nature19946)\n\n[396\\. Direct prediction of intrinsically disordered protein conformational properties from sequence](https://digitalcommons.wustl.edu/context/oa_4/article/4357/viewcontent/s41592_023_02159_5.pdf)\n\n[397\\. HighPlay : Cyclic Peptide Sequence Design Based on Reinforcement Learning and Protein Structure Prediction](https://www.biorxiv.org/content/10.1101/2025.03.17.643626v1.full.pdf)\n\n[398\\. An integrative approach to protein sequence design through multiobjective optimization](https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1011953&type=printable)\n\n[399\\. Self-play reinforcement learning guides protein engineering | Nature Machine Intelligence](https://www.nature.com/articles/s42256-023-00691-9)\n\n[400\\. Efficient Protein Optimization via Structure-Aware Hamiltonian Dynamics](https://openreview.net/pdf/9e1361662d70dc415d6298249933cfb69b0a3d84.pdf)\n\n[401\\. ProteinVAE: Variational AutoEncoder for Translational Protein Design](https://www.biorxiv.org/content/10.1101/2023.03.04.531110v1.full.pdf)\n\n[402\\. Proteus: Exploring Protein Structure Generation for Enhanced Designability and Efficiency](https://raw.githubusercontent.com/mlresearch/v235/main/assets/wang24bi/wang24bi.pdf)"
  },
  {
    "id": 40,
    "question": "What fundamental architectural innovations are needed to enable neural networks to maintain lifelong learning capabilities without catastrophic forgetting?",
    "response": "**Research Report**\n\n**To:** Interested Parties\n\n**From:** Expert Researcher\n\n**Date:** July 27, 2025\n\n**Subject:** Fundamental Architectural Innovations for Lifelong Learning Neural Networks\n\n**1.0 Introduction: The Imperative for Lifelong Learning**\n\nThe field of artificial intelligence (AI) has made remarkable strides, yet a fundamental limitation persists: most state-of-the-art neural networks operate under a static learning paradigm. They are trained on a fixed dataset and, once deployed, their knowledge is frozen. When presented with new information or tasks in a sequential manner, they suffer from a phenomenon known as **catastrophic forgetting**, where the acquisition of new knowledge destructively overwrites previously learned information \\[58\\]\\[284\\]\\[341\\]. This brittleness stands in stark contrast to biological intelligence, which demonstrates a remarkable capacity for **lifelong learning**—the ability to continuously learn, adapt, and accumulate knowledge over time without compromising existing skills.\n\nOvercoming catastrophic forgetting is not merely an incremental improvement; it is a critical step toward developing more robust, autonomous, and generally intelligent AI systems. A true lifelong learning agent must navigate the inherent **stability-plasticity dilemma**: it must be plastic enough to integrate new knowledge, yet stable enough to prevent the erosion of consolidated memories \\[293\\].\n\nThis report synthesizes research on the fundamental **architectural innovations** required to imbue neural networks with this crucial lifelong learning capability. While strategies involving data replay and parameter regularization are important components of the solution, our focus here is on the core structural designs that intrinsically support continuous adaptation and knowledge retention. We will explore how a paradigm shift away from fixed-size models towards dynamic, modular, and biologically-inspired architectures provides a foundational pathway to solving catastrophic forgetting.\n\n**2.0 Dynamic Network Architectures: Building Plasticity into the Model**\n\nThe most direct architectural solution to catastrophic forgetting is to abandon the constraint of a fixed network structure. Dynamic architectures are designed to grow and adapt their capacity and topology in response to the demands of new tasks, thereby creating dedicated neural resources for new knowledge while preserving existing ones.\n\n**2.1 Network Expansion and Growth Models**\n\nThese models directly counteract forgetting by adding new parameters or components to the network for each new task.\n\n**Progressive Neural Networks (PNNs):** The PNN architecture represents a foundational approach to network expansion. For each new task, a new neural network \"column\" is instantiated and added to the overall structure. Crucially, the parameters of all previously trained columns are frozen, making them immune to being overwritten \\[1\\]\\[31\\]\\[90\\]. To enable knowledge transfer, each new column receives lateral connections from all preceding columns, allowing it to leverage previously learned features without altering them \\[2\\]\\[23\\]\\[92\\]. While PNNs can achieve zero forgetting by design, their primary drawback is a linear growth in model size with each new task, leading to significant computational and memory scalability challenges \\[1\\]\\[25\\]\\[25\\].\n\n**Dynamically Expanding Networks (DEN) and Neurogenesis Deep Learning (NDL):** As a more granular alternative to adding entire network columns, these approaches dynamically add new _neurons_ to existing layers \\[2\\]\\[23\\]\\[28\\]. This expansion is not arbitrary; it is triggered when the network's current capacity is insufficient to learn a new task, often determined by monitoring if the training loss remains above a certain threshold after an initial retraining attempt \\[63\\]\\[66\\]. To determine the optimal number of neurons to add, techniques like **group sparse regularization** are employed to identify the necessary capacity increase while preventing the model from becoming unnecessarily complex \\[62\\]\\[62\\]\\[66\\]. Some NDL models also incorporate an \"intrinsic replay\" mechanism to further reinforce the weights associated with older information \\[23\\].\n\n**2.2 Parameter Isolation and Modularization**\n\nA related strategy involves functionally isolating parameters within a single, shared network, creating task-specific pathways or modules.\n\n**Component Sharing and Isolation:** This approach assigns distinct sets of parameters to different tasks. Once a task is learned, its associated parameters are frozen to prevent interference from subsequent learning \\[3\\]. This requires a sophisticated mechanism to decide which parameters to share for knowledge transfer and which to isolate for knowledge preservation \\[3\\]. Methods like **PackNet** implement this by iteratively pruning redundant weights after learning a task, freeing up parameters that can then be used to learn a new task within the same network \\[168\\]. Another technique, **Hard Attention to the Task (HAT)**, uses learned attention masks to create task-specific pathways, effectively activating only a relevant sub-network for each task during both training and inference \\[168\\].\n\n**Task-Specific Adapters:** A highly efficient and modern implementation of modularity involves using lightweight \"adapter\" modules. In this paradigm, a large, pre-trained base model is kept frozen, and for each new task, only a small set of new parameters in the form of an adapter is trained \\[22\\]. These adapters are \"plug-and-play\" modules that specialize in a new task without altering the vast repository of general knowledge in the base model, thus minimizing the risk of catastrophic forgetting while being parameter-efficient \\[42\\].\n\n**3.0 Biologically-Inspired Architectures: Learning from the Brain**\n\nThe brain is the ultimate lifelong learning system, and neuroscience provides a rich source of inspiration for novel AI architectures. These approaches seek to emulate the biological mechanisms that gracefully balance learning and memory.\n\n**3.1 Complementary Learning Systems (CLS)**\n\nThe CLS theory of the brain posits two interacting memory systems: a fast-learning **hippocampus** that rapidly encodes specific, episodic experiences, and a slow-learning **neocortex** that gradually integrates new information into a structured, general knowledge base, largely through memory replay during sleep \\[2\\]\\[2\\]\\[111\\].\n\nComputational models inspired by CLS replicate this duality. They often feature a \"fast\" network for rapid acquisition of task-specific knowledge and a separate \"long-term\" network \\[105\\]\\[112\\]. To prevent catastrophic forgetting in the long-term store, knowledge is transferred from the fast system via the replay of stored data samples or, more sophisticatedly, internally generated pseudo-patterns that capture the essence of past experiences \\[105\\]\\[113\\]. This architectural separation of learning and consolidation directly addresses the stability-plasticity dilemma.\n\n**3.2 Structural Plasticity: Neuron Splitting and Timestamping**\n\nDrawing inspiration from biological neurogenesis and synaptic plasticity, this advanced architectural mechanism addresses a subtle form of forgetting known as \"semantic drift\" \\[2\\]\\[122\\]. Even when a network expands, neurons shared across tasks can have their function gradually altered as they are fine-tuned on new data.\n\n**Neuron Splitting:** To combat semantic drift, the network actively monitors the function of its neurons. This is often implemented by calculating the **L2 distance** of a neuron's incoming weight vector between consecutive tasks \\[186\\]\\[188\\]. If this \"drift\" value exceeds a predefined hyperparameter threshold (), it signifies that the neuron's representative feature has changed substantially. In response, the neuron is \"split\" or duplicated \\[121\\]\\[121\\]. One copy of the neuron retains its original weights, preserving its function for older tasks, while the duplicate is free to adapt to the new task \\[123\\].\n\n**Timestamping:** To manage this dynamic architecture, newly added units—whether from general expansion or neuron splitting—are assigned a **timestamp** corresponding to the task stage at which they were created \\[121\\]\\[123\\]. During inference for a specific task, the model uses a task-specific mask to activate only those neurons that existed up to that task's timestamp. This elegant mechanism ensures that older tasks are not affected by neurons created for future tasks, while still allowing shared neurons to benefit from continued training \\[121\\]\\[121\\].\n\n**4.0 Emerging Architectural Paradigms of 2025**\n\nAs of 2025, research is pushing beyond established methods into novel network paradigms with intrinsic properties conducive to lifelong learning.\n\n**Liquid Neural Networks (LNNs):** LNNs are a class of continuous-time recurrent neural networks whose internal states are governed by systems of differential equations \\[224\\]. Their key innovation is their \"liquid\" or dynamic nature; unlike networks with fixed weights, LNNs continuously evolve their state based on incoming data streams \\[224\\]\\[227\\]. This inherent adaptability allows them to process information in a continuously changing environment. Their ability to prevent catastrophic forgetting stems from this structural fluidity, where adaptive mechanisms like **liquid time constants (LTCs)** allow the network to dynamically alter connection strengths in response to new information while maintaining overall system robustness \\[229\\]. Bounded weights further enhance stability by preventing exploding gradients \\[229\\].\n\n**Transformer-Based Lifelong Learners:** Given the dominance of the Transformer architecture, adapting it for continual learning is a key research area. The **Lifelong Vision Transformer (LVT)** is a notable innovation in this space \\[107\\]\\[164\\]. Its architecture introduces an **inter-task attention mechanism**, which allows the model to implicitly reference and absorb knowledge from previous tasks while training on a new one. This is complemented by a **dual-classifier structure** that separates the injection of new, task-specific representations from the accumulation of general knowledge, further preventing interference \\[107\\].\n\n**Spline-Based Networks (KANs):** Architectures like Kolmogorov-Arnold Networks (KANs) replace the simple weighted sums of traditional neurons with learnable spline functions on the network's edges. This design has a profound implication for catastrophic forgetting due to the **local plasticity** of splines \\[102\\]\\[281\\]. When the network learns, it only needs to update the coefficients of the spline in a localized region. This leaves distant parts of the spline function—and the knowledge encoded therein—unaffected. In contrast, updating a single weight in a standard MLP can have a global effect on the function it represents. This innate locality provides a powerful architectural defense against knowledge overwriting \\[162\\].\n\n**5.0 Benchmarking and Evaluating Performance**\n\nTo empirically validate these architectural innovations, the research community relies on standardized benchmarks and metrics.\n\n**Standard Benchmarks:** Sequential learning tasks are simulated using datasets like **Split-MNIST**, Permuted-MNIST, and Split-CIFAR-10/100 \\[143\\]\\[211\\]\\[324\\]. In Split-MNIST, for instance, the model is trained to classify a pair of digits (e.g., 0 and 1), then a different pair (2 and 3), and so on, sequentially.\n\n**Key Metrics:** Performance is evaluated not just on the final task, but across all tasks learned over the agent's lifetime.\n\n**Average Accuracy (ACC):** The mean accuracy across all tasks after the final task has been learned. This measures overall performance \\[141\\]\\[220\\].\n\n**Forgetting Measure (FM) or Backward Transfer (BWD):** This metric directly quantifies catastrophic forgetting by measuring the average drop in accuracy on a task after training on subsequent tasks, compared to its peak accuracy right after it was trained \\[91\\]\\[159\\]\\[220\\]. A lower value is better.\n\n**Comparative Insights:** Direct, head-to-head quantitative comparisons between all architectures on a single benchmark remain sparse in the literature. However, general trends are observable. PNNs can achieve a forgetting measure of 0.0 on some benchmarks, but at the cost of unbounded growth \\[273\\]. Replay-based and regularization methods, often evaluated on Split-MNIST, show a trade-off; for example, the GEM method achieves ~86% accuracy with ~11% forgetting, while ER-MIR improves this to ~88% accuracy and ~7% forgetting \\[220\\]. This highlights that purely architectural solutions like PNNs offer perfect memory preservation, while more integrated approaches negotiate a balance between performance, memory, and computational cost.\n\n**6.0 Conclusion and Future Directions**\n\nThe journey towards true lifelong learning AI is fundamentally an architectural one. The evidence overwhelmingly indicates that the static, fixed-capacity architectures of the past are insufficient. The necessary innovations lie in creating networks that are **dynamic, modular, and adaptive**.\n\nThe core principle unifying these successful architectures—from the explicit expansion of PNNs and DENs, to the functional isolation of adapters and HAT, to the intrinsic locality of KANs—is the ability to create or allocate new resources for new knowledge (plasticity) while simultaneously protecting or isolating the resources dedicated to consolidated knowledge (stability). Biologically-inspired concepts, such as complementary learning and structural plasticity, provide a powerful conceptual framework for these designs.\n\nLooking forward, several research directions are paramount:\n\n1.  **Efficiency and Scalability:** The primary challenge for dynamic architectures is managing their computational and memory footprint. Future work must focus on more efficient growth strategies and pruning mechanisms to ensure scalability \\[25\\]\\[62\\].\n2.  **Hybrid Systems:** The most robust lifelong learners will likely be hybrid systems that combine the strengths of multiple approaches: dynamic architectures to create capacity, parameter regularization (e.g., EWC) to fine-tune shared components, and efficient replay mechanisms to reinforce knowledge.\n3.  **Deepening Biological Fidelity:** Moving beyond high-level analogies to more faithful modeling of neural circuits and neuromodulatory systems could unlock more powerful and efficient learning mechanisms.\n4.  **Maturing Emerging Paradigms:** Continued exploration of how the unique properties of LNNs, KANs, and next-generation Transformers can be explicitly harnessed for more complex continual learning scenarios will be crucial.\n\nBy fundamentally rethinking the static nature of neural network architectures, we can pave the way for AI systems that learn as biological organisms do: continuously, adaptively, and for a lifetime.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Binghui Peng, Andrej Risteski. “Continual learning: a feature extraction formalization, an efficient algorithm, and fundamental obstructions.” ArXiv](https://doi.org/10.48550/arXiv.2203.14383)\n\n[2\\. G. I. Parisi, Ronald Kemker et al. “Continual Lifelong Learning with Neural Networks: A Review.” Neural networks : the official journal of the International Neural Network Society](https://doi.org/10.1016/j.neunet.2019.01.012.)\n\n[3\\. 图终身学习: 综述](https://crad.ict.ac.cn/cn/article/pdf/preview/10.7544/issn1000-1239.202440204.pdf)\n\n[4\\. J. Kirkpatrick, Razvan Pascanu et al. “Overcoming catastrophic forgetting in neural networks.” Proceedings of the National Academy of Sciences](https://doi.org/10.1073/pnas.1611835114)\n\n[5\\. Toward Continual Learning for Conversational Agents](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/12/Toward-Continual-Learning-for-Conversational-Agents.pdf)\n\n[6\\. Andrei A. Rusu, Neil C. Rabinowitz et al. “Progressive Neural Networks.” ArXiv](https://arxiv.org/abs/1606.04671)\n\n[7\\. 核心：通过认知回放减轻持续学习中的灾难性遗忘](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_2_5/2402.01348.pdf)\n\n[8\\. 人工神经网络中连续学习与情境学习的算法设计与研究](http://ir.ia.ac.cn/bitstream/173211/23683/1/%E5%8D%9A%E5%90%8E%E5%87%BA%E7%AB%99%E6%8A%A5%E5%91%8A-%E9%99%88%E9%98%B3.pdf)\n\n[9\\. 【博士每天一篇文献-算法】改进的PNN架构Lifelong learning ...](https://developer.aliyun.com/article/1580604)\n\n[21\\. TinyCL: An Efficient Hardware Architecture for Continual Learning on Autonomous Systems](https://arxiv.org/pdf/2402.09780)\n\n[22\\. Continual Learning Pytorch Techniques | Restackio](https://www.restack.io/p/continual-learning-answer-pytorch-techniques-cat-ai)\n\n[23\\. Neural networks for continuous lifelong learning: A review](http://www.ijesonline.com/Published%20Paper/Volume%2047/Special_Issue_July_18/150.doc)\n\n[24\\. 持续学习研究进展](https://www.cjig.cn/rc-pub/front/front-article/download/84437594/lowqualitypdf/%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95.pdf)\n\n[25\\. Continual learning with neural networks](https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=1447&context=etd_coll)\n\n[26\\. Adaptive and Efficient Continual Learning in Dynamic Environments](https://hal.science/tel-04943017/file/Thesis_continual_learning_ruiYANG.pdf)\n\n[27\\. Reading notes # 3 : continual learning](https://thethoughtprocess.xyz/language/en/science-en/computer-science/artificial-intelligence/2022/03/10/introduction-to-continual-learning/)\n\n[28\\. Arxiv 8.10 持续学习/增量学习/终身学习 continual learning/incremental learning/lifelong learning](https://zhuanlan.zhihu.com/p/649177641)\n\n[29\\. TinyCL: An Efficient Hardware Architecture for Continu...](http://arxiv.org/html/2402.09780v3)\n\n[30\\. DyTox: Transformers for Continual Learning With DYnamic TOken eXpansion](https://openaccess.thecvf.com/content/CVPR2022/html/Douillard_DyTox_Transformers_for_Continual_Learning_With_DYnamic_TOken_eXpansion_CVPR_2022_paper.html)\n\n[31\\. Continual Learning with Deep Architectures](https://icml.cc/media/icml-2021/Slides/10833_njzbXvu.pdf)\n\n[32\\. Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challenges](https://hal.science/hal-02381343/document)\n\n[33\\. Continual learning, fast and slow](https://ink.library.smu.edu.sg/context/sis_research/article/9622/viewcontent/2209.02370_av.pdf)\n\n[41\\. J. Kirkpatrick, Razvan Pascanu et al. “Overcoming catastrophic forgetting in neural networks.” Proceedings of the National Academy of Sciences](https://doi.org/10.1073/pnas.1611835114)\n\n[42\\. 【INFFUS 2025年】跨通道状态空间交互注意力CSI,即插即用,捕获长...](https://www.bilibili.com/video/BV1fKu2zTEv8/)\n\n[43\\. Sylvestre-Alvise Rebuffi, Alexander Kolesnikov et al. “iCaRL: Incremental Classifier and Representation Learning.” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2017.587)\n\n[44\\. 创智突破：AI首次自主发现106个超越人类设计的神经网络架构](https://www.51cto.com/article/821366.html)\n\n[45\\. Advances in Artificial Intelligence: Current Trends and Future Directions](http://sprcopen.org/index.php/FAIR/article/download/36/32)\n\n[46\\. 最新「大模型简史」整理！从Transformer（2017）到DeepSeek-R1（2025）](https://oss.hermchats.com/file/%E6%9C%80%E6%96%B0%E3%80%8C%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8F%B2%E3%80%8D%E6%95%B4%E7%90%86%EF%BC%81%E4%BB%8ETransformer%EF%BC%882017%EF%BC%89%E5%88%B0DeepSeek-R1%EF%BC%882025%EF%BC%89.pdf)\n\n[47\\. G. I. Parisi, Ronald Kemker et al. “Continual Lifelong Learning with Neural Networks: A Review.” Neural networks : the official journal of the International Neural Network Society](https://doi.org/10.1016/j.neunet.2019.01.012.)\n\n[48\\. 2025年度理工学部授業計画（数学科 専門科目編）](https://www.meiji.ac.jp/sst/6t5h7p00000gq053-att/6t5h7p00003ff9u4.pdf)\n\n[49\\. 2025年,大模型LLM还有哪些可研究的方向?\\_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1T17fzBEcJ/)\n\n[50\\. Recent Breakthroughs in AI Research: Deep Learning's Impact on Reinforcement Learning and Generative Models](https://www.ijirset.com/upload/2024/september/62_Recent.pdf)\n\n[51\\. 2025年机器学习领域新研究动态汇总 - 月光AI博客](https://blog.moontak.com/id/562023/)\n\n[52\\. Zhizhong Li, Derek Hoiem. “Learning without Forgetting.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1007/978-3-319-46493-0_37)\n\n[53\\. Neural networks for continuous lifelong learning: A review](http://www.ijesonline.com/Published%20Paper/Volume%2047/Special_Issue_July_18/150.doc)\n\n[54\\. Advancing Meta-Learning for Enhanced Generalization Across Diverse Tasks](https://www.diva-portal.org/smash/get/diva2:1924707/FULLTEXT01.pdf)\n\n[55\\. M. McCloskey, N. J. Cohen. “Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem.” Psychology of Learning and Motivation](https://doi.org/10.1016/S0079-7421%2808%2960536-8)\n\n[56\\. 生成式人工智能应用发展报告（2024）](https://pdf.dfcfw.com/pdf/H3_AP202412081641219285_1.pdf?1733669948000.pdf)\n\n[57\\. 【LNN】传统离散模型力不从心,液态神经网络高效逆袭!\\_哔哩哔哩...](https://www.bilibili.com/video/BV1ZFNLzKEYj)\n\n[58\\. FLAR: A Unified Prototype Framework for Few-sample Lifelong Active Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Fan_FLAR_A_Unified_Prototype_Framework_for_Few-Sample_Lifelong_Active_Recognition_ICCV_2021_paper.pdf)\n\n[61\\. G. I. Parisi, Ronald Kemker et al. “Continual Lifelong Learning with Neural Networks: A Review.” Neural networks : the official journal of the International Neural Network Society](https://doi.org/10.1016/j.neunet.2019.01.012.)\n\n[62\\. Jaehong Yoon, Eunho Yang et al. “Lifelong Learning with Dynamically Expandable Networks.” ArXiv](https://arxiv.org/abs/1708.01547)\n\n[63\\. Dynamically Expandable Neural Networks](https://hackernoon.com/dynamically-expandable-neural-networks-ce75ff2b69cf)\n\n[64\\. Streamable Neural Fields](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136800580.pdf)\n\n[65\\. Privacy-Aware Lifelong Learning](https://openreview.net/pdf?id=UstOpZCESc)\n\n[66\\. 深度模型的持续学习综述：理论、方法和应用](https://jeit.ac.cn/cn/article/pdf/preview/10.11999/JEIT240095.pdf)\n\n[67\\. Large-scale deep class-incremental learning](https://theses.hal.science/tel-03478553v1/file/2021IMTA0281_Belouadah-Eden.pdf)\n\n[68\\. 面向任务扩展的增量学习动态神经网络:研究进展与展望](https://www.ejournal.org.cn/CN/PDF/10.12263/DZXB.20221226)\n\n[69\\. Rajeev Gupta, Suhani Gupta et al. “Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems.”](https://arxiv.org/abs/2504.20109)\n\n[70\\. LIFELONG LEARNING WITH DYNAMICALLY EXPANDABLE NETWORKS](https://zhuanlan.zhihu.com/p/87775025)\n\n[71\\. J. Kirkpatrick, Razvan Pascanu et al. “Overcoming catastrophic forgetting in neural networks.” Proceedings of the National Academy of Sciences](https://doi.org/10.1073/pnas.1611835114)\n\n[72\\. Arxiv 8.10 持续学习/增量学习/终身学习 continual learning/incremental learning/lifelong learning](https://zhuanlan.zhihu.com/p/649177641)\n\n[81\\. Catastrophic Forgetting in Deep Graph Networks](https://pmc.ncbi.nlm.nih.gov/articles/PMC8855050/)\n\n[82\\. Measuring Catastrophic Forgetting in Neural Networks](https://www.thejournal.club/c/paper/127340/)\n\n[83\\. UNLOCKING THE POWER OF FUNCTION VECTORS FOR CHARACTERIZING AND MITIGATING CATASTROPHIC FORGETTING IN CONTINUAL INSTRUCTION TUNING](https://openreview.net/pdf/be5cf6009ff66d686d7434fb356f403eddb1b9ce.pdf)\n\n[84\\. Yo'LLaVA: Your Personalized Language and Vision Assistant](https://proceedings.neurips.cc/paper_files/paper/2024/file/48088756ec0ce6ba362bddc7ebeb3915-Paper-Conference.pdf)\n\n[85\\. Sylvestre-Alvise Rebuffi, Alexander Kolesnikov et al. “iCaRL: Incremental Classifier and Representation Learning.” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2017.587)\n\n[86\\. Friedemann Zenke, Ben Poole et al. “Continual Learning Through Synaptic Intelligence.” Proceedings of machine learning research](https://www.semanticscholar.org/paper/a99d857ecc78316a0d9a774972b775058d5644ca)\n\n[87\\. I. Goodfellow, Mehdi Mirza et al. “An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks.” CoRR](https://arxiv.org/abs/1312.6211)\n\n[88\\. Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomy](https://journals-sol.sbc.org.br/index.php/jbcs/article/download/3966/2845/21743)\n\n[89\\. Catastrophic Forgetting in LLMs: A Comparative Analysis ...](https://arxiv.org/html/2504.01241v1)\n\n[90\\. Andrei A. Rusu, Neil C. Rabinowitz et al. “Progressive Neural Networks.” ArXiv](https://arxiv.org/abs/1606.04671)\n\n[91\\. Robust Deep Learning in the Open World with Lifelong Learning and Representation Learning](https://deepblue.lib.umich.edu/bitstream/handle/2027.42/162981/kibok_1.pdf?sequence=1)\n\n[92\\. Continual Learning and Catastrophic Forgetting](https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf)\n\n[93\\. Catastrophic Forgetting in Neural Networks](https://dev.co/catastrophic-forgetting-in-neural-networks)\n\n[94\\. 神经网络模型中灾难性遗忘研究的综述](https://journal.bjut.edu.cn/bjgydxxb/cn/article/pdf/preview/10.11936/bjutxb2020120014.pdf)\n\n[95\\. Zhizhong Li, Derek Hoiem. “Learning without Forgetting.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1007/978-3-319-46493-0_37)\n\n[96\\. Predictive Attractor Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/5df4313ecd4875931fbdacc486cc1fcf-Paper-Conference.pdf)\n\n[97\\. G. I. Parisi, Ronald Kemker et al. “Continual Lifelong Learning with Neural Networks: A Review.” Neural networks : the official journal of the International Neural Network Society](https://doi.org/10.1016/j.neunet.2019.01.012.)\n\n[101\\. G. I. Parisi, Ronald Kemker et al. “Continual Lifelong Learning with Neural Networks: A Review.” Neural networks : the official journal of the International Neural Network Society](https://doi.org/10.1016/j.neunet.2019.01.012.)\n\n[102\\. KAN: Kolmogorov–Arnold Networks](http://arxiv.org/pdf/2404.19756)\n\n[103\\. REMIND Your Neural Network to Prevent Catastrophic Forgetting](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530460.pdf)\n\n[104\\. Continual Learning and Catastrophic Forgetting](https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf)\n\n[105\\. Catastrophic forgetting in Continual Learning](https://www.bestpfe.com/catastrophic-forgetting-in-continual-learning/)\n\n[106\\. J. Kirkpatrick, Razvan Pascanu et al. “Overcoming catastrophic forgetting in neural networks.” Proceedings of the National Academy of Sciences](https://doi.org/10.1073/pnas.1611835114)\n\n[107\\. Continual Learning with Lifelong Vision Transformer](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Continual_Learning_With_Lifelong_Vision_Transformer_CVPR_2022_paper.pdf)\n\n[108\\. Neural Topic Modeling with Continual Lifelong Learning](http://proceedings.mlr.press/v119/gupta20a/gupta20a.pdf)\n\n[109\\. Jaehong Yoon, Eunho Yang et al. “Lifelong Learning with Dynamically Expandable Networks.” ArXiv](https://arxiv.org/abs/1708.01547)\n\n[110\\. Catastrophic Forgetting: Understanding AI Memory Loss](https://www.projectpro.io/article/catastrophic-forgetting/1034#:~:text=How%20to%20prevent%20Catastrophic%20Forgetting?,neural%20networks%20and%20machine%20learning.)\n\n[111\\. Neural networks for continuous lifelong learning: A review](http://www.ijesonline.com/Published%20Paper/Volume%2047/Special_Issue_July_18/150.doc)\n\n[112\\. Lifelong Machine Learning](https://www2.cs.uic.edu/~liub/lifelong-learning/Draft-Lifelong_Machine_Learning-2nd-edition.pdf)\n\n[113\\. Catastrophic Interference in Artificial Neural Networks](https://www.knowpia.com/knowpedia/Catastrophic_interference)\n\n[114\\. Unveiling Principles of Neural Computations: From Biological to Artificial Intelligence, and Back](https://klab.tch.harvard.edu/publications/PDFs/gk8125.pdf)\n\n[115\\. CHALLENGING COMMON ASSUMPTIONS ABOUT CATASTROPHIC FORGETTING AND KNOWLEDGE ACCUMULATION](https://proceedings.mlr.press/v232/lesort23a/lesort23a.pdf)\n\n[116\\. Sleep prevents catastrophic forgetting in spiking neural networks by forming joint synaptic weight representations](https://www.biorxiv.org/content/10.1101/688622v2.full.pdf)\n\n[121\\. Jaehong Yoon, Eunho Yang et al. “Lifelong Learning with Dynamically Expandable Networks.” ArXiv](https://arxiv.org/abs/1708.01547)\n\n[122\\. Dynamically Expandable Neural Networks](https://hackernoon.com/dynamically-expandable-neural-networks-ce75ff2b69cf)\n\n[123\\. LIFELONG LEARNING WITH DYNAMICALLY EXPANDABLE NETWORKS](https://zhuanlan.zhihu.com/p/87775025)\n\n[124\\. J. Kirkpatrick, Razvan Pascanu et al. “Overcoming catastrophic forgetting in neural networks.” Proceedings of the National Academy of Sciences](https://doi.org/10.1073/pnas.1611835114)\n\n[125\\. G. I. Parisi, Ronald Kemker et al. “Continual Lifelong Learning with Neural Networks: A Review.” Neural networks : the official journal of the International Neural Network Society](https://doi.org/10.1016/j.neunet.2019.01.012.)\n\n[126\\. Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training](https://proceedings.neurips.cc/paper_files/paper/2024/file/143ea4a156ef64f32d4d905206cf32e1-Paper-Conference.pdf)\n\n[127\\. Jaehong Yoon, Eunho Yang et al. “IFELONG L EARNING WITH D YNAMICALLY E XPANDABLE N ETWORKS.”](https://www.semanticscholar.org/paper/af2c7e3423558abe4d31dea1dc9a080c1434ddb3)\n\n[128\\. David Lopez-Paz, Marc'Aurelio Ranzato. “Gradient Episodic Memory for Continual Learning.” Neural Information Processing Systems](https://arxiv.org/abs/1706.08840)\n\n[129\\. Friedemann Zenke, Ben Poole et al. “Continual Learning Through Synaptic Intelligence.” Proceedings of machine learning research](https://www.semanticscholar.org/paper/a99d857ecc78316a0d9a774972b775058d5644ca)\n\n[130\\. Andrei A. Rusu, Neil C. Rabinowitz et al. “Progressive Neural Networks.” ArXiv](https://arxiv.org/abs/1606.04671)\n\n[131\\. Splitting Steepest Descent for Growing Neural Architectures](http://papers.neurips.cc/paper/9250-splitting-steepest-descent-for-growing-neural-architectures.pdf)\n\n[141\\. Catastrophic Forgetting in Deep Graph Networks](https://pmc.ncbi.nlm.nih.gov/articles/PMC8855050/)\n\n[142\\. Measuring Catastrophic Forgetting in Neural Networks](https://ojs.aaai.org/index.php/AAAI/article/view/11651)\n\n[143\\. Peiqiu Chen, Wei Wei et al. “Overcoming Catastrophic Forgetting by Bayesian Generative Regularization.” International Conference on Machine Learning](https://arxiv.org/abs/1912.01238)\n\n[144\\. Cuong V Nguyen, A. Achille et al. “Toward Understanding Catastrophic Forgetting in Continual Learning.” ArXiv](https://arxiv.org/abs/1908.01091)\n\n[145\\. 函数向量对齐技术，让大模型持续学习不“失忆”](https://m.36kr.com/p/3314742975408128)\n\n[146\\. Hanul Shin, Jung Kwon Lee et al. “Continual Learning with Deep Generative Replay.” Neural Information Processing Systems](https://arxiv.org/abs/1705.08690)\n\n[147\\. Friedemann Zenke, Ben Poole et al. “Continual Learning Through Synaptic Intelligence.” Proceedings of machine learning research](https://www.semanticscholar.org/paper/a99d857ecc78316a0d9a774972b775058d5644ca)\n\n[148\\. Simple Structures in Deep Networks](https://escholarship.org/content/qt8v60c6qg/qt8v60c6qg_noSplash_5393b6fcbf7f647c3296cdf109371728.pdf)\n\n[149\\. Jianshu Zhang, Yankai Fu et al. “CORE: Mitigating Catastrophic Forgetting in Continual Learning through Cognitive Replay.” ArXiv](https://doi.org/10.48550/arXiv.2402.01348)\n\n[150\\. Zhizhong Li, Derek Hoiem. “Learning without Forgetting.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1007/978-3-319-46493-0_37)\n\n[151\\. I. Goodfellow, Mehdi Mirza et al. “An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks.” CoRR](https://arxiv.org/abs/1312.6211)\n\n[152\\. Mitigating Forgetting in Online Continual Learning with Neuron Calibration](https://proceedings.neurips.cc/paper_files/paper/2021/file/54ee290e80589a2a1225c338a71839f5-Paper.pdf)\n\n[153\\. THE EFFECTIVENESS OF MEMORY REPLAY IN LARGE SCALE CONTINUAL LEARNING](https://openreview.net/pdf?id=AGQGZkLBKK)\n\n[154\\. Elephant Neural Networks: Born to Be a Continual Learner](https://openreview.net/pdf?id=kxe0hQ5mxp)\n\n[155\\. M. McCloskey, N. J. Cohen. “Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem.” Psychology of Learning and Motivation](https://doi.org/10.1016/S0079-7421%2808%2960536-8)\n\n[156\\. Mitigating Catastrophic Forgetting in Online Continual Learning by Modeling Previous Task Interrelations via Pareto Optimization](https://raw.githubusercontent.com/mlresearch/v235/main/assets/wu24ab/wu24ab.pdf)\n\n[157\\. Catastrophic Forgetting. “P OISONING G ENERATIVE M ODELS TO P ROMOTE C ATASTROPHIC F ORGETTING.”](https://www.semanticscholar.org/paper/042613b48b8a5b47ab8bc008004b60b90a28b3ca)\n\n[158\\. 马毅团队新作,微调多模态大模型会「灾难性遗忘」,让性能大减-36氪](https://36kr.com/p/2450958713149570)\n\n[159\\. Bo Liu, Mao Ye et al. “Fine-Grained Gradient Restriction: A Simple Approach for Mitigating Catastrophic Forgetting.”](https://arxiv.org/abs/2410.00868)\n\n[161\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[162\\. KAN: Kolmogorov–Arnold Networks](http://arxiv.org/pdf/2404.19756)\n\n[163\\. Catastrophic forgetting in Continual Learning](https://www.bestpfe.com/catastrophic-forgetting-in-continual-learning/)\n\n[164\\. Continual Learning with Lifelong Vision Transformer](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Continual_Learning_With_Lifelong_Vision_Transformer_CVPR_2022_paper.pdf)\n\n[165\\. G. I. Parisi, Ronald Kemker et al. “Continual Lifelong Learning with Neural Networks: A Review.” Neural networks : the official journal of the International Neural Network Society](https://doi.org/10.1016/j.neunet.2019.01.012.)\n\n[166\\. D. Rolnick, Arun Ahuja et al. “Experience Replay for Continual Learning.” Neural Information Processing Systems](https://arxiv.org/abs/1811.11682)\n\n[167\\. Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting](http://proceedings.mlr.press/v97/li19m/li19m.pdf)\n\n[168\\. NEUROCOGNITIVE-INSPIRED PARADIGMS FOR CONTINUAL LEARNING](https://www.iris.unict.it/retrieve/feacd6e6-c517-43eb-ae66-9d1e72d676ae/Giovanni_Bellitto_phd_thesis.pdf)\n\n[169\\. J. Kirkpatrick, Razvan Pascanu et al. “Overcoming catastrophic forgetting in neural networks.” Proceedings of the National Academy of Sciences](https://doi.org/10.1073/pnas.1611835114)\n\n[170\\. A Comprehensive Survey of Continual Learning: Theory, Method and Application](https://arxiv.org/pdf/2302.00487v1)\n\n[171\\. Tyler L. Hayes, Kushal Kafle et al. “REMIND Your Neural Network to Prevent Catastrophic Forgetting.” ArXiv](https://doi.org/10.1007/978-3-030-58598-3_28)\n\n[172\\. Compacting, Picking and Growing for Unforgetting Continual Learning](https://proceedings.neurips.cc/paper/2019/file/3b220b436e5f3d917a1e649a0dc0281c-Paper.pdf)\n\n[173\\. Continual Learning and Catastrophic Forgetting](https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf)\n\n[174\\. CHALLENGING COMMON ASSUMPTIONS ABOUT CATASTROPHIC FORGETTING AND KNOWLEDGE ACCUMULATION](https://proceedings.mlr.press/v232/lesort23a/lesort23a.pdf)\n\n[175\\. HCLmNet: A Unified Hybrid Continual Learning Strategy Multimodal Network for Lung Cancer Survival Prediction](https://www.medrxiv.org/content/10.1101/2024.12.14.24319041.full.pdf)\n\n[176\\. Learning to Continually Learn](https://ecai2020.eu/papers/939_paper.pdf)\n\n[181\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[182\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[183\\. Yann LeCun, L. Bottou et al. “Gradient-based learning applied to document recognition.” Proc. IEEE](https://doi.org/10.1109/5.726791)\n\n[184\\. A. Krizhevsky. “Learning Multiple Layers of Features from Tiny Images.”](https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086)\n\n[185\\. Xavier Glorot, Yoshua Bengio. “Understanding the difficulty of training deep feedforward neural networks.” International Conference on Artificial Intelligence and Statistics](https://www.semanticscholar.org/paper/ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649)\n\n[186\\. Jaehong Yoon, Eunho Yang et al. “Lifelong Learning with Dynamically Expandable Networks.” ArXiv](https://arxiv.org/abs/1708.01547)\n\n[187\\. 三种权重的初始化方法 - 菜鸡一枚 - 博客园](https://www.cnblogs.com/yymn/articles/7069014.htm)\n\n[188\\. Dynamically Expandable Neural Networks](https://hackernoon.com/dynamically-expandable-neural-networks-ce75ff2b69cf)\n\n[189\\. Deep Learning Toolbox](https://raw.githubusercontent.com/stavros99/DeepLearningToolbox_Matlab/master/manual.pdf)\n\n[190\\. Challenges with weight initialization in neural networks](https://www.byteplus.com/en/topic/401470)\n\n[191\\. BrainScale: Enabling Scalable Online Learning in Spiking Neural Networks](https://www.biorxiv.org/content/10.1101/2024.09.24.614728v1.full.pdf)\n\n[192\\. WI-TMLEGA: Weight Initialization and Training Method ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC11353430/)\n\n[193\\. Splitting Steepest Descent for Growing Neural Architectures](http://papers.neurips.cc/paper/9250-splitting-steepest-descent-for-growing-neural-architectures.pdf)\n\n[194\\. Deep Feedforward Networks](https://course.ece.cmu.edu/~ece739/lectures/18739-2020-spring-lecture-06-deep-feedforward-networks.pdf)\n\n[195\\. BrainUnit: Integrating Physical Units into High-Performance AI-Driven Scientific Computing](https://www.biorxiv.org/content/10.1101/2024.09.20.614111v1.full.pdf)\n\n[196\\. Neural Networks and Deep Learning](https://static.latexstudio.net/article/2018/0912/neuralnetworksanddeeplearning.pdf)\n\n[197\\. Deep Learning Methods Applied to Higgs Physics at the LHC](https://cds.cern.ch/record/2791460/files/TS2021_024_3.pdf)\n\n[198\\. Weight Initialization Techniques for Deep Neural Networks](https://www.geeksforgeeks.org/weight-initialization-techniques-for-deep-neural-networks/)\n\n[201\\. Catastrophic Forgetting in Deep Graph Networks](https://pmc.ncbi.nlm.nih.gov/articles/PMC8855050/)\n\n[202\\. Measuring Catastrophic Forgetting in Neural Networks](https://ojs.aaai.org/index.php/AAAI/article/view/11651)\n\n[203\\. Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting](http://proceedings.mlr.press/v97/li19m/li19m.pdf)\n\n[204\\. Mitigating Forgetting in Online Continual Learning with Neuron Calibration](https://proceedings.neurips.cc/paper_files/paper/2021/file/54ee290e80589a2a1225c338a71839f5-Paper.pdf)\n\n[205\\. David Lopez-Paz, Marc'Aurelio Ranzato. “Gradient Episodic Memory for Continual Learning.” Neural Information Processing Systems](https://arxiv.org/abs/1706.08840)\n\n[206\\. Peiqiu Chen, Wei Wei et al. “Overcoming Catastrophic Forgetting by Bayesian Generative Regularization.” International Conference on Machine Learning](https://arxiv.org/abs/1912.01238)\n\n[207\\. Prakhar Kaushik, Alex Gain et al. “Understanding Catastrophic Forgetting and Remembering in Continual Learning with Optimal Relevance Mapping.” ArXiv](https://arxiv.org/abs/2102.11343)\n\n[208\\. Hanul Shin, Jung Kwon Lee et al. “Continual Learning with Deep Generative Replay.” Neural Information Processing Systems](https://arxiv.org/abs/1705.08690)\n\n[209\\. Friedemann Zenke, Ben Poole et al. “Continual Learning Through Synaptic Intelligence.” Proceedings of machine learning research](https://www.semanticscholar.org/paper/a99d857ecc78316a0d9a774972b775058d5644ca)\n\n[210\\. I. Goodfellow, Mehdi Mirza et al. “An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks.” CoRR](https://arxiv.org/abs/1312.6211)\n\n[211\\. Cuong V Nguyen, A. Achille et al. “Toward Understanding Catastrophic Forgetting in Continual Learning.” ArXiv](https://arxiv.org/abs/1908.01091)\n\n[212\\. Catastrophic Forgetting in Neural Networks](https://dev.co/catastrophic-forgetting-in-neural-networks)\n\n[213\\. An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks](https://www.arxiv-vanity.com/papers/1312.6211/)\n\n[214\\. Mitigating Catastrophic Forgetting in Online Continual Learning by Modeling Previous Task Interrelations via Pareto Optimization](https://raw.githubusercontent.com/mlresearch/v235/main/assets/wu24ab/wu24ab.pdf)\n\n[215\\. Zhizhong Li, Derek Hoiem. “Learning without Forgetting.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1007/978-3-319-46493-0_37)\n\n[216\\. Hitesh U. Vaidya. “Reducing Catastrophic Forgetting in Self-Organizing Maps.”](https://www.semanticscholar.org/paper/792133be1576891db87992f19656282b06758a17)\n\n[217\\. M. McCloskey, N. J. Cohen. “Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem.” Psychology of Learning and Motivation](https://doi.org/10.1016/S0079-7421%2808%2960536-8)\n\n[218\\. Timothée Lesort. “Continual Learning: Tackling Catastrophic Forgetting in Deep Neural Networks with Replay Processes.” ArXiv](https://arxiv.org/abs/2007.00487)\n\n[219\\. Meta Networks](http://proceedings.mlr.press/v70/munkhdalai17a/munkhdalai17a.pdf)\n\n[220\\. Online Continual Learning with Maximally Interfered Retrieval](http://papers.neurips.cc/paper/9357-online-continual-learning-with-maximal-interfered-retrieval.pdf)\n\n[221\\. CoRaL: Continual Representation Learning for Overcoming Catastrophic Forgetting](https://www.ifaamas.org/Proceedings/aamas2023/pdfs/p1969.pdf)\n\n[222\\. Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting](https://news.ncsu.edu/2019/05/ai-continual-learning-framework/)\n\n[223\\. Simple Structures in Deep Networks](https://escholarship.org/content/qt8v60c6qg/qt8v60c6qg_noSplash_5393b6fcbf7f647c3296cdf109371728.pdf)\n\n[224\\. Liquid Neural Networks](https://weeklyreport.ai/briefings/liquid-neural-networks/)\n\n[225\\. ConStruct-VL: Data-Free Continual Structured VL Concepts Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Smith_ConStruct-VL_Data-Free_Continual_Structured_VL_Concepts_Learning_CVPR_2023_paper.pdf)\n\n[226\\. G. I. Parisi, Ronald Kemker et al. “Continual Lifelong Learning with Neural Networks: A Review.” Neural networks : the official journal of the International Neural Network Society](https://doi.org/10.1016/j.neunet.2019.01.012.)\n\n[227\\. Liquid Neural Network: A Dynamic and Flexible AI Approach](https://www.ai-demand.com/insights/tech/artificial-intelligence/liquid-neural-network-unveiling-the-fluid-intelligence/)\n\n[228\\. Vector Quantization Prompting for Continual Learning](https://openreview.net/pdf?id=ACCqGLviig)\n\n[229\\. How Liquid AI Is Challenging Transformer-Based AI Models](https://thenewstack.io/how-liquid-ai-is-challenging-transformer-based-ai-models/)\n\n[230\\. Continual learning with neural networks](https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=1447&context=etd_coll)\n\n[231\\. CLR: Channel-wise Lightweight Reprogramming for Continual Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_CLR_Channel-wise_Lightweight_Reprogramming_for_Continual_Learning_ICCV_2023_paper.pdf)\n\n[232\\. Energy Efficiency Scaling for Two Decades Research and Development Roadmap—Version 1.0](https://www.energy.gov/sites/default/files/2024-08/Draft_EES2_Roadmap_AMMTO_August29_2024.pdf)\n\n[233\\. Sylvestre-Alvise Rebuffi, Alexander Kolesnikov et al. “iCaRL: Incremental Classifier and Representation Learning.” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2017.587)\n\n[234\\. Permissioned Distributed Ledger (PDL); Artificial Intelligence for Permissioned Distributed Ledger](https://www.etsi.org/deliver/etsi_gr/PDL/001_099/032/01.01.01_60/gr_PDL032v010101p.pdf)\n\n[235\\. Continual Learning and Catastrophic Forgetting](https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf)\n\n[236\\. 4 ways to enable Continual learning into Neural Networks](https://hub.packtpub.com/4-ways-enable-continual-learning-neural-networks/)\n\n[237\\. Continual Learning with Lifelong Vision Transformer](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Continual_Learning_With_Lifelong_Vision_Transformer_CVPR_2022_paper.pdf)\n\n[238\\. NPCL: Neural Processes for Uncertainty-Aware Continual Learning](https://proceedings.neurips.cc/paper_files/paper/2023/file/6c4a1a3cbe70ef36d7d6332166bba77d-Supplemental-Conference.pdf)\n\n[241\\. Dynamically Expandable Neural Networks](https://hackernoon.com/dynamically-expandable-neural-networks-ce75ff2b69cf)\n\n[242\\. J. Bergstra, Yoshua Bengio. “Random Search for Hyper-Parameter Optimization.” J. Mach. Learn. Res.](https://doi.org/10.5555/2503308.2188395)\n\n[243\\. Jasper Snoek, H. Larochelle et al. “Practical Bayesian Optimization of Machine Learning Algorithms.” Neural Information Processing Systems](https://arxiv.org/abs/1206.2944)\n\n[244\\. J. Bergstra, R. Bardenet et al. “Algorithms for Hyper-Parameter Optimization.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/03911c85305d42aa2eeb02be82ef6fb7da644dd0)\n\n[245\\. Hyperparameter Optimization](https://www.automl.org/wp-content/uploads/2019/05/AutoML_Book_Chapter1.pdf)\n\n[246\\. Real-time deep learning design tool for far-field radiation profile](https://www.researching.cn/articles/OJ906c9d448c2dfd4d)\n\n[247\\. L2-NONEXPANSIVE NEURAL NETWORKS](https://arxiv.org/pdf/1802.07896v3.pdf)\n\n[248\\. Shijin Gong, Xinyu Zhang. “Towards Optimal Neural Networks: the Role of Sample Splitting in Hyperparameter Selection.” ArXiv](https://doi.org/10.48550/arXiv.2307.07726)\n\n[249\\. F. Hutter, H. Hoos et al. “Sequential Model-Based Optimization for General Algorithm Configuration.” Learning and Intelligent Optimization](https://doi.org/10.1007/978-3-642-25566-3_40)\n\n[250\\. Kevin Swersky, Jasper Snoek et al. “Freeze-Thaw Bayesian Optimization.” ArXiv](https://arxiv.org/abs/1406.3896)\n\n[251\\. Neural Network Hyperparameter Optimization with Sparse Grids](https://mediatum.ub.tum.de/doc/1728129/1728129.pdf)\n\n[252\\. Optimal SGD Hyperparameters for Fully Connected Networks](http://bayesiandeeplearning.org/2018/papers/86.pdf)\n\n[253\\. Learning improvement in spiking neural networks: enhancement techniques, design and performance analysis](https://dr.ntu.edu.sg/bitstream/10356/179879/2/thesis_WangSiqi.pdf)\n\n[254\\. 神经网络与深度学习](http://www.liuxiao.org/wp-content/uploads/2016/10/nndl-ebook.pdf)\n\n[255\\. Low-Loss Space in Neural Networks is Continuous and Fully Connected](https://arxiv.org/pdf/2505.02604)\n\n[256\\. Neural Networks Learn Distance Metrics](http://arxiv.org/html/2502.02103v1)\n\n[261\\. A. Krizhevsky. “Learning Multiple Layers of Features from Tiny Images.”](https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086)\n\n[262\\. Online Continual Learning with Maximally Interfered Retrieval](http://papers.neurips.cc/paper/9357-online-continual-learning-with-maximal-interfered-retrieval.pdf)\n\n[263\\. Mitigating Forgetting in Online Continual Learning with Neuron Calibration](https://proceedings.neurips.cc/paper_files/paper/2021/file/54ee290e80589a2a1225c338a71839f5-Paper.pdf)\n\n[264\\. J. Kirkpatrick, Razvan Pascanu et al. “Overcoming catastrophic forgetting in neural networks.” Proceedings of the National Academy of Sciences](https://doi.org/10.1073/pnas.1611835114)\n\n[265\\. PROMPT GRADIENT PROJECTION FOR CONTINUAL LEARNING](https://openreview.net/pdf?id=EH2O3h7sBI)\n\n[266\\. David Lopez-Paz, Marc'Aurelio Ranzato. “Gradient Episodic Memory for Continual Learning.” Neural Information Processing Systems](https://arxiv.org/abs/1706.08840)\n\n[267\\. 核心：通过认知回放减轻持续学习中的灾难性遗忘](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_2_5/2402.01348.pdf)\n\n[268\\. Friedemann Zenke, Ben Poole et al. “Continual Learning Through Synaptic Intelligence.” Proceedings of machine learning research](https://www.semanticscholar.org/paper/a99d857ecc78316a0d9a774972b775058d5644ca)\n\n[269\\. LEARNING TO REMEMBER MORE WITH LESS MEMORIZATION](https://openreview.net/pdf?id=r1xlvi0qYm)\n\n[270\\. Zhizhong Li, Derek Hoiem. “Learning without Forgetting.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1007/978-3-319-46493-0_37)\n\n[271\\. Overcoming Catastrophic Forgetting by Bayesian Generative Regularization](http://proceedings.mlr.press/v139/chen21v/chen21v.pdf)\n\n[272\\. Measuring Catastrophic Forgetting in Neural Networks](https://ojs.aaai.org/index.php/AAAI/article/view/11651)\n\n[273\\. Efficient Learning across Multiple Domains with Deep Neural Networks](https://escholarship.org/content/qt1gd449ns/qt1gd449ns_noSplash_5205a27c32b079e31dcb2292b8e263a4.pdf)\n\n[274\\. An Empirical Investigation of the Role of Pre-training in Lifelong Learning](https://jmlr.org/papers/volume24/22-0496/22-0496.pdf)\n\n[275\\. Prakhar Kaushik, Alex Gain et al. “Understanding Catastrophic Forgetting and Remembering in Continual Learning with Optimal Relevance Mapping.” ArXiv](https://arxiv.org/abs/2102.11343)\n\n[276\\. Learn, don’t forget: constructive methods for effective continual learning](https://upcommons.upc.edu/bitstream/handle/2117/374177/thesis%20Aitor%20Ganuza.pdf?sequence=1)\n\n[277\\. Yunhui Guo, Mingrui Liu et al. “Learning with Long-term Remembering: Following the Lead of Mixed Stochastic Gradient.” ArXiv](https://arxiv.org/abs/1909.11763)\n\n[278\\. Principal Gradient Direction and Confidence Reservoir Sampling for Continual Learning](https://sai.pku.edu.cn/system/_content/download.jsp?urltype=news.DownloadAttachUrl&owner=1887772040&wbfileid=12070500)\n\n[279\\. NeuroGen: Neural Network Parameter Generation via Large Language Models](https://arxiv.org/pdf/2505.12470v2)\n\n[281\\. KAN: Kolmogorov–Arnold Networks](http://arxiv.org/pdf/2404.19756)\n\n[282\\. Learning to Dream, Dreaming to Learn](https://boristheses.unibe.ch/4258/12/23deperrois_nrp.pdf)\n\n[283\\. NPCL: Neural Processes for Uncertainty-Aware Continual Learning](https://proceedings.neurips.cc/paper_files/paper/2023/file/6c4a1a3cbe70ef36d7d6332166bba77d-Paper-Conference.pdf)\n\n[284\\. J. Kirkpatrick, Razvan Pascanu et al. “Overcoming catastrophic forgetting in neural networks.” Proceedings of the National Academy of Sciences](https://doi.org/10.1073/pnas.1611835114)\n\n[285\\. Sleep prevents catastrophic forgetting in spiking neural networks by forming joint synaptic weight representations](https://www.biorxiv.org/content/10.1101/688622v2.full.pdf)\n\n[286\\. Sleep prevents catastrophic forgetting in spiking neural networks by forming a joint synaptic weight representation](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010628)\n\n[287\\. NEUROCOGNITIVE-INSPIRED PARADIGMS FOR CONTINUAL LEARNING](https://www.iris.unict.it/retrieve/feacd6e6-c517-43eb-ae66-9d1e72d676ae/Giovanni_Bellitto_phd_thesis.pdf)\n\n[288\\. Learning to Continually Learn Rapidly from Few and Noisy Data](http://proceedings.mlr.press/v140/kuo21a/kuo21a.pdf)\n\n[289\\. Neural Networks Mimic Brain Circuits for AI Advances](https://www.electropages.com/blog/2025/03/neural-networks-take-inspiration-human-brain)\n\n[290\\. Deep Learning - Continual Learning](https://www.iss.uni-stuttgart.de/en/institute/former_employees/felix-wiewel/)\n\n[291\\. Residual Continual Learning](https://ojs.aaai.org/index.php/AAAI/article/view/5884)\n\n[292\\. Unveiling Principles of Neural Computations: From Biological to Artificial Intelligence, and Back](https://klab.tch.harvard.edu/publications/PDFs/gk8125.pdf)\n\n[293\\. CLR: Channel-wise Lightweight Reprogramming for Continual Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_CLR_Channel-wise_Lightweight_Reprogramming_for_Continual_Learning_ICCV_2023_paper.pdf)\n\n[294\\. Few-Shot Self Reminder to Overcome Catastrophic Forgetting](https://www.borealisai.com/publications/few-shot-self-reminder-overcome-catastrophic-forgetting/)\n\n[301\\. Hyperparameter Optimization of Machine Learning Algorithms](https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms)\n\n[302\\. J. Bergstra, Yoshua Bengio. “Random Search for Hyper-Parameter Optimization.” J. Mach. Learn. Res.](https://doi.org/10.5555/2503308.2188395)\n\n[303\\. Jasper Snoek, H. Larochelle et al. “Practical Bayesian Optimization of Machine Learning Algorithms.” Neural Information Processing Systems](https://arxiv.org/abs/1206.2944)\n\n[304\\. Dynamically Expandable Neural Networks](https://hackernoon.com/dynamically-expandable-neural-networks-ce75ff2b69cf)\n\n[305\\. J. Bergstra, R. Bardenet et al. “Algorithms for Hyper-Parameter Optimization.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/03911c85305d42aa2eeb02be82ef6fb7da644dd0)\n\n[306\\. S. Falkner, Aaron Klein et al. “BOHB: Robust and Efficient Hyperparameter Optimization at Scale.” ArXiv](https://arxiv.org/abs/1807.01774)\n\n[307\\. F. Hutter, H. Hoos et al. “Sequential Model-Based Optimization for General Algorithm Configuration.” Learning and Intelligent Optimization](https://doi.org/10.1007/978-3-642-25566-3_40)\n\n\\[308. C. Alecsa, Titus Pinta. “L G \\] 2 9 A pr 2 01 9 N EW OPTIMIZATION ALGORITHMS FOR NEURAL NETWORK TRAINING USING OPERATOR SPLITTING TECHNIQUES.” \\](https://www.semanticscholar.org/paper/d9c6c6ab4915ecf7e8738a93d7095fa43dc1b0bf)\n\n[309\\. Splitting Steepest Descent for Growing Neural Architectures](http://papers.neurips.cc/paper/9250-splitting-steepest-descent-for-growing-neural-architectures.pdf)\n\n[310\\. Real-time deep learning design tool for far-field radiation profile](https://www.researching.cn/articles/OJ906c9d448c2dfd4d)\n\n[311\\. Neural Network Hyperparameter Optimization with Sparse Grids](https://mediatum.ub.tum.de/doc/1728129/1728129.pdf)\n\n[312\\. Low-Loss Space in Neural Networks is Continuous and Fully Connected](https://arxiv.org/pdf/2505.02604)\n\n[313\\. Hyperparameter Optimization on Classification and Regression Algorithms](https://www.iosrjournals.org/iosr-jce/papers/Vol23-issue4/Ser-1/E2304013450.pdf)\n\n[314\\. Shijin Gong, Xinyu Zhang. “Towards Optimal Neural Networks: the Role of Sample Splitting in Hyperparameter Selection.” ArXiv](https://doi.org/10.48550/arXiv.2307.07726)\n\n[315\\. M²SGD: Learning to Learn Important Weights](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w15/Kuo_M2SGD_Learning_to_Learn_Important_Weights_CVPRW_2020_paper.pdf)\n\n[316\\. Study of optimal design of lattice structures using Deep Learning methods](https://upcommons.upc.edu/bitstream/handle/2117/421739/Study_of_Optimal_design_of_lattice_structures_using_DL_methods_JoelCarrasco.pdf?sequence=2)\n\n[317\\. C. Alecsa, Titus Pinta et al. “New optimization algorithms for neural network training using operator splitting techniques.” Neural networks : the official journal of the International Neural Network Society](https://doi.org/10.1016/j.neunet.2020.03.018)\n\n[318\\. Using Deep Learning to quality control products with challenging morphology](https://nmbu.brage.unit.no/nmbu-xmlui/bitstream/handle/11250/2832834/Julius_Kviman_master_thesis.pdf?sequence=1&isAllowed=y)\n\n[319\\. 神经网络超参数优化的删除垃圾神经元策略](https://wulixb.iphy.ac.cn/fileWLXB/journal/article/wlxb/2022/16/PDF/16-20220436.pdf)\n\n[321\\. 核心：通过认知回放减轻持续学习中的灾难性遗忘](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_2_5/2402.01348.pdf)\n\n[322\\. Catastrophic Forgetting in Deep Graph Networks](https://pmc.ncbi.nlm.nih.gov/articles/PMC8855050/)\n\n[323\\. Overcoming Catastrophic Forgetting by Bayesian Generative Regularization](http://proceedings.mlr.press/v139/chen21v/chen21v.pdf)\n\n[324\\. Prakhar Kaushik, Alex Gain et al. “Understanding Catastrophic Forgetting and Remembering in Continual Learning with Optimal Relevance Mapping.” ArXiv](https://arxiv.org/abs/2102.11343)\n\n[325\\. Mitigating Catastrophic Forgetting in Online Continual Learning by Modeling Previous Task Interrelations via Pareto Optimization](https://raw.githubusercontent.com/mlresearch/v235/main/assets/wu24ab/wu24ab.pdf)\n\n[326\\. 部分盲忘：贝叶斯视角的深度网络类忘却](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_3_26/2403.16246.pdf)\n\n[327\\. Prompt Gradient Projection For Continual Learning](https://iclr.cc/media/iclr-2024/Slides/19110.pdf)\n\n[328\\. Mitigating Forgetting in Online Continual Learning with Neuron Calibration](https://proceedings.neurips.cc/paper_files/paper/2021/file/54ee290e80589a2a1225c338a71839f5-Paper.pdf)\n\n[329\\. Measuring Catastrophic Forgetting in Neural Networks](https://ojs.aaai.org/index.php/AAAI/article/view/11651)\n\n[330\\. David Lopez-Paz, Marc'Aurelio Ranzato. “Gradient Episodic Memory for Continual Learning.” Neural Information Processing Systems](https://arxiv.org/abs/1706.08840)\n\n[331\\. Hitesh U. Vaidya. “Reducing Catastrophic Forgetting in Self-Organizing Maps.”](https://www.semanticscholar.org/paper/792133be1576891db87992f19656282b06758a17)\n\n[332\\. LEARNING TO REMEMBER MORE WITH LESS MEMORIZATION](https://openreview.net/pdf?id=r1xlvi0qYm)\n\n[333\\. Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting](http://proceedings.mlr.press/v97/li19m/li19m.pdf)\n\n[334\\. Hanul Shin, Jung Kwon Lee et al. “Continual Learning with Deep Generative Replay.” Neural Information Processing Systems](https://arxiv.org/abs/1705.08690)\n\n[335\\. Andrei A. Rusu, Neil C. Rabinowitz et al. “Progressive Neural Networks.” ArXiv](https://arxiv.org/abs/1606.04671)\n\n[336\\. Zhizhong Li, Derek Hoiem. “Learning without Forgetting.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1007/978-3-319-46493-0_37)\n\n[337\\. PROMPT GRADIENT PROJECTION FOR CONTINUAL LEARNING](https://openreview.net/pdf?id=EH2O3h7sBI)\n\n[341\\. J. Kirkpatrick, Razvan Pascanu et al. “Overcoming catastrophic forgetting in neural networks.” Proceedings of the National Academy of Sciences](https://doi.org/10.1073/pnas.1611835114)\n\n[342\\. Sleep prevents catastrophic forgetting in spiking neural networks by forming joint synaptic weight representations](https://www.biorxiv.org/content/10.1101/688622v2.full.pdf)\n\n[343\\. Sleep prevents catastrophic forgetting in spiking neural networks by forming a joint synaptic weight representation](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010628)\n\n[344\\. REMIND Your Neural Network to Prevent Catastrophic Forgetting](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530460.pdf)\n\n[345\\. ADVANCING FEW-SHOT CONTINUAL LEARNING VIA SELECTIVE KNOWLEDGE TRANSFER](https://openreview.net/pdf?id=iN7EIQRUbF)\n\n[346\\. Catastrophic Interference in Artificial Neural Networks](https://www.knowpia.com/knowpedia/Catastrophic_interference)\n\n[347\\. Liquid Neural Networks: an introduction](https://indico.in2p3.fr/event/33209/contributions/141769/attachments/87279/131741/THINKII_LLN_081024.pdf)\n\n[348\\. Catastrophic forgetting in Continual Learning](https://www.bestpfe.com/catastrophic-forgetting-in-continual-learning/)\n\n[349\\. Learning to Continually Learn Rapidly from Few and Noisy Data](http://proceedings.mlr.press/v140/kuo21a/kuo21a.pdf)\n\n[350\\. Residual Continual Learning](https://ojs.aaai.org/index.php/AAAI/article/view/5884)\n\n[351\\. CONTINUAL LEARNING VIA WINNING SUBNETWORKS THAT ARISE THROUGH STOCHASTIC LOCAL COMPETITION](https://openreview.net/pdf?id=fUwfjPzI8g)\n\n[352\\. Few-Shot Self Reminder to Overcome Catastrophic Forgetting](https://www.borealisai.com/publications/few-shot-self-reminder-overcome-catastrophic-forgetting/)"
  },
  {
    "id": 41,
    "question": "Could transformer architectures be fundamentally reimagined to process multimodal inputs (video/audio/text) with the same efficiency they process text?",
    "response": "**Research Report**\n\n**To:** Interested Parties\n\n**From:** Expert Researcher\n\n**Date:** July 27, 2025\n\n**Subject:** An In-Depth Analysis on Reimagining Transformer Architectures for Efficient Multimodal Processing\n\n**Executive Summary**\n\nThe transformer architecture, having established unparalleled dominance in text processing, now faces its next frontier: multimodality. The prospect of a single model that can fluidly understand and generate video, audio, and text has captivated the AI research community. However, a fundamental challenge remains: the stark efficiency gap between processing sparse, discrete text and dense, continuous data streams like video and audio. This report investigates whether transformer architectures can be fundamentally reimagined to process these multimodal inputs with the same efficiency they process text.\n\nOur analysis, based on cutting-edge research and benchmarks as of 2025, reveals that while achieving _identical_ efficiency is not currently feasible due to the inherent differences in data dimensionality, the field is undergoing a radical reimagining. Innovations in unified architectural frameworks, intelligent token reduction, efficient attention mechanisms, and sophisticated temporal synchronization are rapidly closing the gap. The path forward lies not in brute-force scaling, but in developing architectures that can intelligently abstract high-dimensional sensory data into a sparse, semantically rich representation akin to text, before applying the full computational might of the transformer. This report provides a comprehensive overview of these architectural shifts, a quantitative analysis of the current efficiency landscape, and a forward-looking perspective on the future of truly efficient multimodal AI.\n\n**1.0 Introduction: The Efficiency Conundrum of Multimodal Transformers**\n\nThe transformer architecture has proven to be a paradigm-shifting force in artificial intelligence, primarily due to its exceptional performance and computational efficiency on natural language processing (NLP) tasks \\[21\\]. Its core mechanism, self-attention, allows it to model complex dependencies within sequential data, making it the foundation for Large Language Models (LLMs) like GPT-4 \\[23\\]\\[67\\].\n\nAs the ambition of AI grows, the focus has shifted from unimodal to multimodal systems—models that can process and integrate information from various sources like text, images, audio, and video \\[135\\]\\[172\\]. This transition is crucial for building AI that can interact with the world in a more human-like manner \\[130\\]. However, this ambition is confronted by a significant computational hurdle.\n\nThe efficiency of text-only transformers stems from the nature of text itself: it is a discrete, symbolic, and relatively low-dimensional data format. In contrast, video and audio are continuous, high-dimensional signals. A few seconds of high-resolution video can contain orders of magnitude more raw data than a lengthy text document. Directly applying the transformer architecture to this deluge of data results in prohibitively long token sequences, which, given the self-attention mechanism's quadratic complexity with respect to sequence length, leads to immense computational and memory costs \\[51\\].\n\nThis report explores the central research question: **Could transformer architectures be fundamentally reimagined to overcome this data-modality efficiency gap?** We will dissect the architectural modifications, tokenization strategies, and attention mechanisms being developed in 2025 to make multimodal processing as efficient, in principle, as text processing.\n\n**2.0 The Efficiency Gap in 2025: A Quantitative Perspective**\n\nTo understand the challenge, it is essential to quantify the efficiency disparity between text-only and multimodal transformers. Efficiency is typically measured through metrics like Floating Point Operations (FLOPs), which represent computational complexity, and latency, which measures real-world inference speed. Other important factors include energy consumption and throughput (tokens per second).\n\n**2.1 The Baseline: Text-Only Transformer Efficiency**\n\nText-only transformers are remarkably efficient for their capabilities. Training a lightweight GPT-2 model on NVIDIA A100 GPUs involves specific FLOPs calculations for its forward and backward passes \\[192\\]. The Vision Transformer (ViT), which adapts the architecture for images, achieves excellent results with substantially fewer computational resources than its convolutional predecessors \\[24\\]\\[347\\]. Inference speed for text generation has also become highly optimized; for a 7B parameter model, generating a token on an A100 GPU can take as little as 16 milliseconds \\[300\\], with some optimized models achieving high throughput rates of over 60 tokens/second \\[336\\].\n\n**2.2 The Multimodal Challenge: A Chasm of Complexity**\n\nMultimodal models are inherently more complex and computationally demanding \\[137\\]. Processing video, audio, and text together requires larger models and more sophisticated data handling, which translates directly to higher FLOPs and latency.\n\nDirect, controlled benchmarks comparing text-only and multimodal models on \"equivalent\" tasks are scarce, as the tasks themselves are fundamentally different \\[65\\]\\[188\\]. A text-only model cannot perform video captioning, and a multimodal model's performance may degrade on text-only tasks \\[241\\]. However, we can glean insights from individual model benchmarks:\n\n**FLOPs and Latency:** The LLaVA-Mini model, a large multimodal model for image and video, demonstrates the scale of computation. Its FLOPs and latency vary significantly with input resolution and the number of visual tokens processed, with all tests conducted on powerful NVIDIA A100 or RTX A6000 GPUs \\[129\\]\\[188\\]\\[352\\]. One multimodal sentiment analysis model, TF-Mamba, was shown to be more efficient than a Transformer-based baseline, but this is a comparison between two multimodal approaches, not against a text-only model \\[127\\]\\[189\\]. Similarly, a video Transformer-based model was found to be 1.3x more efficient in FLOPs than an image Transformer-based one for action recognition, again highlighting relative improvements within the multimodal domain \\[140\\].\n\n**Energy Consumption:** The operational cost of AI is increasingly scrutinized. While task complexity itself may have a minimal impact on inference energy if input/output lengths are fixed \\[277\\]\\[328\\]the vastly longer effective sequence lengths of multimodal inputs inherently drive up energy use. Broad-purpose generative models, which are often multimodal, consume significantly more energy than task-specific models \\[332\\]\\[391\\]. For instance, generating an image can require orders of magnitude more energy per 1,000 queries than simple text classification \\[286\\]. That said, hardware and software optimizations have led to dramatic efficiency gains; modern H100 GPUs with FP8 quantization can perform inference on a 70B parameter model for just 0.39 joules per token \\[389\\], a massive improvement over older estimates.\n\nThe evidence is clear: processing raw multimodal data with a standard transformer architecture is computationally extravagant. The path to text-like efficiency requires a fundamental reimagining of how these models ingest and process information.\n\n**3.0 Architectural Reimaginings for Multimodal Efficiency**\n\nTo bridge the efficiency gap, researchers are pursuing a multi-pronged strategy, rethinking everything from the overall model structure to the fundamental way data is represented and processed.\n\n**3.1 Unifying and Modularizing the Architecture**\n\nTwo dominant architectural philosophies have emerged:\n\n1.  **Unified Attention Frameworks:** Models like the Perceiver family pioneer a single, generic attention framework. They use a small set of latent \"bottleneck\" arrays to iteratively attend to the massive byte arrays of different modalities (video, audio, text), effectively decoupling the main processing complexity from the input size \\[1\\]\\[221\\]. This elegant approach avoids modality-specific pipelines.\n2.  **Modular Architectures with Specialized Encoders:** A more common and pragmatic approach involves a modular design. This typically uses a pre-trained Vision Transformer (ViT) for visual data, an Audio Spectrogram Transformer (AST) or similar for audio, and a standard LLM for text \\[11\\]\\[16\\]\\[16\\]. The outputs of these specialized encoders are then mapped into a shared embedding space via \"projector\" modules. This allows the model to leverage powerful, highly-optimized pre-trained components \\[11\\]\\[172\\]. Fusion then occurs through joint attention or cross-attention layers within a multimodal transformer block \\[7\\]\\[13\\].\n\nThe trend is toward flexible, modular systems like Qwen2.5-Omni, which can process multiple modalities and even generate speech and text concurrently using a \"Thinker-Talker\" architecture \\[10\\]\\[374\\].\n\n**3.2 Taming the Token Explosion: The Key to Efficiency**\n\nThe primary source of inefficiency is the \"token explosion\" that occurs when converting dense video and audio into discrete tokens for the transformer. A standard video might be tokenized into patches for every frame, and audio into spectrogram \"images,\" resulting in sequences tens or hundreds of thousands of tokens long \\[1\\]\\[299\\]. Addressing this is the most critical area of research.\n\n**Token Reduction and Abstraction Strategies:**\n\n**Token Pruning and Merging:** The most direct approach is to reduce the number of tokens. Token pruning methods dynamically identify and discard unimportant or redundant tokens. For example, pruning 70% of input tokens in a video transformer can slash computation by 55.5% with only a 1.7% accuracy drop \\[93\\]. Semantic-aware strategies can prune based on temporal redundancy and semantic importance \\[87\\]. Token merging, seen in ToMe and VidToMe, fuses similar tokens together \\[84\\]. These methods present a clear trade-off: a 50% merge ratio can substantially decrease GFLOPs but may degrade top-5 accuracy by a small margin \\[85\\].\n\n**Token-Importance-Aware Attention (TIAA):** More sophisticated methods like TIAA use a \"token utilization rate\" to intelligently select the most salient tokens from each modality stream, aiming to improve both accuracy and efficiency \\[44\\].\n\n**Bottleneck Architectures:** The Multimodal Bottleneck Transformer (MBT) introduces a small, fixed set of special \"bottleneck tokens\" into the sequence. All cross-modal information fusion is forced to occur through these few tokens, drastically reducing the computational cost of cross-attention while improving performance \\[46\\]\\[47\\]. This is a powerful reimagining, as it forces the model to learn a compressed, abstract representation of each modality.\n\nThese token reduction techniques are the most direct path to making multimodal processing more \"text-like\"—transforming a dense, noisy signal into a sparse, semantically meaningful set of tokens before the expensive quadratic attention is applied.\n\n**3.3 Optimizing Attention for Long Multimodal Sequences**\n\nEven with token reduction, multimodal sequences are often long. Therefore, optimizing the attention mechanism itself is crucial.\n\n**Efficient Attention Mechanisms:** IO-aware algorithms like **FlashAttention** and its successor, FlashAttention-2, have become indispensable. They optimize the attention computation by minimizing slow memory reads/writes to the GPU HBM, parallelizing operations, and reducing non-matrix-multiply FLOPs. These are not just incremental improvements; they are enabling technologies that make training on very long sequences computationally viable \\[45\\]\\[54\\].\n\n**Sparse Attention:** Other techniques like Linformer and ReFormer reduce the O(n²) complexity by restricting attention operations to local windows or sparse patterns, a necessary compromise for extreme sequence lengths \\[51\\].\n\n**3.4 Synchronizing Time: The Role of Advanced Positional Embeddings**\n\nA unique challenge for video and audio is temporal alignment. A video might be sampled at 8 frames per second, while its corresponding audio is sampled at over 31,000 Hz, requiring the model to align events across these different timescales \\[101\\]\\[144\\].\n\nThe solution has emerged in the form of **Time-aligned Multimodal Rotary Positional Embeddings (TMRoPE)** or simply \"Aligned RoPE\" \\[7\\]\\[10\\]\\[104\\]. RoPE encodes position information by rotating the query and key embedding vectors, a method superior to simple additive embeddings for capturing relative temporal relationships \\[112\\]\\[114\\]. In the multimodal setting, aligned RoPE works by scaling the rotational frequencies of the audio and visual streams proportionally to their different sampling rates. This allows the joint attention mechanism to correctly calculate the affinity between a visual token at time t and an audio token at the same time t, even if their sequence indices are vastly different \\[104\\]\\[144\\].\n\nCrucially, the computational overhead of the RoPE mechanism itself is considered negligible, accounting for as little as 0.01% of total FLOPs in one analysis \\[150\\]. While no sources directly quantify the specific overhead of the _alignment_ logic, it is primarily a pre-computation of scaling factors. The immense benefit of achieving temporal synchrony far outweighs this marginal computational cost, making Aligned RoPE a cornerstone of modern multimodal video-audio transformers.\n\n**4.0 Conclusion: A Future of Abstractive Efficiency**\n\nCan transformer architectures be fundamentally reimagined to process multimodal inputs with the same efficiency they process text? As of 2025, the answer is a qualified **no**, but with a clear and promising path forward. The fundamental barrier is not the transformer architecture itself, but the nature of the data. The high dimensionality of raw video and audio cannot be processed with the same efficiency as sparse, symbolic text using identical methods.\n\nHowever, the \"reimagining\" is well underway and is proving remarkably successful. The future of efficient multimodal transformers does not lie in naively scaling computation to handle a flood of raw tokens. Instead, it lies in **intelligent abstraction**.\n\nThe most promising architectural innovations—from Perceiver's latent bottlenecks to MBT's fusion tokens and TIAA's salience-based selection—all share a common goal: to compress the high-dimensional sensory input into a sparse, information-rich, and \"text-like\" set of tokens _before_ the main computational workload. These abstracted representations can then be processed with an efficiency that begins to approach that of text.\n\nSupported by foundational optimizations like FlashAttention and sophisticated temporal alignment from Aligned RoPE, these new architectures are transforming multimodal AI from a computationally prohibitive dream into a practical reality. While the one-to-one efficiency of text may remain an idealized target, the ongoing revolution in architectural design is rapidly narrowing the gap, paving the way for models that can perceive, understand, and reason about our complex, multimodal world with ever-increasing efficiency.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Multimodal AI Trends 2025: Agentic & Embodied AI Future](https://futureagi.com/blogs/multimodal-ai-2025)\n\n[2\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[3\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[4\\. Sepp Hochreiter, J. Schmidhuber. “Long Short-Term Memory.” Neural Computation](https://doi.org/10.1162/neco.1997.9.8.1735)\n\n[5\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[6\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[7\\. MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis](https://arxiv.org/pdf/2412.15322)\n\n[8\\. Evaluating Multimodal AI Systems: A Comparative Analysis of Large Language Model-Based Models for Text, Image, and Video Generation](https://digitalcommons.georgiasouthern.edu/cgi/viewcontent.cgi?article=4167&context=etd)\n\n[9\\. Vision and Multi-modal Transformers](https://hal.science/hal-04413851v1/file/Vision_and_Multi-modal_Transformers.pdf)\n\n[10\\. Qwen2.5-Omni Technical Report](https://www.52nlp.cn/wp-content/uploads/2025/03/Qwen2.5_Omni_%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E8%8B%B1%E4%B8%AD%E5%AF%B9%E7%85%A7%E7%89%88.pdf)\n\n[11\\. Image Captioning Using Multimodal LLMs](https://lup.lub.lu.se/student-papers/record/9185108/file/9185109.pdf)\n\n[12\\. CSE 252D: Advanced Computer Vision](http://cseweb.ucsd.edu/~mkchandraker/classes/CSE252D/Spring2024/Lectures/lec02_visiontransformers.pdf)\n\n[13\\. Diffusion-based Multimodal Video Captioning](https://openaccess.thecvf.com/content/ACCV2024/papers/Kainulainen_Diffusion-based_Multimodal_Video_Captioning_ACCV_2024_paper.pdf)\n\n[14\\. 11th European Workshop on Visual Information Processing (EUVIP)](https://www.euvip2023.org/wp-content/uploads/2023/09/EUVIP2023_Program_Proceedings2023.pdf)\n\n[15\\. Exploring the Future of Multi-Modal AI: Integrating Text, Audio, Images, and Video](https://www.ciscolive.com/c/dam/r/ciscolive/emea/docs/2025/pdf/AIHUB-1970.pdf)\n\n[16\\. AV-PEA: Parameter-Efficient Adapter for Audio-Visual Multimodal Learning](https://www.scitepress.org/Papers/2024/124315/124315.pdf)\n\n[17\\. The Rise of Multimodal AI: Beyond Text in 2025](https://providentiatech.ai/blog/the-rise-of-multimodal-ai-beyond-text-in-2025/)\n\n[18\\. The Indian Journal of Technical Education](http://www.isteonline.in/Datafiles/cms//Spl%20Issue%20Jan%202024%20for%20web%20uploading.pdf)\n\n[19\\. Past vs. Present: Key Differences Between Conventional Machine Learning and Transformer Architectures](https://internationalpubls.com/index.php/anvi/article/download/2537/1671/5042)\n\n[21\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[22\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[23\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[24\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[25\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[26\\. TF-Mamba: Text-enhanced Fusion Mamba with Missing Modalities for Robust Multimodal Sentiment Analysis](https://arxiv.org/pdf/2505.14329)\n\n[27\\. ON THE EFFICIENCY OF TRANSFORMERS](https://asset.library.wisc.edu/1711.dl/L5G4PGWKGIGKX8Q/R/file-99b91.pdf)\n\n[28\\. Efficiency Evaluation of Mobile Vision Transformers](https://oa.upm.es/81020/2/paper.pdf)\n\n[29\\. Hanrui Wang, Zhanghao Wu et al. “HAT: Hardware-Aware Transformers for Efficient Natural Language Processing.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/2020.acl-main.686)\n\n[30\\. Continuous Patient State Attention Models](https://www.medrxiv.org/content/10.1101/2022.12.23.22283908v1.full.pdf)\n\n[31\\. Transformer FLOPs](https://www.adamcasson.com/transformer-flops.pdf)\n\n[32\\. Partial Large Kernel CNNs for Efficient Super-Resolution](https://arxiv.org/html/2404.11848v1)\n\n[33\\. HR-NAS: Searching Efficient High-Resolution Neural Architectures with Lightweight Transformers](https://openaccess.thecvf.com/content/CVPR2021/papers/Ding_HR-NAS_Searching_Efficient_High-Resolution_Neural_Architectures_With_Lightweight_Transformers_CVPR_2021_paper.pdf)\n\n[34\\. AlignMamba: Enhancing Multimodal Mamba with Local and ...](http://arxiv.org/html/2412.00833v1)\n\n[35\\. A. Ulhaq, Naveed Akhtar et al. “Vision Transformers for Action Recognition: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2209.05700)\n\n[36\\. Towards Robust and Efficient Multimodal Representation Learning and Fusion](https://dr.ntu.edu.sg/bitstream/10356/182226/2/Guo%20Xiaobao_Thesis_final_version_signed%20%281%29.pdf)\n\n[37\\. Encode Once and Decode in Parallel: Efficient Transformer Decoding](https://openreview.net/pdf?id=tBCC63Yl_x)\n\n[38\\. An attention-based multiscale transformer network for remote sensing image change detection](https://uwaterloo.ca/geospatial-intelligence/sites/default/files/uploads/documents/an-attention-based-multiscale-transformer-network-for-remote-sensing-image.pdf)\n\n[39\\. Rethinking LLM Advancement: Compute-Dependent and Independent Paths to Progress](https://www.arxiv.org/pdf/2505.04075)\n\n[41\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[42\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[43\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[44\\. Computation and Parameter Efficient Multi-Modal Fusion ...](https://arxiv.org/abs/2401.17604)\n\n[45\\. Lean Attention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers](https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/Lean_Attention___arxiv_version.pdf)\n\n[46\\. Attention Bottlenecks for Multimodal Fusion](https://proceedings.neurips.cc/paper/2021/file/76ba9f564ebbc35b1014ac498fafadd0-Paper.pdf)\n\n[47\\. Multimodal Transformer with a Low-Computational-Cost ...](https://arxiv.org/html/2402.15096v1)\n\n[48\\. TOKENFORMER: RETHINKING TRANSFORMER SCALING WITH TOKENIZED MODEL PARAMETERS](https://arxiv.org/pdf/2410.23168)\n\n[49\\. Hugo Touvron, M. Cord et al. “Training data-efficient image transformers & distillation through attention.” International Conference on Machine Learning](https://arxiv.org/abs/2012.12877)\n\n[50\\. Lei Liu, Li Liu et al. “Computation and Parameter Efficient Multi-Modal Fusion Transformer for Cued Speech Recognition.” IEEE/ACM Transactions on Audio, Speech, and Language Processing](https://doi.org/10.1109/TASLP.2024.3363446)\n\n[51\\. LLAVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models](https://openreview.net/pdf?id=gZnBI7WS1K)\n\n[52\\. 【CVPR】Multimodal Token Fusion for Vision Transformers](https://robotics.pkusz.edu.cn/weeklyreportENG/20230914.%20%E3%80%90CVPR%E3%80%91Multimodal%20Token%20Fusion%20for%20Vision%20Transformers/?back=1)\n\n[53\\. Optimizing Attention](https://openreview.net/forum?id=vnp2LtLlQg)\n\n[54\\. LeanAttention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers](https://arxiv.org/pdf/2405.10480)\n\n[55\\. Bandit-based Attention Mechanism in Vision Transformers](https://openaccess.thecvf.com/content/WACV2025/papers/Chowdhury_Bandit_Based_Attention_Mechanism_in_Vision_Transformers_WACV_2025_paper.pdf)\n\n[56\\. Multi-Token突破注意力机制瓶颈,Meta发明一种很新的Transformer](http://finance.sina.com.cn/roll/2025-04-04/doc-ineryuys7338057.shtml)\n\n[57\\. Improving Transformer with an Admixture of Attention Heads](https://proceedings.neurips.cc/paper_files/paper/2022/file/b2e4edd53059e24002a0c916d75cc9a3-Paper-Conference.pdf)\n\n[58\\. Deficient Executive Control in Transformer Attention](https://www.biorxiv.org/content/biorxiv/early/2025/01/23/2025.01.22.634394.full.pdf)\n\n[61\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[62\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[63\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[64\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[65\\. TF-Mamba: Text-enhanced Fusion Mamba with Missing Modalities for Robust Multimodal Sentiment Analysis](https://arxiv.org/pdf/2505.14329)\n\n[66\\. Mixed Sparsity Training: Achieving 4× FLOP Reduction for Transformer Pretraining](https://openreview.net/pdf?id=XosdLS7KVE)\n\n[67\\. Comparison of Major LLM Architectures (2017– 2025)](https://skillenai.com/competition-post/comparison-of-major-llm-architectures-2017-2025/)\n\n[68\\. Alex Wang, Amanpreet Singh et al. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” BlackboxNLP@EMNLP](https://doi.org/10.18653/v1/W18-5446)\n\n[69\\. A Benchmark for ML Inference Latency on Mobile Devices](https://qed.usc.edu/paolieri/papers/2024_edgesys_mobile_inference_benchmark.pdf)\n\n[70\\. Embodied - Paper Reading](http://paperreading.club/category?cate=Embodied)\n\n[71\\. Evaluating Transformer Architectures: Metrics & Benchmarks](https://futureagi.com/blogs/evaluating-transformer-architectures-key-metrics-and-performance-benchmarks)\n\n[72\\. Deep Learning Transformer and Newer Architectures](https://deeplearning.cs.cmu.edu/S25/document/slides/lec19.transformer.pdf)\n\n[73\\. BalanceBenchmark: A Survey for Multimodal Imbalance Learning](https://arxiv.org/pdf/2502.10816)\n\n[74\\. ON THE EFFICIENCY OF TRANSFORMERS](https://asset.library.wisc.edu/1711.dl/L5G4PGWKGIGKX8Q/R/file-99b91.pdf)\n\n[75\\. Enhancing Transformer Models for Dialogue Summarization](https://digital.lib.washington.edu/researchworks/bitstreams/f7e27e4e-8223-475e-bbe4-f57ef586260d/download)\n\n[76\\. EECS 570 Lecture 16 GPU Optimizations](http://www.eecs.umich.edu/courses/eecs570/lectures/16.pdf)\n\n[77\\. Learned Thresholds Token Merging and Pruning for Vision Transformers](https://openreview.net/pdf?id=WYKTCKpImz)\n\n[81\\. TOKEN PRUNING MEETS AUDIO: INVESTIGATING UNIQUE BEHAVIORS IN VISION TRANSFORMER-BASED AUDIO CLASSIFICATION](https://openreview.net/pdf/d397f9d53fadc4a82ac37a617a6963700fd0591d.pdf)\n\n[82\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[83\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[84\\. Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models](https://www.arxiv.org/pdf/2503.16980v2)\n\n[85\\. Efficient Video Transformers via Spatial-temporal Token Merging for Action Recognition](http://www.jdl.link/doc/2011/20241220_3633781.pdf)\n\n[86\\. Ze Liu, Yutong Lin et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00986)\n\n[87\\. Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation](https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_Prune_Spatio-temporal_Tokens_by_Semantic-aware_Temporal_Accumulation_ICCV_2023_paper.pdf)\n\n[88\\. Advancing Transformer Efficiency with Token Pruning](https://www.preprints.org/frontend/manuscript/d18db08b59a7525c85d3fb802c4c1947/download_pub)\n\n[89\\. VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text](https://openreview.net/pdf?id=RzYrn625bu8)\n\n[90\\. Kunchang Li](https://www.catalyzex.com/author/Kunchang%20Li)\n\n[91\\. vid-TLDR: Training Free Token merging for Light-weight Video Transformer](https://openaccess.thecvf.com/content/CVPR2024/papers/Choi_vid-TLDR_Training_Free_Token_Merging_for_Light-weight_Video_Transformer_CVPR_2024_paper.pdf)\n\n[92\\. Adaptive Token Sampling For Efficient Vision Transformers](https://web.cs.ucdavis.edu/~hpirsiav/papers/ats_eccv22.pdf)\n\n[93\\. Hao Wang, Wenjia Zhang et al. “TSNet: Token Sparsification for Efficient Video Transformer.” Applied Sciences](https://doi.org/10.3390/app131910633)\n\n[94\\. Yifei Chen, Dapeng Chen et al. “Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition.” ArXiv](https://doi.org/10.48550/arXiv.2311.15619)\n\n[95\\. Hugo Touvron, M. Cord et al. “Training data-efficient image transformers & distillation through attention.” International Conference on Machine Learning](https://arxiv.org/abs/2012.12877)\n\n[96\\. MA-AVT: Modality Alignment for Parameter-Efficient Audio-Visual Transformers](https://openaccess.thecvf.com/content/CVPR2024W/ECV24/supplemental/Mahmud_MA-AVT_Modality_Alignment_CVPRW_2024_supplemental.pdf)\n\n[97\\. Efficient Video Transformers with Spatial-Temporal Token Selection](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136950068.pdf)\n\n[101\\. MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis](https://arxiv.org/pdf/2412.15322)\n\n[102\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[103\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[104\\. ...Multimodal Joint Training for High-Quality Video-to...](http://arxiv.org/html/2412.15322v1)\n\n[105\\. Ho Kei Cheng, Masato Ishii et al. “Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis.”](https://arxiv.org/abs/2412.15322)\n\n[106\\. Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis](https://arxiv.org/html/2412.15322v1)\n\n[107\\. Qwen2.5-Omni Technical Report](https://www.52nlp.cn/wp-content/uploads/2025/03/Qwen2.5_Omni_%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E8%8B%B1%E4%B8%AD%E5%AF%B9%E7%85%A7%E7%89%88.pdf)\n\n[108\\. Jiasen Lu, Dhruv Batra et al. “ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks.” Neural Information Processing Systems](https://arxiv.org/abs/1908.02265)\n\n[109\\. AV-Link: 跨模态音视频生成的时间对齐扩散特征](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_12_20/2412.15191.pdf)\n\n[110\\. Hao Hao Tan, Mohit Bansal. “LXMERT: Learning Cross-Modality Encoder Representations from Transformers.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D19-1514)\n\n[111\\. Weijie Su, Xizhou Zhu et al. “VL-BERT: Pre-training of Generic Visual-Linguistic Representations.” ArXiv](https://arxiv.org/abs/1908.08530)\n\n[112\\. Rotary Positional Embeddings (RoPE)](https://nn.labml.ai/transformers/rope/index.html)\n\n[113\\. Tengda Han, Weidi Xie et al. “Temporal Alignment Networks for Long-term Video.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.00292)\n\n[114\\. TIMER-XL: LONG-CONTEXT TRANSFORMERS FOR UNIFIED TIME SERIES FORECASTING](https://arxiv.org/pdf/2410.04803v1)\n\n[115\\. CS 224N Spring 2024 Assignment 4 Self-Attention, Transformers, and Pretraining](https://web.stanford.edu/class/cs224n/assignments/a4_spr24_student_handout.pdf)\n\n[116\\. Longitudinal Multimodal Transformer Integrating Imaging ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC11110542/)\n\n[117\\. Shashwat Bhatnagar, Aman Singh et al. “Combining Data Mining and Visualization: UMAP on ROPE.” 2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS)](https://doi.org/10.1109/ICICNIS64247.2024.10823114)\n\n[118\\. Multi-Modal Beat Alignment Transformers for Dance Quality ...](https://www.jmis.org/archive/view_article?pid=jmis-11-2-149)\n\n[119\\. Multistage Time-Optimal Control for Synchronization Process in Electric-Driven Mechanical Transmission With Angle Alignment Considering Torque Response Process](https://pangea.stanford.edu/ERE/pdf/OnoriPDF/Journals/61.pdf)\n\n[121\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[122\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[123\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[124\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[125\\. ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHING](https://openreview.net/pdf?id=yYZbZGo4ei)\n\n[126\\. Hugo Touvron, M. Cord et al. “Training data-efficient image transformers & distillation through attention.” International Conference on Machine Learning](https://arxiv.org/abs/2012.12877)\n\n[127\\. TF-Mamba: Text-enhanced Fusion Mamba with Missing Modalities for Robust Multimodal Sentiment Analysis](https://arxiv.org/pdf/2505.14329)\n\n[128\\. MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis](https://arxiv.org/pdf/2412.15322)\n\n[129\\. LLAVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://openreview.net/pdf/efd2169a71f1800808f58038f0bf1023ce051103.pdf)\n\n[130\\. HiPEAC Vision 2025: High Performance, Edge and Cloud Computing](https://vision.hipeac.net/pdf/hipeac-vision-2025.pdf)\n\n[131\\. A Benchmark for ML Inference Latency on Mobile Devices](https://qed.usc.edu/paolieri/papers/2024_edgesys_mobile_inference_benchmark.pdf)\n\n[132\\. Evaluating Transformer Architectures: Metrics & Benchmarks](https://futureagi.com/blogs/evaluating-transformer-architectures-key-metrics-and-performance-benchmarks)\n\n[133\\. Comparison of Major LLM Architectures (2017– 2025)](https://skillenai.com/competition-post/comparison-of-major-llm-architectures-2017-2025/)\n\n[134\\. Enhancing Transformer Models for Dialogue Summarization](https://digital.lib.washington.edu/researchworks/bitstreams/f7e27e4e-8223-475e-bbe4-f57ef586260d/download)\n\n[135\\. Large Multimodal Models (LMMs) vs LLMs in 2025](https://research.aimultiple.com/large-multimodal-models/#:~:text=LMMs:%20They%20are%20designed%20to,data%20types%20like%20sensory%20data.)\n\n[136\\. HoliTom 🧸 : Holistic Token Merging for Fast Video Large Language Models](https://arxiv.org/pdf/2505.21334)\n\n[137\\. The Complete LLM Model Comparison Guide (2025)](https://www.helicone.ai/blog/the-complete-llm-model-comparison-guide)\n\n[138\\. LONGVIDEOBENCH: A Benchmark for Long-context Interleaved Video-Language Understanding](https://proceedings.neurips.cc/paper_files/paper/2024/file/329ad516cf7a6ac306f29882e9c77558-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[139\\. Understanding the Performance of Transformer Inference](https://dspace.mit.edu/bitstream/handle/1721.1/151543/ouyang-aouyang-meng-eecs-2023-thesis.pdf?sequence=1&isAllowed=y)\n\n[140\\. AVT: AUDIO-VIDEO TRANSFORMER FOR MULTIMODAL ACTION RECOGNITION](https://openreview.net/pdf?id=yFuHxmSwGus)\n\n[141\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[142\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[143\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[144\\. ...Multimodal Joint Training for High-Quality Video-to...](http://arxiv.org/html/2412.15322v1)\n\n[145\\. Qwen2.5-Omni Technical Report](https://www.52nlp.cn/wp-content/uploads/2025/03/Qwen2.5_Omni_%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E8%8B%B1%E4%B8%AD%E5%AF%B9%E7%85%A7%E7%89%88.pdf)\n\n[146\\. Peter Shaw, Jakob Uszkoreit et al. “Self-Attention with Relative Position Representations.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N18-2074)\n\n[147\\. Building DeepSeek AI Models: Architecture, Implementation, and Optimization](https://walzone.com/books/wp-content/uploads/2025/03/document.pdf)\n\n[148\\. Abstracts: July 29, 2024 - Microsoft Research](https://www.microsoft.com/en-us/research/podcast/abstracts-july-29-2024/)\n\n[149\\. DeepGene: An Efficient Foundation Model for Genomics based on Pan-genome Graph Transformer](https://www.biorxiv.org/content/10.1101/2024.04.24.590879v2.full.pdf)\n\n[150\\. Rotary Position Embedding for Vision Transformer](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01584.pdf)\n\n[151\\. How Multimodal LLMs Work - The Vision Story](https://www.analyticsvidhya.com/blog/2025/06/multimodal-llm/)\n\n[152\\. A Comparative Study of RoPE-based Positional Encodings from A Scaling Perspective](https://openreview.net/pdf?id=Y6yz85kqL9)\n\n[153\\. Llama 2, a new intelligent Open Source Language Model](https://www.e2enetworks.com/blog/llama-2-the-new-open-source-language-model)\n\n[154\\. Jianlin Su, Yu Lu et al. “RoFormer: Enhanced Transformer with Rotary Position Embedding.” ArXiv](https://doi.org/10.1016/j.neucom.2023.127063)\n\n[155\\. Rotatory Position Embedding (RoPE)](https://karthick.ai/blog/2024/Rotatory-Position-Embedding-%28RoPE%29/)\n\n[156\\. A Unified, Scalable Framework for Neural Population Decoding](https://proceedings.neurips.cc/paper_files/paper/2023/file/8ca113d122584f12a6727341aaf58887-Paper-Conference.pdf)\n\n[157\\. Falcon LLM: Comprehensive Guide](https://www.geeksforgeeks.org/falcon-llm-comprehensive-guide/)\n\n[158\\. Revisiting Positional Information in Transformers in the era of Fused Attention](https://openreview.net/pdf?id=1Iq1qIsc2s)\n\n[159\\. Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis](https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_Exploring_the_Deep_Fusion_of_Large_Language_Models_and_Diffusion_CVPR_2025_paper.pdf)\n\n[160\\. Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model](https://seaweed.video/seaweed.pdf)\n\n[161\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[162\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[163\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[164\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[165\\. TxT: Crossmodal End-to-End Learning with Transformers](https://arxiv.org/abs/2109.04422)\n\n[166\\. The analysis of transformer end-to-end model in Real-time ...](https://www.nature.com/articles/s41598-025-02904-0)\n\n[167\\. An Empirical Study of Training End-to-End Vision-and-Language Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Dou_An_Empirical_Study_of_Training_End-to-End_Vision-and-Language_Transformers_CVPR_2022_paper.pdf)\n\n[168\\. Investigating the Efficacy of Multimodal Large Language Models in Cross-Domain Knowledge Transfer](https://premierscience.com/wp-content/uploads/2025/02/pjai-24-436.pdf)\n\n[169\\. MMBench: Benchmarking End-to-End Multi-modal DNNs and Understanding Their Hardware-Software Implications](https://www.cs.sjtu.edu.cn/~lichao/publications/MMBench_Benchmarking_IISWC-2023-Xu.pdf)\n\n[170\\. Cross-Modal Multitask Transformer for End-to-End Multimodal Aspect ...](https://www.x-mol.com/paper/1555022014473990144?adv)\n\n[171\\. M6: Multi-Modality-to-Multi-Modality Multitask Mega-transformer for Unified Pretraining](https://keg.cs.tsinghua.edu.cn/jietang/publications/KDD21-Lin-et-al-M6.pdf)\n\n[172\\. What are multimodal transformers and how do they work? - Milvus](https://milvus.io/ai-quick-reference/what-are-multimodal-transformers-and-how-do-they-work#:~:text=Multimodal%20transformers%20are%20machine%20learning,to%20analyze%20relationships%20within%20data.)\n\n[173\\. Modal Contrastive Learning Based End-to-End Text Image Machine Translation](https://nlpr.ia.ac.cn/cip/English/ZongPublications/2024/2024-MaCong-TASLP.pdf)\n\n[174\\. Data, Architecture, or Losses: What Contributes Most to Multimodal Transformer Success?](https://deepmind.google/discover/blog/data-architecture-or-losses-what-contributes-most-to-multimodal-transformer-success/)\n\n[175\\. Benchmarking Multimodal AutoML for Tabular Data with Text Fields](https://openreview.net/pdf?id=Q0zOIaec8HF)\n\n[176\\. Renyu Zhu, Chengcheng Han et al. “Exchanging-based Multimodal Fusion with Transformer.” ArXiv](https://doi.org/10.48550/arXiv.2309.02190)\n\n[177\\. What Contributes Most to Multimodal Transformer Success?](https://todaysainews.com/index.php/2023/01/29/what-contributes-most-to-multimodal-transformer-success/)\n\n[178\\. Deblina Bhattacharjee, Tong Zhang et al. “MuIT: An End-to-End Multitask Learning Transformer.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01172)\n\n[179\\. Are Multimodal Transformers Robust to Missing Modality?](https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Are_Multimodal_Transformers_Robust_to_Missing_Modality_CVPR_2022_paper.pdf)\n\n[180\\. Min Peng, Chongyang Wang et al. “Efficient End-to-End Video Question Answering with Pyramidal Multimodal Transformer.” AAAI Conference on Artificial Intelligence](https://doi.org/10.48550/arXiv.2302.02136)\n\n[181\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[182\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[183\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[184\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[185\\. Hugo Touvron, M. Cord et al. “Training data-efficient image transformers & distillation through attention.” International Conference on Machine Learning](https://arxiv.org/abs/2012.12877)\n\n[186\\. MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis](https://arxiv.org/pdf/2412.15322)\n\n[187\\. Top AI Hardware Trends Shaping 2025 - Trio Dev](https://trio.dev/ai-hardware-trends/#:~:text=In%20order%20to%20get%20the,an%20effort%20to%20reduce%20costs.)\n\n[188\\. LLAVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://openreview.net/pdf/efd2169a71f1800808f58038f0bf1023ce051103.pdf)\n\n[189\\. TF-Mamba: Text-enhanced Fusion Mamba with Missing Modalities for Robust Multimodal Sentiment Analysis](https://arxiv.org/pdf/2505.14329)\n\n[190\\. HoliTom 🧸 : Holistic Token Merging for Fast Video Large Language Models](https://arxiv.org/pdf/2505.21334)\n\n[191\\. Learned Thresholds Token Merging and Pruning for Vision Transformers](https://openreview.net/pdf?id=WYKTCKpImz)\n\n[192\\. Mixed Sparsity Training: Achieving 4× FLOP Reduction for Transformer Pretraining](https://openreview.net/pdf?id=XosdLS7KVE)\n\n[193\\. Enhancing Transformer Models for Dialogue Summarization](https://digital.lib.washington.edu/researchworks/bitstreams/f7e27e4e-8223-475e-bbe4-f57ef586260d/download)\n\n[194\\. Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training](https://arxiv.org/pdf/2411.13055)\n\n[195\\. Evaluating Transformer Architectures: Metrics & Benchmarks](https://futureagi.com/blogs/evaluating-transformer-architectures-key-metrics-and-performance-benchmarks)\n\n[196\\. The Rise of Multimodal AI: Beyond Text in 2025](https://providentiatech.ai/blog/the-rise-of-multimodal-ai-beyond-text-in-2025/)\n\n[197\\. Qwen2.5-Omni Technical Report](https://www.52nlp.cn/wp-content/uploads/2025/03/Qwen2.5_Omni_%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E8%8B%B1%E4%B8%AD%E5%AF%B9%E7%85%A7%E7%89%88.pdf)\n\n[198\\. Data-Efficient Multimodal Fusion on a Single GPU](https://www.cs.toronto.edu/~mvolkovs/CVPR2024_FuseMix.pdf)\n\n[199\\. Encode Once and Decode in Parallel: Efficient Transformer Decoding](https://openreview.net/pdf?id=tBCC63Yl_x)\n\n[201\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[202\\. ...Multimodal Joint Training for High-Quality Video-to...](http://arxiv.org/html/2412.15322v1)\n\n[203\\. AlignMamba: Enhancing Multimodal Mamba with Local and ...](http://arxiv.org/html/2412.00833v1)\n\n[204\\. Rotary Position Embedding for Vision Transformer](http://arxiv.org/html/2403.13298v2)\n\n[205\\. Modular - NEW PRODUCTION](https://www.modular.com/ai-resources/rotary-position-embedding-rope)\n\n[206\\. Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis](https://arxiv.org/html/2412.15322v1)\n\n[207\\. Falcon LLM: Comprehensive Guide](https://www.geeksforgeeks.org/falcon-llm-comprehensive-guide/)\n\n[208\\. Computational Object-Wrapping Rope Nets](https://cfcs.pku.edu.cn/baoquan/docs/2021-09/20210907113205858304.pdf)\n\n[209\\. UNDERSTANDING AND OPTIMIZING COMMUNICATION OVERHEAD IN DISTRIBUTED TRAINING](https://jscholarship.library.jhu.edu/bitstream/handle/1774.2/68210/ZHANG-DISSERTATION-2023.pdf?sequence=1)\n\n[210\\. “闭门造车”之多模态思路浅谈（三）：位置编码](https://spaces.ac.cn/archives/10352/comment-page-1)\n\n[211\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[212\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[213\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[214\\. Investigating the Efficacy of Multimodal Large Language Models in Cross-Domain Knowledge Transfer](https://premierscience.com/wp-content/uploads/2025/02/pjai-24-436.pdf)\n\n[215\\. Large Multimodal Models (LMMs) vs LLMs in 2025 - Research AIMultiple](https://research.aimultiple.com/large-multimodal-models/#:~:text=Multimodal%20AI%20agents%20are%20systems,both%20digital%20and%20physical%20environments.)\n\n[216\\. TxT: Crossmodal End-to-End Learning with Transformers](https://arxiv.org/abs/2109.04422)\n\n[217\\. SHOW-o: ONE SINGLE TRANSFORMER TO UNIFY MULTIMODAL UNDERSTANDING AND GENERATION](https://openreview.net/pdf?id=o6Ynz6OIQ6)\n\n[218\\. Cross-modal Information Flow in Multimodal Large Language...](https://tool.lu/index.php/vi_VN/deck/Rx/detail)\n\n[219\\. Jiasen Lu, Dhruv Batra et al. “ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks.” Neural Information Processing Systems](https://arxiv.org/abs/1908.02265)\n\n[220\\. Deficient Executive Control in Transformer Attention](https://www.biorxiv.org/content/biorxiv/early/2025/01/23/2025.01.22.634394.full.pdf)\n\n[221\\. Multimodal AI Trends 2025: Agentic & Embodied AI Future](https://futureagi.com/blogs/multimodal-ai-2025)\n\n[222\\. nlp - Natural language processing](https://github.com/topics/nlp?o=desc&s=stars)\n\n[223\\. Hao Hao Tan, Mohit Bansal. “LXMERT: Learning Cross-Modality Encoder Representations from Transformers.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D19-1514)\n\n[224\\. Latest Transformer Architecture Trends and Innovations 2024](https://moldstud.com/articles/p-latest-transformer-architecture-trends-and-innovations-2024)\n\n[225\\. Large Multimodal Models (LMMs) vs LLMs in 2025](https://research.aimultiple.com/large-multimodal-models/#:~:text=LMMs:%20They%20are%20designed%20to,data%20types%20like%20sensory%20data.)\n\n[226\\. M^3SAT: A SPARSELY ACTIVATED TRANSFORMER FOR EFFICIENT MULTI-TASK LEARNING FROM MULTIPLE MODALITIES](https://openreview.net/pdf?id=_QkHfB07QMN)\n\n[227\\. Findings of WMT2024 English-to-Low Resource Multimodal Translation Task](https://www2.statmt.org/wmt24/pdf/2024.wmt-1.56.pdf)\n\n[228\\. DCU ADAPT at WMT24: English to Low-resource Multi-Modal Translation Task](https://www2.statmt.org/wmt24/pdf/2024.wmt-1.75.pdf)\n\n[229\\. From Multimodal LLM to Human-level AI: Modality, Instruction, Reasoning, Efficiency and beyond](http://www.lrec-conf.org/proceedings/lrec-coling-2024/tutorials/LREC-2024-Tutorials.pdf)\n\n[230\\. Attention is All You Need: Still True in 2025? - Notes](https://notes.suhaib.in/docs/tech/llms/attention-is-all-you-need-still-true-in-2025/)\n\n[231\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[232\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[233\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[234\\. K. Papineni, Salim Roukos et al. “Bleu: a Method for Automatic Evaluation of Machine Translation.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.3115/1073083.1073135)\n\n[235\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[236\\. Enhancing Transformer Models for Dialogue Summarization](https://digital.lib.washington.edu/researchworks/bitstreams/f7e27e4e-8223-475e-bbe4-f57ef586260d/download)\n\n[237\\. Mixed Sparsity Training: Achieving 4× FLOP Reduction for Transformer Pretraining](https://openreview.net/pdf?id=XosdLS7KVE)\n\n[238\\. Measuring the Contributions of Vision and Text Modalities in Multimodal Transformers](https://archiv.ub.uni-heidelberg.de/volltextserver/35753/1/30-Nov-2024_final_parcalabescu_letitia_dissertation-pdf-a.pdf)\n\n[239\\. Towards Multimodal Empathetic Response Generation: A Rich Text-Speech-Vision Avatar-based Benchmark](https://openreview.net/pdf?id=36Qk76s74U)\n\n[240\\. EFFICIENT VIDEO DIFFUSION MODELS VIA CONTENT-FRAME MOTION-LATENT DECOMPOSITION](https://openreview.net/pdf?id=dQVtTdsvZH)\n\n[241\\. META to Unveil State-of-the-art Multi-modal LLM Chameleon](https://cioinfluence.com/natural-language/meta-to-unveil-state-of-the-art-multi-modal-llm-chameleon/)\n\n[242\\. MULTI-GRANULARITY CORRESPONDENCE LEARNING FROM LONG-TERM NOISY VIDEOS](https://openreview.net/pdf?id=9Cu8MRmhq2)\n\n[243\\. On the Alignment, Robustness, and Generalizability of Multimodal Learning](http://reports-archive.adm.cs.cmu.edu/anon/home/ftp/2024/CMU-CS-24-101.pdf)\n\n[244\\. Huayang Li, Siheng Li et al. “TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild.” ArXiv](https://doi.org/10.48550/arXiv.2309.08637)\n\n[245\\. LLAVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://openreview.net/pdf/88e4723973f4cd22956468b3533ee5253ab7add9.pdf)\n\n[246\\. MONO-INTERNVL: PUSHING THE BOUNDARIES OF MONOLITHIC MULTIMODAL LARGE LANGUAGE MODELS WITH ENDOGENOUS VISUAL PRE-TRAINING](https://arxiv.org/pdf/2410.08202v1)\n\n[247\\. From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping](https://www.ijcai.org/proceedings/2023/0481.pdf)\n\n[248\\. Valentin Gabeur, Chen Sun et al. “Multi-modal Transformer for Video Retrieval.” ArXiv](https://doi.org/10.1007/978-3-030-58548-8_13)\n\n[249\\. NVIDIA A100 GPU Overview](https://cdn-prod.scdn6.secure.raxcdn.com/static/media/DAM_521f2ecd-92a5-41ac-a46c-b3db24f03ca3.pdf)\n\n[251\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[252\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[253\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[254\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[255\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[256\\. Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis](https://arxiv.org/html/2412.15322v1)\n\n[257\\. AlignMamba: Enhancing Multimodal Mamba with Local and ...](http://arxiv.org/html/2412.00833v1)\n\n[258\\. LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions](https://raw.githubusercontent.com/mlresearch/v235/main/assets/agostinelli-iii24a/agostinelli-iii24a.pdf)\n\n[259\\. A SURVEY OF RESOURCE-EFFICIENT LLM AND MULTIMODAL FOUNDATION MODELS](https://arxiv.org/pdf/2401.08092)\n\n[260\\. Rethinking LLM Advancement: Compute-Dependent and Independent Paths to Progress](https://www.arxiv.org/pdf/2505.04075)\n\n[261\\. Scaling Laws of RoPE-based Extrapolation](https://openreview.net/pdf?id=JO7k0SJ5V6)\n\n[262\\. Ting Chen, Lala Li. “FIT: Far-reaching Interleaved Transformers.” ArXiv](https://doi.org/10.48550/arXiv.2305.12689)\n\n[263\\. Efficiency Evaluation of Mobile Vision Transformers](https://oa.upm.es/81020/2/paper.pdf)\n\n[264\\. Lightweight Infrared Small Target Detection Method Based ...](https://www.mdpi.com/2072-4292/17/12/2016)\n\n[265\\. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://proceedings.neurips.cc/paper/2020/file/2cd2915e69546904e4e5d4a2ac9e1652-Paper.pdf)\n\n[266\\. CS 224N Spring 2024 Assignment 4 Self-Attention, Transformers, and Pretraining](https://web.stanford.edu/class/cs224n/assignments/a4_spr24_student_handout.pdf)\n\n[267\\. Mixed Sparsity Training: Achieving 4× FLOP Reduction for Transformer Pretraining](https://openreview.net/pdf?id=XosdLS7KVE)\n\n[268\\. RoFormer: Enhanced Transformer with Rotary Position Embedding - CatalyzeX](https://www.catalyzex.com/paper/arxiv:2104.09864)\n\n[269\\. RoFormer: Enhanced transformer with Rotary Position Embedding](https://www.sciencedirect.com/science/article/abs/pii/S0925231223011864)\n\n[271\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[272\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[273\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[274\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[275\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[276\\. WHAT TO ALIGN IN MULTIMODAL CONTRASTIVE LEARNING?](https://openreview.net/pdf/2ed91a549d961b24a80ff1aad405fe8c17326fbd.pdf)\n\n[277\\. Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models](https://arxiv.org/pdf/2502.05610)\n\n[278\\. Evaluating Multimodal AI Systems: A Comparative Analysis of Large Language Model-Based Models for Text, Image, and Video Generation](https://digitalcommons.georgiasouthern.edu/cgi/viewcontent.cgi?article=4167&context=etd)\n\n[279\\. EVOLUTIONARY PROMPT OPTIMIZATION DISCOVERS EMERGENT MULTIMODAL REASONING STRATEGIES IN VISION-LANGUAGE MODELS](https://openreview.net/pdf?id=u8BO0NFF21)\n\n[280\\. Approximating vision transformers for edge: variational inference and mixed-precision for multi-modal data](https://research.tudelft.nl/files/240702219/s00607-025-01427-w.pdf)\n\n[281\\. Energy-Based Transformers are Scalable Learners and ...](https://arxiv.org/abs/2507.02092)\n\n[282\\. M&M: Multimodal-Multitask Model Integrating Audiovisua...](http://arxiv.org/html/2403.09451v1)\n\n[283\\. ECS Strategic Research and Innovation Agenda 2024](https://ecssria.eu/ECS-SRIA-2024.pdf)\n\n[284\\. ENERGY EFFICIENCY MEASUREMENT IN OPTIMIZATION AND INFERENCE OF ML MODELS](https://upcommons.upc.edu/bitstream/handle/2117/391670/177940.pdf?sequence=2)\n\n[285\\. Meta-Transformer: A Unified Framework for Multimodal Learning](http://export.arxiv.org/pdf/2307.10802)\n\n[286\\. OverThink: Slowdown Attacks on Reasoning LLMs](http://arxiv.org/html/2502.02542v1)\n\n[287\\. Findings of WMT2024 English-to-Low Resource Multimodal Translation Task](https://www2.statmt.org/wmt24/pdf/2024.wmt-1.56.pdf)\n\n[288\\. Power Usage and Energy Efficiency](https://llm-tracker.info/_TOORG/Power-Usage-and-Energy-Efficiency)\n\n[289\\. From Multimodal LLM to Human-level AI: Modality, Instruction, Reasoning, Efficiency and beyond](http://www.lrec-conf.org/proceedings/lrec-coling-2024/tutorials/LREC-2024-Tutorials.pdf)\n\n[290\\. Large Multimodal Models (LMMs) vs LLMs in 2025](https://research.aimultiple.com/large-multimodal-models/#:~:text=LMMs:%20They%20are%20designed%20to,data%20types%20like%20sensory%20data.)\n\n[291\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[292\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[293\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[294\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[295\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[296\\. VCC: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens](https://papers.nips.cc/paper_files/paper/2023/file/4054556fcaa934b0bf76da52cf4f92cb-Paper-Conference.pdf)\n\n[297\\. Mixed Sparsity Training: Achieving 4× FLOP Reduction for Transformer Pretraining](https://openreview.net/pdf?id=XosdLS7KVE)\n\n[298\\. LLAVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://openreview.net/pdf/efd2169a71f1800808f58038f0bf1023ce051103.pdf)\n\n[299\\. MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis](https://arxiv.org/pdf/2412.15322)\n\n[300\\. A guide to LLM inference and performance](https://www.baseten.co/blog/llm-transformer-inference-guide/)\n\n[301\\. Transformer FLOPs](https://www.adamcasson.com/transformer-flops.pdf)\n\n[302\\. LORA-GEN: SPECIALIZING LANGUAGE MODEL VIA ONLINE LORA GENERATION](https://openreview.net/pdf/9036606b7b30b4554bc9e09eb13fca8d97901a8b.pdf)\n\n[303\\. Measuring the Contributions of Vision and Text Modalities in Multimodal Transformers](https://archiv.ub.uni-heidelberg.de/volltextserver/35753/1/30-Nov-2024_final_parcalabescu_letitia_dissertation-pdf-a.pdf)\n\n[304\\. Enhancing Transformer Models for Dialogue Summarization](https://digital.lib.washington.edu/researchworks/bitstreams/f7e27e4e-8223-475e-bbe4-f57ef586260d/download)\n\n[305\\. 2025 Outlook - AI and Innovative Technology Active ETF (3006)](http://investments.miraeasset.com.hk/docs/ETF/25-01-Global-AI-and-Innovative-Tech-Outlook_GX.pdf)\n\n[306\\. VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation](https://openreview.net/pdf?id=NKPXHzYusG)\n\n[307\\. TOKEN-SUPERVISED VALUE MODELS FOR ENHANCING MATHEMATICAL PROBLEM-SOLVING CAPABILITIES OF LARGE LANGUAGE MODELS](https://openreview.net/notes/edits/attachment?id=3kqj8YmeNZ&name=pdf)\n\n[308\\. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://dai.sjtu.edu.cn/my_file/pdf/56ebcd39-ce4f-4800-9f74-7aa108345782.pdf)\n\n[309\\. MONO-INTERNVL: PUSHING THE BOUNDARIES OF MONOLITHIC MULTIMODAL LARGE LANGUAGE MODELS WITH ENDOGENOUS VISUAL PRE-TRAINING](https://arxiv.org/pdf/2410.08202v1)\n\n[311\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[312\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[313\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[314\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[315\\. MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis](https://arxiv.org/pdf/2412.15322)\n\n[316\\. Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis](https://arxiv.org/html/2412.15322v1)\n\n[317\\. AlignMamba: Enhancing Multimodal Mamba with Local and ...](http://arxiv.org/html/2412.00833v1)\n\n[318\\. Enhanced Transformer with Rotary Position Embedding - ar5iv](https://ar5iv.labs.arxiv.org/html/2104.09864)\n\n[319\\. Modular - NEW PRODUCTION](https://www.modular.com/ai-resources/rotary-position-embedding-rope)\n\n[320\\. 十分钟读懂旋转编码（RoPE）](https://zhuanlan.zhihu.com/p/647109286)\n\n[321\\. Pretraining vs. finetuning + Modern Transformers (RoPE, GQA, Longformer) + CNNs](http://www.cs.cmu.edu/~mgormley/courses/10423-f24/slides/lecture4-vision-ink.pdf)\n\n[322\\. UNDERSTANDING AND OPTIMIZING COMMUNICATION OVERHEAD IN DISTRIBUTED TRAINING](https://jscholarship.library.jhu.edu/bitstream/handle/1774.2/68210/ZHANG-DISSERTATION-2023.pdf?sequence=1)\n\n[323\\. RoPE外推的缩放法则](https://zhuanlan.zhihu.com/p/660073229)\n\n[324\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[325\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[326\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[327\\. Matthew E. Peters, Mark Neumann et al. “Deep Contextualized Word Representations.” ArXiv](https://doi.org/10.18653/v1/N18-1202)\n\n[328\\. Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models](https://arxiv.org/pdf/2502.05610)\n\n[329\\. Pranav Rajpurkar, Jian Zhang et al. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D16-1264)\n\n[330\\. Power Usage and Energy Efficiency](https://llm-tracker.info/_TOORG/Power-Usage-and-Energy-Efficiency)\n\n[331\\. Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning](https://m.riunet.upv.es/bitstream/handle/10251/204447/DesislavovMartinez-PlumedHernandez-Orallo%20-%20Trends%20in%20AI%20inference%20energy%20consumption%20Beyond%20the%20....pdf?sequence=1&isAllowed=y)\n\n[332\\. Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI](https://arxiv.org/pdf/2409.14160)\n\n[333\\. Attention is All You Need: Still True in 2025? - Notes](https://notes.suhaib.in/docs/tech/llms/attention-is-all-you-need-still-true-in-2025/)\n\n[334\\. OVERTHINK: Slowdown Attacks on Reasoning LLMs](https://arxiv.org/pdf/2502.02542)\n\n[335\\. Token-Budget-Aware LLM Reasoning](https://arxiv.org/pdf/2412.18547)\n\n[336\\. Gpt-4o tokens per second comparable to gpt-3.5-turbo. Data and analsys](https://community.openai.com/t/gpt-4o-tokens-per-second-comparable-to-gpt-3-5-turbo-data-and-analsys/768559)\n\n[337\\. Energy-Based Transformers are Scalable Learners and ...](https://arxiv.org/abs/2507.02092)\n\n[338\\. Energy-efficient Online Scheduling of Transformer Inference Services on GPU Servers](https://repository.hkust.edu.hk/ir/bitstream/1783.1-118320/1/043422_1.pdf)\n\n[339\\. Investigating Energy Efficiency and Performance Trade-...](http://arxiv.org/html/2501.08219v1)\n\n[340\\. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://dai.sjtu.edu.cn/my_file/pdf/56ebcd39-ce4f-4800-9f74-7aa108345782.pdf)\n\n[344\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[345\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[346\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[347\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[348\\. Token Turing Machines are Efficient Vision Models](https://www.arxiv.org/pdf/2409.07613)\n\n[349\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[350\\. VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation](https://openreview.net/pdf?id=NKPXHzYusG)\n\n[351\\. VCC: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens](https://papers.nips.cc/paper_files/paper/2023/file/4054556fcaa934b0bf76da52cf4f92cb-Paper-Conference.pdf)\n\n[352\\. LLAVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://openreview.net/pdf/efd2169a71f1800808f58038f0bf1023ce051103.pdf)\n\n[353\\. 2025 Outlook - AI and Innovative Technology Active ETF (3006)](http://investments.miraeasset.com.hk/docs/ETF/25-01-Global-AI-and-Innovative-Tech-Outlook_GX.pdf)\n\n[354\\. Transformer FLOPs](https://www.adamcasson.com/transformer-flops.pdf)\n\n[355\\. MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis](https://arxiv.org/pdf/2412.15322)\n\n[356\\. MONO-INTERNVL: PUSHING THE BOUNDARIES OF MONOLITHIC MULTIMODAL LARGE LANGUAGE MODELS WITH ENDOGENOUS VISUAL PRE-TRAINING](https://arxiv.org/pdf/2410.08202v1)\n\n[357\\. SIMPLE LINEAR ATTENTION LANGUAGE MODELS BALANCE THE RECALL-THROUGHPUT TRADEOFF](https://openreview.net/pdf/0779c6e31781100fde299327f8dfeafff66de03e.pdf)\n\n[358\\. Measuring the Contributions of Vision and Text Modalities in Multimodal Transformers](https://archiv.ub.uni-heidelberg.de/volltextserver/35753/1/30-Nov-2024_final_parcalabescu_letitia_dissertation-pdf-a.pdf)\n\n[359\\. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://dai.sjtu.edu.cn/my_file/pdf/56ebcd39-ce4f-4800-9f74-7aa108345782.pdf)\n\n[360\\. A guide to LLM inference and performance](https://www.baseten.co/blog/llm-transformer-inference-guide/)\n\n[361\\. Enhancing Transformer Models for Dialogue Summarization](https://digital.lib.washington.edu/researchworks/bitstreams/f7e27e4e-8223-475e-bbe4-f57ef586260d/download)\n\n[362\\. AI算力浪潮下数通业务量价齐升 买入（首次）](https://pdf.dfcfw.com/pdf/H3_AP202304251585789247_1.pdf)\n\n[363\\. Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models](https://www.arxiv.org/pdf/2503.16980v2)\n\n[364\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[365\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[366\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[367\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[368\\. Enhanced Transformer with Rotary Position Embedding - ar5iv](https://ar5iv.labs.arxiv.org/html/2104.09864)\n\n[369\\. Jianlin Su, Yu Lu et al. “RoFormer: Enhanced Transformer with Rotary Position Embedding.” ArXiv](https://doi.org/10.1016/j.neucom.2023.127063)\n\n[370\\. Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis](https://arxiv.org/html/2412.15322v1)\n\n[371\\. Scaling Laws of RoPE-based Extrapolation](https://openreview.net/pdf?id=JO7k0SJ5V6)\n\n[372\\. Rethinking LLM Advancement: Compute-Dependent and Independent Paths to Progress](https://www.arxiv.org/pdf/2505.04075)\n\n[373\\. Modular - NEW PRODUCTION](https://www.modular.com/ai-resources/rotary-position-embedding-rope)\n\n[374\\. Qwen2.5-Omni Technical Report](https://www.52nlp.cn/wp-content/uploads/2025/03/Qwen2.5_Omni_%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E8%8B%B1%E4%B8%AD%E5%AF%B9%E7%85%A7%E7%89%88.pdf)\n\n[375\\. Base of RoPE Bounds Context Length](http://arxiv.org/html/2405.14591v1)\n\n[376\\. VRoPE: Rotary Position Embedding for Video Large Language Models](https://arxiv.org/pdf/2502.11664)\n\n[377\\. Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling](https://arxiv.org/pdf/2402.00522)\n\n[378\\. Rotary Positional Embeddings (RoPE)](https://nn.labml.ai/transformers/rope/index.html)\n\n[379\\. Resonance RoPE: Improving Context Length Generalization of Large ...](https://yiyibooks.cn/__trs__/arxiv/2403.00071v1/index.html)\n\n[380\\. Transformer Block Coupling and its Correlation with...](http://arxiv.org/html/2407.07810v2)\n\n[381\\. Abstracts: July 29, 2024 - Microsoft Research](https://www.microsoft.com/en-us/research/podcast/abstracts-july-29-2024/)\n\n[382\\. Wonderful Matrices: More Efficient and Effective Architecture for Language Modeling Tasks](https://arxiv.org/pdf/2407.16958v3)\n\n[383\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[384\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[385\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[386\\. Alex Wang, Amanpreet Singh et al. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” BlackboxNLP@EMNLP](https://doi.org/10.18653/v1/W18-5446)\n\n[387\\. Iz Beltagy, Matthew E. Peters et al. “Longformer: The Long-Document Transformer.” ArXiv](https://arxiv.org/abs/2004.05150)\n\n[388\\. Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models](https://arxiv.org/pdf/2502.05610)\n\n[389\\. Power Usage and Energy Efficiency](https://llm-tracker.info/_TOORG/Power-Usage-and-Energy-Efficiency)\n\n[390\\. An Empirical Study of Mamba-based Language Models](https://paperswithcode.com/paper/an-empirical-study-of-mamba-based-language)\n\n[391\\. Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI](https://arxiv.org/pdf/2409.14160)\n\n[392\\. Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning](https://m.riunet.upv.es/bitstream/handle/10251/204447/DesislavovMartinez-PlumedHernandez-Orallo%20-%20Trends%20in%20AI%20inference%20energy%20consumption%20Beyond%20the%20....pdf?sequence=1&isAllowed=y)\n\n[393\\. Attention is All You Need: Still True in 2025? - Notes](https://notes.suhaib.in/docs/tech/llms/attention-is-all-you-need-still-true-in-2025/)\n\n[394\\. OVERTHINK: Slowdown Attacks on Reasoning LLMs](https://arxiv.org/pdf/2502.02542)\n\n[395\\. Energy-Based Transformers are Scalable Learners and ...](https://arxiv.org/abs/2507.02092)\n\n[396\\. TOKEN STATISTICS TRANSFORMER: LINEAR-TIME ATTENTION VIA VARIATIONAL RATE REDUCTION](https://openreview.net/pdf?id=lXRDQsiP2v)\n\n[397\\. Token-Budget-Aware LLM Reasoning](https://arxiv.org/pdf/2412.18547)\n\n[398\\. Neuromorphic Principles for Efficient Large Language Models on Intel Loihi 2](https://openreview.net/pdf?id=qaDM1R2nlm)\n\n[399\\. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://dai.sjtu.edu.cn/my_file/pdf/a43c4dc7-6132-49cb-b737-311eefde646f.pdf)\n\n[400\\. A Preliminary Empirical Study on Energy Consumption in ...](https://arxiv.org/html/2402.13640v1)\n\n[401\\. Inf2 Inference Performance — AWS Neuron Documentation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/benchmarks/inf2/inf2-performance.html)"
  },
  {
    "id": 42,
    "question": "How might federated learning combined with model distillation techniques overcome both technical and legal barriers in sensitive domains like healthcare?",
    "response": "**Research Report: Overcoming Technical and Legal Barriers in Sensitive Domains with Federated Distillation**\n\n**Report Date:** July 27, 2025\n\n**Author:** Expert AI Researcher\n\n**1\\. Executive Summary**\n\nThe advancement of artificial intelligence (AI) in sensitive domains, particularly healthcare, is paradoxically constrained by its greatest need: access to large, diverse, and representative datasets. While deep learning models thrive on data, stringent legal and ethical frameworks like the Health Insurance Portability and Accountability Act (HIPAA) and the General Data Protection Regulation (GDPR) severely restrict the centralization and sharing of patient data. This creates a fundamental tension between innovation and privacy. This report examines how the synergistic combination of **Federated Learning (FL)** and **Model Distillation (MD)**, collectively known as **Federated Distillation (FD)**, provides a powerful paradigm to resolve this tension.\n\nOur analysis reveals that this combined approach offers a multi-pronged solution. Firstly, it fundamentally realigns AI development with privacy-by-design principles, directly addressing the legal mandates of HIPAA and GDPR by keeping sensitive data localized. Secondly, it systematically dismantles the key technical barriers that have plagued multi-institutional collaborations, namely prohibitive communication overhead, statistical and system heterogeneity, and high computational costs. By shifting from exchanging large, complex model parameters to sharing compact, distilled knowledge, FD enables the development of robust, generalizable, and efficient medical AI. This report will detail the specific mechanisms of this synergy, evaluate its performance against traditional methods, analyze its pathway to regulatory approval, and assess its potential for real-world clinical deployment.\n\n**2\\. Overcoming Legal and Privacy Barriers: A New Paradigm for Compliance**\n\nThe primary obstacle to large-scale AI in healthcare is not a lack of data, but the inability to legally and ethically aggregate it. Federated Distillation offers a technical architecture that inherently respects these boundaries.\n\n**2.1 The Regulatory Gauntlet: HIPAA and GDPR**\n\nHIPAA in the United States and GDPR in the European Union establish strict rules governing the use and disclosure of Protected Health Information (PHI) and personal data. Traditional AI development, which requires pooling data into a central repository, creates significant compliance burdens. This model necessitates complex, often-insufficient de-identification procedures and cumbersome data-sharing agreements between institutions, hindering rapid and scalable research \\[21\\]\\[22\\].\n\nFederated Learning (FL) was conceived as a direct response to this challenge. Its core principle is to move the computation to the data, not the other way around. In an FL network, multiple healthcare institutions can collaboratively train a shared AI model without ever exchanging raw patient data. Instead, only anonymized model updates (parameters or gradients) are sent to a central server for aggregation \\[22\\]\\[27\\]. This decentralized approach inherently aligns with the data minimization principle of GDPR and the security rules of HIPAA, dramatically simplifying the legal and administrative overhead of multi-site studies \\[21\\].\n\n**2.2 Federated Distillation: Deepening Privacy Protections**\n\nWhile standard FL is a major leap forward, sharing model parameters is not without risk. Sophisticated attacks, such as model inversion and membership inference, can potentially reverse-engineer information about the private training data from the shared model updates \\[21\\]\\[30\\].\n\nFederated Distillation enhances privacy by changing _what_ is shared. Instead of transmitting the entire set of model parameters, participants share only the model's outputs (e.g., logits or predictions) on a given set of inputs \\[9\\]\\[7\\]\\[70\\]. This method, often termed **Ensemble Distillation**, is significantly more privacy-preserving because these outputs are a higher-level abstraction of the model's knowledge and contain far less granular information about the underlying training data compared to the millions of individual weights in a deep neural network \\[66\\]\\[77\\]. Adding noise to these outputs can further strengthen privacy guarantees \\[70\\].\n\nFurthermore, some advanced FD techniques eliminate the need for any shared data whatsoever, even a public dataset. **Data-Free Knowledge Distillation (DFKD)** frameworks use a generative model on the server to synthesize a small, representative dataset. This synthetic data, which is low-fidelity to the original patient data, is then used to query the local models and distill their collective knowledge, effectively circumventing privacy leakage risks associated with using any real data for the transfer process \\[24\\]\\[73\\]. By severing the link to both private and public datasets, these methods offer a more robust solution for maintaining HIPAA compliance and satisfying GDPR's strict consent and purpose limitation requirements \\[5\\]\\[76\\].\n\n**3\\. Tackling Technical Hurdles in Multi-Institutional Collaboration**\n\nBeyond legal compliance, the practicalities of coordinating research across multiple hospitals present formidable technical challenges. Federated Distillation provides an elegant suite of solutions to these operational roadblocks.\n\n**3.1 Challenge: Prohibitive Communication Overhead**\n\nIn standard FL, deep learning models for tasks like medical imaging can have hundreds of millions of parameters, resulting in updates that are hundreds of megabytes or even gigabytes in size. Transmitting these large updates frequently between hospitals and a central server consumes significant bandwidth and can be infeasible for institutions with limited network infrastructure \\[123\\]\\[124\\]\\[126\\].\n\n**FD Solution:** Federated Distillation dramatically reduces this communication burden. The information being transferred—model predictions (logits)—is typically orders of magnitude smaller than the full model parameter set \\[9\\]\\[14\\]. One study demonstrated a communication overhead reduction of approximately 26 times while achieving high accuracy \\[278\\]\\[336\\]. Several specific techniques facilitate this efficiency:\n\n**Model Update Distillation (MUD):** This framework synthesizes and transmits compact tensor sequences that encode only the essential information needed for synchronization, rather than the entire model \\[4\\].\n\n**Dynamic Parameter Difference Compression (DPDC):** This method dynamically adjusts compression thresholds to reduce parameter size, achieving communication cost reductions of over 30% \\[5\\].\n\n**FedCompress:** This approach combines dynamic weight clustering with server-side knowledge distillation to produce generalizable models with low communication costs \\[13\\].\n\n**3.2 Challenge: Statistical Heterogeneity (Non-IID Data)**\n\nPerhaps the most significant technical challenge in federated learning is dealing with non-independent and identically distributed (Non-IID) data. Data from different hospitals naturally varies due to differences in imaging equipment, scanning protocols, patient populations, and even labeling practices \\[128\\]\\[126\\]\\[126\\]. In standard FL (e.g., using the FedAvg algorithm), simply averaging the parameters of models trained on such heterogeneous data can lead to a phenomenon known as \"client drift,\" where the aggregated global model performs poorly for everyone \\[131\\]\\[337\\].\n\n**FD Solution:** Federated Distillation is inherently more robust to Non-IID data. Instead of naively averaging divergent model weights, it distills a consensual, generalized knowledge base from an ensemble of specialized local \"teacher\" models. The global \"student\" model learns the collective intelligence, smoothing out the biases from individual sites.\n\n**Federated Distillation Fusion (FedDF):** This popular method leverages ensemble learning principles, demonstrating that fusing knowledge through distillation is more effective than averaging parameters, especially in Non-IID settings \\[45\\]\\[45\\].\n\n**Adaptive Mutual Distillation:** This technique encourages a reciprocal learning process between smaller \"student\" models and larger local \"mentor\" models, allowing models to adapt to local data specifics while contributing to a robust global consensus \\[12\\].\n\n**Quantitative Improvements:** Studies consistently show significant accuracy gains. For instance, **DaFKD** improved model accuracy by up to 6.02% over baselines on Non-IID data \\[277\\]. Combining FD with **Federated Augmentation (FAug)** has been shown to boost test accuracy by 7-22% \\[278\\]\\[336\\]. Another method, **FedGMKD**, achieved global accuracy gains of up to 3.37% over state-of-the-art methods in highly non-IID scenarios \\[275\\].\n\n**3.3 Challenge: System Heterogeneity and Computational Constraints**\n\nCollaborating hospitals often possess diverse IT infrastructure, with varying computational power (CPU/GPU) and software environments \\[128\\]\\[131\\]. Traditional FL often imposes a rigid requirement that all participating clients use the exact same model architecture, which is impractical and stifles innovation \\[246\\].\n\n**FD Solution:** Model distillation decouples the client models from the server model. Because knowledge is transferred via a standardized format (predictions/logits), each participating institution is free to design and train its own unique model architecture, tailored to its specific data and computational resources \\[25\\]\\[31\\]\\[175\\]. This flexibility is a key enabler for real-world collaboration. Frameworks like **FedMD** are explicitly designed to support this model heterogeneity, allowing participants with different intellectual property or technical capabilities to collaborate effectively \\[235\\]\\[295\\].\n\n**4\\. Performance, Application, and the Path to Clinical Deployment**\n\nFor any new technology to be adopted in healthcare, it must not only be compliant and technically feasible but also demonstrably effective and safe.\n\n**4.1 Diagnostic Accuracy: Closing the Gap with Centralized Training**\n\nA critical question is whether a decentralized approach can match the performance of a \"gold standard\" centralized model trained on all data. Evidence suggests that Federated Distillation is closing this gap rapidly.\n\n**Comparative Performance:** Multiple studies show that FD methods can achieve diagnostic accuracy comparable to, or in some cases superior to, centralized training, especially when data is noisy or heterogeneous \\[45\\]. While a basic FL model might lag, advanced FD techniques often excel. For instance, on the MIMIC-III dataset, a basic FL model's performance quickly became similar to a centralized model after initial training rounds \\[48\\].\n\n**Medical Imaging Success:** On the MedMNIST medical imaging datasets, the **MetaFed** framework achieved accuracy improvements of up to 12.44% over other federated methods \\[44\\]. The **FEDLGD** approach increased test accuracy by 3.1% over the best baseline on the RETINA dataset \\[47\\]. The **FEDMIC** framework consistently outperformed baselines on four different medical image classification datasets under simulated Non-IID conditions \\[348\\].\n\n**Beyond Imaging:** While imaging is a primary use case, techniques like **FedKD** have also shown effectiveness in handling Non-IID data for medical Natural Language Processing (NLP) tasks \\[288\\].\n\n**4.2 Efficiency for Real-Time Applications: ICU Monitoring and Edge Computing**\n\nIn critical care settings like an Intensive Care Unit (ICU), AI models must provide insights in real-time. Sending sensor data to a remote cloud server for processing introduces unacceptable latency. Model distillation is key to solving this by creating small, highly efficient \"student\" models that can be deployed directly on **edge devices** (e.g., bedside monitors, local hospital servers) \\[153\\].\n\n**Latency Reduction:** By processing data locally, edge computing drastically reduces response times. Studies show that edge deployments can be 10-100ms faster than cloud providers and can reduce overall latency by as much as 90% \\[144\\]\\[149\\]. For time-sensitive ICU monitoring, this speed can be the difference between proactive intervention and reactive response.\n\n**4.3 The Regulatory Pathway: FDA Approval**\n\nFor an FD-trained model to be used in a diagnostic medical device, it must gain clearance or approval from the U.S. Food and Drug Administration (FDA). The FDA regulates AI/ML software through established risk-based pathways like Premarket Notification (510(k)), Premarket Approval (PMA), and the De Novo classification \\[101\\]\\[102\\]\\[103\\].\n\nWhile the FDA has not yet published explicit validation protocols _specifically for federated learning systems_, the requirements can be inferred from its existing framework for AI/ML devices \\[199\\]\\[255\\]\\[309\\]. The FDA requires robust **analytical and clinical validation** to ensure the device is safe and effective \\[192\\]\\[254\\]. For a federated system, demonstrating **model consistency across sites** will be crucial. The required validation evidence will likely include:\n\n1.  **Site-Specific Performance Data:** Demonstrating that the final model performs effectively and safely not just on average, but within the specific patient population of _each_ participating institution. This guards against models that are biased or underperform for certain subgroups \\[205\\].\n2.  **Robustness to Heterogeneity:** Providing evidence that the model is robust to the Non-IID data distributions inherent in the federated network, a key claim of FD systems.\n3.  **Comprehensive Documentation:** Clear documentation of the federated training and distillation protocol, the aggregation methods used, and the quality control measures for each participating site's data \\[327\\].\n4.  **Large-Scale Generalizability:** Federated learning's ability to train on diverse, multi-institutional data is a strength. The FDA emphasizes the need for large-scale validation to evaluate generalizability, a requirement that FL is uniquely positioned to meet \\[209\\]\\[260\\].\n\n**4.4 Real-World Implementation: From Research to Practice**\n\nThe transition from research frameworks to fully operational clinical systems is ongoing. While most documented examples are research frameworks like **FedMIC** for medical imaging \\[178\\] and **MH-pFLID** for personalized models \\[186\\], there are promising signs of real-world application. The **FLuID** framework, for instance, was validated in a real-world research collaboration involving eight pharmaceutical companies for drug discovery, demonstrating the model's viability in a multi-stakeholder, privacy-sensitive environment \\[239\\]\\[297\\]. These pioneering efforts are solving key implementation challenges and paving the way for broader adoption in clinical diagnostics.\n\n**5\\. Conclusion and Future Outlook**\n\nThe combination of federated learning and model distillation represents a paradigm shift for AI in healthcare. It is not merely an incremental improvement but a foundational rethinking of how to build intelligent systems in a world governed by strict privacy laws and practical data silos. By providing a technical solution that is privacy-preserving by design, this synergy directly addresses the legal and ethical imperatives of our time.\n\nFederated Distillation systematically dismantles the core technical barriers to collaboration—communication costs, data heterogeneity, and system incompatibility—while producing models that are robust, efficient, and increasingly accurate compared to traditional methods. It offers a clear path toward developing AI that is not only powerful but also equitable and generalizable, having learned from the rich diversity of data held across multiple institutions.\n\nThe road ahead involves continued innovation and standardization. Key future directions include:\n\n**Formalized Regulatory Guidance:** Collaboration between researchers, industry, and regulatory bodies like the FDA to establish clear, standardized validation protocols for federated AI systems.\n\n**Advanced Data-Free Techniques:** Further refinement of data-free distillation methods to completely eliminate reliance on any proxy data, maximizing both privacy and efficiency.\n\n**Turnkey Infrastructure:** The development of open-source platforms and common frameworks that lower the barrier to entry, enabling more hospitals and clinics to participate in federated networks.\n\n**Expansion to New Modalities:** Moving beyond medical imaging to tackle more complex, multimodal challenges involving genomics, electronic health records, and real-time clinical text and sensor data.\n\nUltimately, Federated Distillation provides a viable and compelling path to unlock the immense potential of artificial intelligence in medicine. It enables a future where we can build smarter, more effective diagnostic tools that benefit all patients, without ever having to compromise on the fundamental right to privacy.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[2\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[3\\. Knowledge Distillation in Federated Learning: A Practical Guide](https://arxiv.org/pdf/2211.04742)\n\n[4\\. Communication-Efficient Federated Learning via Model ...](https://openreview.net/forum?id=Zh9gz3CaWm)\n\n[5\\. Communication-Efficient Federated Self-Distillation Method for Medical Image Segmentation](http://article.isciencegroup.com/pdf/10240074.pdf)\n\n[6\\. On Learning Representations for Tabular Data Distillation](https://openreview.net/pdf?id=GXlsrvOGIK)\n\n[7\\. Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data](https://arxiv.org/pdf/1811.11479v1)\n\n[8\\. Jakub Konecný, H. B. McMahan et al. “Federated Learning: Strategies for Improving Communication Efficiency.” ArXiv](https://arxiv.org/abs/1610.05492)\n\n[9\\. Distributed Learning for Wireless Communications: Methods, Applications and Challenges](https://hal.science/hal-03843797/file/%5B8%5D.pdf)\n\n[10\\. Security of Federated Learning: Attacks, Defensive Mechanisms, and Challenges](https://hal.science/hal-03620400v1/file/RIA%2013896%20-%20FL%20Security.pdf)\n\n[11\\. COMMUNICATION-EFFICIENT FEDERATED LEARNING BASED ON EXPLANATION-GUIDED PRUNING FOR REMOTE SENSING IMAGE CLASSIFICATION](https://arxiv.org/pdf/2501.11493)\n\n[12\\. Communication-efficient federated learning via knowledge distillation | Nature Communications](https://www.nature.com/articles/s41467-022-29763-x)\n\n[13\\. GitHub - FederatedML/FedCompress: Communication-Efficient Federated ...](https://github.com/FederatedML/FedCompress)\n\n[14\\. 知识蒸馏研究综述](http://cjc.ict.ac.cn/online/bfpub/hzhxv-2022124104143.pdf)\n\n[15\\. Federated Learning: A Privacy-Preserving Approach to ...](https://www.netguru.com/blog/federated-learning)\n\n[16\\. Daliang Li, Junpu Wang. “FedMD: Heterogenous Federated Learning via Model Distillation.” ArXiv](https://arxiv.org/abs/1910.03581)\n\n[21\\. Federated Learning Based Artificial Intelligence Systems with Blockchain Security for Global Healthcare Collaboration and Patient Centric Data Privacy](https://www.atlantis-press.com/article/126011345.pdf)\n\n[22\\. Optimizing Federated Learning Workloads: A Practical ...](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Optimizing-Federated-Learning-Workloads-A-Practical-Evaluation/post/1638458?profile.language=zh-CN)\n\n[23\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[24\\. Federated Learning on Virtual Heterogeneous Data with Local-Global Dataset Distillation](https://openreview.net/pdf/e166177fc47948a430b6470e8c95eac2c0678e17.pdf)\n\n[25\\. Daliang Li, Junpu Wang. “FedMD: Heterogenous Federated Learning via Model Distillation.” ArXiv](https://arxiv.org/abs/1910.03581)\n\n[26\\. PRIVACY PRESERVING FREDERATED MACHINE LEARNING IN HEALTH CARE](https://www.ijprems.com/uploadedfiles/paper/volume_4/issue_11_november_2024/37349/1732888257.docx)\n\n[27\\. AI and Machine Learning in Healthcare: Federated ...](https://www.sprypt.com/blog/ai-machine-learning-healthcare-federated-learning-privacy)\n\n[28\\. Federated learning: Data for AI in the age of data regulations](https://doku.lrz.de/files/10333405/10333424/1/1684600428147/FL_Intel_LRZ_October.pdf)\n\n[29\\. AI in healthcare: Predictive modeling, explainability and clinical impact](https://wjarr.com/sites/default/files/WJARR-2023-1986.pdf)\n\n[30\\. Modular Federated Learning: A Meta-Framework Perspective](https://arxiv.org/pdf/2505.08646)\n\n[31\\. A novel decentralized federated learning approach to train on globally distributed, poor quality, and protected private medical data](https://www.nature.com/articles/s41598-022-12833-x.pdf)\n\n[32\\. Data privacy in the era of AI: Navigating regulatory landscapes for global businesses](https://ijsra.net/sites/default/files/IJSRA-2024-2396.pdf)\n\n[33\\. Inclusive, Differentially Private Federated Learning for ...](https://arxiv.org/html/2505.22108v1)\n\n[34\\. P. Kairouz, H. B. McMahan et al. “Advances and Open Problems in Federated Learning.” Found. Trends Mach. Learn.](https://doi.org/10.1561/2200000083)\n\n[35\\. Federated Learning with Privacy-Preserving Ensemble ...](https://arxiv.org/abs/2210.08464)\n\n[36\\. Federated Learning for Medical Applications: A Taxonomy, Current Trends, Challenges, and Future Research Directions](https://sintef.brage.unit.no/sintef-xmlui/bitstream/handle/11250/3128355/Rauniyar_2023_Federated_learning_AAM.pdf?sequence=4)\n\n[37\\. MH-pFLID: Model Heterogeneous personalized Federated Learning via Injection and Distillation for Medical Data Analysis](http://www.paperreading.club/page?id=226615)\n\n[38\\. Proceedings of the 2019 USENIX Conference on Operational Machine Learning](https://www.usenix.org/sites/default/files/opml19_full_proceedings.pdf)\n\n[39\\. Global AI and Data Science](https://community.ibm.com/community/user/datascience/blogs/nathalie-baracaldo1/2020/01/03/federated-learning-part-2)\n\n[41\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[42\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[43\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[44\\. MetaFed: Federated Learning among Federations with Cyclic Knowledge Distillation for Personalized Healthcare](https://federated-learning.org/fl-ijcai-2022/Papers/FL-IJCAI-22_paper_10.pdf)\n\n[45\\. A novel decentralized federated learning approach to train on globally distributed, poor quality, and protected private medical data](https://www.nature.com/articles/s41598-022-12833-x.pdf)\n\n[46\\. Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Fine-Tuning_Global_Model_via_Data-Free_Knowledge_Distillation_for_Non-IID_Federated_CVPR_2022_paper.pdf)\n\n[47\\. Federated Learning on Virtual Heterogeneous Data with Local-Global Dataset Distillation](https://openreview.net/pdf?id=QplBL2pV4Z)\n\n[48\\. Federated Learning on Clinical Benchmark Data](https://pmc.ncbi.nlm.nih.gov/articles/PMC7652692/)\n\n[49\\. Anit Kumar Sahu, Tian Li et al. “Federated Optimization in Heterogeneous Networks.” arXiv: Learning](https://arxiv.org/abs/1812.06127)\n\n[50\\. FEDERATED LEARNING FOR MEDICAL IMAGE CLASSIFICATION](https://fau.digital.flvc.org/islandora/object/fau%3A97102/datastream/OBJ/view/FEDERATED_LEARNING_FOR_MEDICAL_IMAGE_CLASSIFICATION.pdf)\n\n[51\\. Communication-Efficient Federated Self-Distillation Method for Medical Image Segmentation](http://article.isciencegroup.com/pdf/10240074.pdf)\n\n[52\\. Ensemble Distillation for Robust Model Fusion in Federated Learning](https://proceedings.neurips.cc/paper_files/paper/2020/file/18df51b97ccd68128e994804f3eccc87-Paper.pdf)\n\n[53\\. FedRCIL: Federated Knowledge Distillation for Representation based Contrastive Incremental Learning](https://openaccess.thecvf.com/content/ICCV2023W/VCL/papers/Psaltis_FedRCIL_Federated_Knowledge_Distillation_for_Representation_based_Contrastive_Incremental_Learning_ICCVW_2023_paper.pdf)\n\n[54\\. Daliang Li, Junpu Wang. “FedMD: Heterogenous Federated Learning via Model Distillation.” ArXiv](https://arxiv.org/abs/1910.03581)\n\n[55\\. Centralized and Federated Heart Disease Classification Models Using UCI Dataset and their Shapley-value Based Intepretability](https://arxiv.org/pdf/2408.06183)\n\n[56\\. A multicenter bladder cancer MRI dataset and baseline evaluation of federated learning in clinical application](https://www.nature.com/articles/s41597-024-03971-0.pdf)\n\n[61\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[62\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[63\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[64\\. PFDP: Privacy-preserving Federated Distillation Method for Pretraining Language Models](https://www.researchsquare.com/article/rs-4247440/v1.pdf?c=1713257510000)\n\n[65\\. A Survey of Recent Advances for Tackling Data Heterogeneity in Federated Learning](https://www.preprints.org/frontend/manuscript/b22bf0467a103724f92a01967066b908/download_pub)\n\n[66\\. Tao Lin, Lingjing Kong et al. “Ensemble Distillation for Robust Model Fusion in Federated Learning.” ArXiv](https://arxiv.org/abs/2006.07242)\n\n[67\\. Applied and Computational Engineering](https://www.ewadirect.com/media/var/media/upload/vol_pdf/ace/40.pdf)\n\n[68\\. Daliang Li, Junpu Wang. “FedMD: Heterogenous Federated Learning via Model Distillation.” ArXiv](https://arxiv.org/abs/1910.03581)\n\n[69\\. MH-pFLID: Model Heterogeneous personalized Federated Learning via Injection and Distillation for Medical Data Analysis](http://www.paperreading.club/page?id=226615)\n\n[70\\. Xuan Gong, Abhishek Sharma et al. “Preserving Privacy in Federated Learning with Ensemble Cross-Domain Knowledge Distillation.” ArXiv](https://doi.org/10.1609/aaai.v36i11.21446)\n\n[71\\. Jiawei Shao, Fangzhao Wu et al. “Selective knowledge sharing for privacy-preserving federated distillation without a good teacher.” Nature Communications](https://doi.org/10.1038/s41467-023-44383-9)\n\n[72\\. Lichao Sun, L. Lyu. “Federated Model Distillation with Noise-Free Differential Privacy.” International Joint Conference on Artificial Intelligence](https://doi.org/10.24963/ijcai.2021/216)\n\n[73\\. Ming Li, Guang Yang. “Data-Free Distillation Improves Efficiency and Privacy in Federated Thorax Disease Analysis.” 2023 IEEE EMBS Special Topic Conference on Data Science and Engineering in Healthcare, Medicine and Biology](https://doi.org/10.1109/IEEECONF58974.2023.10405205)\n\n[74\\. Federated Learning on Virtual Heterogeneous Data with Local-Global Dataset Distillation](https://openreview.net/pdf/e166177fc47948a430b6470e8c95eac2c0678e17.pdf)\n\n[75\\. Federated Learning via Input-Output Collaborative Distillation](https://openreview.net/forum?id=3WjgGnswHy&referrer=%5Bthe%20profile%20of%20Xuan%20Gong%5D%28/profile?id%3D~Xuan_Gong1%29)\n\n[76\\. Haonan Shi, Ouyang Tu et al. “Unveiling Client Privacy Leakage from Public Dataset Usage in Federated Distillation.”](https://arxiv.org/abs/2502.08001)\n\n[77\\. FedED: 基于集合蒸馏的联合学习医疗关系提取-信息检索研究室](https://ir.dlut.edu.cn/info/1007/1311.htm)\n\n[78\\. Communication-Efficient Federated Self-Distillation Method for Medical Image Segmentation](http://article.isciencegroup.com/pdf/10240074.pdf)\n\n[81\\. Olga Russakovsky, Jia Deng et al. “ImageNet Large Scale Visual Recognition Challenge.” International Journal of Computer Vision](https://doi.org/10.1007/s11263-015-0816-y)\n\n[82\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[83\\. Sergey Zagoruyko, N. Komodakis. “Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer.” ArXiv](https://arxiv.org/abs/1612.03928)\n\n[84\\. Model Distillation AI Starter Guide: Techniques, Benefits ...](https://www.intellectyx.com/model-distillation-ai-starter-guide-techniques-benefits-and-applications/)\n\n[85\\. Cristian Bucila, R. Caruana et al. “Model compression.” Knowledge Discovery and Data Mining](https://doi.org/10.1145/1150402.1150464)\n\n[86\\. 2025 AI Model Benchmark Report: Accuracy, Cost, Latency, SVI](https://www.allaboutai.com/resources/ai-statistics/ai-models/)\n\n[87\\. Model Distillation](https://humanloop.com/blog/model-distillation)\n\n[88\\. Jianping Gou, B. Yu et al. “Knowledge Distillation: A Survey.” International Journal of Computer Vision](https://doi.org/10.1007/s11263-021-01453-z)\n\n[89\\. atp magazin](https://www.ifg.kit.edu/downloads/atp_08_2022.pdf)\n\n[90\\. CUPID: Curriculum Learning Based Real-Time Prediction using Distillation](https://assets.amazon.science/e2/d0/84e0075d4a53af807b30e2306141/cupid-curriculum-learning-based-real-time-prediction-using-distillation.pdf)\n\n[91\\. Rep-MedSAM: Towards Real-time and Universal Medical Image Segmentation](https://openreview.net/pdf?id=yqf77n9Kfw)\n\n[92\\. USABILITY OF LOCAL LARGE LANGUAGE MODELS AND RETRIEVAL AUGMENTED GENERATION IN HEALTH CARE](https://oulurepo.oulu.fi/bitstream/handle/10024/55664/nbnfioulu-202505083171.pdf?sequence=1&isAllowed=y)\n\n[93\\. AMD: Automatic Multi-step Distillation of Large-scale Vision Models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08272.pdf)\n\n[94\\. Real-Time Diagnostics in Critical Care: AI for Rapid Decision-Making and Continuous Monitoring](https://www.carijournals.org/journals/index.php/IJCE/article/download/2658/3074/7580?srsltid=AfmBOorR9thusXkoDU95VUxdTsufgi4h5SMpmZKGLDlJTDdN5ozA935E)\n\n[95\\. 一文读懂到底什么是“模型蒸馏(Model Distillation)”技术...](https://news.qq.com/rain/a/20250503A06J2I00)\n\n[96\\. Model Distillation Vs Fine Tuning | Restackio](https://www.restack.io/p/model-distillation-answer-vs-fine-tuning-cat-ai)\n\n[97\\. DIFFUSION MODELS ARE REAL-TIME GAME ENGINES](https://mimanizalesdelalma.com/wp-content/uploads/2024/09/2408.14837v1-IA-genera-videojuego-en-tiempo-real.pdf)\n\n[98\\. Inference-Time Diffusion Model Distillation](https://arxiv.org/pdf/2412.08871)\n\n[99\\. PFGE: Parsimonious Fast Geometric Ensembling of DNNs](https://arxiv.org/pdf/2202.06658v2)\n\n[100\\. TUNING TIMESTEP-DISTILLED DIFFUSION MODEL USING PAIRWISE SAMPLE OPTIMIZATION](https://openreview.net/pdf/3665030bef7787c9a984634fc505af1a567a3f72.pdf)\n\n[101\\. AI-Enabled Medical Devices: Transformation and Regulation](https://www.lexology.com/library/detail.aspx?g=f133ee05-a503-4aae-8d28-08a8db48d184)\n\n[102\\. FDA Regulation of Clinical Microbiology Diagnostic Devices](https://pmc.ncbi.nlm.nih.gov/articles/PMC3185846/)\n\n[103\\. Better Tests, Better Care: The Promise of Next Generation Diagnostics](https://www.idsociety.org/globalassets/idsa/policy--advocacy/current_topics_and_issues/diagnostics/statements/better-tests-better-care-for-policymakers.pdf)\n\n[104\\. Regulatory challenges in ai-based diagnostics: legal implications of ai use in medical diagnostics](https://www.bio-conferences.org/articles/bioconf/pdf/2025/03/bioconf_ichbs2025_01034.pdf)\n\n[105\\. Medicare and the Health Care Delivery System](https://www.medpac.gov/wp-content/uploads/2024/06/Jun24_MedPAC_Report_To_Congress_SEC.pdf)\n\n[106\\. The Pros and Cons of 3 Regulatory Pathways](https://www.avaniaclinical.com/blog/pros-and-cons-regulatory-pathways/)\n\n[107\\. Biodesix Annual Report 2021 Form 10-K (NASDAQ:BDSX)](https://stocklight.com/stocks/us/nasdaq-bdsx/biodesix/annual-reports/nasdaq-bdsx-2021-10K-21745283.pdf)\n\n[108\\. FDA Device Regulation - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC6140070/)\n\n[109\\. Regulatory Approval Pathways for Molecular Diagnostic Technology](https://pubmed.ncbi.nlm.nih.gov/22081361/)\n\n[110\\. Public evidence on AI products for digital pathology](https://www.nature.com/articles/s41746-024-01294-3.pdf)\n\n[111\\. 医疗器械的FDA认可共识标准现状介绍](https://tbt.sist.org.cn/cslm/zhwxk/201202/P020171222592547636827.pdf)\n\n[112\\. FORM 10-Q](https://s29.q4cdn.com/260940227/files/doc_financials/2023/q3/bio-20230930.pdf)\n\n[113\\. Navigating Health Tech: Regulations for AI/ML in Medical Devices and Software](https://www.mintz.com/sites/default/files/media/documents/2024-04-16/Mintz%20-%20Health%20Law%20AI%20Webinar%20April%202024.pdf)\n\n[114\\. 2019-2020 Undergraduate Bulletin, Santa Clara University](https://www.scu.edu/bulletin/undergraduate/2019-2020-Undergraduate-Bulletin.pdf)\n\n[115\\. 全球市场医疗技术（MedTech）审批流程指南](https://mindmachineco.com/navigating-the-medtech-maze-a-guide-to-the-medtech-approval-process-in-global-markets/importation-to-us/)\n\n[116\\. OncoCyte Annual Report 2025 Form 10-K (NASDAQ:OCX)](https://stocklight.com/stocks/us/nasdaq-ocx/oncocyte/annual-reports/nasdaq-ocx-2025-10K-25764357.pdf)\n\n[117\\. Regulatory Approval for Medical Devices](https://biotility.research.ufl.edu/industry-courses/citi-program/device-approval/)\n\n[118\\. F. Goodsaid. “The Labyrinth of Product Development and Regulatory Approvals in Liquid Biopsy Diagnostics.” Clinical and Translational Science](https://doi.org/10.1111/cts.12657)\n\n[119\\. Hyeon Ki Jeong, Christine Park et al. “Deep Learning in Dermatology: A Systematic Review of Current Approaches, Outcomes, and Limitations.” JID Innovations](https://doi.org/10.1016/j.xjidi.2022.100150)\n\n[121\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[122\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[123\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[124\\. Medical](https://paperreading.club/category?cate=Medical&page=5)\n\n[125\\. 联邦领域适应知识蒸馏与迁移学习](https://max.book118.com/try_down/865320020003011140.pdf)\n\n[126\\. Sufen Ren, Yule Hu et al. “Federated Distillation for Medical Image Classification: Towards Trustworthy Computer-Aided Diagnosis.”](https://arxiv.org/abs/2407.02261)\n\n[127\\. Tao Lin, Lingjing Kong et al. “Ensemble Distillation for Robust Model Fusion in Federated Learning.” ArXiv](https://arxiv.org/abs/2006.07242)\n\n[128\\. Federated learning of biomedical data in multicentric imaging studies](https://theses.hal.science/tel-04417044v1/file/2023COAZ4056.pdf)\n\n[129\\. Daliang Li, Junpu Wang. “FedMD: Heterogenous Federated Learning via Model Distillation.” ArXiv](https://arxiv.org/abs/1910.03581)\n\n[130\\. MH-pFLID: Model Heterogeneous personalized Federated Learning via Injection and Distillation for Medical Data Analysis](http://www.paperreading.club/page?id=226615)\n\n[131\\. Federated Learning of Biomedical Data in Multicentric Imaging Studies](https://inria.hal.science/tel-04417044/file/PhD_Manuscript_Santiago_SILVA.pdf)\n\n[132\\. Vision Foundation Models in Medical Image Analysis: Advances and Challenges](https://arxiv.org/pdf/2502.14584)\n\n[133\\. Communication-Efficient Federated Self-Distillation Method for Medical Image Segmentation](http://article.isciencegroup.com/pdf/10240074.pdf)\n\n[134\\. Nikolas Koutsoubis, Yasin Yilmaz et al. “Privacy Preserving Federated Learning in Medical Imaging with Uncertainty Estimation.” ArXiv](https://doi.org/10.48550/arXiv.2406.12815)\n\n[135\\. Zhe Li, Bernhard Kainz. “Image Distillation for Safe Data Sharing in Histopathology.” ArXiv](https://doi.org/10.48550/arXiv.2406.13536)\n\n[136\\. Liuyan Yang, Juanjuan He et al. “Federated Learning for Medical Imaging Segmentation via Dynamic Aggregation on Non-IID Data Silos.” Electronics](https://doi.org/10.3390/electronics12071687)\n\n[137\\. Federated Learning in Medical Image Analysis: A Systematic Survey](https://sigarra.up.pt/feup/en/pub_geral.show_file?pi_doc_id=440469)\n\n[138\\. ENHANCING MEDICAL IMAGE SEGMENTATION: STUDIES IN MODEL ACCURACY, PRIVACY, AND EFFICIENCY](https://dspace.jaist.ac.jp/dspace/bitstream/10119/19388/2/paper.pdf)\n\n[141\\. Weisong Shi, Jie Cao et al. “Edge Computing: Vision and Challenges.” IEEE Internet of Things Journal](https://doi.org/10.1109/JIOT.2016.2579198)\n\n[142\\. Yuyi Mao, Changsheng You et al. “A Survey on Mobile Edge Computing: The Communication Perspective.” IEEE Communications Surveys & Tutorials](https://doi.org/10.1109/COMST.2017.2745201)\n\n[143\\. Cloud Computing vs Edge Computing: 10 Major Differences](https://www.distilnfo.com/itadvisory/2020/06/29/cloud-computing-vs-edge-computing-10-major-differences/)\n\n[144\\. Latency Comparison of Cloud Datacenters and Edge Servers](https://par.nsf.gov/servlets/purl/10184999)\n\n[145\\. Alistair E. W. Johnson, T. Pollard et al. “MIMIC-III, a freely accessible critical care database.” Scientific Data](https://doi.org/10.1038/sdata.2016.35)\n\n[146\\. Edge Computing vs. Cloud Computing: A Comparative Analysis for Real-Time AI Applications](https://www.ijfmr.com/papers/2024/5/29316.pdf)\n\n[147\\. M. Satyanarayanan. “The Emergence of Edge Computing.” Computer](https://doi.org/10.1109/MC.2017.9)\n\n[148\\. Edge Computing](https://www.forbesness.com/edge-computing/)\n\n[149\\. Edge Computing Benefits: Edge Vs Cloud Computing](https://quantumzeitgeist.com/edge-computing-benefits-edge-vs-cloud-computing/)\n\n[150\\. A Black-Box Fork-Join Latency Prediction Model for Data-Intensive Applications](https://ranger.uta.edu/~jiang/publication/Journals/2020/2020-IEEE-TPDS-Fork-Join.pdf)\n\n[151\\. Edge cloud for video analysis in autonomous vehicles](https://libstore.ugent.be/fulltxt/RUG01/003/063/814/RUG01-003063814_2022_0001_AC.pdf)\n\n[152\\. Kubernetes Edge/Cloud Continuum Task Offloading Framework for Vehicular Computing](https://ceur-ws.org/Vol-3776/shortpaper13.pdf)\n\n[153\\. Valerie Lim, Kai Wen Ng et al. “Contrastive Learning in Distilled Models.” ArXiv](https://doi.org/10.48550/arXiv.2401.12472)\n\n[154\\. Automatic Edge App placement for personalized heart attack predictions](https://assets-eu.researchsquare.com/files/rs-2864678/v1/1022aeae6f0d88cdb0f271d0.docx)\n\n[161\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[162\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[163\\. Eunjeong Jeong, Seungeun Oh et al. “Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data.” ArXiv](https://arxiv.org/abs/1811.11479)\n\n[164\\. Rohan Anil, Gabriel Pereyra et al. “Large scale distributed neural network training through online distillation.” ArXiv](https://arxiv.org/abs/1804.03235)\n\n[165\\. MetaFed: Federated Learning among Federations with Cyclic Knowledge Distillation for Personalized Healthcare](https://federated-learning.org/fl-ijcai-2022/Papers/FL-IJCAI-22_paper_10.pdf)\n\n[166\\. Verification Vs. Validation Of Medical Devices](https://synectic.net/verification-vs-validation-of-medical-devices/)\n\n[167\\. Validation](https://biomanufacturing.org/uploads/files/925838416610429644-chapter-4-restricted.pdf)\n\n[168\\. Heterogeneous Federated Learning via Model Distillation](https://towardsdatascience.com/fedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4/)\n\n[169\\. Ensemble distillation for robust model fusion in feder...](https://dl.acm.org/doi/abs/10.5555/3495724.3495922)\n\n[170\\. Model-Decoupling-Based Federated Learning with ...](https://openreview.net/forum?id=h5lqXPd9JN)\n\n[172\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[173\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[174\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[175\\. Tao Lin, Lingjing Kong et al. “Ensemble Distillation for Robust Model Fusion in Federated Learning.” ArXiv](https://arxiv.org/abs/2006.07242)\n\n[176\\. Laiqiao Qin, Tianqing Zhu et al. “Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions.” ArXiv](https://doi.org/10.48550/arXiv.2406.10861)\n\n[177\\. Daliang Li, Junpu Wang. “FedMD: Heterogenous Federated Learning via Model Distillation.” ArXiv](https://arxiv.org/abs/1910.03581)\n\n[178\\. Medical](https://paperreading.club/category?cate=Medical&page=5)\n\n[179\\. Sufen Ren, Yule Hu et al. “Federated Distillation for Medical Image Classification: Towards Trustworthy Computer-Aided Diagnosis.”](https://arxiv.org/abs/2407.02261)\n\n[180\\. Chung-ju Huang, Leye Wang et al. “Vertical Federated Knowledge Transfer via Representation Distillation for Healthcare Collaboration Networks.” Proceedings of the ACM Web Conference 2023](https://doi.org/10.1145/3543507.3583874)\n\n[181\\. Communication-Efficient Federated Self-Distillation Method for Medical Image Segmentation](http://article.isciencegroup.com/pdf/10240074.pdf)\n\n[182\\. A comprehensive survey of federated transfer learning: challenges, methods and applications](https://link.springer.com/content/pdf/10.1007/s11704-024-40065-x.pdf)\n\n[183\\. Vertical Federated Knowledge Transfer via Representation ...](https://arxiv.org/abs/2302.05675)\n\n[184\\. A. Qayyum, Kashif Ahmad et al. “Collaborative Federated Learning for Healthcare: Multi-Modal COVID-19 Diagnosis at the Edge.” IEEE Open Journal of the Computer Society](https://doi.org/10.1109/OJCS.2022.3206407)\n\n[185\\. A Survey of Recent Advances for Tackling Data Heterogeneity in Federated Learning](https://www.preprints.org/frontend/manuscript/b22bf0467a103724f92a01967066b908/download_pub)\n\n[186\\. MH-pFLID: Model Heterogeneous personalized Federated Learning via Injection and Distillation for Medical Data Analysis](http://www.paperreading.club/page?id=226615)\n\n[192\\. Artificial intelligence in managing clinical trial design and conduct: Man and machine still on the learning curve?](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8011519/)\n\n[193\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[194\\. FDA Regulation on Artificial Intelligence](https://finarbconsulting.com/blog-fda-regulation-on-artificial-intelligence)\n\n[195\\. P. Kairouz, H. B. McMahan et al. “Advances and Open Problems in Federated Learning.” Found. Trends Mach. Learn.](https://doi.org/10.1561/2200000083)\n\n[196\\. How to build medical AI: Combining best practices from machine learning and medical device development](https://www.zuehlke.com/en/system/files?file=documents/medical-ai-whitepaper.pdf)\n\n[197\\. FDA Testbed for AI/ML-enabled Medical Devices](https://idsc.miami.edu/magazine/fda-testbed-for-ai-ml-enabled-medical-devices/)\n\n[198\\. FDA approved Artificial Intelligence and Machine Learning (AI/ML)-Enabled Medical Devices: An updated landscape](https://www.medrxiv.org/content/10.1101/2022.12.07.22283216v3.full.pdf)\n\n[199\\. 中华人民共和国医药行业标准 人工智能医疗器械 质量要求和评价 第1部分:术语](https://www.cmde.org.cn/hbpdf/YY1833.1-2022.pdf)\n\n[200\\. Federated Learning AI Approach Allows Hospitals to Share Patient Data Privately](https://healthcare-in-europe.com/en/news/federated-learning-ai-approach-allows-hospitals-to-share-patient-data-privately.html)\n\n[201\\. Federated learning: a collaborative effort to achieve better ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC7779924/)\n\n[202\\. U.S. Food & Drug Administration](https://www.accessdata.fda.gov/cdrh_docs/pdf23/K232322.pdf)\n\n[203\\. Akin Intelligence](https://www.akingump.com/a/web/5vfWsz9su3FDpL8yNZ4XX3/a1fuEo/akin-intelligence-nov-dec-2024.pdf)\n\n[204\\. AI in healthcare case study: A research proposal involving the use of AI technologies in the diagnostic support and prevention of breast cancer](https://classroom.eneri.eu/sites/default/files/2024-12/Case%20study%20AI%20in%20healthcare%20%281%29.pdf)\n\n[205\\. Regulating AI Adaptation: An Analysis of AI Medical Device ...](https://www.medrxiv.org/content/10.1101/2024.06.26.24309506v1.full-text)\n\n[206\\. ChatGPT in veterinary medicine: a practical guidance of generative artificial intelligence in clinics, education, and research](https://www.frontiersin.org/journals/veterinary-science/articles/10.3389/fvets.2024.1395934/pdf)\n\n[207\\. Artificial Intelligence Techniques in Health Diagnostics: A Systematic Review](https://knowledge.uchicago.edu/record/12279/files/Yau%2C%20Josephine%20-%20Artificial%20Intelligence%20Techniques%20in%20Health%20Diagnostics.pdf)\n\n[208\\. MPAI-AIH Use Cases and Functional Requirements WD0.5](https://mpai.community/wp-content/uploads/2023/07/N1302-MPAI-AIH-Use-Cases-and-Functional-Requirements-WD0.5.docx)\n\n[209\\. A. Karargyris, R. Umeton et al. “Federated benchmarking of medical artificial intelligence with MedPerf.” Nature machine intelligence](https://doi.org/10.1038/s42256-023-00652-2)\n\n[210\\. \"Nutrition Facts Labels\" for Artificial Intelligence/Machine Learning-Based Medical Devices—The Urgent Need for Labeling Standards](https://www.gwlr.org/wp-content/uploads/2023/03/91-Geo.-Wash.-L.-Rev.-79-2023.pdf)\n\n[211\\. Public evidence on AI products for digital pathology](https://www.nature.com/articles/s41746-024-01294-3.pdf)\n\n[212\\. A. Krizhevsky. “Learning Multiple Layers of Features from Tiny Images.”](https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086)\n\n[213\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[214\\. Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data](https://arxiv.org/pdf/1811.11479v1)\n\n[215\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[216\\. A novel decentralized federated learning approach to train on globally distributed, poor quality, and protected private medical data](https://www.nature.com/articles/s41598-022-12833-x.pdf)\n\n[217\\. Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Fine-Tuning_Global_Model_via_Data-Free_Knowledge_Distillation_for_Non-IID_Federated_CVPR_2022_paper.pdf)\n\n[218\\. \\[1806.00582\\] Federated Learning with Non-IID Data](https://arxiv.org/abs/1806.00582)\n\n[219\\. Chung-ju Huang, Leye Wang et al. “Vertical Federated Knowledge Transfer via Representation Distillation for Healthcare Collaboration Networks.” Proceedings of the ACM Web Conference 2023](https://doi.org/10.1145/3543507.3583874)\n\n[220\\. Hyowoon Seo, Jihong Park et al. “Federated Knowledge Distillation.” ArXiv](https://arxiv.org/abs/2011.02367)\n\n[221\\. A review on different techniques used to combat the non-iid ...](https://arxiv.org/html/2401.00809v1)\n\n[222\\. FedTweet: Two-fold Knowledge Distillation for non-IID Federated Learning](https://www.sciencedirect.com/science/article/abs/pii/S0045790623004913)\n\n[223\\. Efficient Federated Learning for AIoT Applications Using Knowledge Distillation](https://arxiv.org/pdf/2111.14347v1)\n\n[224\\. Anit Kumar Sahu, Tian Li et al. “Federated Optimization in Heterogeneous Networks.” arXiv: Learning](https://arxiv.org/abs/1812.06127)\n\n[225\\. Federated Learning on Virtual Heterogeneous Data with Local-Global Dataset Distillation](https://openreview.net/pdf/e166177fc47948a430b6470e8c95eac2c0678e17.pdf)\n\n[226\\. DEPTHFL: DEPTHWISE FEDERATED LEARNING FOR HETEROGENEOUS CLIENTS](https://openreview.net/pdf?id=pf8RIZTMU58)\n\n[227\\. FedCD: Personalized Federated Learning via Collaborative Distillation](https://hpc.ec.tuwien.ac.at/files/FedCD_UCC2022.pdf)\n\n[228\\. Communication-Efficient Federated Self-Distillation Method for Medical Image Segmentation](http://article.isciencegroup.com/pdf/10240074.pdf)\n\n[229\\. A Survey of Federated Learning on Non-IID Data](https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-cn/mediares/magazine/publication/com_en/article/202203/202203003.pdf)\n\n[230\\. One-shot Federated Learning via Synthetic Distiller-Distillate Communication](https://proceedings.neurips.cc/paper_files/paper/2024/file/ba0ad9d1e0c737800b2340b9cd68c208-Paper-Conference.pdf)\n\n[232\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[233\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[234\\. Anit Kumar Sahu, Tian Li et al. “Federated Optimization in Heterogeneous Networks.” arXiv: Learning](https://arxiv.org/abs/1812.06127)\n\n[235\\. Daliang Li, Junpu Wang. “FedMD: Heterogenous Federated Learning via Model Distillation.” ArXiv](https://arxiv.org/abs/1910.03581)\n\n[236\\. Eunjeong Jeong, Seungeun Oh et al. “Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data.” ArXiv](https://arxiv.org/abs/1811.11479)\n\n[237\\. Case studies in the modeling and control of continuous pharmaceutical manufacturing processes](https://dspace.mit.edu/bitstream/handle/1721.1/158315/maloney-phd-cheme-2021.pdf?sequence=-1&isAllowed=y)\n\n[238\\. FedCD: Personalized Federated Learning via Collaborative Distillation](https://hpc.ec.tuwien.ac.at/files/FedCD_UCC2022.pdf)\n\n[239\\. Q2 2025 Federated Computing Updates](https://www.rhinofcp.com/blog/1q25-federated-computing-update)\n\n[240\\. Data Lake Documentation: Real-World Case Studies](https://docs.data-lake.co/data-lake-chain-and-app/data-lake-app/case-studies)\n\n[241\\. Case Studies](https://sandbox.sebokwiki.org/index.php?title=Case_Studies&printable=yes)\n\n[242\\. Data-driven federated learning in drug discovery with ...](https://www.nature.com/articles/s42256-025-00991-2)\n\n[243\\. Security of Federated Learning: Attacks, Defensive Mechanisms, and Challenges](https://hal.science/hal-03620400v1/file/RIA%2013896%20-%20FL%20Security.pdf)\n\n[244\\. Framework for Co-distillation Driven Federated Learning to ...](https://www.chatpaper.com/chatpaper/zh-CN/paper/76643)\n\n[245\\. MetaFed: Federated Learning among Federations with Cyclic Knowledge Distillation for Personalized Healthcare](https://federated-learning.org/fl-ijcai-2022/Papers/FL-IJCAI-22_paper_10.pdf)\n\n[246\\. Lin Li, Jianping Gou et al. “Federated Distillation: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2404.08564)\n\n[247\\. Efficient Federated Learning for AIoT Applications Using Knowledge Distillation](https://arxiv.org/pdf/2111.14347v1)\n\n[248\\. MH-pFLID: Model Heterogeneous personalized Federated Learning via Injection and Distillation for Medical Data Analysis](http://www.paperreading.club/page?id=226615)\n\n[249\\. Knowledge Distillation in Federated Learning: A Practical Guide](https://arxiv.org/pdf/2211.04742)\n\n[250\\. Sufen Ren, Yule Hu et al. “Federated Distillation for Medical Image Classification: Towards Trustworthy Computer-Aided Diagnosis.”](https://arxiv.org/abs/2407.02261)\n\n[252\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[253\\. FDA Testbed for AI/ML-enabled Medical Devices](https://idsc.miami.edu/magazine/fda-testbed-for-ai-ml-enabled-medical-devices/)\n\n[254\\. FDA Regulation on Artificial Intelligence](https://finarbconsulting.com/blog-fda-regulation-on-artificial-intelligence)\n\n[255\\. Artificial intelligence in managing clinical trial design and conduct: Man and machine still on the learning curve?](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8011519/)\n\n[256\\. U.S. Food & Drug Administration](https://www.accessdata.fda.gov/cdrh_docs/pdf23/K232322.pdf)\n\n[257\\. Health AI for Good Rather Than Evil? The Need for a New Regulatory Framework for AI-Based Medical Devices](https://ideas.dickinsonlaw.psu.edu/cgi/viewcontent.cgi?article=1286&context=fac-works)\n\n[258\\. Alejandro Barredo Arrieta, Natalia Díaz Rodríguez et al. “Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI.” Inf. Fusion](https://doi.org/10.1016/j.inffus.2019.12.012)\n\n[259\\. Artificial Intelligence (AI)-Enabled Medical Devices – SFDA Regulatory Practice](https://ahwp.info/sites/default/files/No.6-1%20Artificial%20Intelligence%20Medical%20Device%20-Regulatory-%20Hala%20Alhodaib.pdf)\n\n[260\\. A. Karargyris, R. Umeton et al. “Federated benchmarking of medical artificial intelligence with MedPerf.” Nature machine intelligence](https://doi.org/10.1038/s42256-023-00652-2)\n\n[261\\. Exploring the Role of Reading Centers in the Era of Artificial Intelligence](https://www.ophth.wisc.edu/wp-content/uploads/2019/03/Ocular-Imaging-Reading-Centers-Annual-Meeting-2018-Retina-Times-1-1.pdf)\n\n[262\\. Many FDA-authorized AI devices lack validation data](https://www.medtechdive.com/news/nature-ai-medical-devices-clinical-evidence/726449/)\n\n[263\\. AI医学影像辅助决策软件FDA临床要求浅析](http://220.249.113.234:8088/KCMS/detail/detail.aspx?filename=YISZ202305002&dbcode=CHKJ&dbname=)\n\n[264\\. Federated Learning AI Approach Allows Hospitals to Share Patient Data Privately](https://healthcare-in-europe.com/en/news/federated-learning-ai-approach-allows-hospitals-to-share-patient-data-privately.html)\n\n[265\\. Regulating the Use of AI in Drug Approvals](https://uclajolt.com/wp-content/uploads/2024/09/JOLT29-2_Chen.pdf)\n\n[266\\. Nicola Rieke, Jonny Hancox et al. “The future of digital health with federated learning.” NPJ Digital Medicine](https://doi.org/10.1038/s41746-020-00323-1)\n\n[267\\. Diagnostic Accuracy of Artificial Intelligence in Glaucoma Screening and Clinical Practice](https://cdn-links.lww.com/permalink/ijg/a/ijg_2022_03_02_absdh_jog-d-21-0673_sdc1.docx)\n\n[268\\. 近半数AI医疗设备缺乏适当的FDA评估 - The HighWire](https://thehighwire.com/news/nearly-half-of-ai-medical-devices-lack-proper-fda-evaluation-new-report-reveals/)\n\n[269\\. Research Collective's Summary of FDA's AI-Enabled ...](https://research-collective.com/research-collectives-summary-of-fdas-ai-enabled-device-draft-guidance/)\n\n[272\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[273\\. FedCD: Personalized Federated Learning via Collaborative Distillation](https://hpc.ec.tuwien.ac.at/files/FedCD_UCC2022.pdf)\n\n[274\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[275\\. FedGMKD: An Efficient Prototype Federated Learning Framework through Knowledge Distillation and Discrepancy-Aware Aggregation](https://proceedings.neurips.cc/paper_files/paper/2024/file/d6520fa7f71dc8e09ed5939a60a64218-Paper-Conference.pdf)\n\n[276\\. Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Fine-Tuning_Global_Model_via_Data-Free_Knowledge_Distillation_for_Non-IID_Federated_CVPR_2022_paper.pdf)\n\n[277\\. DaFKD: Domain-aware Federated Knowledge Distillation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_DaFKD_Domain-Aware_Federated_Knowledge_Distillation_CVPR_2023_paper.pdf)\n\n[278\\. Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data](https://arxiv.org/pdf/1811.11479v1)\n\n[279\\. P. Kairouz, H. B. McMahan et al. “Advances and Open Problems in Federated Learning.” Found. Trends Mach. Learn.](https://doi.org/10.1561/2200000083)\n\n[280\\. Federated Learning with Extremely Noisy Clients via Negative Distillation](https://arxiv.org/pdf/2312.12703v1)\n\n[281\\. Federated Learning in Non-IID Settings Aided by Differentially Private Synthetic Data](https://openaccess.thecvf.com/content/CVPR2023W/FedVision/papers/Chen_Federated_Learning_in_Non-IID_Settings_Aided_by_Differentially_Private_Synthetic_CVPRW_2023_paper.pdf)\n\n[282\\. Anomaly detection and defense techniques in federated learning: a comprehensive review](https://link.springer.com/content/pdf/10.1007/s10462-024-10796-1.pdf)\n\n[283\\. Harmonizing Generalization and Personalization in Federated Prompt Learning](https://openreview.net/attachment?id=YYwERRXsJW&name=pdf)\n\n[284\\. Handling Non-IID Data in Federated Learning: An Experimental Evaluation Towards Unified Metrics](https://bth.diva-portal.org/smash/get/diva2:1832968/FULLTEXT01.pdf)\n\n[285\\. Xiaolin Zheng, Senci Ying et al. “Federated Learning on Non-iid Data via Local and Global Distillation.” 2023 IEEE International Conference on Web Services (ICWS)](https://doi.org/10.1109/ICWS60048.2023.00083)\n\n[286\\. Anit Kumar Sahu, Tian Li et al. “Federated Optimization in Heterogeneous Networks.” arXiv: Learning](https://arxiv.org/abs/1812.06127)\n\n[287\\. Efficient Federated Learning for AIoT Applications Using Knowledge Distillation](https://arxiv.org/pdf/2111.14347v1)\n\n[288\\. Chuhan Wu, Fangzhao Wu et al. “Communication-efficient federated learning via knowledge distillation.” Nature Communications](https://doi.org/10.1038/s41467-022-29763-x)\n\n[292\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[293\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[294\\. Tao Lin, Lingjing Kong et al. “Ensemble Distillation for Robust Model Fusion in Federated Learning.” ArXiv](https://arxiv.org/abs/2006.07242)\n\n[295\\. Daliang Li, Junpu Wang. “FedMD: Heterogenous Federated Learning via Model Distillation.” ArXiv](https://arxiv.org/abs/1910.03581)\n\n[296\\. Eunjeong Jeong, Seungeun Oh et al. “Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data.” ArXiv](https://arxiv.org/abs/1811.11479)\n\n[297\\. Data-driven federated learning in drug discovery with ...](https://www.nature.com/articles/s42256-025-00991-2)\n\n[298\\. Research Center for Ubiquitous Computing Systems Wins ...](http://english.ict.cas.cn/events/annu/202305/t20230526_331007.html)\n\n[299\\. Lin Li, Jianping Gou et al. “Federated Distillation: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2404.08564)\n\n[300\\. Minh-Duong Nguyen, Viet Quoc Pham et al. “Label driven Knowledge Distillation for Federated Learning with non-IID Data.”](https://arxiv.org/abs/2209.14520)\n\n[301\\. Sufen Ren, Yule Hu et al. “Federated Distillation for Medical Image Classification: Towards Trustworthy Computer-Aided Diagnosis.”](https://arxiv.org/abs/2407.02261)\n\n[302\\. Chunxu Zhang, Guodong Long et al. “Federated Adaptation for Foundation Model-based Recommendations.” ArXiv](https://doi.org/10.48550/arXiv.2405.04840)\n\n[303\\. Framework for Co-distillation Driven Federated Learning to ...](https://chatpaper.com/chatpaper/ja/paper/76643)\n\n[304\\. MH-pFLID: Model Heterogeneous personalized Federated Learning via Injection and Distillation for Medical Data Analysis](http://www.paperreading.club/page?id=226615)\n\n[305\\. Evaluating the Security Posture of Real-World FIDO2 Deployments](https://faculty.cc.gatech.edu/~frankli/papers/kuchhal_ccs23.pdf)\n\n[306\\. MetaFed: Federated Learning among Federations with Cyclic Knowledge Distillation for Personalized Healthcare](https://federated-learning.org/fl-ijcai-2022/Papers/FL-IJCAI-22_paper_10.pdf)\n\n[307\\. GitHub - Chung-ju/VFedTrans: Paper: Vertical Federated Knowledge ...](https://github.com/Chung-ju/VFedTrans)\n\n[308\\. Federated Distillation: A Survey | Papers With Code](https://paperswithcode.com/paper/federated-distillation-a-survey)\n\n[309\\. Federated Learning in Healthcare: Challenges and Opportunities](https://comp-path.bwh.harvard.edu/2024/05/)\n\n[312\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[313\\. P. Kairouz, H. B. McMahan et al. “Advances and Open Problems in Federated Learning.” Found. Trends Mach. Learn.](https://doi.org/10.1561/2200000083)\n\n[314\\. Federated Learning Based Artificial Intelligence Systems with Blockchain Security for Global Healthcare Collaboration and Patient Centric Data Privacy](https://www.atlantis-press.com/article/126011345.pdf)\n\n[315\\. DeepSeek AI: A comprehensive guide for enterprise implementation](https://aigc.idigital.com.cn/djyanbao/%E3%80%90DeepSeek%20AI%E3%80%91%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%9E%E6%96%BD%E5%85%A8%E9%9D%A2%E6%8C%87%E5%8D%97-2025-06-02.pdf)\n\n[316\\. Federated learning: a collaborative effort to achieve better ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC7779924/)\n\n[317\\. U.S. Food & Drug Administration](https://www.accessdata.fda.gov/cdrh_docs/pdf23/K232322.pdf)\n\n[318\\. Personalized Mixture of Experts for Multi-Site Medical Image Segmentation](https://openaccess.thecvf.com/content/WACV2025/papers/Rahman_Personalized_Mixture_of_Experts_for_Multi-Site_Medical_Image_Segmentation_WACV_2025_paper.pdf)\n\n[319\\. A Syntactic Approach for Privacy-Preserving Federated Learning](https://ecai2020.eu/papers/1591_paper.pdf)\n\n[320\\. A. Karargyris, R. Umeton et al. “Federated benchmarking of medical artificial intelligence with MedPerf.” Nature machine intelligence](https://doi.org/10.1038/s42256-023-00652-2)\n\n[321\\. Xiao Chen, Shunan Zhang et al. “Federated Data Model.” ArXiv](https://doi.org/10.48550/arXiv.2403.08887)\n\n[322\\. 中华人民共和国医药行业标准 人工智能医疗器械 质量要求和评价 第1部分:术语](https://www.cmde.org.cn/hbpdf/YY1833.1-2022.pdf)\n\n[323\\. Neutrosophic Sets and Systems](https://sciencesforce.com/NSS/NSS-66.pdf)\n\n[324\\. AI in healthcare case study: A research proposal involving the use of AI technologies in the diagnostic support and prevention of breast cancer](https://classroom.eneri.eu/sites/default/files/2024-12/Case%20study%20AI%20in%20healthcare%20%281%29.pdf)\n\n[325\\. FDA Testbed for AI/ML-enabled Medical Devices](https://idsc.miami.edu/magazine/fda-testbed-for-ai-ml-enabled-medical-devices/)\n\n[326\\. Nicola Rieke, Jonny Hancox et al. “The future of digital health with federated learning.” NPJ Digital Medicine](https://doi.org/10.1038/s41746-020-00323-1)\n\n[327\\. Medical Data Labeling: FDA compliance for AI](https://www.innovatiana.com/en/post/medical-data-labeling-compliant-with-fda)\n\n[328\\. AI & BI: THE FUTURE OF INTELLIGENT HEALTHCARE ANALYTICS](https://www.irjmets.com/uploadedfiles/paper/issue_3_march_2025/69945/final/fin_irjmets1742887652.pdf)\n\n[329\\. Micah J. Sheller, Brandon Edwards et al. “Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data.” Scientific Reports](https://doi.org/10.1038/s41598-020-69250-1)\n\n[332\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[333\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[334\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[335\\. Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Fine-Tuning_Global_Model_via_Data-Free_Knowledge_Distillation_for_Non-IID_Federated_CVPR_2022_paper.pdf)\n\n[336\\. Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data](https://arxiv.org/pdf/1811.11479v1)\n\n[337\\. Anit Kumar Sahu, Tian Li et al. “Federated Optimization in Heterogeneous Networks.” arXiv: Learning](https://arxiv.org/abs/1812.06127)\n\n[338\\. FedCD: Personalized Federated Learning via Collaborative Distillation](https://hpc.ec.tuwien.ac.at/files/FedCD_UCC2022.pdf)\n\n[339\\. Harmonizing Generalization and Personalization in Federated Prompt Learning](https://openreview.net/attachment?id=YYwERRXsJW&name=pdf)\n\n[340\\. Federated Learning with Extremely Noisy Clients via Negative Distillation](https://arxiv.org/pdf/2312.12703v1)\n\n[341\\. 结合知识蒸馏与互信息的多模态MRI疾病预后](https://cjig.cn/rc-pub/front/front-article/download/75674364/lowqualitypdf/%E7%BB%93%E5%90%88%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E4%B8%8E%E4%BA%92%E4%BF%A1%E6%81%AF%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81MRI%E7%96%BE%E7%97%85%E9%A2%84%E5%90%8E.pdf)\n\n[342\\. DEPTHFL: DEPTHWISE FEDERATED LEARNING FOR HETEROGENEOUS CLIENTS](https://openreview.net/pdf?id=pf8RIZTMU58)\n\n[343\\. Efficient Federated Learning for AIoT Applications Using Knowledge Distillation](https://arxiv.org/pdf/2111.14347v1)\n\n[344\\. Ensemble Distillation for Robust Model Fusion in Federated Learning](https://proceedings.neurips.cc/paper/2020/file/18df51b97ccd68128e994804f3eccc87-Paper.pdf)\n\n[345\\. Federated Learning in Non-IID Settings Aided by Differentially Private Synthetic Data](https://openaccess.thecvf.com/content/CVPR2023W/FedVision/papers/Chen_Federated_Learning_in_Non-IID_Settings_Aided_by_Differentially_Private_Synthetic_CVPRW_2023_paper.pdf)\n\n[346\\. Xiufang Shi, Wei Zhang et al. “Dataset Distillation-based Hybrid Federated Learning on Non-IID Data.”](https://arxiv.org/abs/2409.17517)\n\n[347\\. PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08492.pdf)\n\n[348\\. Sufen Ren, Yule Hu et al. “Federated Distillation for Medical Image Classification: Towards Trustworthy Computer-Aided Diagnosis.”](https://arxiv.org/abs/2407.02261)\n\n[352\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[353\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[354\\. P. Kairouz, H. B. McMahan et al. “Advances and Open Problems in Federated Learning.” Found. Trends Mach. Learn.](https://doi.org/10.1561/2200000083)\n\n[355\\. Tao Lin, Lingjing Kong et al. “Ensemble Distillation for Robust Model Fusion in Federated Learning.” ArXiv](https://arxiv.org/abs/2006.07242)\n\n[356\\. Federated Adaptation for Foundation Model-based Recommendations](https://www.ijcai.org/proceedings/2024/0603.pdf)\n\n[357\\. Eunjeong Jeong, Seungeun Oh et al. “Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data.” ArXiv](https://arxiv.org/abs/1811.11479)\n\n[358\\. MetaFed: Federated Learning among Federations with Cyclic Knowledge Distillation for Personalized Healthcare](https://federated-learning.org/fl-ijcai-2022/Papers/FL-IJCAI-22_paper_10.pdf)\n\n[359\\. Data-driven federated learning in drug discovery with ...](https://www.nature.com/articles/s42256-025-00991-2)\n\n[360\\. Research Center for Ubiquitous Computing Systems Wins ...](http://english.ict.cas.cn/events/annu/202305/t20230526_331007.html)\n\n[361\\. Lin Li, Jianping Gou et al. “Federated Distillation: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2404.08564)\n\n[362\\. Framework for Co-distillation Driven Federated Learning to ...](https://chatpaper.com/chatpaper/ja/paper/76643)\n\n[363\\. Server-Client Collaborative Distillation for Federated Reinforcement Learning](https://www.comp.hkbu.edu.hk/~ymc/papers/journal/TKDD-2023-01-0001-publication-version.pdf)\n\n[364\\. Evaluating the Security Posture of Real-World FIDO2 Deployments](https://faculty.cc.gatech.edu/~frankli/papers/kuchhal_ccs23.pdf)\n\n[365\\. Minh-Duong Nguyen, Viet Quoc Pham et al. “Label driven Knowledge Distillation for Federated Learning with non-IID Data.”](https://arxiv.org/abs/2209.14520)\n\n[366\\. GitHub - Chung-ju/VFedTrans: Paper: Vertical Federated Knowledge ...](https://github.com/Chung-ju/VFedTrans)\n\n[367\\. MH-pFLID: Model Heterogeneous personalized Federated Learning via Injection and Distillation for Medical Data Analysis](http://www.paperreading.club/page?id=226615)"
  },
  {
    "id": 43,
    "question": "What overlooked system architecture challenges need solving to fully realize AI's potential across cloud and edge computing?",
    "response": "**Research Report**\n\n**To:** Interested Parties\n\n**From:** Expert Researcher\n\n**Date:** July 27, 2025\n\n**Subject:** Unlocking AI's Potential: An Analysis of Overlooked System Architecture Challenges in Hybrid Cloud-Edge Environments\n\n**Executive Summary**\n\nAs of 2025, the deployment of Artificial Intelligence (AI) has moved beyond centralized cloud data centers into a complex, distributed paradigm spanning the cloud-edge continuum. While the potential of this hybrid model—combining the massive computational power of the cloud with the low-latency responsiveness of edge devices—is immense, its full realization is hindered by a series of deeply interconnected and often overlooked system architecture challenges. This report provides a comprehensive analysis of these challenges, drawing on recent findings to illuminate the critical problems that require novel solutions. The primary obstacles are not merely technological limitations but complex trade-offs at the intersection of system integration, performance optimization, and security. Key overlooked challenges include the profound difficulty of unifying fragmented and legacy systems, the cascading architectural implications of latency and bandwidth constraints, and the unique, expanded security vulnerabilities inherent in distributed AI. Addressing these challenges will require a fundamental shift towards holistic hardware-software co-design, advanced privacy-preserving paradigms like federated learning, and rigorous methods for quantifying the complex trade-offs between performance, accuracy, and security.\n\n**1\\. The Foundational Challenge: System Fragmentation and Integration Complexity**\n\nThe promise of cloud-edge AI rests on the seamless flow of data and computational workloads across a distributed infrastructure. However, the foundational challenge that many organizations face is the profound fragmentation of the underlying systems. This is not a simple interoperability issue but a multi-faceted problem that stifles scalability and innovation.\n\nA primary hurdle is the integration of modern AI platforms with entrenched legacy systems \\[7\\]\\[20\\]. These older systems, often critical to core business operations, were not designed for the flexibility and data velocity required by today's AI. This incompatibility creates stubborn data silos, where valuable information remains trapped, preventing the creation of unified data fabrics essential for training and deploying robust AI models \\[3\\]\\[15\\]. The result is a fragmented data landscape that limits the value of enterprise data assets \\[15\\].\n\nThis fragmentation is compounded by the inherent complexity of distributed architectures. Managing real-time data flows, complex AI/ML pipelines, and workloads spread across on-premises data centers, multiple public clouds, and a growing number of edge nodes requires highly specialized skills and sophisticated orchestration tools \\[15\\]. Without a unified management plane, organizations suffer from a lack of visibility into operations, ownership, and performance, making it exceedingly difficult to diagnose problems, manage costs, and ensure compliance \\[18\\]. This operational blindness is a significant and often underestimated barrier to scaling AI deployments effectively.\n\n**2\\. The Performance Conundrum: Navigating the Triad of Latency, Accuracy, and Cost**\n\nPerformance in a cloud-edge environment is not a single metric but a complex balancing act. The physical distance between the cloud and the edge introduces fundamental constraints of latency and bandwidth that dictate architectural choices and force difficult trade-offs.\n\n**2.1. The Architectural Dominance of Latency and Bandwidth**\n\nThe cloud, despite its immense processing power, is often ill-suited for applications requiring real-time responses due to network latency, which can range from 100-150ms for a round trip \\[17\\]\\[23\\]. For applications in autonomous vehicles, industrial automation, and interactive augmented reality, such delays are unacceptable \\[25\\]\\[25\\]. This has been the primary driver for adopting hybrid models where time-critical AI inference tasks are pushed to the edge \\[17\\]. Edge computing can slash this latency to as low as 5-25ms \\[23\\].\n\nConsequently, system architects are forced to design systems that intelligently partition AI workloads. Computationally intensive model training, which can process vast historical datasets, remains in the cloud, while latency-sensitive inference happens locally on edge devices \\[27\\]\\[33\\]. This partitioning strategy also mitigates bandwidth constraints, as processing data locally reduces the volume of raw data that must be transmitted to the cloud, lowering operational costs and improving privacy \\[21\\]\\[28\\]. However, this creates a new set of challenges, as edge devices are severely constrained in terms of compute power, memory, and energy \\[21\\]\\[24\\]\\[30\\].\n\n**2.2. The Inevitable Trade-off: Model Accuracy vs. Edge Efficiency**\n\nThe resource constraints of edge devices mean that large, state-of-the-art AI models often cannot be deployed directly. This reality has spurred the development of model optimization techniques, but each comes with a trade-off, most notably a potential degradation in model accuracy.\n\n**Model compression** is a critical technique for deploying complex models like Vision Transformers (ViTs) on resource-constrained hardware. Research shows a clear, quantifiable relationship between the compression ratio and accuracy loss. For example, aggressive pruning techniques can reduce a ViT's computational cost (FLOPs) and parameter count by 30-37%, making it viable for edge deployment, but this can come at the cost of a 7-10% drop in accuracy \\[143\\]. More sophisticated methods, such as patch slimming, demonstrate the nuanced nature of this trade-off, achieving a 40.4% reduction in FLOPs with only a marginal 0.4% accuracy decrease \\[155\\]. For a VTP model, pruning 20% of its dimensions results in a manageable 0.96% accuracy drop, which worsens to 1.92% at a 40% pruning rate \\[211\\].\n\nThis trade-off forces developers to make difficult decisions based on application requirements. For a latency-sensitive mobile application, a 10% accuracy loss might be an acceptable price for a one-third reduction in inference cost \\[143\\]. For a medical diagnostic tool, however, even a 1% drop in accuracy could be unacceptable. This decision calculus is a central, and often overlooked, challenge in designing effective edge AI systems. It highlights that there is no \"one-size-fits-all\" model; rather, models must be co-designed with the target hardware and application constraints in mind \\[207\\].\n\n**3\\. The Security and Privacy Imperative in Distributed AI**\n\nDistributing AI systems across the cloud-edge continuum dramatically expands the attack surface and introduces unique security and privacy vulnerabilities that are not present in centralized architectures. Addressing these vulnerabilities adds another layer of complexity and performance overhead.\n\n**3.1. An Expanded and Porous Attack Surface**\n\nThe decentralized nature of cloud-edge AI, with its thousands of dispersed endpoints, creates numerous entry points for malicious actors \\[45\\]\\[52\\]. Edge locations often lack the robust physical security of a traditional data center, making devices vulnerable to physical tampering \\[52\\].\n\nBeyond physical threats, the architecture itself creates new vectors for attack. In a distributed inference system, data and intermediate computations flow between nodes, creating opportunities for interception and manipulation \\[42\\]. A compromised or \"rogue\" index server could maliciously redirect data flows to an attacker-controlled node, leading to data leakage \\[41\\]\\[41\\]. Furthermore, AI models themselves are a target. Adversaries can mount attacks to tamper with model weights, poison training data, or execute adversarial attacks that manipulate a model's output with subtle, imperceptible changes to the input \\[45\\]\\[57\\]. Protecting user data privacy is also a major concern, as distributing data across nodes for processing creates risks of unauthorized access and reconstruction of sensitive information \\[41\\]\\[53\\].\n\n**3.2. The Hidden Cost: Performance Overhead of Security Protocols**\n\nSecuring these distributed systems is non-negotiable, but it is not without cost. The implementation of security protocols, particularly encryption, introduces computational overhead that directly translates to increased latency—a critical performance metric for the edge. This trade-off is a frequently overlooked system architecture challenge.\n\nFor instance, securing data in transit between the edge and cloud typically relies on protocols like Transport Layer Security (TLS). While modern versions like TLS 1.3 are optimized to reduce the latency of the initial handshake by minimizing round trips \\[181\\]\\[183\\]the cryptographic operations themselves still consume CPU cycles. The core of TLS encryption often relies on ciphers like AES-GCM, which, while highly efficient on modern hardware with dedicated acceleration (e.g., Intel AES-NI), can still introduce measurable latency \\[125\\]\\[132\\]. Without hardware acceleration, software-based AES-GCM can cause significant performance degradation and battery drain on resource-constrained edge devices \\[131\\].\n\nA critical and overlooked gap as of 2025 is the lack of standardized metrics and empirical benchmarks to quantify this trade-off precisely. While we can measure the number of security incidents \\[102\\] and the inference latency of a model \\[109\\] as separate metrics, there is little research providing direct quantitative data on the specific latency increase incurred by implementing protocols like TLS 1.3 during real-time AI inference on specific edge hardware like an NVIDIA Jetson Nano \\[182\\]\\[200\\]. This lack of data forces architects to rely on general principles rather than precise engineering calculations when balancing security requirements against performance budgets.\n\n**4\\. Emerging Architectural Solutions and Co-Design Strategies**\n\nSolving these interconnected challenges requires moving beyond siloed optimizations and embracing holistic system design philosophies. Emerging strategies focus on co-designing hardware and software, leveraging open standards, and adopting new distributed training paradigms.\n\n**4.1. Hardware-Software Co-Design and Intelligent Orchestration**\n\nThe most promising path forward is a tight integration of hardware and software design. This involves using specialized hardware accelerators for AI workloads, such as NVIDIA's Jetson platform or Intel's Movidius processors at the edge, in concert with AI-optimized cloud infrastructure \\[63\\]\\[69\\].\n\nHowever, co-design goes deeper than just using specialized chips. It involves creating performant inference engines with minimal components to maximize hardware efficiency \\[62\\] and employing software-level model optimization techniques like quantization and pruning to fit models within the power and memory envelopes of the hardware \\[63\\]\\[63\\]. Furthermore, AI itself is being used to solve these orchestration problems. Machine learning-driven schedulers can intelligently distribute and place AI workloads across a heterogeneous mix of hardware (CPUs, GPUs, NPUs) and locations (cloud, edge) based on real-time demands, cost, and latency requirements \\[63\\]\\[68\\]. Open-source frameworks like TensorFlow Lite, ONNX, and Apache TVM are also crucial for enabling this vision by providing a standardized layer that allows models to be run across diverse hardware stacks, mitigating the fragmentation problem \\[63\\]\\[73\\].\n\n**4.2. Federated Learning: A Paradigm for Privacy and Scale**\n\nFederated Learning (FL) has emerged as a powerful architectural pattern to address the challenges of data privacy and bandwidth limitations in distributed AI \\[83\\]\\[89\\]. In the FL model, a global AI model is trained collaboratively across numerous decentralized devices without the raw data ever leaving the local device. Instead, only anonymized model updates (e.g., gradients or parameters) are sent to a central server for aggregation \\[82\\]\\[91\\]. This fundamentally preserves data privacy by design \\[81\\].\n\nHowever, FL introduces its own set of architectural trade-offs. The most significant is a potential decrease in model accuracy compared to traditional centralized training, especially when the data across devices is not independent and identically distributed (Non-IID)—a common scenario in the real world \\[87\\]\\[92\\]\\[96\\].\n\nTo mitigate both privacy risks and accuracy degradation, FL architectures are becoming increasingly sophisticated.\n\n**Secure Aggregation (SA)** protocols are used in conjunction with the standard federated averaging (FedAvg) algorithm. SA uses cryptographic techniques like secure multi-party computation to allow the central server to compute the sum of the model updates without being able to see any individual update, thus preventing inference attacks on a user's contribution \\[168\\]\\[170\\]\\[165\\].\n\n**Differential Privacy** provides mathematical guarantees of privacy by adding precisely calibrated noise to the model updates, making it impossible to reverse-engineer an individual's data \\[87\\]\\[90\\]. However, this introduces a direct trade-off, as too much noise can degrade model accuracy \\[100\\].\n\nWhile these techniques are powerful, they add significant computational and communication overhead \\[169\\]\\[175\\]. As with security protocols, a major research gap exists in the form of comprehensive, empirical benchmarks. There is a lack of published data directly comparing the training time, communication cost, and final model accuracy of a standard FedAvg implementation versus one augmented with Secure Aggregation for training large language models like BERT on standard NLP benchmarks such as GLUE \\[222\\]\\[234\\]. Such benchmarks are essential for architects to make informed decisions about implementing these privacy-preserving technologies.\n\n**5\\. Conclusion**\n\nThe journey to fully realize the potential of AI across cloud and edge computing is less about a single breakthrough and more about diligently solving a web of interconnected system architecture challenges. The overlooked issues are not the obvious need for faster chips or bigger models, but the nuanced interplay between integration, performance, and security. The fragmentation of legacy and modern systems creates a brittle foundation. The physical laws of latency and bandwidth dictate a hybrid architecture fraught with difficult trade-offs between model accuracy and on-device efficiency. Finally, the distributed nature of this new paradigm creates a porous security perimeter that requires robust, and often costly, protective measures.\n\nThe path forward is clear: a holistic, co-design approach is paramount. Solutions cannot be developed in isolation. AI models must be designed with an awareness of the target hardware's constraints. Security protocols must be evaluated not only for their robustness but also for their performance impact. And new paradigms like federated learning must be embraced to solve the fundamental challenges of data privacy and sovereignty. Continued research and, critically, the development of standardized metrics and empirical benchmarks to quantify these complex trade-offs will be the key to navigating these challenges and unlocking the next wave of AI-driven innovation.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. 计算2030](https://www-file.huawei.com/-/media/corp2020/pdf/giv/2024/computing_2030_cn.pdf)\n\n[2\\. 智能世界2030](https://www-file.huawei.com/-/media/corp2020/pdf/giv/2024/intelligent_world_2030_2024_cn.pdf)\n\n[3\\. 云计算2030](https://www-file.huawei.com/-/media/corp2020/pdf/giv/2024/cloud_computing_whitepaper_2030_cn.pdf)\n\n[4\\. The State of AI Infrastructure at Scale 2024: Unveiling Future Landscapes, Key Insights, and Business Benchmarks](https://ai-infrastructure.org/wp-content/uploads/2024/03/The-State-of-AI-Infrastructure-at-Scale-2024.pdf)\n\n[5\\. Xing Hao, Guigang Zhang et al. “Deep Learning.” Int. J. Semantic Comput.](https://doi.org/10.1142/S1793351X16500045)\n\n[6\\. Overcoming Technical Hurdles in AI Deployment](https://www.alphabold.com/overcoming-technical-hurdles-in-ai-deployment/)\n\n[7\\. Cloudera 2025 代理AI 调查报告：57% 企业已部署](https://i-newcar.com/index.php?m=home&c=View&a=index&aid=4286)\n\n[8\\. Unlocking AI and Generative AI Use Cases with AI-Ready Hybrid Cloud Infrastructure](https://cdrdv2-public.intel.com/833841/Nutanix%20CP%20AI%20WP.pdf)\n\n[9\\. AI and Machine Learning for Predictive Performance Engineering in Cloud Platforms](https://www.ijirset.com/upload/2024/august/160_AI.pdf)\n\n[10\\. Futurum Research 2025 Key Issues & Predictions](https://futurumgroup.com/wp-content/uploads/2025/04/Futurum-Research-2025-Key-Issues-Predictions-04112025.pdf)\n\n[11\\. 25+ AI Data Center Statistics & Trends (2025 Updated)](https://thenetworkinstallers.com/blog/ai-data-center-statistics/)\n\n[12\\. AI / ML Reference Architecture Overview](https://www.f5.com/pdf/reference-guide/f5-ai-reference-architecture.pdf)\n\n[13\\. Challenges and Future Directions in AI-Enabled Cloud Security](https://www.irjet.net/archives/V11/i10/IRJET-V11I1030.pdf)\n\n[14\\. How to Overcome the Challenges of Extending AI Applications Anywhere](https://www.nutanix.com/content/dam/nutanix/en/resources/ebook/eb-ai-mod-apps.pdf)\n\n[15\\. System Integration Challenges in 2025 & their solution](https://www.zigiwave.com/resources/system-integration-challenges-2025)\n\n[16\\. The 2025 Edge AI Technology Report](https://www.hkdca.com/wp-content/uploads/2025/06/edge-ai-technology-report-2025.pdf)\n\n[17\\. Edge vs Cloud in 2025: Why AI Needs Compute Closer to the ...](https://www.techi.com/edge-vs-cloud-in-2025-ai-compute-shift/)\n\n[18\\. Hybrid Computing in 2025: No Longer Just About Cloud vs. On-Prem](https://www.melillo.com/2025/05/14/hybrid-computing-in-2025-no-longer-just-about-cloud-vs-on-prem/#:~:text=96%25%20of%20businesses%20use%20public%20cloud%20in%202025.&text=75%25%20of%20enterprise%20data%20is,real-time%20processing%20and%20automation.)\n\n[19\\. Top 3 Most Common Threats Accelerated by Hybrid Cloud Environments](https://www.vectra.ai/blog/top-3-most-common-threats-accelerated-by-hybrid-cloud-environments)\n\n[20\\. 9 AI problems in 2025: Common challenges and solutions](https://lumenalta.com/insights/ai-problems-9-common-challenges-and-solutions)\n\n[21\\. Finding the balance between edge AI vs. cloud AI](https://www.techtarget.com/searchenterpriseai/feature/Finding-the-balance-between-edge-AI-vs-cloud-AI?track=NL-1816&ad=937223&asrc=EM_NLN_143764699&utm_medium=EM&utm_source=NLN&utm_campaign=20201231_The%20differences%20between%20data%20warehouses%20and%20data%20lakes)\n\n[22\\. AI and Machine Learning for Predictive Performance Engineering in Cloud Platforms](https://www.ijirset.com/upload/2024/august/160_AI.pdf)\n\n[23\\. Edge computing and ai for real-time enterprise innovation: Transforming business operations through low-latency analytics](https://journalwjarr.com/sites/default/files/fulltext_pdf/WJARR-2025-1180.pdf)\n\n[24\\. Edge AI Explained in 5 Minutes or Less](https://geekflare.com/edge-ai/)\n\n[25\\. Edge Computing vs. Cloud Computing: A Comparative Analysis for Real-Time AI Applications](https://www.ijfmr.com/papers/2024/5/29316.pdf)\n\n[26\\. The State of Edge AI](https://www.hkdca.com/wp-content/uploads/2024/10/state-of-edge-ai.pdf)\n\n[27\\. The 2025 Edge AI Technology Report](https://www.ceva-ip.com/wp-content/uploads/2025-Edge-AI-Technology-Report.pdf)\n\n[28\\. Edge AI: Definitions, Advantages, Use Cases](https://www.nutanix.com/info/artificial-intelligence/edge-ai)\n\n[29\\. Edge technologies in the automotive industry: An experimental latency evaluation of AWS Greengrass usage](https://odr.chalmers.se/server/api/core/bitstreams/9eb3a8af-3908-414a-b54c-821a063d11f3/content)\n\n[30\\. Edge AI & Edgification of Models](https://namla.cloud/blog/edge-ai-and-edgification-of-models)\n\n[31\\. Information Fusion on Delivery: A Survey on the Roles of Mobile Edge Caching Systems](https://www.uclab.re.kr/publications_openaccesspdf/nguyen_elsevierif2023_open.pdf)\n\n[32\\. AIOTI Strategic Research and Innovation Agenda](https://aioti.eu/wp-content/uploads/2023/01/AIOTI-SRIA-Final.pdf)\n\n[33\\. Yuanming Shi, Kai Yang et al. “Communication-Efficient Edge AI: Algorithms and Systems.” IEEE Communications Surveys & Tutorials](https://doi.org/10.1109/COMST.2020.3007787)\n\n[34\\. AI INERENCE AT THE EDGE](https://upcommons.upc.edu/bitstream/handle/2117/427108/192715.pdf?sequence=2&isAllowed=y)\n\n[35\\. Weisong Shi, Jie Cao et al. “Edge Computing: Vision and Challenges.” IEEE Internet of Things Journal](https://doi.org/10.1109/JIOT.2016.2579198)\n\n[36\\. 6G Native AI Architecture and Technologies White Paper (2022)](https://www.zgc-xnet.com/6g/6G%20Native%20AI%20Architecture%20and%20Technologies.pdf)\n\n[41\\. Security Risks of AI Hardware for Personal and Edge Computing Devices](https://www.nccgroup.com/media/glzdsluo/ncc_group_google_e010491_report_2024-07-12.pdf)\n\n[42\\. The 2025 Edge AI Technology Report](https://www.hkdca.com/wp-content/uploads/2025/06/edge-ai-technology-report-2025.pdf)\n\n[43\\. AI Security Protecting AI models in the cloud and on edge](https://www.valoremreply.com/resources/insights/blog/2021/november/ai-security-protecting-ai-models-in-the-cloud-and-on-the-edge/)\n\n[44\\. Intelligent Edge Computing for IOT: AI-Powered Decision Making at the Edge](https://www.ijraset.com/best-journal/intelligent-edge-computing-for-iot-aipowered-decision-making-at-the-edge)\n\n[45\\. 2030 年人工智慧基础设施市场预测：按产品、部署模式、技术](https://cn.gii.tw/report/smrc1725195-ai-infrastructure-market-forecasts-global-analysis.html)\n\n[46\\. Edge Computing with Artificial Intelligence: A Machine Learning Perspective](http://elop.org.cn/elopriit/journal/jcao_j_csur.pdf)\n\n[47\\. Reference Architecture #1 - Cloud-to-Edge AI Integration](https://www.vamsitalkstech.com/edge-computing/architecting-intelligent-edge-reference-architecture-1-cloud-to-edge-ai-integration/)\n\n[48\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[49\\. Reducing AI's Vulnerable Attack Surface with Edge Computing](https://www.wevolver.com/article/reducing-ais-vulnerable-attack-surface-with-edge-computing)\n\n[50\\. AI EdgeLabs与ZEDEDA的战略合作](https://edgelabs.ai/blog/partnership-with-zededa/)\n\n[51\\. 人工智能应用系统内生安全蓝皮书（2024年）](https://hrssit.cn/Uploads/file/20241202/1733111352347868.pdf)\n\n[52\\. Securing the Edge: Tackling Distributed Security Challenges](https://avassa.io/articles/securing-the-edge-tackling-distributed-security-challenges/)\n\n[53\\. The State of Edge AI](https://www.hkdca.com/wp-content/uploads/2024/10/state-of-edge-ai.pdf)\n\n[54\\. Jing Liu, Yao Du et al. “Edge-Cloud Collaborative Computing on Distributed Intelligence and Model Optimization: A Survey.”](https://arxiv.org/abs/2505.01821)\n\n[55\\. Distributed AI-defense for Cyber Threats on Edge Computing Systems](https://rrpress.utsa.edu/items/848f2f6e-9866-4b6f-a42f-014bdd4b5b2e)\n\n[56\\. AI ML and cloud computing: exploring models, challenges and opportunities](https://journalwjarr.com/sites/default/files/fulltext_pdf/WJARR-2025-0430.pdf)\n\n[57\\. The Challenges of Deploying AI to the Edge](https://www.sealevel.com/the-challenges-of-deploying-ai-edge-computing/?srsltid=AfmBOoqvhnrgzyb_vBNzpODcnouxZohkhBYyiFvJl77phLG_BcG19h85)\n\n[61\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[62\\. Optimizing the AI Stack for the Age of Inferencing: Maximizing Infrastructure Utility for Scalable AI](https://4149027.fs1.hubspotusercontent-na1.net/hubfs/4149027/WSAICA25%20speaker%20ppts/15_25%20-%2015_45%2C%20Yujing%20Qian.pptx.pdf)\n\n[63\\. The 2025 Edge AI Technology Report](https://www.ceva-ip.com/wp-content/uploads/2025-Edge-AI-Technology-Report.pdf)\n\n[64\\. Review, Tutorials and Introduction to Cloud Platforms for Agentic GenAI: A Comparative Studies](https://www.ijariit.com/manuscripts/v11i1/V11I1-1395.pdf)\n\n[65\\. IPCEI-CIS Progress Report 2025](https://www.bmwk.de/Redaktion/DE/Downloads/I/ipcei-cis-progress-report-2025.pdf?__blob=publicationFile&v=4)\n\n[66\\. EFFICIENT AI HARDWARE ACCELERATION](https://www.ideals.illinois.edu/items/126747/bitstreams/414224/data.pdf)\n\n[67\\. Exploring Edge AI Inference in Heterogeneous Environments: Requirements, Challenges, and Solutions](https://researchportal.helsinki.fi/files/287133168/Exploring_Edge_AI_Inference_in_Heterogeneous_Environments_Requirements_Challenges_and_Solutions.pdf)\n\n[68\\. HiPEAC Vision 2025: High Performance, Edge and Cloud Computing](https://vision.hipeac.net/pdf/hipeac-vision-2025.pdf)\n\n[69\\. AI Hardware in 2025: The Ultimate Guide to Cutting-Edge ...](https://www.humai.blog/ai-hardware-in-2025-the-ultimate-guide-to-cutting-edge-gadgets-and-innovations/)\n\n[70\\. AI and Machine Learning for Predictive Performance Engineering in Cloud Platforms](https://www.ijirset.com/upload/2024/august/160_AI.pdf)\n\n[71\\. Weka 推出 Neuralmesh：人工智能創新的智能自適應基礎，並為推理時代而有目的地建立](https://wire.expertini.com/article/weka-tui-chu-neuralmesh-ren-gong-zhi-neng-chuang-xin-de-zhi-neng-zi-shi-ying-ji-chu-bing-wei-tui-li-shi-dai-er-you-mu-de-di-jian-li--2025-06-18.pdf)\n\n[72\\. Unlock the Future of AI with Heterogeneous Computing](https://newsroom.arm.com/blog/unlock-the-future-of-ai-with-heterogeneous-computing)\n\n[73\\. AI engine of innovation 2025](https://www.itsprodigy.com/en/news/2025-06-18-ai-engine-of-innovation-2025/)\n\n[74\\. Best AI Development Frameworks In 2025](https://savvycomsoftware.com/blog/ai-development-framework/)\n\n[75\\. 2025年数据中心五大趋势](https://www.ichub.com/srvContent/view?columnType=4&id=1046691805922230273&column_id=788622946322448385)\n\n[76\\. Weisong Shi, Jie Cao et al. “Edge Computing: Vision and Challenges.” IEEE Internet of Things Journal](https://doi.org/10.1109/JIOT.2016.2579198)\n\n[81\\. Privacy-Enhancing and Privacy-Preserving Technologies in AI: Enabling Data Use and Operationalizing Privacy by Design and Default](https://www.informationpolicycentre.com/uploads/5/7/1/0/57104281/cipl_pets_and_ppts_in_ai_mar25.pdf)\n\n[82\\. Federated Learning: A Privacy-Preserving Approach to ...](https://www.netguru.com/blog/federated-learning)\n\n[83\\. Federated Learning Enhances Data Privacy](https://connectcx.ai/federated-learning-enhances-data-privacy/)\n\n[84\\. The convergence of AI and SAP: A technical deep dive into enterprise innovation](https://journalwjaets.com/sites/default/files/fulltext_pdf/WJAETS-2025-0421.pdf)\n\n[85\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[86\\. 联邦学习的隐私保护与安全防御研究综述](http://cjc.ict.ac.cn/online/onlinepaper/xiaox-2023429163623.pdf)\n\n[87\\. What is the trade-off between model accuracy and privacy ...](https://milvus.io/ai-quick-reference/what-is-the-tradeoff-between-model-accuracy-and-privacy-in-federated-learning)\n\n[88\\. Privacy-Preserving Search Systems: A Comprehensive Analysis of Advanced Techniques and Real-World Implementations](https://www.ijfmr.com/papers/2024/6/34087.pdf)\n\n[89\\. The 2025 Edge AI Technology Report](https://www.ceva-ip.com/wp-content/uploads/2025-Edge-AI-Technology-Report.pdf)\n\n[90\\. Federated Learning for AI-Powered Privacy in Distributed ...](https://www.ijcesen.com/index.php/ijcesen/article/view/2505)\n\n[91\\. Generative AI model privacy: a survey](https://web.comp.polyu.edu.hk/csbxiao/paper/2024/AI-privacy-review.pdf)\n\n[92\\. Efficient Federated and Privacy-preserving Machine Learning](https://mediatum.ub.tum.de/doc/1732268/mykpuqvdgwrvevyneka0yt2vy.Reza_Phd_Dissertation_Final.pdf)\n\n[93\\. A Comprehensive Research of Data Privacy Based on Federated Learning](https://www.scitepress.org/Papers/2024/128325/128325.pdf)\n\n[94\\. An Alternative Approach to Federated Learning for Model Security and Data Privacy](https://www.scitepress.org/Papers/2025/132375/132375.pdf)\n\n[95\\. Scalable Backend Architecture for New Age AI-Powered SaaS Applications: Supporting Billions of Users](https://www.jetir.org/papers/JETIR2502131.pdf)\n\n[96\\. Yue Zhao, Meng Li et al. “Federated Learning with Non-IID Data.” ArXiv](https://doi.org/10.48550/arXiv.1806.00582)\n\n[97\\. Unlocking the Power of Federated and Transfer Learning: A Unified Architecture for Enhanced Privacy and Scalability in AI](https://austinpublishinggroup.com/business-administration-and-management/fulltext/ajbam-v8-id1077.pdf)\n\n[98\\. Robin C. Geyer, T. Klein et al. “Differentially Private Federated Learning: A Client Level Perspective.” ArXiv](https://arxiv.org/abs/1712.07557)\n\n[99\\. Permissioned Distributed Ledger (PDL); Artificial Intelligence for Permissioned Distributed Ledger](https://docbox.etsi.org/ISG/PDL/Open/0032_AI_4PDL/PDL-0032_AI_4PDLv008.docx)\n\n[100\\. Synergizing Intelligence and Privacy: A Review of Integrating Internet of Things, Large Language Models, and Federated Learning in Advanced Networked Systems](https://www.preprints.org/manuscript/202504.2082/v1/download)\n\n[101\\. Edge computing and ai for real-time enterprise innovation: Transforming business operations through low-latency analytics](https://journalwjarr.com/sites/default/files/fulltext_pdf/WJARR-2025-1180.pdf)\n\n[102\\. The KPIs That Matter for Generative AI, Edge & Quantum](https://hypersense-software.com/blog/2025/07/19/kpis-generative-ai-edge-quantum-emerging-tech/)\n\n[103\\. Yuyi Mao, Changsheng You et al. “A Survey on Mobile Edge Computing: The Communication Perspective.” IEEE Communications Surveys & Tutorials](https://doi.org/10.1109/COMST.2017.2745201)\n\n[104\\. Zhi Zhou, Xu Chen et al. “Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing.” Proceedings of the IEEE](https://doi.org/10.1109/JPROC.2019.2918951)\n\n[105\\. AI at the Edge: Solving Real-World Problems with Embedded Machine Learning](https://6371311.fs1.hubspotusercontent-na1.net/hubfs/6371311/Content%20and%20Docs/AI%20at%20the%20Edge%20-%20full%20book%20-%20compressed.pdf)\n\n[106\\. Edge AI Systems: Achieving High Accuracy and Low Error Rates in Real-Time Data Processing on Edge Devices](https://www.ijirset.com/upload/2023/september/111_Edge%20AI.pdf)\n\n[107\\. Communication-Efficient Edge AI Inference over Wireless Networks](https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-cn/mediares/magazine/publication/com_en/article/202002/202002005.pdf)\n\n[108\\. Pavel Mach, Zdenek Becvar. “Mobile Edge Computing: A Survey on Architecture and Computation Offloading.” IEEE Communications Surveys & Tutorials](https://doi.org/10.1109/COMST.2017.2682318)\n\n[109\\. Adaptive neural networks for edge intelligence](https://dr.ntu.edu.sg/bitstream/10356/172595/2/MyThesis.pdf)\n\n[110\\. Jaskirat Singh, Bram Adams et al. “On the Impact of Black-box Deployment Strategies for Edge AI on Latency and Model Performance.” ArXiv](https://doi.org/10.48550/arXiv.2403.17154)\n\n[111\\. Microservices vs. Monolithic Architectures: Strategic Trade-Offs in the AI Era](https://ijsrcseit.com/index.php/home/article/download/CSEIT251112269/CSEIT251112269/1862)\n\n[112\\. Wireless Sensing and Networking for the Internet of Things](https://mdpi-res.com/bookfiles/book/7236/Wireless_Sensing_and_Networking_for_the_Internet_of_Things.pdf?v=1738116439)\n\n[113\\. A Survey of AI Agent Protocols](https://arxiv.org/pdf/2504.16736)\n\n[114\\. AI and Machine Learning for Predictive Performance Engineering in Cloud Platforms](https://www.ijirset.com/upload/2024/august/160_AI.pdf)\n\n[115\\. Deploying AI on Edge: Advancement and Challenges in ...](https://www.mdpi.com/2227-7390/13/11/1878)\n\n[116\\. Surat Teerapittayanon, Bradley McDanel et al. “Distributed Deep Neural Networks Over the Cloud, the Edge and End Devices.” 2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)](https://doi.org/10.1109/ICDCS.2017.226)\n\n[117\\. Surat Teerapittayanon, Bradley McDanel et al. “BranchyNet: Fast inference via early exiting from deep neural networks.” 2016 23rd International Conference on Pattern Recognition (ICPR)](https://doi.org/10.1109/ICPR.2016.7900006)\n\n[121\\. TLS 1.3 Performance Analysis – Throughput](https://www.wolfssl.com/tls-1-3-performance-analysis-throughput/)\n\n[122\\. Apache Kafka Optimization & Benchmarking Guide](https://www.intel.cn/content/www/cn/zh/developer/articles/guide/kafka-optimization-and-benchmarking-guide.html)\n\n[123\\. K. Bhargavan, Antoine Delignat-Lavaud et al. “Edinburgh Research Explorer Implementing and Proving the TLS 1.3 Record Layer.”](https://www.semanticscholar.org/paper/3f4ec516aa4eaf52019a2bfced52e786190e5dc2)\n\n[124\\. Implementing and Proving the TLS 1.3 Record Layer](https://eprint.iacr.org/2016/1178.pdf)\n\n[125\\. AES-GCM、ECDH、HMAC-SHA256与零知识证明-云社区](https://bbs.huaweicloud.cn/blogs/455180)\n\n[126\\. M. Dworkin. “Recommendation for Block Cipher Modes of Operation: Galois/Counter Mode (GCM) and GMAC.”](https://doi.org/10.6028/NIST.SP.800-38D)\n\n[127\\. IETF QUIC Acceleration using Intel® QuickAssist Technology (Intel® QAT)](https://cdrdv2-public.intel.com/787600/IETF%20QUIC%20Acceleration%20using%20Intel%C2%AE%20QuickAssist%20Technology%20-Intel%20QAT.pdf)\n\n[128\\. The Multi-User Security of Authenticated Encryption: AES-GCM in TLS 1.3](https://eprint.iacr.org/2016/564.pdf)\n\n[129\\. TLS 1.2 vs. 1.3—Handshake, Performance, and Other ...](https://www.catchpoint.com/http2-vs-http3/tls1-2-vs-1-3#:~:text=As%20discussed%20in%20the%20previous,and%20the%20selected%20cipher%20suites.)\n\n[130\\. Tim Dierks, E. Rescorla. “The Transport Layer Security (TLS) Protocol Version 1.2.” RFC](https://doi.org/10.17487/RFC5246)\n\n[131\\. Analysis of AES-GCM Cipher Suites in TLS](https://link.springer.com/chapter/10.1007/978-3-319-68385-0_9)\n\n[132\\. SmartDIMM: In-Memory Acceleration of Upper Layer Protocols](https://alian.csl.cornell.edu/papers/smartdimm-hpca24.pdf)\n\n[133\\. Robert Lychev, Samuel Jero et al. “How Secure and Quick is QUIC? Provable Security and Performance Analyses.” 2015 IEEE Symposium on Security and Privacy](https://doi.org/10.1109/SP.2015.21)\n\n[134\\. Status of This Memo The Transport Layer Security (TLS) Protocol.](https://www.semanticscholar.org/paper/7cbb18046cc39a85b027dcb38162b1a68f561728)\n\n[135\\. ...Less Latency - Improved Handshakes in TLS version 1.3](https://timtaubert.de/blog/2015/11/more-privacy-less-latency-improved-handshakes-in-tls-13/)\n\n[141\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[142\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[143\\. Compression of vision transformer models with AIminify](https://aiminify.com/compressing-vision-attention-models-with-aiminfy/)\n\n[144\\. Geoffrey E. Hinton, O. Vinyals et al. “Distilling the Knowledge in a Neural Network.” ArXiv](https://arxiv.org/abs/1503.02531)\n\n[145\\. Ze Liu, Yutong Lin et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00986)\n\n[146\\. OATS: OUTLIER-AWARE PRUNING THROUGH SPARSE AND LOW RANK DECOMPOSITION](https://openreview.net/pdf/80939b0c99d88d6d9fffad4fbeb35719cd663617.pdf)\n\n[147\\. Optimizing Transformer Models for Resource-Constrained Environments: A Study on Compression Techniques for Edge Computing](https://hal.science/hal-04670072/document)\n\n[148\\. Compressing Vision Transformers for Low-Resource Visual ...](https://paperreading.club/page?id=181669)\n\n[149\\. Approximating vision transformers for edge: variational inference and mixed-precision for multi-modal data](https://research.tudelft.nl/files/240702219/s00607-025-01427-w.pdf)\n\n[150\\. Object detection in video surveillance using MobileNetV2 on resource-constrained low-power edge devices](https://beei.org/index.php/EEI/article/download/8131/4043)\n\n[151\\. MiniViT: Compressing Vision Transformers with Weight Multiplexing](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_MiniViT_Compressing_Vision_Transformers_With_Weight_Multiplexing_CVPR_2022_paper.pdf)\n\n[152\\. Hugo Touvron, M. Cord et al. “Training data-efficient image transformers & distillation through attention.” International Conference on Machine Learning](https://arxiv.org/abs/2012.12877)\n\n[153\\. Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy](https://paperreading.club/page?id=208389)\n\n[154\\. Vision Transformers 2: Quantization and Sparsity - Myrtle.ai](https://myrtle.ai/resources/leo-2-quantization-and-sparsity/)\n\n[155\\. Patch Slimming for Efficient Vision Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Patch_Slimming_for_Efficient_Vision_Transformers_CVPR_2022_paper.pdf)\n\n[156\\. Evaluating and Accelerating Vision Transformers on GPU-based Embedded Edge AI Systems](https://assets-eu.researchsquare.com/files/rs-5083258/v1_covered_7dcbed04-40c0-4c57-8c47-02fc11107836.pdf?c=1733374368)\n\n[161\\. Eluding Secure Aggregation in Federated Learning via Model Inconsistency](https://arxiv.org/pdf/2111.07380v4)\n\n[162\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[163\\. On Using Secure Aggregation in Differentially Private Federated Learning with Multiple Local Steps](https://openreview.net/pdf?id=uxyWlXPuIg)\n\n[164\\. Feature Norm Regularized Federated Learning: Utilizing Data Disparities for Model Performance Gains](https://www.ijcai.org/proceedings/2024/0457.pdf)\n\n[165\\. SAFELearn: Secure Aggregation for private FEderated Learning (Full Version)\\*](https://eprint.iacr.org/2021/386.pdf)\n\n[166\\. P. Kairouz, H. B. McMahan et al. “Advances and Open Problems in Federated Learning.” Found. Trends Mach. Learn.](https://doi.org/10.1561/2200000083)\n\n[167\\. SAFER: Sparse Secure Aggregation for Federated Learning](https://www.owkin.com/publications/safer-sparse-secure-aggregation-for-federated-learning)\n\n[168\\. The Distributed Discrete Gaussian Mechanism for Federated Learning with Secure Aggregation](https://proceedings.mlr.press/v139/kairouz21a/kairouz21a-supp.pdf)\n\n[169\\. Efficient Differentially Private Secure Aggregation for Federated Learning via Hardness of Learning with Errors](https://www.usenix.org/system/files/sec22-stevens.pdf)\n\n[170\\. SPFL: a self-purified federated learning method against poisoning attacks](https://dr.ntu.edu.sg/bitstream/10356/180384/2/SPFL_TIFS.pdf)\n\n[171\\. DESIGN, IMPLEMENTATION, AND ANALYSIS OF A CLOUD FEDERATED LEARNING ARCHITECTURE](https://upcommons.upc.edu/bitstream/handle/2117/401815/179029.pdf;jsessionid=CB9DCC6DE91BB9FA5334A901A82E4A47?sequence=2)\n\n[172\\. ACORN: Input Validation for Secure Aggregation](https://eprint.iacr.org/2022/1461.pdf)\n\n[173\\. Advances, Challenges & Recent Developments in Federated Learning](https://www.scirp.org/pdf/oalib2024%2011null_1112239.pdf)\n\n[174\\. INTRODUCTION TO FEDERATED LEARNING](http://researchers.lille.inria.fr/abellet/talks/federated_learning_introduction.pdf)\n\n[175\\. ...Aggregation for Privacy-Preserving Federated Learni...](https://ui.adsabs.harvard.edu/abs/arXiv:2009.11248)\n\n[176\\. Yunbo Li, Jiaping Gui et al. “An Experimental Study of Different Aggregation Schemes in Semi-Asynchronous Federated Learning.” ArXiv](https://doi.org/10.48550/arXiv.2405.16086)\n\n[177\\. Federated Learning for Healthcare: A Comprehensive Review - MDPI](https://www.mdpi.com/2673-4591/59/1/230)\n\n[181\\. Analyzing TLS 1.3 Features on Popular Websites](https://www.duo.uio.no/bitstream/handle/10852/112564/Analyzing_TLS13_Features_On_Popular_Websites.pdf?sequence=1&isAllowed=y)\n\n[182\\. Floating inference time on NVIDIA Jetson Nano #21286](https://github.com/pytorch/pytorch/issues/21286)\n\n[183\\. TLSv1.2 vs TLSv1.3: A Deep Dive into Handshake Efficiency](https://www.go-euc.com/tls12-vs-tlsv13-a-deep-dive-into-handshake-efficiency/)\n\n[184\\. Overview and Analysis of Overhead Caused by TLS - IETF Datatracker](https://datatracker.ietf.org/doc/draft-mattsson-uta-tls-overhead/00/)\n\n[185\\. Seamless Transition to Post-Quantum TLS 1.3: A Hybrid Approach Using Identity-Based Encryption](https://inspirehep.net/files/cdf982882a99103e49baf67ed2b4b7da)\n\n[186\\. TLS 1.2 vs. 1.3—Handshake, Performance, and Other ...](https://www.catchpoint.com/http2-vs-http3/tls1-2-vs-1-3#:~:text=As%20discussed%20in%20the%20previous,and%20the%20selected%20cipher%20suites.)\n\n[187\\. The impact of data-heavy, post-quantum TLS 1.3 on the Time-To-Last-Byte of real-world connections](https://eprint.iacr.org/2024/176.pdf)\n\n[188\\. AI inference using Images, RTSP Video Stream on NVIDIA Jetson Nano ...](https://www.cnx-software.com/2019/12/07/rtsp-stream-nvidia-jetson-nano-devkit/)\n\n[189\\. Dimitrios Sikeridis, Panos Kampanakis et al. “Post-Quantum Authentication in TLS 1.3: A Performance Study.” IACR Cryptol. ePrint Arch.](https://doi.org/10.14722/ndss.2020.24203)\n\n[190\\. Everything About TLS 1.3: TLS 1.2 Vs TLS 1.3 Difference](https://certera.com/blog/tls-1-3-everything-you-need-to-know/)\n\n[191\\. Petter Solnør. “A Cryptographic Toolbox for Feedback Control Systems.”](https://doi.org/10.4173/mic.2020.4.3)\n\n[192\\. TLS 1.3: An Overview of Benefits and Risks](https://www.fortinet.com/blog/business-and-technology/tls-is-here-what-this-means-for-you?utm_source=blog&utm_campaign=2020-q4-tls-is-here)\n\n[193\\. What is TLS 1.3? What to Know About the Latest TLS Version](https://comparecheapssl.com/what-is-tls-1-3-everything-you-need-to-know/)\n\n[194\\. Erik Sy, Moritz Moennich et al. “Enhanced performance for the encrypted web through TLS resumption across hostnames.” Proceedings of the 15th International Conference on Availability, Reliability and Security](https://doi.org/10.1145/3407023.3407067)\n\n[195\\. AI inference using Images, RTSP Video Stream on NVIDIA...](https://www.cnx-software.com/2019/12/07/getting-started-with-nvidia-jetson-nano-devkit-inference-using-images-rtsp-video-stream/)\n\n[196\\. Dimitrios Sikeridis, Panos Kampanakis et al. “Assessing the overhead of post-quantum cryptography in TLS 1.3 and SSH.” Proceedings of the 16th International Conference on emerging Networking EXperiments and Technologies](https://doi.org/10.1145/3386367.3431305)\n\n[197\\. Joppe W. Bos, Craig Costello et al. “Post-Quantum Key Exchange for the TLS Protocol from the Ring Learning with Errors Problem.” 2015 IEEE Symposium on Security and Privacy](https://doi.org/10.1109/SP.2015.40)\n\n[198\\. 406 - Not Acceptable](https://www.indusface.com/blog/enabling-tls-1-3-certificate-are-you-ready-for-moving-forward/)\n\n[199\\. TLS 1.3 Is Here](https://www.geocerts.com/blog/tls-13-is-here)\n\n[200\\. Edge Devices Inference Performance Comparison](http://jcse.kiise.org/files/V17N2-02.pdf)\n\n[201\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[202\\. K. Simonyan, Andrew Zisserman. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” CoRR](https://arxiv.org/abs/1409.1556)\n\n[203\\. A. Krizhevsky. “Learning Multiple Layers of Features from Tiny Images.”](https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086)\n\n[204\\. Efficient Patch Pruning for Vision Transformers via Patch Similarity](https://www.preprints.org/frontend/manuscript/5f70c44bfd81d2de6821139e2224021f/download_pub)\n\n[205\\. Efficient Mixed-Precision Inference for Vision Transformers](https://zenodo.org/records/15043962/files/Piotr%20Kluska%20PhD%20Thesis.pdf?download=1)\n\n[206\\. SliceViT](https://cs231n.stanford.edu/2024/papers/slicevit.pdf)\n\n[207\\. APPROXHPVM: A RETARGETABLE COMPILER FRAMEWORK FOR ACCURACY-AWARE OPTIMIZATIONS](https://www.ideals.illinois.edu/items/118308/bitstreams/388463/data.pdf)\n\n[208\\. A Survey of Algorithmic and Hardware Optimization Techniques for Vision Convolutional Neural Networks on FPGAs](https://lirias.kuleuven.be/retrieve/619685)\n\n[209\\. DATA-INDEPENDENT MODULE-AWARE PRUNING FOR HIERARCHICAL VISION TRANSFORMERS](https://oar.a-star.edu.sg/storage/q/qr8pe06x68/2024-iclr-swin-camere-ready-arxiv-version.pdf)\n\n[210\\. Patch Slimming for Efficient Vision Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Patch_Slimming_for_Efficient_Vision_Transformers_CVPR_2022_paper.pdf)\n\n[211\\. Mingjian Zhu, K. Han et al. “Vision Transformer Pruning.”](https://arxiv.org/abs/2104.08500)\n\n[212\\. Neural Network Quantization with Scale-Adjusted Training](https://www.bmvc2020-conference.com/assets/papers/0634.pdf)\n\n[213\\. Too Large; Data Reduction for Vision-Language Pre-Training](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Too_Large_Data_Reduction_for_Vision-Language_Pre-Training_ICCV_2023_paper.pdf)\n\n[214\\. Isomorphic Pruning for Vision Models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04364.pdf)\n\n[215\\. ...Pruning for Vision Models | Computer Vision – ECCV...](https://dl.acm.org/doi/10.1007/978-3-031-73404-5_14)\n\n[216\\. Vision Transformers 2: Quantization and Sparsity - Myrtle.ai](https://myrtle.ai/resources/leo-2-quantization-and-sparsity/)\n\n[217\\. Low-Power Semi-structured Pruning for Vision Transformers](https://arxiv.org/abs/2407.02068)\n\n[218\\. Hugo Touvron, M. Cord et al. “Training data-efficient image transformers & distillation through attention.” International Conference on Machine Learning](https://arxiv.org/abs/2012.12877)\n\n[221\\. Johannes Blömer. “How to share a secret.” Commun. ACM](https://doi.org/10.1145/359168.359176)\n\n[222\\. H. B. McMahan, Eider Moore et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” International Conference on Artificial Intelligence and Statistics](https://arxiv.org/abs/1602.05629)\n\n[223\\. FEDBFPT: An Efficient Federated Learning Framework for BERT Further Pre-training](https://www.ijcai.org/proceedings/2023/0483.pdf)\n\n[224\\. Mario: Multi-round Multiple-Aggregator Secure Aggregation with Robustness against Malicious Actors](https://eprint.iacr.org/2024/1428.pdf)\n\n[225\\. P. Kairouz, H. B. McMahan et al. “Advances and Open Problems in Federated Learning.” Found. Trends Mach. Learn.](https://doi.org/10.1561/2200000083)\n\n[226\\. Enhancing Federated Learning with In-Cloud Unlabeled Data](http://home.ustc.edu.cn/~lyl617/paper/icde2022_wanglun.pdf)\n\n[227\\. A Fast Federated Learning-based Crypto-aggregation Scheme and Its Simulation Analysis](https://www.china-simulation.com/EN/article/downloadArticleFile.do?attachType=PDF&id=3537)\n\n[228\\. Distributed ML System for Large-scale Models: Dynamic Distributed Training and Scalable Federated Learning](https://minghsiehece.usc.edu/wp-content/uploads/2021/11/Chaoyang-He.pdf)\n\n[229\\. 基于联邦学习的bert模型高效训练框架 - 软件学报](https://jos.org.cn/jos/ch/reader/view_abstract.aspx?flag=2&file_no=202403200000003&journal_id=jos)\n\n[230\\. The Fundamental Price of Secure Aggregation in Differentially Private Federated Learning](https://proceedings.mlr.press/v162/chen22c/chen22c.pdf)\n\n[231\\. Meng Jiang, Taeho Jung et al. “Federated Dynamic GNN with Secure Aggregation.” ArXiv](https://arxiv.org/abs/2009.07351)\n\n[232\\. Keith Bonawitz, Hubert Eichner et al. “Towards Federated Learning at Scale: System Design.” ArXiv](https://arxiv.org/abs/1902.01046)\n\n[233\\. Keith Bonawitz, Vladimir Ivanov et al. “Practical Secure Aggregation for Privacy-Preserving Machine Learning.” Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security](https://doi.org/10.1145/3133956.3133982)\n\n[234\\. MosaicBERT: How to Train BERT with a Lunch Money Budget](https://openreview.net/forum?id=WH1S0gonzR)\n\n[235\\. Communication Costs in Federated Learning](https://encyclopedia.pub/entry/48637)\n\n[236\\. Resource-Efficient Federated Learning with Hierarchical Aggregation in Edge Computing](http://staff.ustc.edu.cn/~lyl617/paper/Infocom21_wangzhiyuan.pdf)"
  },
  {
    "id": 44,
    "question": "What would a 'PhD-level' AI capability look like in practice, and how might that force us to re-evaluate our current educational accreditation systems?",
    "response": "**Research Report: The Emergence of PhD-Level AI and the Impending Challenge to Educational Accreditation**\n\n**Date of Report:** July 27, 2025\n\n**Author:** Expert Researcher\n\n**Executive Summary**\n\nAs of mid-2025, artificial intelligence is demonstrating capabilities that are increasingly described as \"PhD-level\" within specific, narrow domains. This report analyzes the practical manifestations of this advanced AI, examining the technical benchmarks that define it and the verifiable instances of AI-driven scientific contribution. It concurrently investigates the response of the global higher education sector, including universities and international accreditation bodies, to this technological sea change.\n\nThe key findings are twofold. First, while AI has not yet autonomously produced a complete body of work equivalent to a human PhD dissertation, it is achieving or exceeding expert-level performance on complex, specialized benchmarks in fields like theoretical physics and molecular biology. Furthermore, documented instances from 2024 and 2025 show AI systems independently deriving new physical insights and solving problems previously thought to require human ingenuity.\n\nSecond, the response from educational institutions and accreditation authorities is lagging significantly behind the technology's advancement. Universities are primarily focused on reactive measures, such as updating academic integrity policies and integrating AI literacy into curricula. International bodies like UNESCO and the Bologna Process signatories are developing ethical guidelines and competency frameworks but have not yet proposed formal modifications to doctoral accreditation standards. This creates an \"accreditation lag,\" where the definition of an original doctoral contribution is becoming increasingly misaligned with the reality of AI-assisted and AI-driven research. The report concludes that a fundamental re-evaluation of the purpose, process, and validation of doctoral education is now an urgent necessity.\n\n**1\\. Defining and Demonstrating 'PhD-Level' AI Capabilities**\n\nThe term \"PhD-level AI\" has moved from speculative fiction to a tangible concept discussed by industry leaders and demonstrated in practice. However, its meaning is nuanced, referring not to a general artificial intelligence but to highly specialized systems capable of expert-level reasoning and novel discovery within tightly defined domains.\n\n**1.1. Conceptualizing \"PhD-Level Intelligence\" in AI**\n\nThe prevailing industry view, as articulated by OpenAI's Chief Technology Officer Mira Murati, is that emerging models like the anticipated GPT-5 will achieve \"Ph.D. intelligence for specific tasks\" \\[7\\]. This concept is critical: the intelligence is task-specific, not generalized. These systems exhibit advanced memory and reasoning capabilities that allow them to achieve human-expert performance in certain areas while still lagging in others \\[7\\]. This specialized cognitive prowess forms the foundation of what can be considered PhD-level capability—not a simulation of a human student, but an engine for performing the discrete, high-level intellectual tasks that comprise doctoral research.\n\n**1.2. Benchmarking Advanced AI in Scientific Disciplines**\n\nTo move beyond conceptual discussions, the research community has developed sophisticated benchmarks to quantitatively assess AI capabilities against expert-level human performance.\n\n**In Theoretical Physics:** A significant development is the **TPBench (Theoretical Physics Benchmark)**, introduced in February 2025. This benchmark comprises 57 problems of escalating difficulty, from undergraduate to active research level, centered on high-energy theory and cosmology \\[1\\]\\[8\\]\\[9\\]. As of mid-2025, state-of-the-art models show impressive progress on the lower-level problems, but the majority of research-level challenges remain unsolved \\[1\\]\\[8\\]. This indicates that while AI can be a powerful assistant, it is not yet a replacement for human theoretical physicists. A common failure mode observed is the generation of solutions that are plausible on the surface but are logically inconsistent or contain subtle physical errors, highlighting the need for improved reasoning and self-correction mechanisms \\[3\\]. Similarly, the **FrontierMath** benchmark for advanced mathematical reasoning shows that top AI models currently solve less than 2% of its problems, underscoring the significant gap that still exists in abstract mathematical expertise \\[3\\].\n\n**In Molecular Biology and Life Sciences:** The field has seen remarkable AI advancements. A comprehensive 2025 study evaluated 27 large language models across eight biological domains, including molecular biology and genetics, using a suite of difficult benchmarks like **GPQA-Bio** and **CloningScenarios** \\[10\\]. The results revealed that top-tier models were achieving or even exceeding the performance of human experts on these tasks, demonstrating a capacity for complex biological reasoning that was previously the exclusive domain of highly trained scientists \\[10\\]. The success of systems like AlphaFold in protein structure prediction continues to have a profound impact, with its use in academic publications growing significantly; citations increased by 62% in 2023 alone, and its adoption is widespread across universities of all tiers \\[273\\]\\[260\\]\\[260\\].\n\nThese benchmarks reflect a broader evolution in AI evaluation. The focus has expanded beyond computational power to include metrics like energy efficiency, ethical alignment, cross-domain adaptability, and, most importantly, the ability to solve tangible, real-world problems \\[4\\].\n\n**1.3. Milestones in AI-Driven Scientific Discovery (2024-2025)**\n\nWhile no AI has autonomously authored a full, peer-reviewed PhD dissertation \\[47\\]\\[49\\]\\[62\\]the period from 2024 to mid-2025 has been marked by a series of unprecedented achievements where AI systems have generated novel scientific and mathematical contributions with minimal or no human intervention.\n\n**Autonomous Mathematical Discovery:** In a landmark 2024 _Nature_ publication, an AI system was shown to be capable of \"solving olympiad geometry without human demonstrations,\" proving complex theorems at a level that challenges the brightest human students \\[219\\]\\[220\\]\\[221\\]. This demonstrated a capacity for symbolic reasoning and logical deduction far beyond simple pattern recognition.\n\n**Derivation of Physical Laws:** In a pivotal April 2025 study, researchers demonstrated that an AI system could independently discover and propose Hamiltonian physical quantities from observational data without any prior knowledge of physics \\[225\\]. This aligns with other findings showing AI can rediscover fundamental physical principles like Kepler's and Newton's laws from data alone, suggesting a nascent ability to perform autonomous science \\[233\\].\n\n**AI-Generated Peer-Reviewed Research:** In a notable case from April 2025, the company Sakana AI announced that its AI system had independently generated a research paper that subsequently passed peer review for a workshop at the prestigious ICLR 2025 conference \\[223\\]. While the company acknowledged the need for establishing clear norms around AI-generated science, this event marks a critical threshold in AI's ability to participate in the formal process of scientific dissemination.\n\nThese cases, while not equivalent to the sustained, multi-faceted research project of a PhD, represent the successful automation of core components of a dissertation: literature synthesis, hypothesis generation, experimentation (in silico), and the articulation of novel findings. They signal that the building blocks for an AI-generated dissertation chapter or even a full paper are now in place \\[111\\]\\[118\\].\n\n**2\\. The Response of Higher Education and Accreditation Systems**\n\nFaced with these rapid technological advancements, the global higher education sector is grappling with the implications. The response thus far has been largely tactical and reactive, focused on managing the immediate disruptions of generative AI rather than undertaking a strategic, structural re-evaluation of advanced degrees and their accreditation.\n\n**2.1. University-Level Adaptations: A Focus on Integrity and Integration**\n\nThe primary institutional response since 2023 has been directed at academic integrity. Universities worldwide are scrambling to develop and update policies to address the use of AI tools by students, with a strong focus on preventing plagiarism and defining acceptable use \\[22\\]\\[26\\].\n\nBeyond policing, a more constructive trend is the integration of AI into the educational fabric. Institutions are:\n\n**Adapting Curricula:** Incorporating AI competencies and AI literacy into programs across disciplines \\[27\\]\\[30\\].\n\n**Adjusting Assessments:** Rethinking examination formats and thesis requirements to include, for example, mandatory documentation of AI tool usage \\[27\\]. Some educators are even viewing AI proficiency as a skill to be assessed, differentiating students based on their ability to leverage these tools effectively \\[32\\].\n\n**Launching New Programs:** There has been a significant proliferation of AI-specific degree programs. As of early 2024, 67 U.S. universities offered AI degrees or certificates, and the demand for AI master's degrees is particularly high \\[29\\]\\[23\\].\n\nDespite these activities, a comprehensive review of the search results reveals **no direct evidence that universities have modified their core accreditation requirements for doctoral degrees** in response to AI's research capabilities \\[22\\]\\[32\\]query summary). The focus remains on student conduct and curriculum modernization, not on redefining the nature of an original scholarly contribution at the PhD level.\n\n**2.2. Impact on Doctoral Programs: Enrollment, Training, and Career Trajectories**\n\nThe rise of PhD-level AI has not yet created a clear, measurable impact on PhD application volumes. The search results contain no studies or data showing a statistically significant correlation between AI research milestones and STEM PhD application rates at top universities since 2023 (Query results on correlation coefficients).\n\nHowever, other profound shifts are underway within doctoral education:\n\n**Shifting Training Paradigms:** PhD programs are beginning to adapt by integrating AI as a core research tool. Initiatives like the \"Wallenberg AI, Autonomous Systems and Software Program\" aim to train PhD students to use AI effectively in their own research, enhancing their overall research competence \\[35\\]\\[41\\].\n\n**The Industry 'Brain Drain':** A powerful trend shaping the landscape is the destination of AI talent. A vast majority of new AI PhD graduates are opting for careers in private industry over academia. In 2021, 65.4% of new AI PhDs took industry jobs, compared to just 28.2% who entered academia, a trend that has been accelerating for a decade \\[200\\]. This exodus starves the academic pipeline of the very experts needed to train the next generation and lead university research, potentially ceding the frontier of AI discovery to the private sector. This is occurring even as demand for PhDs specializing in foundation models grows by over 400% year-on-year \\[100\\].\n\n**2.3. The Stance of International Accreditation and Policy Bodies**\n\nAt the highest levels of educational policy, the response has been one of guidance and deliberation, not of regulatory change. Major international bodies are aware of the challenge but have not yet translated this awareness into concrete modifications for doctoral accreditation.\n\n**UNESCO:** UNESCO has been active in publishing guidance on the use of AI. Its 2023 \"Guidance for generative AI in education and research\" provides a framework for ethical implementation and policy considerations \\[180\\]\\[181\\]\\[239\\]. It has also developed competency frameworks for educators and researchers \\[70\\]\\[76\\]. However, these documents are recommendations, not binding standards, and they do not specify accreditation requirements for validating AI-generated knowledge within a doctoral thesis (Query on UNESCO policy documents).\n\n**The Bologna Process:** Within the European Higher Education Area, the Bologna Process has established working groups to provide recommendations on the ethical use of digital tools and AI in learning and teaching \\[129\\]\\[179\\]. The process is fundamentally concerned with quality assurance and degree recognition \\[128\\]\\[136\\]. Yet, despite discussions around digitization, the search results show **no official policy documents or proposed formal changes to the Bologna framework's accreditation standards for doctoral programs** that specifically address how to handle, validate, or credit AI-generated knowledge (Query on Bologna Process changes).\n\nThe collective stance is one of cautious observation. While bodies like the American Medical Association (AMA) are encouraging accrediting bodies to study the issue \\[73\\], the formal machinery of accreditation, which operates on long timescales, has yet to be engaged.\n\n**3\\. Analysis and Future Outlook**\n\nThe evidence presents a clear and growing schism: the capabilities of research AI are advancing exponentially, while the educational structures that certify advanced human knowledge remain largely unchanged. This \"accreditation lag\" poses a fundamental challenge to the integrity and relevance of the doctorate.\n\n**3.1. The Accreditation Lag: Why Education Systems Trail Technology**\n\nThe lag between AI capability and accreditation reform is a product of institutional inertia, the inherent complexity of the issue, and a focus on immediate, first-order problems. Universities have prioritized the visible threat of student plagiarism because it fits within existing frameworks of academic misconduct. The more profound, second-order problem—that the very definition of \"original human contribution\" is becoming blurred—is far more difficult to address. It calls into question the foundational premise of the PhD, an exercise designed over centuries to be the ultimate test of a single individual's capacity for independent, creative, and rigorous scholarship.\n\n**3.2. Re-evaluating the Doctorate: From Sole Contributor to Human-AI Research Director**\n\nThe path forward will require a paradigm shift. The PhD can no longer be seen solely as a test of a student's ability to perform every step of the research process manually. Instead, it must evolve to certify a student's ability to **direct the process of knowledge creation in a human-AI collaborative environment.**\n\nA future-proofed PhD program might evaluate a candidate on their ability to:\n\n1.  **Identify and Frame Important Questions:** Posing novel, significant, and researchable problems remains a deeply human skill.\n2.  **Strategically Deploy AI Tools:** Selecting, fine-tuning, and prompting the appropriate AI systems to generate data, synthesize literature, or run simulations.\n3.  **Critically Evaluate AI Outputs:** Vetting AI-generated hypotheses, code, and text for logical fallacies, subtle biases, and factual inaccuracies—a skill that requires deeper expertise than ever before.\n4.  **Synthesize and Integrate Knowledge:** Weaving together human insights and AI-generated results into a coherent, novel argument that advances the field.\n\nIn this model, the dissertation would become a portfolio of human-AI collaboration. The final oral defense, or _viva voce_, would become even more critical, serving as the ultimate forum to probe the candidate's true depth of understanding and verify that they were the intellectual author and director of the project, not merely a passive operator of a machine.\n\n**3.3. Conclusion**\n\nAs of July 2025, the world has witnessed the dawn of PhD-level AI in specific, powerful applications. These systems are crossing thresholds of scientific and mathematical reasoning previously thought to be exclusively human. Yet, the institutions that bestow the title of \"Doctor of Philosophy\" are just beginning to grapple with the most superficial implications of this new reality.\n\nThe current focus on academic integrity is a necessary but insufficient first step. The far greater challenge is to redefine doctoral originality and reform accreditation standards to reflect a world where knowledge creation is an act of human-AI partnership. Without such a fundamental re-evaluation, the PhD, the highest symbol of intellectual achievement, risks becoming an anachronism, disconnected from the true frontier of research and discovery. The coming years will determine whether higher education can adapt to this new era or be rendered a lagging spectator to it.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. \\[2502.15815\\] Theoretical Physics Benchmark (TPBench)](https://arxiv.org/abs/2502.15815)\n\n[2\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[3\\. State of AI Reasoning for Theoretical Physics - Insights from the TPBench Project](https://pdf.pirsa.org/files/25040061.pdf)\n\n[4\\. Deep Learning Benchmarks 2025](https://www.byteplus.com/en/topic/466080)\n\n[5\\. 高教动态](https://udp.cufe.edu.cn/system/_content/download.jsp?urltype=news.DownloadAttachUrl&owner=2070134866&wbfileid=37AF05E57F7F90965ED4A6E7BE08C0C6)\n\n[6\\. AI and the Future of Theoretical Physics](https://esp.mit.edu/download/a012b0fc-52db-48aa-aee6-47d3fbd6260d/S16186_ai_splash.pdf)\n\n[7\\. OpenAI's GPT-5 Pushed Back to Late 2025, but Promises PhD-Level Abilities - FUTURES - ON AI](https://futures.webershandwick.com/2024/06/22/openais-gpt-5-pushed-back-to-late-2025-but-promises-phd-level-abilities/)\n\n[8\\. Daniel J. H. Chung, Zhiqi Gao et al. “Theoretical Physics Benchmark (TPBench) -- a Dataset and Study of AI Reasoning Capabilities in Theoretical Physics.”](https://arxiv.org/abs/2502.15815)\n\n[9\\. Daniel J.H. Chung, Zhiqi Gao et al. “Theoretical Physics Benchmark (TPBench) -- a Dataset and Study of AI Reasoning Capabilities in Theoretical Physics.”](https://arxiv.org/abs/2502.15815)\n\n[10\\. LLMs outperform experts on multiple biology benchmarks](https://arxiv.org/pdf/2505.06108)\n\n[21\\. Tiffany H. Kung, Morgan Cheatham et al. “Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models.” PLOS Digital Health](https://doi.org/10.1371/journal.pdig.0000198)\n\n[22\\. Debby R. E. Cotton, Peter A. Cotton et al. “Chatting and cheating: Ensuring academic integrity in the era of ChatGPT.” Innovations in Education and Teaching International](https://doi.org/10.1080/14703297.2023.2190148)\n\n[23\\. New Academic Degree Program Request to Establish](https://provost.charlotte.edu/wp-content/uploads/sites/887/2025/04/RTE-Revised.Final_.4.15.25-fully-signed.locked.pdf)\n\n[24\\. Lijia Chen, Pingping Chen et al. “Artificial Intelligence in Education: A Review.” IEEE Access](https://doi.org/10.1109/ACCESS.2020.2988510)\n\n[25\\. J. Crawford, Michael Cowling et al. “Leadership is needed for ethical ChatGPT: Character, assessment, and learning using artificial intelligence (AI).” Journal of University Teaching and Learning Practice](https://doi.org/10.53761/1.20.3.02)\n\n[26\\. Academic integrity considerations of AI Large Language Models in the post-pandemic era: ChatGPT and beyond.Journal of University Teaching and Learning Practice](https://doi.org/10.53761/1.20.02.07)\n\n[27\\. Joschka Mütterlein, Tamara Ranner et al. “Artificial Intelligence at Universities: Impact on Grades, Student Experience, and Teaching.”](https://doi.org/10.56843/jm001tr002)\n\n[28\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[29\\. Artificial Intelligence Scholarship for Service Initiative: Need, Feasibility, and Implementation](https://nsf-gov-resources.nsf.gov/files/2024SFSAIReport-r.pdf)\n\n[30\\. JUCE Journal 大学教育と情報](https://www.juce.jp/LINK/journal/2404/pdf/2404_all.pdf)\n\n[31\\. Transforming University Academic Management into Data Aggregation of Artificial Intelligence](https://www.scirp.org/pdf/ce2025163_86308537.pdf)\n\n[32\\. Higher education sector trends analysis February 2024](https://cdn.csu.edu.au/__data/assets/pdf_file/0006/4286679/Tertiary-Sector-Trends-February-2024.pdf)\n\n[33\\. University Rules and Syllabuses for Degrees, Diplomas and Certificates offered in the Faculty of Science for the 2023 Academic Year](https://www.wits.ac.za/media/wits-university/students/academic-matters/2023%20Science%20Rules%20and%20Syllabuses%20Final.pdf)\n\n[34\\. Lijia Chen, Pingping Chen et al. “Artificial Intelligence in Education: A Review.” IEEE Access](https://doi.org/10.1109/ACCESS.2020.2988510)\n\n[35\\. WASP-HS: The Wallenberg AI, Autonomous Systems and Software Program - Humanities and Society 2023](https://wasp-hs.org/wp-content/uploads/2024/01/This-is-WASP-HS-2023-Webb.pdf)\n\n[36\\. Yogesh Kumar Dwivedi, Laurie Hughes et al. “Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy.” International Journal of Information Management](https://doi.org/10.1016/J.IJINFOMGT.2019.08.002)\n\n[37\\. Olaf Zawacki-Richter, Victoria I. Marín et al. “Systematic review of research on artificial intelligence applications in higher education – where are the educators?.” International Journal of Educational Technology in Higher Education](https://doi.org/10.1186/s41239-019-0171-0)\n\n[38\\. A. Kaplan, M. Haenlein. “Siri, Siri, in my hand: Who’s the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence.” Business Horizons](https://doi.org/10.1016/J.BUSHOR.2018.08.004)\n\n[39\\. Stefan A. D. Popenici, Sharon Kerr. “Exploring the impact of artificial intelligence on teaching and learning in higher education.” Research and Practice in Technology Enhanced Learning](https://doi.org/10.1186/s41039-017-0062-8)\n\n[40\\. DIKWP Model Based Changes in Academic Evaluation](https://zhuanlan.zhihu.com/p/682491969)\n\n[41\\. Artificial intelligence in developing doctoral students' research competencies](https://revistaeduweb.org/index.php/eduweb/article/download/654/969/1481)\n\n[42\\. International Doctoral Business Research Conference 2024](https://www.westernsydney.edu.au/__data/assets/pdf_file/0007/2056498/IDBRC_Conference_2024_Final_5-11.pdf)\n\n[43\\. Autonomous artificial intelligence increases real-world ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC10550906/)\n\n[44\\. The Impact of Artificial Intelligence on Higher Education: An Empirical Study](https://files.eric.ed.gov/fulltext/EJ1384682.pdf)\n\n[45\\. Autonomous artificial intelligence increases real-world specialist clinic productivity in a cluster-randomized trial | npj Digital Medicine](https://www.nature.com/articles/s41746-023-00931-7)\n\n[46\\. The Impact of Artificial Intelligence Literacy on the Expected Outcome of AI Tool's Integration into Higher Education Institutions: A Systematic Review of EFL Departments](https://journals.e-palli.com/home/index.php/ajet/article/download/3977/2266/29857)\n\n[47\\. ReviewEval: An Evaluation Framework for AI-Generated Reviews](https://arxiv.org/pdf/2502.11736)\n\n[48\\. Systematic analysis of generative AI tools integration in academic research and peer review](https://www.ojcmt.net/download/systematic-analysis-of-generative-ai-tools-integration-in-academic-research-and-peer-review-15832.pdf)\n\n[49\\. Quality and Effectiveness of AI Tools for Students ...](https://pubmed.ncbi.nlm.nih.gov/38682531/?utm_source=FeedFetcher&utm_medium=rss&utm_campaign=None&utm_content=0nK-FGmcPzruPpJferMpyd8mRld7hsB1ra05TRUDXN6&fc=None&ff=20240429121914&v=2.18.0.post9%20e462414)\n\n[50\\. THE CAPCO INSTITUTE JOURNAL OF FINANCIAL TRANSFORMATION](https://www.hkdca.com/wp-content/uploads/2025/02/journal-financial-transformation-capco.pdf)\n\n[51\\. Personal experience with AI-generated peer reviews](https://pmc.ncbi.nlm.nih.gov/articles/PMC11974187/)\n\n[52\\. Personal experience with AI-generated peer reviews: a case ...](https://researchintegrityjournal.biomedcentral.com/articles/10.1186/s41073-025-00161-3)\n\n[53\\. AI SINGAPORE RESEARCH PROGRAMME AI SINGAPORE PhD FELLOWSHIP PROGRAMME 2024](https://aisingapore.org/wp-content/uploads/2023/12/2.1-AISG-Research-PhD-Fellowship-Application-Guideline_Aug-2024-Intake_Applicant_v1_clean.pdf)\n\n[54\\. 再谈同行评议新范式](https://chinaxiv.org/user/download.htm?uuid=250a831b-c04c-4aff-bca6-24252aefbe21)\n\n[55\\. GRADUATION THESIS BUILDING A MANAGEMENT SYSTEM FOR A CHAIN OF PRIVATE TRAINING AND STUDENT CARE CENTERS USING AI TECHNOLOGY](https://elib.vku.udn.vn/bitstream/123456789/4790/1/20SE6-20IT423.%20Hoang%20Ha.pdf)\n\n[56\\. LLMs Learn to Collaborate and Reason: December 2024 Update to “Generative AI for Economic Research: Use Cases and Implications for Economists,” Published in the Journal of Economic Literature 61(4)](https://www.aeaweb.org/content/file?id=21904&trk=public_post_comment-text)\n\n[57\\. Debby R. E. Cotton, Peter A. Cotton et al. “Chatting and cheating: Ensuring academic integrity in the era of ChatGPT.” Innovations in Education and Teaching International](https://doi.org/10.1080/14703297.2023.2190148)\n\n[58\\. Effective Practices in AI Literacy Education: Case Studies and Reflections](https://www.emerald.com/insight/content/doi/10.1108/978-1-83608-852-320241022/full/pdf?title=prelims)\n\n[59\\. Richard Smith. “Peer Review: A Flawed Process at the Heart of Science and Journals.” Journal of the Royal Society of Medicine](https://doi.org/10.1177/014107680609900414)\n\n[60\\. An Evaluation Framework for AI-Generated Reviews](https://arxiv.org/html/2502.11736v1)\n\n[61\\. ARTIFICIAL INTELLIGENCE MAY SOON EMULATE PICASSO AND TOLKIEN—IS PORTER NEXT? AN OPTIMIZATION OF ORGANIZATION STRATEGIC DECISION-MAKING WITH AI TECHNIQUES](https://dt.athabascau.ca/jspui/bitstream/10791/489/3/SBhasinDissertation.pdf)\n\n[62\\. 港大90后开源，OpenAI 2万刀博士级AI智能体平替！自主研究媲美顶会论文](https://finance.sina.com.cn/roll/2025-03-15/doc-ineptnsn1855481.shtml)\n\n[63\\. Understanding the Application of AI in Higher Education Systems: Global Perspectives From a Local AI Ecosystem](https://openpraxis.org/articles/725/files/67498e9c0c8a0.pdf)\n\n[64\\. Dongyeop Kang, Bridger Waleed Ammar et al. “A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications.” ArXiv](https://doi.org/10.18653/v1/N18-1149)\n\n[67\\. Informe d'Acreditació del Programa de Doctorat (IAPD) Intel.ligència Artificial](https://gpaq.upc.edu/resultatsVSMA/documents/informes/VSMA/Autoinforme%20Acreditacio%20IA_2018.pdf)\n\n[68\\. Artificial Intelligence Master of Science Degree COMMENCING FALL 2021 Full Proposal](https://provostdata.kent.edu/roadmapweb/02/EPC_20may_attach3.pdf)\n\n[69\\. 信息参考](https://lib.qau.edu.cn/attachdownload?url=%2Fuserfiles%2Fimage%2Ftsg%2F2024%2F11%2F20241120092326827.pdf&t=1750218830356&contentid=906fe18d9e6946aba67dfa57d70f40d2&passcode=619681c6e6215f03f2785c78385b576a)\n\n[70\\. 高教参考](https://jfzx.gzhmu.edu.cn/__local/C/C7/5C/9E20E461E81105A910D8995FD01_265345F1_1E37D6.pdf)\n\n[71\\. Olaf Zawacki-Richter, Victoria I. Marín et al. “Systematic review of research on artificial intelligence applications in higher education – where are the educators?.” International Journal of Educational Technology in Higher Education](https://doi.org/10.1186/s41239-019-0171-0)\n\n[72\\. European Year of Youth Flagship Initiatives](https://www.crui.it/documenti/54/New-category/939/20220221-Flagships.pdf)\n\n[73\\. REFERRAL CHANGES AND OTHER REVISIONS 2024 Annual Meeting](https://www.ama-assn.org/system/files/a24-meeting-tote.pdf)\n\n[74\\. Curriculum Frameworks and Educational Programs in AI for Medical Students, Residents, and Practicing Physicians: Scoping Review](https://pdfs.semanticscholar.org/5f28/0cd40931be2bd45e21549cae9022c6585c50.pdf)\n\n[75\\. B. Meskó, Gergely Hetényi et al. “Will artificial intelligence solve the human resource crisis in healthcare?.” BMC Health Services Research](https://doi.org/10.1186/s12913-018-3359-4)\n\n[76\\. 专家观点丨AI时代的大学之变](https://mp.weixin.qq.com/s?__biz=MzI3ODM5NzgzNA%3D%3D&mid=2247492696&idx=1&sn=8de9ca7261a266eacdc4be8c18d2a307&chksm=eadc21cab774aa940c32707a3e540584aa019899e43377d53da7674049c2753260a09678a2e4&scene=27)\n\n[77\\. Computación para el Desarrollo - XVI Congreso](http://compdes.org/libros/compdes2023.pdf)\n\n[78\\. ICLR'25| 真正「Deep」的「Research」,通过强化学习实现可自主进化的科研智能体](https://b23.tv/BV1GhZ4YSED8?t=784)\n\n[79\\. Toolkit for Artificial Intelligence Readiness and Capacity Assessment](https://www.gov.br/mcom/pt-br/acesso-a-informacao/governanca/governanca-de-tic-1/documentos-g20/p4-g20-dewg-brasil-2024-toolkit-for-ai-readiness-and-capacity-assessment.pdf)\n\n[80\\. Lewis Ho, Joslyn Barnhart et al. “International Institutions for Advanced AI.” ArXiv](https://doi.org/10.48550/arXiv.2307.04699)\n\n[81\\. Mehul Mahrishi, A. Abbas et al. “Global Initiatives Towards Regulatory Frameworks for Artificial Intelligence (AI) in Higher Education.” Digital Government: Research and Practice](https://doi.org/10.1145/3672462)\n\n[82\\. 高等教育动态](https://gjzx.nwu.edu.cn/__local/F/C1/2E/FFF08C917F378897C99B060FB90_035E0DAE_5B219.pdf)\n\n[83\\. European Conference on Academic Integrity and Plagiarism 2021](https://academicintegrity.eu/conference/proceedings/2021/book_of_abstracts2021.pdf)\n\n[84\\. AI 大模型驱动高校人才培养改革](http://csteic.org/poster/web/202412V12N6/202412V12N6_17.pdf)\n\n[86\\. EXPLAINABLE AI FOR DEFENCE SYSTEMS](https://www.unicasd.it/pluginfile.php/22712/mod_page/content/199/Claudio%20Stanzione%20%281%29.pdf?time=1742823487308)\n\n[87\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2024.” ArXiv](https://doi.org/10.48550/arXiv.2405.19522)\n\n[88\\. A Glimpse in ChatGPT Capabilities and its impact for AI research](https://www.honda-ri.de/pubs/pdf/5402.pdf)\n\n[89\\. Ежемесячный мониторинг глобальных стратегических трендов](https://iwmes.hse.ru/mirror/pubs/share/1026062368.pdf)\n\n[90\\. Research trends in the use of artificial intelligence in higher education](https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2024.1438715/pdf)\n\n[91\\. Science & Technology Trends 2023-2043](https://www.nato.int/nato_static_fl2014/assets/pdf/2023/3/pdf/stt23-vol2.pdf)\n\n[92\\. Stanford University Releases 2024 AI Index Report](https://www.icdrex.com/stanford-university-releases-2024-ai-index-report/)\n\n[93\\. Artificial Intelligence Index Report 2023](https://milanoluisshub.it/wp-content/uploads/2023/04/HAI_AI-Index_Report_2023_compressed.pdf)\n\n[94\\. Science & Technology Trends 2023-2043 Across the Physical, Biological, and Information Domains](https://cesmar.it/wp-content/uploads/2023/04/stt23-vol1.pdf)\n\n[95\\. The Artificial Intelligence Patent Dataset (AIPD) 2023 update](https://www.uspto.gov/sites/default/files/documents/oce-aipd-2023.pdf)\n\n[96\\. 2023年度 博士学位論文 内容の要旨および審査結果の要旨 第35号 (2023年9月授与)](https://www.kitakyu-u.ac.jp/env/uploads/ee75c49a4ea6698038a50c6858347f1a.pdf)\n\n[97\\. Arslan Akram. “Quantitative Analysis of AI-Generated Texts in Academic Research: A Study of AI Presence in Arxiv Submissions using AI Detection Tool.” ArXiv](https://doi.org/10.48550/arXiv.2403.13812)\n\n[98\\. AAISS 2023 : Special Issue on Advances in Artificial...](http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=169954&copyownerid=166176)\n\n[99\\. Artificial Intelligence Index Report 2023 - European Commission](https://ec.europa.eu/newsroom/rtd/redirection/item/813110#:~:text=The%202023%20report%20includes%20new,2022%20to%20127%20in%202023.)\n\n[100\\. Rising demand in AI creates job opportunities_Shenzhen Daily](https://szdaily.sznews.com/PC/content/202306/09/content_3076424.html)\n\n[101\\. PhD Thesis Defenses](https://www.graduateschool-computerscience.de/doctoral-students/phd-defenses-2023/)\n\n[106\\. Research Papers: AI, peer-reviewed, published 2024-2025](https://websets.exa.ai/research-papers-ai-peer-reviewed-published-2024-2025-cm8yjralw02q9kl0jl8ijvgjy)\n\n[107\\. Identifying AI-Generated Research Papers: Methods and Considerations](https://goldenratio.id/index.php/grdis/article/download/489/506/5174)\n\n[108\\. Publikationsbericht 2024](https://www21.ovgu.de/unimagdeburg_media/Forschung+_+Wirtschaft/Dokumente/2024_Publikationsbericht_OVGU.pdf)\n\n[109\\. Publications](https://www.icip.org.cn/pulications/)\n\n[110\\. 2024-- 智能信息处理重点实验室网站](https://iip.ict.ac.cn/lwfb/2024/)\n\n[111\\. Towards Scientific Discovery with Generative AI](https://arxiv.org/html/2412.11427v1)\n\n[112\\. National Conference Proceedings of NCCCMEA 2024](https://casmvk.kerala.gov.in/wp-content/uploads/2025/03/NCCCMEA-2024.pdf)\n\n[113\\. An Academic Viewpoint (2025) on the Integration ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC12020443/)\n\n[114\\. (PDF) Generative artificial intelligence and academic ...](https://www.academia.edu/124665754/Generative_artificial_intelligence_and_academic_writing_an_analysis_of_the_perceptions_of_researchers_in_training)\n\n[115\\. DeepDiveAI: Identifying AI Related Documents in Large Scale Literature Dataset](https://arxiv.org/pdf/2408.12871)\n\n[116\\. International Doctoral Business Research Conference 2024](https://www.westernsydney.edu.au/__data/assets/pdf_file/0007/2056498/IDBRC_Conference_2024_Final_5-11.pdf)\n\n[117\\. Large language models for automated scholarly paper review: A survey](https://arxiv.org/pdf/2501.10326)\n\n[118\\. Aniketh Garikaparthi, Manasi Patwardhan et al. “IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery.”](https://arxiv.org/abs/2504.16728)\n\n[119\\. ...Department, College of Engineering, University of Michigan...](https://www.aminer.cn/profile/j-e-laird/53f59124dabfaede0bf8045b)\n\n[120\\. Aishik Sanyal, Samuel Schapiro et al. “Spark: A System for Scientifically Creative Idea Generation.”](https://arxiv.org/abs/2504.20090)\n\n[121\\. Academic Publishing Trends in 2025 - Changes to Know](https://blog.lap-publishing.com/current-trends-in-academic-publishing-you-should-know/#:~:text=In%202025,%20the%20academic%20publishing%20industry%20is%20prioritizing%20the%20delicate,comply%20with%20evolving%20legal%20standards.)\n\n[122\\. HeartDJ - Music Recommendation and Generation through Biofeedback from Heart Rate Variability](https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi?article=1224&context=masters_theses)\n\n[123\\. 2024년 춘계전국학술대회 발표자료집](https://konige.kr/files/sub0203/konige202408081451370.pdf)\n\n[124\\. Preparing National Research Ecosystems for AI: strategies and progress in 2024](https://council.science/wp-content/uploads/2024/03/Preparing-National-Research-Ecosystems-for-AI-5.pdf)\n\n[125\\. More than 5 Million Scholarly Articles Were Published in ...](https://publishingstate.com/more-than-5-million-scholarly-articles-were-published-in-2024-and-here-are-more-interesting-facts/2025/)\n\n[126\\. Bologna Process Explained](https://everything.explained.today/Bologna_Process/)\n\n[127\\. Tanja Milati. “Towards the European Higher Education Area.” European Journal of Social Work](https://doi.org/10.1080/714889991)\n\n[128\\. The Bologna Process Implementation Report Working Group Monitoring Proposal to revise and amend Key Commitment Scorecard Indicators](https://ehea.info/Upload/BFUG_SE_BA_84_WG_Monitoring_Revised_scorecard_indicators_for_2024_BPIR_1_.pdf)\n\n[129\\. Bologna with Students Eyes 2024](https://esu-online.org/wp-content/uploads/2024/06/ESU-BWSE-2024-1.pdf)\n\n[130\\. Follow-up Report, Prague Conferences. “Trends in Learning Structures in Higher Education.”](https://www.semanticscholar.org/paper/8f1a60aed6def0edd26752c4b46a1b19b8fcb972)\n\n[131\\. Main publications of UNESCO on digital learning and AI in Education](https://www.lamar.edu/lu-online/_files/documents/luonline/ctle/unesco_publications_on_digital_learning_ai_in_education_1711974706.pdf)\n\n[132\\. B. Kehm. “The Future of the Bologna Process — The Bologna Process of the Future.” European Journal of Education](https://doi.org/10.1111/J.1465-3435.2010.01453.X)\n\n[133\\. Law, Science and Technology - University of Bologna](https://www.unibo.it/en/study/phd-professional-masters-specialisation-schools-and-other-programmes/phd/2022-2023/law-science-and-technology-1)\n\n[134\\. Universities' contribution to the Bologna Process An introduction](https://tuningacademy.org/wp-content/uploads/2014/02/Universities-Contribution_EN.pdf)\n\n[135\\. Making Technology Work for Education Transformation Goals: A Partnership Approach for Action](https://www.globalpartnership.org/node/document/download?file=document/file/2024-10-making-technology-education-transformation-goals-partnership-approach.pdf)\n\n[136\\. New Zealand and the Bologna Process](https://www.ehea.info/media.ehea.info/file/EHEA_in_a_Global_Context/31/6/NZandBologna_595316.pdf)\n\n[137\\. Seventh Annual Groningen Declaration Network Meeting](https://www.groningendeclaration.org/wp-content/uploads/2020/06/2018-Groningen-Exec-Summary_final.pdf)\n\n[138\\. Higher Education Management and Policy](https://www.oecd.org/content/dam/oecd/en/publications/reports/2012/06/higher-education-management-and-policy-volume-24-issue-1_g1g1687d/hemp-v24-1-en.pdf)\n\n[139\\. Xing Hao, Guigang Zhang et al. “Deep Learning.” Int. J. Semantic Comput.](https://doi.org/10.1142/S1793351X16500045)\n\n[140\\. China's universities outpace US peers amid tech competition](https://www.fdiintelligence.com/content/d0a58f39-0ed0-4b58-8c51-477133b6d9e1)\n\n[141\\. Artificial Intelligence Index Report 2023](https://aiindex.stanford.edu/wp-content/uploads/2023/04/HAI_AI-Index-Report-2023_CHAPTER_1-1.pdf)\n\n[142\\. AI100: 人工智能杰出成果](https://evaluation.benchcouncil.org/ai/cn/annual.html)\n\n[143\\. Lijia Chen, Pingping Chen et al. “Artificial Intelligence in Education: A Review.” IEEE Access](https://doi.org/10.1109/ACCESS.2020.2988510)\n\n[144\\. RECRUS Research Newsletter](https://hsaas.upm.edu.my/upload/dokumen/20240112104545RECRUS_Newsletter_January_2024_Volume_4_Issues_26.pdf)\n\n[145\\. Top 50 Surprising Artificial Intelligence Facts & Statistics \\[2024\\]](https://digitaldefynd.com/IQ/surprising-artificial-intelligence-facts-statistics/)\n\n[146\\. Current Academic Studies in Technology and Education 2023](https://www.isres.org/books/978-625-6959-35-4_31-12-2023.pdf)\n\n[147\\. Artificial Intelligence: An Untapped Opportunity for Equity ...](https://www.mdpi.com/2227-7102/15/1/68)\n\n[148\\. AI100: 人工智能杰出成果 - BenchCouncil](https://www.benchcouncil.org/evaluation/ai/cn/annual.html)\n\n[149\\. Science & Technology Trends 2023-2043](https://atelierdesfuturs.org/wp-content/uploads/2023/09/NATO_stt23-vol2.pdf)\n\n[150\\. Artificial Intelligence Index Report 2023 - European Commission](https://ec.europa.eu/newsroom/rtd/redirection/item/813110#:~:text=The%202023%20report%20includes%20new,2022%20to%20127%20in%202023.)\n\n[151\\. AI Index 2023 주요 내용과 시사점 Summary and Implications of 2023 AI Index Report](https://spri.kr/download/23262)\n\n[152\\. 报告揭示AI科研之路：告别“广撒网” 进入应用导向新阶段](https://finance.sina.com.cn/jjxw/2025-07-07/doc-infervxe9628653.shtml)\n\n[153\\. Global Trends in R&D 2023: Activity, Productivity, and Enablers](https://www.farmindustria.it/app/uploads/2023/02/iqvia-institute-global-trends-in-rd-2023-forweb.pdf)\n\n[154\\. Jeong-Soo Lee, Jungwon Cho. “Artificial Intelligence Curriculum Development for Intelligent System Experts in University.” International Journal on Advanced Science, Engineering and Information Technology](https://doi.org/10.18517/ijaseit.14.2.18860)\n\n[155\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2024.” ArXiv](https://doi.org/10.48550/arXiv.2405.19522)\n\n[156\\. Artificial Intelligence for Science - Adoption Trends and Future Development Pathways](https://www.conradsanderson.id.au/pdfs/hajkowicz_AI_for_science_CSIRO_2022.pdf)\n\n[157\\. A decade of research contributions and emerging trends in the ...](https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-025-00533-7)\n\n[159\\. Scientific AI for the Physical Sciences](http://www.jasonmcewen.org/talks/ScientificAI_ICML-London_2024.pdf)\n\n[160\\. Generative AI in Physics Education: Evaluating Tools and ...](https://link.springer.com/chapter/10.1007/978-3-031-94171-9_38)\n\n[161\\. The Nobel Prize in Physics 2024](https://www.drishtiias.com/pdf/1729589110.pdf)\n\n[162\\. AI Magazine](https://aiinstitutes.org/wp-content/uploads/AI-Magazine-Spring-2024-NSFs-National-AI-Institutes.pdf)\n\n[163\\. M. Behandish, J. Maxwell et al. “AI Research Associate for Early-Stage Scientific Discovery.” ArXiv](https://arxiv.org/abs/2202.03199)\n\n[164\\. Sustainable AI 2025: AI for a Greener Planet](https://nanoschool.in/wp-content/uploads/2024/12/Sustainable-AI-2025-Brochure-4.pdf?srsltid=AfmBOoqLkVCE6WL0SAei7qDtf-wxcSnay2k223hyxq6pRx27Pf3RPNyh)\n\n[165\\. AI meets physics: a comprehensive survey](https://link.springer.com/content/pdf/10.1007/s10462-024-10874-4.pdf)\n\n[166\\. 人工智能风云录与OpenAI权力的游戏](https://lib.scu.edu.cn/sites/default/files/2025-03/20250310.pdf)\n\n[167\\. 人工智能的诺奖时刻：重塑科学的未来](https://news.qq.com/rain/a/20250203A039A700)\n\n[168\\. International Journal of Artificial Intelligence (AI) in Scientific Disciplines (IJAISD)](https://www.igi-global.com/journal/international-journal-artificial-intelligence/334590)\n\n[169\\. GitHub - openags/Awesome-AI-Scientist-Papers: A collec...](https://github.com/openags/Awesome-AI-Scientist-Papers)\n\n[170\\. Boston University Photonics Center Annual Report 2024](https://www.bu.edu/photonics/files/2024/10/Photonics-Annual-Report-2024.pdf)\n\n[171\\. AI Magazine, Volume 45](https://dblp.org/db/journals/aim/aim45)\n\n[172\\. State estimation of Takagi–Sugeno systems with unmeas...](https://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal&amp;id=ICTADW000004000005000897000001&amp;idtype=cvips&amp;gifs=Yes)\n\n[173\\. Development of Physics-Based AI Systems: Focusing on Neural Operator and PINN](https://www.matlabexpo.com/content/dam/mathworks/mathworks-dot-com/company/events/conferences/matlab-expo-korea/2024/kr-expo-2024-postech-physics-based-ai-rev.pdf)\n\n[174\\. AI for Science: In-Depth Analysis of 2024 Nobel Prize Results and Suggestions for Guangdong's Development](https://www.atlantis-press.com/article/126009935.pdf)\n\n[175\\. Hot Paper 新上线学术论文 \\[2024.03.01\\]](https://zhuanlan.zhihu.com/p/684817931)\n\n[176\\. Claire Malone. “Trust me? I’m an AI!.” Physics World](https://doi.org/10.1088/2058-7058/37/12/25)\n\n[177\\. Role of AI in experimental materials science](https://par.nsf.gov/servlets/purl/10420938)\n\n[178\\. Materials Futures - 期刊影响因子12.06 - ivySCI](https://ivysci.com/journals/2752-5724/?lang=zh)\n\n[179\\. Bologna with Students Eyes 2024](https://esu-online.org/wp-content/uploads/2024/06/ESU-BWSE-2024-1.pdf)\n\n[180\\. AI政策文件](http://www.lib.dlut.edu.cn/index/xxsyjy/AIsyzt/AIzcwj.htm)\n\n[181\\. AI and Digital Inequities: Policy Insights](https://repository.graduateinstitute.ch/record/302738/files/policy-insights-ai-and-digital-inequities.pdf)\n\n[182\\. Орта білім беру жүйесінде жасанды интеллектті колдану бойынша әдістемелік ұсынымдар](https://uba.edu.kz/storage/app/media/ISKUST%20INTELLEKT%20%20KAZ%2026.06.24.pdf)\n\n[183\\. The Bologna Process as a Multidimensional Architecture of Policy Diffusion in Western Europe | SpringerLink](https://link.springer.com/chapter/10.1007/978-3-031-25867-1_18)\n\n[184\\. Generative AI in Higher Education: The ChatGPT Effect](https://trascendit-corp.com/wp-content/uploads/2024/10/LIBRO-02-IA-Generativa-para-Educacion-Superior-El-Efecto-ChatGPT.pdf)\n\n[185\\. THE POTENTIAL IMPACT OF ARTIFICIAL INTELLIGENCE ON EQUITY AND INCLUSION IN EDUCATION](https://www.oecd.org/content/dam/oecd/en/publications/reports/2024/08/the-potential-impact-of-artificial-intelligence-on-equity-and-inclusion-in-education_0d7e9e00/15df715b-en.pdf)\n\n[186\\. Unesco「教育研究における生成aiに関するガイダンス」を一部修正 - 大阪教育文化センター](https://osaka-kyoubun.org/archives/7104)\n\n[187\\. 教育研究における生成AIに関するガイダンス](https://osaka-kyoubun.org/wp/wp-content/uploads/386693eng_jpn.pdf)\n\n[188\\. UNESCO Guidance for generative AI in education and research and AI Competency Frameworks](https://neqmap.bangkok.unesco.org/wp-content/uploads/2023/09/UNESCO-Guidance-and-AI-frameworks.pdf)\n\n[189\\. Self-Evaluation Institutional Report](https://360.uaic.ro/wp-content/uploads/raportautoevaluare.pdf)\n\n[190\\. R. Keeling. “The Bologna Process and the Lisbon Research Agenda: the European Commission’s expanding role in higher education discourse.” European Journal of Education](https://doi.org/10.1111/J.1465-3435.2006.00256.X)\n\n[191\\. Pauline Ravinet. “From Voluntary Participation to Monitored Coordination: why European countries feel increasingly bound by their commitment to the Bologna Process.” European Journal of Education](https://doi.org/10.1111/J.1465-3435.2008.00359.X)\n\n[192\\. Taina Saarinen, Timo Ala-Vähälä. “Accreditation, the Bologna Process and National Reactions: Accreditation as Concept and Action.” Higher Education in Europe](https://doi.org/10.1080/03797720802066195)\n\n[193\\. Tanja Milati. “Towards the European Higher Education Area.” European Journal of Social Work](https://doi.org/10.1080/714889991)\n\n[194\\. Main publications of UNESCO on digital learning and AI in Education](https://www.lamar.edu/lu-online/_files/documents/luonline/ctle/unesco_publications_on_digital_learning_ai_in_education_1711974706.pdf)\n\n[195\\. C. Chan, T. Colloton. “Generative AI in Higher Education.”](https://doi.org/10.4324/9781003459026)\n\n[196\\. Overview and summary of AI competency framework for ...](https://www.degruyterbrill.com/document/doi/10.1515/gme-2024-0029/html?lang=en&srsltid=AfmBOoq5qjUD0KCo3jNoaaf8J_zimUyecX1vgdm6xMDPLfqys-kCBihZ)\n\n[197\\. Artificial Intelligence — University of Bologna](https://www.unibo.it/en/university/statute-standards-strategies-and-reports/artificial-intelligence)\n\n[199\\. NUS Giving Report 2022/2023](https://nus.edu.sg/nusgiving/docs/default-source/default-document-library/nusgiving23-flipbook.pdf)\n\n[200\\. Artificial Intelligence Index Report 2023](https://event-cdn.baai.ac.cn/file/file-browser/C7FA4aFhrT2Hrnm77AKZPww62Ywm7Pyk.pdf)\n\n[201\\. Lijia Chen, Pingping Chen et al. “Artificial Intelligence in Education: A Review.” IEEE Access](https://doi.org/10.1109/ACCESS.2020.2988510)\n\n[202\\. Artificial Intelligence Index Report 2024](https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024.pdf)\n\n[203\\. The Artificial Intelligence Application Model In Higher Education Towards World-Class University](https://www.atlantis-press.com/article/126008471.pdf)\n\n[204\\. Current Academic Studies in Technology and Education 2023](https://www.isres.org/books/978-625-6959-35-4_31-12-2023.pdf)\n\n[205\\. A decade of research contributions and emerging trends in the ...](https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-025-00533-7)\n\n[206\\. Doctoral Consortium 2023](https://chitaly2023.it/call-for-doctoral-consortium-papers/)\n\n[207\\. THE STANFORD EMERGING TECHNOLOGY REVIEW 2023](https://setr.stanford.edu/sites/default/files/2023-11/SETR_web_231120.pdf)\n\n[208\\. The Adoption of Artificial Intelligence in Firms: New Evidence for Policymaking](https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/05/the-adoption-of-artificial-intelligence-in-firms_8fab986b/f9ef33c3-en.pdf)\n\n[209\\. Årsredovisning 2023](https://www.sh.se/download/18.31ef176418d82bc19609aad4/1708696032245/SH%20%C3%85rsredovisning%202023.pdf)\n\n[210\\. Global Data Consortium: Artificial Intelligence’s Essential Role in the Future of Universities](https://www.acenet.edu/Documents/Global-Data-Consortium-Working-Draft.pdf)\n\n[211\\. Olaf Zawacki-Richter, Victoria I. Marín et al. “Systematic review of research on artificial intelligence applications in higher education – where are the educators?.” International Journal of Educational Technology in Higher Education](https://doi.org/10.1186/s41239-019-0171-0)\n\n[212\\. Research trends in the use of artificial intelligence in higher education](https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2024.1438715/pdf)\n\n[213\\. 斯坦福大学 AI 报告：AI 博士毕业生向产业界的流动加速_ZAKER新闻](https://www.myzaker.com/article/662095708e9f092f887b5dd1)\n\n[214\\. Francisco-Javier Hinojo-Lucena, Inmaculada Aznar-Díaz et al. “Artificial Intelligence in Higher Education: A Bibliometric Study on its Impact in the Scientific Literature.” Education Sciences](https://doi.org/10.3390/EDUCSCI9010051)\n\n[215\\. Artificial intelligence in higher education institutions: review of ...](https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2025.1530247/full#:~:text=Artificial%20Intelligence%20tools%20are%20found,issues%20and%20data%20fabrication%20issues.)\n\n[216\\. Annual Report 2023](https://hai.stanford.edu/sites/default/files/2024-04/2023-Annual-Report.pdf)\n\n[217\\. Bo ram Cho. “Research Trend of University AI-based education.” Korean Association For Learner-Centered Curriculum And Instruction](https://doi.org/10.22251/jlcci.2025.25.1.447)\n\n[219\\. Generative AI Act II: Test Time Scaling Drives Cognition Engineering](https://openreview.net/pdf/4fb6e82aa8b81d04641c3561e9acb0d2175e2698.pdf)\n\n[220\\. AI-driven research in pure mathematics and theoretical physics](https://www.nature.com/articles/s42254-024-00740-1)\n\n[221\\. TOWARDS AGENTIC AI FOR SCIENCE: HYPOTHESIS GENERATION, COMPREHENSION, QUANTIFICATION, AND VALIDATION](https://openreview.net/pdf?id=EDr8rXrtPt)\n\n[222\\. PAI3: Artificial Intelligence that Belongs to the People](https://files.gitbook.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FAcwxwOlsxqPn3KqGT1FU%2Fuploads%2F8oCEc9WCYlAXBpfV2s9K%2FPAI3-%20%20Artificial%20Intelligence%20that%20Belongs%20to%20the%20People%20-%20White%20paper.pdf?alt=media&token=816a7093-15a0-4d)\n\n[223\\. SPRI AI Brief | 2025년 4월호](https://spri.kr/download/23629)\n\n[224\\. 人工智能加速科学仿真、设计、控制和发现](https://www.360doc.cn/article/47115229_1151770585.html)\n\n[225\\. 2025 年4月20 日随笔档案- 吴建明wujianming](https://www.cnblogs.com/wujianming-110117/p/archive/2025/04/20)\n\n[226\\. AI-Enabled Paradigms for Non-Intrusive Screening: Use Case Concepts and Considerations](https://www.dhs.gov/sites/default/files/2025-02/25_0211_st_ai-enabled_paradigms_for_non-intrusive_screening.pdf)\n\n[227\\. Physics and AI: A physics community perspective](https://www.iop.org/sites/default/files/2025-03/Physics-and-AI-A-physics-community-perspective.pdf)\n\n[228\\. Artificial Intelligence Index Report 2024](https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_Chapter2.pdf)\n\n[229\\. A Inteligência Artificial em 2024 – balanço e pontes para 2025](https://interactideas.pt/guia2024/guia.pdf)\n\n[230\\. Trieu H. Trinh, Yuhuai Wu et al. “Solving olympiad geometry without human demonstrations.” Nature](https://doi.org/10.1038/s41586-023-06747-5)\n\n[231\\. Generative Physical AI in Vision: A Survey](https://arxiv.org/pdf/2501.10928)\n\n[232\\. Frontiers in Robotics and Automation Vol. 1 No. 01 (2024) The Role of Artificial Intelligence in Robotics and Automation](https://sprcopen.org/FRA/article/download/130/102)\n\n[233\\. Bao-Bing Li, Yi Gu et al. “Discover Physical Concepts and Equations with Machine Learning.”](https://arxiv.org/abs/2412.12161)\n\n[234\\. 理化学研究所 2024-2025](https://www.riken.jp/medialibrary/riken/pr/publications/pamphlets/r-pamph/r-pamph.pdf)\n\n[235\\. International AI Safety Report](https://italianelfuturo.com/wp-content/uploads/2025/01/International_AI_Safety_Report_2025_accessible_f_250130_073143.pdf)\n\n[236\\. Michael D. Schmidt, Hod Lipson. “Distilling Free-Form Natural Laws from Experimental Data.” Science](https://doi.org/10.1126/science.1165893)\n\n[237\\. Derive Like Crazy](https://www.amazon.com/dp/9798531741011)\n\n[238\\. Tailin Wu, Max Tegmark. “Toward an AI Physicist for Unsupervised Learning.” Physical review. E](https://doi.org/10.1103/PhysRevE.100.033311)\n\n[239\\. AI and Digital Inequities: Policy Insights](https://repository.graduateinstitute.ch/record/302738/files/policy-insights-ai-and-digital-inequities.pdf)\n\n[240\\. Overview and summary of AI competency framework for ...](https://www.degruyterbrill.com/document/doi/10.1515/gme-2024-0029/html?lang=en&srsltid=AfmBOoq5qjUD0KCo3jNoaaf8J_zimUyecX1vgdm6xMDPLfqys-kCBihZ)\n\n[241\\. 教育研究における生成AIに関するガイダンス](https://osaka-kyoubun.org/wp/wp-content/uploads/386693eng_jpn.pdf)\n\n[242\\. AI政策文件](http://www.lib.dlut.edu.cn/index/xxsyjy/AIsyzt/AIzcwj.htm)\n\n[243\\. 米国における教育のデータ駆動化に関する調査報告書（第三部）](https://www.rois.ac.jp/research/pdf/20240322_Survey_Report.pdf)\n\n[244\\. UNESCO and the European Union](https://www.unesco.org/en/fieldoffice/brussels)\n\n[245\\. Guidance for generative AI in education and research](https://www.oph.fi/sites/default/files/documents/Guidance%20for%20generative%20AI%20in%20education%20and%20research.pdf)\n\n[246\\. UNESCO Guidance for generative AI in education and research and AI Competency Frameworks](https://neqmap.bangkok.unesco.org/wp-content/uploads/2023/09/UNESCO-Guidance-and-AI-frameworks.pdf)\n\n[247\\. UNESCO-ICHEI's magazine CLOUD: Academic integrity in ...](https://norvalid.com/blog/unesco-ichei-magazine-cloud)\n\n[248\\. Integrating AI in Education: Navigating UNESCO Global Guidelines, Emerging Trends, and Its Intersection with Sustainable Development Goals](https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/67d5cbf681d2151a0281479b/original/integrating-ai-in-education-navigating-unesco-global-guidelines-emerging-trends-and-its-intersection-with-sustainable-development-goals.pdf)\n\n[249\\. AI in Australian Education Snapshot: Principles, Policy and Practice](https://www.esa.edu.au/docs/default-source/default-document-library/ai-in-australian-education-snapshot---principles-policy-and-practice_august-2023.pdf)\n\n[250\\. Toward an AI Policy Framework for Research Institutions](https://www.cigionline.org/documents/2520/DPH-paper-daMota.pdf)\n\n[251\\. The 22nd International Conference for Media in Education: Conference Proceedings](https://2024.icome.education/wp-content/uploads/2024/10/ICoME2024_Proceedings_final.pdf)\n\n[252\\. Hisar School Artificial Intelligence Education Policy](https://www.hisarschool.k12.tr/wp-content/uploads/2025/03/AI-Education-Policy-2025.pdf)\n\n[253\\. Main publications of UNESCO on digital learning and AI in Education](https://www.lamar.edu/lu-online/_files/documents/luonline/ctle/unesco_publications_on_digital_learning_ai_in_education_1711974706.pdf)\n\n[254\\. UNESCO: Governments Must Quickly Regulate Generative ...](https://www.wedo2018.com.cn/media/index_2381.html)\n\n[255\\. INTERNATIONAL JOURNAL OF EDUCATION & PHILOLOGY](https://www.ibu.edu.mk/wp-content/uploads/2020/05/IJEP-JOURNAL-VOLUME-4-ISSUE-2_2023-FINAL.pdf)\n\n[256\\. K. Yeung. “Recommendation of the Council on Artificial Intelligence (OECD).” International Legal Materials](https://doi.org/10.1017/ilm.2020.5)\n\n[257\\. UNESCO publishes global guidance on generative AI in education and research – Access](https://librarylearningspace.com/unesco-publishes-global-guidance-on-generative-ai-in-education-and-research/)\n\n[258\\. UNESCO-ICHEI White Paper on Higher Ed in the Era of AI](https://mooc.global/alliance/unesco-ichei-white-paper-on-higher-ed-in-the-era-of-ai/)\n\n[259\\. J. Jumper, Richard Evans et al. “Highly accurate protein structure prediction with AlphaFold.” Nature](https://doi.org/10.1038/s41586-021-03819-2)\n\n[260\\. AI-qualizing Science](https://www.biorxiv.org/content/10.1101/2025.02.11.637417v1.full.pdf)\n\n[261\\. M. Mirdita, S. Ovchinnikov et al. “ColabFold: making protein folding accessible to all.” Nature Methods](https://doi.org/10.1038/s41592-022-01488-1)\n\n[262\\. Richard Evans, Michael O’Neill et al. “Protein complex prediction with AlphaFold-Multimer.” bioRxiv](https://doi.org/10.1101/2021.10.04.463034)\n\n[263\\. Baek M, DiMaio F et al. “Accurate prediction of protein structures and interactions using a 3-track neural network.” Science (New York, N.Y.)](https://doi.org/10.1126/science.abj8754)\n\n[264\\. The 2023 Nucleic Acids Research Database Issue and the online molecular biology database collection](https://pdfs.semanticscholar.org/8938/58634fa02905a9792d540aa44bd510678716.pdf)\n\n[265\\. Kathryn Tunyasuvunakool, J. Adler et al. “Highly accurate protein structure prediction for the human proteome.” Nature](https://doi.org/10.1038/s41586-021-03828-1)\n\n[266\\. RSC Interest Group Chemical Information and Computer Applications Group Newsletter Winter 2023-24](https://www.rsccicag.org/index_htm_files/CICAG%20Newsletter%20Winter%202023-24%20FINAL.pdf)\n\n[267\\. AlphaFold结构预测的重大突破及其对蛋白质研究的影响与挑战](https://www.pibb.ac.cn/pibbcn/article/html/20240374?st=article_issue)\n\n[268\\. AlphaFold and what is next: bridging functional, systems ...](https://pubmed.ncbi.nlm.nih.gov/39824781/)\n\n[269\\. AlphaFold3 in Drug Discovery: A Comprehensive Assessment of Capabilities, Limitations, and Applications](https://www.biorxiv.org/content/10.1101/2025.04.07.647682v1.full.pdf)\n\n[270\\. AlphaFold: A Comprehensive Guide to the Revolutionary AI Tool and Its Methodology](https://ajosr.org/wp-content/uploads/journal/published_paper/volume-3/issue-1/ajsr2024_7ya2YQbT.pdf)\n\n[271\\. AlphaFold prediction and analysis of Adhesion-family G protein coupled receptor (aGPCR) structures.](https://www.biorxiv.org/content/10.1101/2024.12.17.628075.full.pdf)\n\n[272\\. Investigating Protein Flexibility Using Molecular Dynamics Simulations of α-1 Acid Glycoprotein and Large-Scale Normal Mode Analysis of AlphaFold Models](https://www.ugent.be/ea/ibitech/en/about-us/biommeda-staff-1/phd/phd_bhawna_dixit-2025.pdf)\n\n[273\\. Nobel Prize in Chemistry 2024](https://www.nature.com/research-intelligence/Nobel-Prize-Chemistry-2024)\n\n[274\\. 蛋白质结构预测模型AlphaFold2的应用进展](https://cjb.ijournals.cn/html/cjbcn/2024/5/gc24051406.htm)"
  },
  {
    "id": 45,
    "question": "What is MCP (Model Context Protocol)? How does it address the data connectivity challenges in LLM applications, and what are the differences compared to Function Calling and AI Agents?",
    "response": "**Research Report: Model Context Protocol (MCP) in LLM Applications**\n\n**1\\. Introduction to Data Connectivity Challenges in LLM Applications**\n\nLarge Language Models (LLMs) face significant data connectivity limitations that restrict their practical application. These include:\n\n**Context Window Constraints**: Inability to process large datasets or maintain long conversation histories\n\n**Integration Fragmentation**: Proprietary APIs requiring custom development for each external data source or tool\n\n**Security Risks**: Potential exposure of sensitive data when processed by cloud-based LLMs\n\n**Limited Autonomy**: Inability to dynamically discover and combine tools during operation\n\nThese challenges necessitate standardized protocols to bridge LLMs with external systems while maintaining security and flexibility.\n\n**2\\. What is Model Context Protocol (MCP)?**\n\nThe **Model Context Protocol (MCP)** is an open-source communication standard developed to enable seamless interaction between LLMs and external resources (data sources, tools, services). Created as a solution to LLM interoperability challenges, MCP functions as a \"universal adapter\" similar to USB's role in hardware connectivity \\[3\\]\\[4\\].\n\n**2.1 Core Technical Architecture**\n\nMCP employs a **multi-layered client-server architecture** with these components:\n\n|     |     |     |\n| --- | --- | --- |\n| **Layer** | **Function** | **Key Components** |\n| **Presentation** | Handles client requests & response formatting | Request routers, response serializers |\n| **Session** | Manages conversation state & context tokens | Context tracking, token management |\n| **Transport** | Handles model-server communication | Stdio transport, JSON-RPC messaging |\n| **Resource** | Manages data/tool access | Load balancing, caching systems |\n| **Protocol** | Standardizes message handling | Protocol, Client, Server classes |\n\n**(Source: \\[3\\]**\\[4\\]\\[14\\]\n\n**Operational Flow**:\n\n1.  **Host** (LLM application) initiates connection through embedded **Client**\n2.  Client connects to one or more **Servers** (lightweight data/tool interfaces)\n3.  Servers provide structured access to **Resources** (data), **Tools** (executable actions), and **Prompts** (templates)\n4.  Contextual information is integrated into the LLM's processing workflow\n\n**(Source:** \\[3\\]\\[4\\]\n\n**2.2 Core Functional Components**\n\n**Resources**: Contextual data endpoints (GET-like interfaces) exposing information to LLMs \\[10\\]\\[12\\]\n\n**Tools**: Remote-executable actions invoked by agents (e.g., database queries, API calls) \\[9\\]\\[11\\]\n\n**Prompts**: Reusable templates for consistent interaction patterns \\[10\\]\\[11\\]\n\n**(Source:** \\[9\\]\\[12\\]\n\n**3\\. How MCP Solves Data Connectivity Challenges**\n\nMCP addresses LLM integration limitations through several key innovations:\n\n**3.1 Standardization & Interoperability**\n\nReplaces fragmented APIs with a **universal JSON-RPC-based protocol** \\[4\\]\\[25\\]\n\nReduces integration complexity from MxN to M+N connections \\[28\\]\n\nEnables cross-platform compatibility through server-agnostic design \\[32\\]\n\n**3.2 Security Architecture**\n\nImplements RBAC (Role-Based Access Control) and temporary credentials \\[26\\]\\[25\\]\n\nPrevents credential proliferation through dynamic tool chaining \\[26\\]\n\nSupports OAuth 2.0 authorization framework \\[30\\]\n\nDecouples sensitive data processing from LLM execution (local handling) \\[42\\]\n\n**3.3 Context Management**\n\nDynamically tracks and preserves conversation context across sessions \\[14\\]\n\nMaintains stateful sessions for coherent multi-turn interactions \\[26\\]\n\nUses intelligent token management to optimize context window usage\n\n**3.4 Developer Experience**\n\nProvides SDKs in Python, TypeScript, Java, Kotlin, and C# \\[4\\]\\[15\\]\n\nOffers pre-built integrations for common data systems \\[25\\]\\[37\\]\n\nEnables vendor-agnostic LLM deployment \\[4\\]\n\n**Comparative Performance**:\n\nReduces integration development time by 50-70% vs. custom APIs \\[28\\]\n\nImproves response accuracy by 30-45% through better context handling \\[28\\]\n\nLowers latency by minimizing redundant data processing \\[28\\]\n\n**4\\. Comparative Analysis: MCP vs. Function Calling vs. AI Agents**\n\n**4.1 Functional Comparison**\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n| **Capability** | **Function Calling** | **MCP** | **AI Agents** |\n| **External Tool Access** | Predefined functions only | Dynamic tool discovery | Programmatic access |\n| **Context Awareness** | Limited to current query | Multi-session stateful context | Varies by implementation |\n| **Security Model** | Data exposed to LLM | Local data handling + RBAC | Implementation-dependent |\n| **Standardization** | Provider-specific | Open protocol | No inherent standard |\n| **Autonomy Level** | Caller-directed | Agent-driven discovery | Goal-oriented autonomy |\n| **Use Cases** | Simple data retrieval | Enterprise integrations | Complex workflows |\n\n**(Source: \\[43\\]**\\[45\\]\\[42\\]\n\n**4.2 Architectural Differences**\n\n**Function Calling**:\n\nTightly coupled with LLM architecture\n\nRequires predefined JSON schemas for each function\n\nProcesses data within LLM execution context (security risk)\n\nLimited to synchronous operations \\[55\\]\n\n**MCP**:\n\nDecoupled client-server model\n\nStateless protocol with stateful session management\n\nTransport-layer security with end-to-end encryption\n\nSupports asynchronous operations \\[26\\]\\[30\\]\n\n**AI Agents**:\n\nTypically incorporate MCP or similar protocol as a component\n\nInclude additional layers: memory systems, planning engines \\[42\\]\\[42\\]\n\nFocus on goal decomposition and workflow orchestration\n\n**(Source: \\[3\\]**\\[26\\]\\[42\\]\n\n**4.3 Security & Data Handling**\n\n**Function Calling**: Sensitive data transmitted to LLM providers \\[42\\]\n\n**MCP**: Implements zero-trust principles with local execution of sensitive operations \\[42\\]\n\n**AI Agents**: Security depends on implementation; may inherit MCP's advantages when integrated\n\n**4.4 Ecosystem Flexibility**\n\nMCP enables a **composable integration marketplace** where data sources and tools can be mixed dynamically \\[30\\]\\[25\\]\n\nFunction Calling creates **vendor lock-in** to specific LLM providers \\[45\\]\n\nAI Agents use MCP to create **interoperable agent networks** \\[42\\]\\[42\\]\n\n**5\\. Challenges and Limitations of MCP**\n\nDespite its advantages, MCP faces implementation hurdles:\n\n**Initial Setup Complexity**: Learning curve for protocol implementation \\[28\\]\n\n**Resource Management**: Potential overhead when running multiple servers \\[29\\]\n\n**Evolving Standards**: Rapid protocol development requires continual updates \\[29\\]\n\n**Tool Discovery**: Requires registry systems for dynamic resource location\n\n**6\\. Conclusion: The Evolving LLM Connectivity Landscape**\n\nModel Context Protocol represents a paradigm shift in LLM application design by:\n\n1.  **Standardizing** the connection layer between LLMs and external systems\n2.  **Enabling secure**, context-aware interactions previously impossible with function calling\n3.  **Empowering AI agents** with dynamic tool discovery and composition capabilities\n4.  **Reducing development friction** through open protocols and multi-language SDKs\n\nWhile function calling serves basic integration needs, MCP provides the infrastructure for enterprise-grade AI applications. Its decoupled architecture makes it particularly valuable for sensitive or complex implementations where data security and system interoperability are paramount. AI agents leverage MCP as a foundational component, using it to access the tools and contextual information needed for sophisticated autonomous operations. As the protocol matures, MCP is positioned to become the backbone of next-generation LLM ecosystems.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. 2025 年7月9 日随笔档案- 葡萄城技术团队](https://www.cnblogs.com/powertoolsteam/p/archive/2025/07/09)\n\n[2\\. What is Model Context Protocol? | A Practical Guide by K2view](https://www.k2view.com/what-is-model-context-protocol/#:~:text=Model%20Context%20Protocol%20%28MCP%29,-Objective&text=MCP%20establishes%20a%20secure,%20standardized,%28accessed%20via%20MCP%20servers%29.&text=MCP%20defines%20a%20protocol%20for,its%20servers%20%28data%20sources%29.)\n\n[3\\. Core architecture - Model Context Protocol](https://modelcontextprotocol.io/docs/concepts/architecture)\n\n[4\\. What is the Model Context Protocol (MCP)?](https://learnprompting.org/blog/what-is-model-context-protocol?srsltid=AfmBOop11bAOKrvb4JV7xvC-KXYN76-SM3-QNdMdMeJ-chb5_Bjm-YKz)\n\n[5\\. Understanding the Model Context Protocol (MCP)](https://nebius.com/blog/posts/understanding-model-context-protocol-mcp-architecture)\n\n[6\\. Model Context Protocol (MCP) Server Development Guide](https://github.com/cyanheads/model-context-protocol-resources/blob/main/guides/mcp-server-development-guide.md)\n\n[7\\. Model Context Protocol · GitHub](https://github.com/modelcontextprotocol/)\n\n[8\\. Model Context Protocol (MCP)](https://www.ymshici.com/tech/2376.html)\n\n[9\\. A Study on the MCP × A2A Framework for Enhancing Interoperability of LLM-based Autonomous Agents](https://arxiv.org/pdf/2506.01804)\n\n[10\\. Model Context Protocol (MCP): Everything you need to know about connecting your data services to LLM Agents](https://www.jfokus.se/aifokus25-preso/MCP-for-LLM-Agents-Infrastructure-APIs-and-Data-Services.pdf)\n\n[11\\. Beyond the Protocol: Unveiling Attack Vectors in the Model Context Protocol Ecosystem](http://www.arxiv.org/pdf/2506.02040)\n\n[12\\. MCP: Model Context Protocol](https://lamroger.com/slides/MCP:%20Model%20Context%20Protocol.pdf)\n\n[13\\. Introduction - Model Context Protocol](https://modelcontextprotocol.io/introduction?ref=localfirstnews.com)\n\n[14\\. Model Context Protocol Server Setup Guide](https://www.byteplus.com/en/topic/541333)\n\n\\[15. Model Context Protocol - GitHub \\](https://github.com/modelcontextprotocol#:~:text=The%20Model%20Context%20Protocol%20(MCP,external%20data%20sources%20and%20tools.)\n\n[16\\. What is Model Context Protocol? | A Practical Guide by ...](https://www.k2view.com/model-context-protocol/)\n\n[17\\. MCP راهنمای کاربردی](https://www.dntips.ir/exports/news/www.dntips.ir-news-20368.pdf)\n\n[18\\. A Survey of the Model Context Protocol (MCP): Standardizing Context to Enhance Large Language Models (LLMs)](https://www.preprints.org/manuscript/202504.0245/download/final_file)\n\n[21\\. ...LLM applications and external data sources and tools.](https://github.com/WebSynapse-com/mcp-go)\n\n[22\\. Model Context Protocol (MCP): Everything you need to know about connecting your data services to LLM Agents](https://www.jfokus.se/aifokus25-preso/MCP-for-LLM-Agents-Infrastructure-APIs-and-Data-Services.pdf)\n\n[23\\. MCP: Model Context Protocol](https://lamroger.com/slides/MCP:%20Model%20Context%20Protocol.pdf)\n\n[24\\. Technology Convergence Report](https://reports.weforum.org/docs/WEF_Technology_Convergence_Report_2025.pdf)\n\n[25\\. A Survey of the Model Context Protocol (MCP): Standardizing Context to Enhance Large Language Models (LLMs)](https://www.preprints.org/manuscript/202504.0245/download/final_file)\n\n[26\\. MODEL CONTEXT PROTOCOL: THE USB-C FOR AGENTIC AI](https://methodhub.in/wp-content/uploads/2025/05/White-Paper_7_14-May.pdf)\n\n[27\\. Model Context Protocol](https://github.com/modelcontextprotocol)\n\n[28\\. Model Context Protocol Mcp Enabling Scalable Ai Data Integration](https://www.ijfmr.com/papers/2025/2/43583.pdf)\n\n[29\\. Data Science for Losers](https://www.dntips.ir/exports/newstag/www.dntips.ir-newstag-2110.pdf)\n\n[30\\. Jan-Niclas Hilgert, Carlo Jakobs et al. “Chances and Challenges of the Model Context Protocol in Digital Forensics and Incident Response.”](https://arxiv.org/abs/2506.00274)\n\n[31\\. What Is the Model Context Protocol (MCP) and How It Works](https://www.descope.com/learn/post/mcp#:~:text=Several%20code%20editors%20and%20IDEs,which%20implements%20MCP%20through%20OpenCtx)\n\n[32\\. GitHub - PederHP/mcpdotnet: .NET implementation of the...](https://github.com/PederHP/mcpdotnet)\n\n[33\\. MCP enterprise deployments combine company data with ...](https://www.k2view.com/blog/mcp-enterprise)\n\n[34\\. Introduction - Model Context Protocol](https://modelcontextprotocol.io/introduction?ref=localfirstnews.com)\n\n[35\\. MCP-Solver: Integrating Language Models with Constrain...](http://arxiv.org/html/2501.00539v1)\n\n[36\\. Technology Radar: An opinionated guide to today's technology landscape](https://www.thoughtworks.cn/content/dam/thoughtworks/documents/radar/2025/04/tr_technology_radar_vol_32_en.pdf)\n\n[37\\. Model Context Protocol (MCP) :: Spring AI Reference](https://docs.spring.io/spring-ai/reference/api/model-context-protocol.html)\n\n[41\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[42\\. A Survey of AI Agent Protocols](https://media.licdn.com/dms/document/media/v2/D4E1FAQGEX5OgViSaHg/feedshare-document-pdf-analyzed/B4EZaXRu.lHYAY-/0/1746294740993?e=1747267200&v=beta&t=kZ3AjxGVZLywF378HjkBea5Tae_okm6C3LCe6jh_jWA)\n\n[43\\. Agent学习之：MCP和Function Call - 极客产品GeekPM](https://geekpm.com/archives/mcp-fuctioncalling)\n\n[44\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[45\\. AI Agent深度（二）：2025 Agent元年，AI从L2向L3发展](https://pdf.dfcfw.com/pdf/H3_AP202505041667413598_1.pdf?1746459426000.pdf)\n\n[46\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[47\\. 比较LLM的function calling，Agent 和MCP - iTech](https://www.cnblogs.com/itech/p/18799272)\n\n[48\\. MCP (Model Context Protocol) ovvero come liberare gli LLM e dargli accesso a tutto il mondo](https://agoracdn.blob.core.windows.net/cms/Content/Uploads/Talks/Files/f5d651f4-ae60-44ed-9fb9-6635566454fd/250508_GENAI_MCP.pdf)\n\n[49\\. Xi Wang, Hongliang Dai et al. “Characteristic AI Agents via Large Language Models.” International Conference on Language Resources and Evaluation](https://doi.org/10.48550/arXiv.2403.12368)\n\n[50\\. AI 领域的 Agent 是什么？](https://weite.ltd/usr/uploads/2025/04/1480904696.pdf)\n\n[51\\. AI 发展 && MCP-CSDN博客](https://blog.csdn.net/weixin_42525672/article/details/149494133)\n\n[52\\. LLM ：Function Call、MCP协议与A2A协议原创](https://blog.csdn.net/dongnihao/article/details/147774522)\n\n[53\\. MCP 和 Function Calling_function calling和mcp-CSDN博客](https://blog.csdn.net/a1256afafaafr/article/details/148556415)\n\n[54\\. Model Context Protocol 是什么？它和Function Calling、AI ...](https://cloud.tencent.com.cn/developer/article/2518470?policyId=1003)\n\n[55\\. Abul Ehtesham, Aditi Singh et al. “A survey of agent interoperability protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP).”](https://arxiv.org/abs/2505.02279)"
  },
  {
    "id": 46,
    "question": "How should the development of generative AI evolve: focusing on dialogue-based systems (Chat) or autonomous action-taking systems (Agent)? What are the key differences, technological requirements, and future implications of each approach?",
    "response": "**The Evolution of Generative AI: Dialogue-Based Systems vs. Autonomous Agents**\n\n**I. Introduction**\n\nGenerative AI development stands at a crossroads: **dialogue-based systems** (e.g., chatbots) focus on human-like conversation, while **autonomous agents** prioritize decision-making and physical action. As of July 2025, this tension shapes AI's future trajectory. This report analyzes architectural differences, technological requirements, safety implications, and performance metrics to guide development priorities.\n\n**II. Core Architectural and Functional Differences**\n\n**A. Dialogue-Based Systems (Chat)**\n\n**Architecture**: Rely on transformer models with self-attention mechanisms, optimized for sequential data processing and contextual coherence \\[1\\]\\[15\\].\n\n**Functionality**: Excel at language tasks (e.g., sentiment analysis, Q&A), generating responses to user prompts without autonomous action \\[7\\]\\[10\\]\\[13\\].\n\n**Limitations**: Lack environmental interaction and task execution capabilities; constrained to text or voice outputs \\[6\\]\\[13\\].\n\n**B. Autonomous Agents (Agent)**\n\n**Architecture**: Combine reinforcement learning (RL) with foundation models (e.g., LLMs) for cognition, integrating modules for memory, planning, and tool use \\[6\\]\\[14\\]\\[11\\].\n\n**Functionality**: Perform actions in physical/digital environments (e.g., supply chain optimization, robotic manipulation), learning via reward signals \\[7\\]\\[16\\]\\[19\\].\n\n**Advantages**: Execute multi-step tasks independently (e.g., inventory management, patient diagnostics) \\[10\\]\\[13\\].\n\n**Key Distinction**: Chat systems focus on \"**language exchange**\"; agents prioritize \"**thinking, decision-making, and task execution**\" \\[6\\]\\[10\\]\\[13\\].\n\n**III. Technological Requirements and Implementation Challenges**\n\n**A. Dialogue Systems**\n\n1.  **Context Understanding**: Must handle sarcasm, slang, and multilingual inputs \\[63\\].\n2.  **Emotional Intelligence**: Require advanced sentiment interpretation for natural responses \\[63\\]\\[73\\].\n3.  **Data Pipelines**: Need high-quality annotated datasets to mitigate biases and hallucinations \\[63\\].\n4.  **Integration**: Struggle with legacy systems (e.g., EHRs) and customization for niche scenarios \\[28\\]\\[67\\].\n\n**B. Autonomous Agents**\n\n1.  **Safety Constraints**: Critical for action systems; require explicit risk-mitigation modules (e.g., inhibiting hazardous actions) \\[35\\].\n2.  **Multi-Step Reasoning**: Demand robust planning architectures to avoid error cascades in long chains \\[66\\]\\[135\\].\n3.  **Infrastructure**: Depend on modern data cores and digital platforms; legacy systems hinder deployment \\[57\\]\\[72\\].\n4.  **Cost**: RL training and LLM inference incur high computational expenses \\[58\\]\\[135\\].\n\n**Critical Gap**: Agents face **reliability challenges** (58% single-step success; 35% multi-step) vs. chatbots’ lower-stakes but persistent hallucination risks \\[135\\]\\[218\\].\n\n**IV. Performance and Reliability Metrics**\n\n**A. Task Success and Failure Rates (2024–2025)**\n\n**Agents**: Achieve 35% multi-step success in enterprise tasks (e.g., supply chain planning); failures spike with task complexity \\[135\\]\\[218\\].\n\n**Chatbots**: Attain 88.3% task completion in controlled dialog benchmarks but struggle with ambiguity \\[46\\]\\[46\\].\n\n**B. Hallucination Comparison**\n\n|     |     |     |\n| --- | --- | --- |\n| **System Type** | **Hallucination Rate** | **Context** |\n| Transformer Chatbots | 15–34% | General Q&A (e.g., GPT-4 at 15.07%) \\[82\\]\\[88\\] |\n| RL Agents | 6–85% | Varies by task; ReAct reduces rates to 6% in HotpotQA \\[125\\]\\[199\\] |\n\n**Reactivity Matters**: Agents using **ReAct paradigms** or **multi-agent debates** cut hallucinations by 8–20% via iterative reasoning \\[190\\]\\[199\\]\\[237\\].\n\n**V. Safety and Ethical Implications**\n\n**A. Healthcare Case Study**\n\n**Chatbots**: Risk misdiagnosis if hallucinations occur but allow human oversight; require HIPAA-compliant data handling \\[23\\]\\[27\\].\n\n**Agents**: Demand rigorous clinical validation for autonomous actions (e.g., surgery); errors have immediate life-or-death consequences \\[26\\]\\[30\\]\\[35\\].\n\n**B. Ethical Frameworks**\n\n**2025 Trends**: EU’s AI Act (based on AI4People principles) emphasizes **explainability** and **human accountability** for agents \\[97\\]\\[106\\].\n\n**Physical Actions**: Require \"**safety constraints**\" (e.g., blocking unsafe decisions) absent in text systems \\[35\\].\n\n**Global Gaps**: No agent-specific framework exists; regulations lag behind embodied AI deployment \\[96\\]\\[107\\].\n\n**VI. Future Implications**\n\n**A. Economic Impact**\n\n**Agents**: Will automate 15% of daily tasks by 2028 (e.g., supply chain logistics), saving billions but requiring $20B+ in infrastructure \\[128\\]\\[141\\].\n\n**Chatbots**: Reduce labor costs (e.g., $80B in contact centers) but create fewer transformative workflows \\[225\\].\n\n**B. Strategic Recommendations**\n\n1.  **Hybrid Systems**: Deploy chatbots for user interfaces atop agentic backends (e.g., customer service → inventory automation).\n2.  **Safety Investment**: Prioritize agent reliability via ReAct/multi-agent debate to cut multi-step failures below 50% by 2023.\n3.  **Regulation**: Fast-track standards for autonomous actions in healthcare/safety-critical domains.\n\n**VII. Conclusion**\n\nGenerative AI’s evolution **must integrate both paradigms**: chatbots for accessible human-AI interaction, and agents for tangible value creation. However, **autonomous systems demand disproportionate investment** in safety, infrastructure, and ethical guardrails. By 2030, agents will dominate enterprise applications, but their success hinges on solving reliability gaps exposed in 2025 benchmarks \\[135\\]\\[218\\]. Development should prioritize modular designs where chatbots handle user input while agents execute actions, ensuring scalability without sacrificing safety.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[2\\. R. S. Sutton, A. Barto. “Reinforcement Learning: An Introduction.” IEEE Trans. Neural Networks](https://doi.org/10.1109/TNN.1998.712192)\n\n[3\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Playing Atari with Deep Reinforcement Learning.” ArXiv](https://arxiv.org/abs/1312.5602)\n\n[4\\. B. Kröse. “Learning from delayed rewards.” Robotics Auton. Syst.](https://doi.org/10.1016/0921-8890%2895%2900026-C)\n\n[5\\. Jiwei Li, Will Monroe et al. “Deep Reinforcement Learning for Dialogue Generation.” ArXiv](https://doi.org/10.18653/v1/D16-1127)\n\n[6\\. AI 领域的 Agent 是什么？](https://weite.ltd/usr/uploads/2025/04/1480904696.pdf)\n\n[7\\. 人工智能概述](http://www.tup.tsinghua.edu.cn/upload/books/yz/101881-01.pdf)\n\n[8\\. Sheetal Kusal, S. Patil et al. “AI-Based Conversational Agents: A Scoping Review From Technologies to Future Directions.” IEEE Access](https://doi.org/10.1109/ACCESS.2022.3201144)\n\n[9\\. Hao Liu, P. Abbeel. “Emergent Agentic Transformer from Chain of Hindsight Experience.” ArXiv](https://doi.org/10.48550/arXiv.2305.16554)\n\n[10\\. 生成式 AI 的发展方向,是 Chat 还是 Agent? - 季春二九 - ...](https://www.cnblogs.com/jichun29/p/18728428)\n\n[11\\. Prashik Buddhaghosh Bansod. “Distinguishing Autonomous AI Agents from Collaborative Agentic Systems: A Comprehensive Framework for Understanding Modern Intelligent Architectures.”](https://arxiv.org/abs/2506.01438)\n\n[12\\. Jennifer She, Jayesh K. Gupta et al. “Agent-Time Attention for Sparse Rewards Multi-Agent Reinforcement Learning.” Adaptive Agents and Multi-Agent Systems](https://doi.org/10.5555/3535850.3536089)\n\n[13\\. 生成式 AI 的发展方向,是 Chat 还是 Agent?\\_手机新浪网](http://finance.sina.com.cn/cj/2025-01-17/doc-ineffpzi5551918.shtml)\n\n[14\\. Johannes Schneider. “Generative to Agentic AI: Survey, Conceptualization, and Challenges.”](https://arxiv.org/abs/2504.18875)\n\n[15\\. The transformer architecture](https://www.bilibili.com/video/av654528419)\n\n[16\\. Smita Adhikari, Bhawana Dhakal. “Revolutionizing Natural Language Processing with GPT-based Chatbots: A Review.” Technical Journal](https://doi.org/10.3126/tj.v3i1.61943)\n\n[17\\. 生成式 AI 的发展方向:Chat 还是 Agent?-腾讯云开发者社区...](https://cloud.tencent.com/developer/article/2478387)\n\n[18\\. M. Nandakishor, M. Anjali. “Continuous Learning Conversational AI: A Personalized Agent Framework via A2C Reinforcement Learning.”](https://arxiv.org/abs/2502.12876)\n\n[19\\. N.P. Novikov, V. Vinogradov. “Experience in Using the Transformer Network Architecture to Approximate Agent’s Policy in Reinforcement Learning.” Моделирование и анализ данных](https://doi.org/10.17759/mda.2024140201)\n\n[21\\. Safety, Security, and Reliability of Autonomous Vehicle Software](https://reu.techconf.org/document/06-Reading-materials/IEEE-Computer-2021-08-Autonomous-Vehicle-Software.pdf)\n\n[22\\. Conversational agents in health care: expert interviews to inform the definition, classification, and conceptual framework](https://dr.ntu.edu.sg/bitstream/10356/173147/2/PDF.pdf)\n\n[23\\. Conversational AI in healthcare: 8 real-world use cases](https://orangesoft.co/blog/conversational-ai-in-healthcare)\n\n[24\\. Guide to Conversational AI in Healthcare](https://www.salesforce.com/healthcare-life-sciences/healthcare-artificial-intelligence/ai-in-healthcare/conversational-ai/)\n\n[25\\. J. Weizenbaum. “ELIZA—a computer program for the study of natural language communication between man and machine.” Communications of the ACM](https://doi.org/10.1145/365153.365168)\n\n[26\\. Health Care AI: Law, Regulation, and Policy](https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1357&context=book_chapters)\n\n[27\\. Conversational AI in healthcare: Just what the doctor ordered](https://www.k2view.com/blog/conversational-ai-in-healthcare/)\n\n[28\\. Conversational AI in healthcare - Version 2](https://version-2.com/zh/2024/11/conversational-ai-in-healthcare/)\n\n[29\\. L. Laranjo, A. Dunn et al. “Conversational agents in healthcare: a systematic review.” Journal of the American Medical Informatics Association : JAMIA](https://doi.org/10.1093/jamia/ocy072)\n\n[30\\. Artificial Intelligence in Healthcare](https://www.ama.com.au/sites/default/files/2023-08/Artificial%20Intelligence%20in%20Healthcare%20-%20AMA.pdf)\n\n[31\\. CONVERSATIONAL MEDICAL AI: READY FOR PRACTICE](http://arxiv.org/pdf/2411.12808)\n\n[32\\. K. Fitzpatrick, Alison M Darcy et al. “Delivering Cognitive Behavior Therapy to Young Adults With Symptoms of Depression and Anxiety Using a Fully Automated Conversational Agent (Woebot): A Randomized Controlled Trial.” JMIR Mental Health](https://doi.org/10.2196/mental.7785)\n\n[33\\. When Friends Turn Fatal: Conversational AI’s Safety Meltdown](https://www.shannonnovak.com/s/Conversational-AI-agent-safety-review-Novak-2025.pdf)\n\n[34\\. Conversational AI in Healthcare: Benefits, 5 Real-life Cases](https://spsoft.com/tech-insights/implementing-conversational-ai-in-healthcare/)\n\n[35\\. Paul Festor, Yan Jia et al. “Assuring the safety of AI-based clinical decision support systems: a case study of the AI Clinician for sepsis treatment.” BMJ Health & Care Informatics](https://doi.org/10.1136/bmjhci-2022-100549)\n\n[36\\. Frontiers | Key Considerations for Incorporating...](https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2019.00746/full)\n\n[41\\. L. Rojas-Barahona, M. Gašić et al. “A Network-based End-to-End Trainable Task-oriented Dialogue System.” Conference of the European Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/V1/E17-1042)\n\n[42\\. M. Walker, D. Litman et al. “PARADISE: A Framework for Evaluating Spoken Dialogue Agents.” ArXiv](https://doi.org/10.3115/976909.979652)\n\n[43\\. J. Schatzmann, Blaise Thomson et al. “Agenda-Based User Simulation for Bootstrapping a POMDP Dialogue System.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.3115/1614108.1614146)\n\n[44\\. M. Walker, C. Kamm et al. “Towards developing general models of usability with PARADISE.” Natural Language Engineering](https://doi.org/10.1017/S1351324900002503)\n\n[45\\. Assessing Dialogue Systems with Distribution Distances](https://bigdata.ustc.edu.cn/paper_pdf/2021/Jiannan-Xiang-ACL-IJCNLP.pdf)\n\n[46\\. Anh Nguyen, W. Wobcke. “An agent-based approach to dialogue management in personal assistants.” Proceedings of the 10th international conference on Intelligent user interfaces](https://doi.org/10.1145/1040830.1040865)\n\n[47\\. DOI: 10.1017/S000000000000000 Printed in the United Kingdom A Survey of Statistical User Simulation Techniques for Reinforcement-Learning of Dialogue Management Strategies.](https://www.semanticscholar.org/paper/e3d8e925ec4f938620a8d76bb50a1b2fc95e7f5e)\n\n[48\\. 认知型口语交互系统中的对话管理技术](https://www.ccf.org.cn/upload/resources/file/2021/11/22/177704.pdf)\n\n[49\\. Zhongyu Wei, Qianlong Liu et al. “Task-oriented Dialogue System for Automatic Diagnosis.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/P18-2033)\n\n[50\\. User Satisfaction Estimation with Sequential Dialogue Act Modeling in Goal-oriented Conversational Systems](https://www.atailab.cn/seminar2022Spring/pdf/2022_WWW_User%20Satisfaction%20Estimation%20with%20Sequential%20Dialogue%20Act%20Modeling%20in%20Goal-oriented%20Conversational%20Systems.pdf)\n\n[51\\. Ryan Fellows, H. Ihshaish et al. “Task-oriented Dialogue Systems: performance vs. quality-optima, a review.” ArXiv](https://doi.org/10.5121/csit.2022.121306)\n\n[52\\. Xiujun Li, Zachary Chase Lipton et al. “A User Simulator for Task-Completion Dialogues.” ArXiv](https://arxiv.org/abs/1612.05688)\n\n[53\\. Anouck Braggaar, Christine Liebrecht et al. “Evaluating Task-oriented Dialogue Systems: A Systematic Review of Measures, Constructs and their Operationalisations.” ArXiv](https://doi.org/10.48550/arXiv.2312.13871)\n\n[54\\. Shuo Zhang, Junzhou Zhao et al. “\"Think Before You Speak\": Improving Multi-Action Dialog Policy by Planning Single-Action Dialogs.” International Joint Conference on Artificial Intelligence](https://doi.org/10.48550/arXiv.2204.11481)\n\n[55\\. Harshit Joshi, Shicheng Liu et al. “LLM-Based Open-Domain Integrated Task and Knowledge Assistants with Programmable Policies.”](https://arxiv.org/abs/2407.05674)\n\n[57\\. 人与AI](https://www.accenture.cn/content/dam/accenture/final/accenture-com/document-3/Zhan-Wang-2025-Humans-and-AI.pdf)\n\n[58\\. 大模型驱动的汽车行业群体智能技术白皮书](https://runwise.oss-accelerate.aliyuncs.com/sites/15/2024/04/%E5%88%9B%E6%96%B0%E7%A0%94%E6%8A%A5_%E6%98%93%E6%85%A7%E6%99%BA%E8%83%BD%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%EF%BC%9A2024%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%A9%B1%E5%8A%A8%E7%9A%84%E6%B1%BD%E8%BD%A6%E8%A1%8C%E4%B8%9A%E7%BE%A4%E4%BD%93%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E7%99%BD%E7%9A%AE%E4%B9%A6-1.pdf)\n\n[59\\. 智慧灯塔，照亮企业AI Agent实施明路](https://m.book118.com/try_down/648022037133006121.pdf)\n\n[60\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[61\\. ChatGPT火爆背后，对话式AI在企业服务场景面临三大挑战](https://www.thepaper.cn/newsDetail_forward_21252464)\n\n[62\\. 落地智能体需要具备的六大核心技术能力-腾讯云开发者社区-腾讯云](https://cloud.tencent.com/developer/article/2535509)\n\n[63\\. Sheetal Kusal, S. Patil et al. “AI-Based Conversational Agents: A Scoping Review From Technologies to Future Directions.” IEEE Access](https://doi.org/10.1109/ACCESS.2022.3201144)\n\n[64\\. DeepSeek、Manus与AI Agent行业现状](https://zjh-space.oss-cn-hangzhou.aliyuncs.com/space/post/330000/2025/05/06/681993eed9c3f/1746506734_8917_77027.pdf)\n\n[65\\. 从试点到生产始终恪守负责任的 AI 理念](https://appen-website.oss-cn-shanghai.aliyuncs.com/prod/uploads/responsible_ai_f1cf7f8fa3.pdf)\n\n[66\\. 企业落地 Agent现在主要面临哪些困境？](https://b23.tv/BV1VbGtznEUQ)\n\n[67\\. Yara Rizk, Abhishek Bhandwalder et al. “A Unified Conversational Assistant Framework for Business Process Automation.” ArXiv](https://arxiv.org/abs/2001.03543)\n\n[68\\. 中关村科金张杰：ChatGPT火爆背后，对话式AI在企业服务场景面临三大挑战｜MEET2023](https://www.bilibili.com/video/av436283406?t=211)\n\n[69\\. Saizheng Zhang, Emily Dinan et al. “Personalizing Dialogue Agents: I have a dog, do you have pets too?.” ArXiv](https://doi.org/10.18653/v1/P18-1205)\n\n[70\\. Yanran Li, Hui Su et al. “DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset.” ArXiv](https://arxiv.org/abs/1710.03957)\n\n[71\\. Prashik Buddhaghosh Bansod. “Distinguishing Autonomous AI Agents from Collaborative Agentic Systems: A Comprehensive Framework for Understanding Modern Intelligent Architectures.”](https://arxiv.org/abs/2506.01438)\n\n[72\\. AI落地并非易事，深入分析其中的摩擦与权限问题 · 构建你的智能应用，使用蓝莺Chat AI SDK](https://docs.lanyingim.com/news/ai-implementation-challenges-39-20240721-3-3-1721505642.html)\n\n[73\\. Hannah Rashkin, Eric Michael Smith et al. “Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/P19-1534)\n\n[74\\. 企业在构建AI智能体问答助手可能会遇到哪些挑战及痛点？](https://www.waytoagi.com/zh/question/72986)\n\n[75\\. M. Owoc, A. Sawicka et al. “Artificial Intelligence Technologies in Education: Benefits, Challenges and Strategies of Implementation.” ArXiv](https://doi.org/10.1007/978-3-030-85001-2_4)\n\n[77\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[78\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[79\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[80\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[81\\. M. Lewis, Yinhan Liu et al. “BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/2020.acl-main.703)\n\n[82\\. AgentInstruct: Toward Generative Teaching with Agentic Flows](https://yiyibooks.cn/__trs__/arxiv/2407.03502v1/index.html)\n\n[83\\. Ziwei Ji, Nayeon Lee et al. “Survey of Hallucination in Natural Language Generation.” ACM Computing Surveys](https://doi.org/10.1145/3571730)\n\n[84\\. Anne Dominique Salamin, David Russo et al. “ChatGPT, an Excellent Liar: How Conversational Agents’ Hallucinations Impact Learning and Teaching.” Proceedings of the 7th International Conference on Teaching, Learning and Education](https://doi.org/10.33422/6th.iacetl.2023.11.100)\n\n[85\\. Qingxing Cao, Liang Lin et al. “Attention-Aware Face Hallucination via Deep Reinforcement Learning.” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2017.180)\n\n[86\\. Yukai Shi, Guanbin Li et al. “Face Hallucination by Attentive Sequence Optimization with Reinforcement Learning.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1109/TPAMI.2019.2915301)\n\n[87\\. Reinforcement_Learning - Paper Reading](http://paperreading.club/category?cate=Reinforcement_Learning)\n\n[88\\. Evaluating the accuracy of large language models in pulmonary medicine scientific writing](https://cdn.amegroups.cn/journals/jlpm/files/journals/33/articles/9759/public/9759-PB1-4022-R1.pdf?filename=JMAI-24-287-final.pdf&t=1741551147)\n\n[89\\. A Survey of LLM Datasets: From Autoregressive Model to AI Chatbot](https://jcst.ict.ac.cn/cn/article/pdf/preview/10.1007/s11390-024-3767-3.pdf)\n\n[96\\. Ethical governance of artificial intelligence: An integrated analytical framework](http://aiig.tsinghua.edu.cn/__local/1/E2/7F/46511D27E433E93DAF3ADB878EC_B5FE7CA6_151E05.pdf)\n\n[97\\. 欧盟寻求人工智能监管规则型领导地位：认知与路径](https://www.ciis.org.cn/gjwtyj/dqqk/202505/P020250512583201774405.pdf)\n\n[98\\. M. Cannarsa. “Ethics Guidelines for Trustworthy AI.” The Cambridge Handbook of Lawyering in the Digital Age](https://doi.org/10.1017/9781108936040.022)\n\n[99\\. Anna Jobin, M. Ienca et al. “The global landscape of AI ethics guidelines.” Nature Machine Intelligence](https://doi.org/10.1038/s42256-019-0088-2)\n\n[100\\. Anna Jobin, M. Ienca et al. “Artificial Intelligence: the global landscape of ethics guidelines.” ArXiv](https://doi.org/10.1038/s42256-019-0088-2)\n\n[101\\. L. Floridi, Josh Cowls et al. “AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations.” Minds and Machines](https://doi.org/10.1007/s11023-018-9482-5)\n\n[102\\. Thilo Hagendorff. “The Ethics of AI Ethics: An Evaluation of Guidelines.” Minds and Machines](https://doi.org/10.1007/s11023-020-09517-8)\n\n[103\\. AI Agent伦理争议：自主决策边界与人类监督博弈](https://shengxinai.com/archives/ai-agentlun-li-zheng-yi-zi-zhu-jue-ce-bian-jie-yu-ren-lei-jian-du-bo-yi)\n\n[104\\. 人工智能法律政策图景研究报告（2025年）](https://www.ciplawyer.cn/uploads/1/file/2025/06/20250623101813936_8pdn6tzhumtu.pdf)\n\n[105\\. 2025年，人工智能如何进化](http://paper.jzxww.com.cn/jzwb/page/26/2025-01/06/11/2025010611_pdf.pdf)\n\n[106\\. 2024年人工智能智能体(AI Agent)发展回顾与2025年趋势展望](https://blog.csdn.net/2401_84495872/article/details/147637287)\n\n[107\\. S. Liao. “Ethics of AI and Health Care: Towards a Substantive Human Rights Framework.” Topoi](https://doi.org/10.1007/s11245-023-09911-8)\n\n[108\\. Amelie Schmid, Manuel Wiesche. “The Importance of an Ethical Framework for Trust Calibration in AI.” IEEE Intelligent Systems](https://doi.org/10.1109/MIS.2023.3320443)\n\n[109\\. 2025年人工智能指数报告](https://pdf.dfcfw.com/pdf/H3_AP202506201694220072_1.pdf?1750413587000.pdf)\n\n[110\\. A. Khan, M. Akbar et al. “AI Ethics: An Empirical Study on the Views of Practitioners and Lawmakers.” IEEE Transactions on Computational Social Systems](https://doi.org/10.1109/TCSS.2023.3251729)\n\n[111\\. Zoe Porter, I. Habli et al. “A principles-based ethics assurance argument pattern for AI and autonomous systems.” AI and Ethics](https://doi.org/10.1007/s43681-023-00297-2)\n\n[112\\. Z. Assaad, Christine Boshuijzen-van Burken. “Ethics and Safety of Human-Machine Teaming.” Proceedings of the First International Symposium on Trustworthy Autonomous Systems](https://doi.org/10.1145/3597512.3600205)\n\n[113\\. An Overview of Artificial Intelligence Ethics](https://static.aminer.cn/upload/pdf/1101/1616/467/64bb058e3fda6d7f06031f4b_0.pdf)\n\n[114\\. Juan M. Garc'ia-G'omez, V. Blanes-Selva et al. “Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare.” ArXiv](https://doi.org/10.48550/arXiv.2309.10424)\n\n[116\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[117\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[118\\. R. S. Sutton, A. Barto. “Reinforcement Learning: An Introduction.” IEEE Trans. Neural Networks](https://doi.org/10.1109/TNN.1998.712192)\n\n[119\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[120\\. Volodymyr Mnih, K. Kavukcuoglu et al. “Human-level control through deep reinforcement learning.” Nature](https://doi.org/10.1038/nature14236)\n\n[121\\. Transformer in reinforcement learning for decision-making: a survey](https://www.fitee.zjujournals.com/rc-pub/front/front-article/download/46620264/lowqualitypdf/%E5%9F%BA%E4%BA%8ETransformer%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%9C%A8%E6%99%BA%E8%83%BD%E5%86%B3%E7%AD%96%E9%A2%86%E5%9F%9F%E7%9A%84%E5%BA%94%E7%94%A8%EF%BC%9A%E7%BB%BC%E8%BF%B0.pdf)\n\n[122\\. Qingxing Cao, Liang Lin et al. “Attention-Aware Face Hallucination via Deep Reinforcement Learning.” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2017.180)\n\n[123\\. Ziwei Ji, Nayeon Lee et al. “Survey of Hallucination in Natural Language Generation.” ACM Computing Surveys](https://doi.org/10.1145/3571730)\n\n[124\\. A Survey of LLM Datasets: From Autoregressive Model to AI Chatbot](https://jcst.ict.ac.cn/cn/article/pdf/preview/10.1007/s11390-024-3767-3.pdf)\n\n[125\\. Reinforcement_Learning - Paper Reading](http://paperreading.club/category?cate=Reinforcement_Learning)\n\n[126\\. Gartner 预测2027年40%的AI 项目将失败；Salesforce 发布 ...](https://www.niutoushe.com/96096)\n\n[127\\. 2025中国AI Agent行业研究报告（二）](https://www.aibook.ren/upload/2025%E5%B9%B4%E4%B8%AD%E5%9B%BDAI%20Agent%E8%A1%8C%E4%B8%9A%E7%A0%94%E7%A9%B6%E6%8A%A5%E5%91%8A.pdf)\n\n[128\\. Quality management in supply chain: Strategic implications and the paradox of AI inspection](https://www.ceibs.edu/sites/portal.prod1.dpmgr.ceibs.edu/files/2025%20DSJ%20AI.pdf)\n\n[129\\. AI AGENT 技术跃迁赋能药企研发智能化](https://pdf.dfcfw.com/pdf/H3_AP202504221660302969_1.pdf?1745316426000.pdf)\n\n[130\\. J. Weizenbaum. “ELIZA—a computer program for the study of natural language communication between man and machine.” Communications of the ACM](https://doi.org/10.1145/365153.365168)\n\n[131\\. George Baryannis, Sahar Validi et al. “Supply chain risk management and artificial intelligence: state of the art and future research directions.” International Journal of Production Research](https://doi.org/10.1080/00207543.2018.1530476)\n\n[132\\. 2025年及以后的人工智能预测](https://www.gjgxy.cn/shownews.php?id=3290)\n\n[133\\. Yogesh Kumar Dwivedi, Laurie Hughes et al. “Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy.” International Journal of Information Management](https://doi.org/10.1016/J.IJINFOMGT.2019.08.002)\n\n[134\\. Baha M. Mohsen. “Impact of Artificial Intelligence on Supply Chain Management Performance.” Journal of Service Science and Management](https://doi.org/10.4236/jssm.2023.161004)\n\n[135\\. “亲手做了12个AI Agent,我并不看好2025年的智能体!”|调用|代码|...](https://www.163.com/dy/article/K53DFCH10511FQO9.html)\n\n[136\\. Reza Toorajipour, Vahid Sohrabpour et al. “Artificial intelligence in supply chain management: A systematic literature review.” Journal of Business Research](https://doi.org/10.1016/J.JBUSRES.2020.09.009)\n\n[137\\. Rameshwar Dubey, A. Gunasekaran et al. “Big data analytics and artificial intelligence pathway to operational performance under the effects of entrepreneurial orientation and environmental dynamism: A study of manufacturing organisations.” International Journal of Production Economics](https://doi.org/10.1016/j.ijpe.2019.107599)\n\n[138\\. 2025年人工智能的演变：从资本注入到自主系统和垂直应用的 ...](https://blog.moontak.com/id/460205/)\n\n[139\\. F5《2025年应用战略现状》报告:AI 落地加速,企业战略从讨论迈向...](https://www.donews.com/news/detail/4/5242108.html)\n\n[140\\. 计算机行业 2025 年 04 月投资策略 Agent 时代到来，Know How 智能实现落地](https://pdf.dfcfw.com/pdf/H3_AP202504021650205252_1.pdf?1743591672000.pdf)\n\n[141\\. 企业级Agent厮杀升级!李开复:成败关键在于 “交付价值”,零一万物...](http://app.myzaker.com/news/article.php?m=1753188135&pk=687f8669b15ec028e3309a99)\n\n[142\\. 7月2日全球AI资讯 | 中国AI检早期胃癌，AI打击医疗欺诈，AI可减碳排放，AI代理任务失败率70%，AI音乐冲击市场，Meta挖角OpenAI，微软裁员](https://b23.tv/BV1GD3azZEi4?t=80)\n\n[143\\. Abeer Aljohani. “Predictive Analytics and Machine Learning for Real-Time Supply Chain Risk Mitigation and Agility.” Sustainability](https://doi.org/10.3390/su152015088)\n\n[146\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[147\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[148\\. Zihao Wu. “Autono: ReAct-Based Highly Robust Autonomous Agent Framework.”](https://arxiv.org/abs/2504.04650)\n\n[149\\. OpenAI Josh Achiam, Steven Adler et al. “GPT-4 Technical Report.”](https://arxiv.org/abs/2303.08774)\n\n[150\\. Ziwei Ji, Nayeon Lee et al. “Survey of Hallucination in Natural Language Generation.” ACM Computing Surveys](https://doi.org/10.1145/3571730)\n\n[151\\. AI Agent：基于大模型的自主智能体，在探索AGI的道路上前进](https://bigdata-s3.wmcloud.com/researchreport/2023-08/6a60dd73e0e1ba35100343d73ecfedd8.pdf)\n\n[152\\. Anna Rohrbach, Lisa Anne Hendricks et al. “Object Hallucination in Image Captioning.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D18-1437)\n\n[153\\. Andrea Madotto, Zhaojiang Lin et al. “Few-Shot Bot: Prompt-Based Learning for Dialogue Systems.” ArXiv](https://arxiv.org/abs/2110.08118)\n\n[154\\. Nouha Dziri, Andrea Madotto et al. “Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding.” ArXiv](https://doi.org/10.18653/v1/2021.emnlp-main.168)\n\n[155\\. Matthew Jörke, Shardul Sapkota et al. “Supporting Physical Activity Behavior Change with LLM-Based Conversational Agents.” ArXiv](https://doi.org/10.48550/arXiv.2405.06061)\n\n[156\\. Xinrun Xu, Yuxin Wang et al. “A Survey on Game Playing Agents and Large Models: Methods, Applications, and Challenges.” ArXiv](https://doi.org/10.48550/arXiv.2403.10249)\n\n[157\\. Hallucination Mitigation using Agentic AI Natural Language ...](https://hub.baai.ac.cn/paper/c4774fae-6004-46c5-87a4-79338e37d90b)\n\n[158\\. Zehang Deng, Yongjian Guo et al. “AI Agents Under Threat: A Survey of Key Security Challenges and Future Pathways.” ArXiv](https://doi.org/10.48550/arXiv.2406.02630)\n\n[159\\. Diego Gosmar, Deborah A. Dahl et al. “AI Multi-Agent Interoperability Extension for Managing Multiparty Conversations.”](https://arxiv.org/abs/2411.05828)\n\n[160\\. Yi Yang, Yitong Ma et al. “Minimizing Hallucinations and Communication Costs: Adversarial Debate and Voting Mechanisms in LLM-Based Multi-Agents.” Applied Sciences](https://doi.org/10.3390/app15073676)\n\n[161\\. Rafael Arias Gonzalez, Steve DiPaola. “Exploring Augmentation and Cognitive Strategies for AI based Synthetic Personae.” ArXiv](https://doi.org/10.48550/arXiv.2404.10890)\n\n[162\\. Yue Zhang, Yafu Li et al. “Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2309.01219)\n\n[163\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[164\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[165\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[166\\. Tianyi Zhang, Varsha Kishore et al. “BERTScore: Evaluating Text Generation with BERT.” ArXiv](https://arxiv.org/abs/1904.09675)\n\n[167\\. Zhilin Yang, Peng Qi et al. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D18-1259)\n\n[168\\. Yejin Bang, Samuel Cahyawijaya et al. “A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity.” ArXiv](https://doi.org/10.18653/v1/2023.ijcnlp-main.45)\n\n[169\\. Abhilasha Ravichander, Shrusti Ghela et al. “HALoGEN: Fantastic LLM Hallucinations and Where to Find Them.”](https://arxiv.org/abs/2501.08292)\n\n[170\\. Magnetic-One: A Generalist Multi-Agent System for Solving Complex Tasks](https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Magentic-One.pdf)\n\n[171\\. Yuxiang Zhang, Jing Chen et al. “ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2406.20015)\n\n[172\\. Shuyan Zhou, Frank F. Xu et al. “WebArena: A Realistic Web Environment for Building Autonomous Agents.” ArXiv](https://doi.org/10.48550/arXiv.2307.13854)\n\n[173\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[174\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[175\\. Daniel De Freitas, Minh-Thang Luong et al. “Towards a Human-like Open-Domain Chatbot.” ArXiv](https://arxiv.org/abs/2001.09977)\n\n[176\\. Matthew Henderson, Blaise Thomson et al. “The Second Dialog State Tracking Challenge.” SIGDIAL Conference](https://doi.org/10.3115/v1/W14-4337)\n\n[177\\. Lova Rajaobelina, Sandrine Prom Tep et al. “Creepiness: Its antecedents and impact on loyalty when interacting with a chatbot.” Psychology & Marketing](https://doi.org/10.1002/MAR.21548)\n\n[178\\. F5《2025年应用战略现状》报告:AI 落地加速,企业战略从讨论迈向...](https://www.donews.com/news/detail/4/5242108.html)\n\n[179\\. Boost.ai：2024驾驭客户体验的未来：对话式人工智能趋势 ...](https://www.sgpjbg.com/bgdown/186855.html)\n\n[180\\. Artificial Intelligence Index Report 2023](https://event-cdn.baai.ac.cn/file/file-browser/C7FA4aFhrT2Hrnm77AKZPww62Ywm7Pyk.pdf)\n\n[181\\. 2190.2024年，最大的AI失败案例-MIT](https://www.bilibili.com/video/av113917632192287?t=10)\n\n[182\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2024.” ArXiv](https://doi.org/10.48550/arXiv.2405.19522)\n\n[183\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[184\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[185\\. ReflAct: 通过目标状态反思进行世界落地的决策在LLM代理中](https://www.xueshuxiangzi.com/downloads/2025_5_22/2505.15182.pdf)\n\n[186\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[187\\. Bowen Jiang, Yangxinyu Xie et al. “Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2406.00252)\n\n[188\\. 2024年大模型多智能体workflow技术之Multi-Agent Debate](https://www.cnblogs.com/ExMan/p/18727514)\n\n[189\\. Xuezhi Wang, Jason Wei et al. “Self-Consistency Improves Chain of Thought Reasoning in Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2203.11171)\n\n[190\\. Zihao Wu. “Autono: ReAct-Based Highly Robust Autonomous Agent Framework.”](https://arxiv.org/abs/2504.04650)\n\n[191\\. Single Agent Prompt](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_11_27/2411.17636.pdf)\n\n[192\\. Huaben Chen, Wenkang Ji et al. “Multi-Agent Consensus Seeking via Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2310.20151)\n\n[193\\. Yilun Du, Shuang Li et al. “Improving Factuality and Reasoning in Language Models through Multiagent Debate.” ArXiv](https://doi.org/10.48550/arXiv.2305.14325)\n\n[194\\. AUTOGEN STUDIO: A No-Code Developer Tool for Building and Debugging Multi-Agent Systems](https://www.microsoft.com/en-us/research/uploads/prod/2024/08/AutoGen_Studio-12.pdf)\n\n[195\\. Noah Shinn, Beck Labash et al. “Reflexion: an autonomous agent with dynamic memory and self-reflection.” ArXiv](https://doi.org/10.48550/arXiv.2303.11366)\n\n[196\\. Luke Yoffe, Alfonso Amayuelas et al. “DebUnc: Mitigating Hallucinations in Large Language Model Agent Communication with Uncertainty Estimations.”](https://arxiv.org/abs/2407.06426)\n\n[197\\. Sugyeong Eo, Hyeonseok Moon et al. “Debate Only When Necessary: Adaptive Multiagent Collaboration for Efficient LLM Reasoning.”](https://arxiv.org/abs/2504.05047)\n\n[198\\. Bin Zhang, Hangyu Mao et al. “Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach.” ArXiv](https://doi.org/10.48550/arXiv.2311.13884)\n\n[199\\. Tula Masterman, Sandi Besen et al. “The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2404.11584)\n\n[200\\. 万字长文!AI Agent架构概况:关于推理、规划和工具调用](https://www.360doc.cn/article/170868_1126838491.html)\n\n[203\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[204\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[205\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[206\\. Zhilin Yang, Peng Qi et al. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D18-1259)\n\n[207\\. Stephanie C. Lin, Jacob Hilton et al. “TruthfulQA: Measuring How Models Mimic Human Falsehoods.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/2022.acl-long.229)\n\n[208\\. 探索AI的力量：Web of Science平台上的智能化科研解决方案](https://www.lib.stu.edu.cn/sites/default/files/jiangzuo/20250508tan_suo_aide_li_liang_webofscienceping_tai_shang_de_zhi_neng_ke_yan_jie_jue_fang_an_jin_gong_xue_xi_jiao_liu_shi_yong_.pdf)\n\n[209\\. Shuyan Zhou, Frank F. Xu et al. “WebArena: A Realistic Web Environment for Building Autonomous Agents.” ArXiv](https://doi.org/10.48550/arXiv.2307.13854)\n\n[210\\. Yejin Bang, Samuel Cahyawijaya et al. “A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity.” ArXiv](https://doi.org/10.18653/v1/2023.ijcnlp-main.45)\n\n[216\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[217\\. J. Weizenbaum. “ELIZA—a computer program for the study of natural language communication between man and machine.” Communications of the ACM](https://doi.org/10.1145/365153.365168)\n\n[218\\. Gartner 预测2027年40%的AI 项目将失败；Salesforce 发布 ...](https://www.niutoushe.com/96096)\n\n[219\\. N. Radziwill, Morgan C. Benton. “Evaluating Quality of Chatbots and Intelligent Conversational Agents.” ArXiv](https://arxiv.org/abs/1704.04579)\n\n[220\\. 克服障碍，实现下一代供应链创新](https://www.headscm.com/uploads/file1/20190601/5cf215ea95b29.pdf)\n\n[221\\. Ulrich Gnewuch, Stefan Morana et al. “Towards Designing Cooperative and Social Conversational Agents for Customer Service.” International Conference on Interaction Sciences](https://www.semanticscholar.org/paper/61ba3584885142e46673943142a4f2280ac14387)\n\n[222\\. F5《2025年应用战略现状》报告:AI 落地加速,企业战略从讨论迈向...](https://www.donews.com/news/detail/4/5242108.html)\n\n[223\\. Lova Rajaobelina, Sandrine Prom Tep et al. “Creepiness: Its antecedents and impact on loyalty when interacting with a chatbot.” Psychology & Marketing](https://doi.org/10.1002/MAR.21548)\n\n[224\\. Nestor Maslej, Loredana Fattorini et al. “Artificial Intelligence Index Report 2024.” ArXiv](https://doi.org/10.48550/arXiv.2405.19522)\n\n[225\\. Boost.ai：2024驾驭客户体验的未来：对话式人工智能趋势 ...](https://www.sgpjbg.com/bgdown/186855.html)\n\n[226\\. 2190.2024年，最大的AI失败案例-MIT](https://www.bilibili.com/video/av113917632192287?t=272)\n\n[228\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[229\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[230\\. ReflAct: 通过目标状态反思进行世界落地的决策在LLM代理中](https://www.xueshuxiangzi.com/downloads/2025_5_22/2505.15182.pdf)\n\n[231\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[232\\. Bowen Jiang, Yangxinyu Xie et al. “Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2406.00252)\n\n[233\\. Zihao Wu. “Autono: ReAct-Based Highly Robust Autonomous Agent Framework.”](https://arxiv.org/abs/2504.04650)\n\n[234\\. EXACT: TEACHING AI AGENTS TO EXPLORE WITH REFLECTIVE-MCTS AND EXPLORATORY LEARNING](https://www.microsoft.com/en-us/research/uploads/prod/2024/12/2410.02052v3.pdf)\n\n[235\\. Shunyu Yao, Jeffrey Zhao et al. “ReAct: Synergizing Reasoning and Acting in Language Models.” ArXiv](https://arxiv.org/abs/2210.03629)\n\n[236\\. 2024年大模型多智能体workflow技术之Multi-Agent Debate](https://www.cnblogs.com/ExMan/p/18727514)\n\n[237\\. Yilun Du, Shuang Li et al. “Improving Factuality and Reasoning in Language Models through Multiagent Debate.” ArXiv](https://doi.org/10.48550/arXiv.2305.14325)\n\n[238\\. Noah Shinn, Beck Labash et al. “Reflexion: an autonomous agent with dynamic memory and self-reflection.” ArXiv](https://doi.org/10.48550/arXiv.2303.11366)\n\n[239\\. Huaben Chen, Wenkang Ji et al. “Multi-Agent Consensus Seeking via Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2310.20151)\n\n[240\\. Luke Yoffe, Alfonso Amayuelas et al. “DebUnc: Mitigating Hallucinations in Large Language Model Agent Communication with Uncertainty Estimations.”](https://arxiv.org/abs/2407.06426)\n\n[241\\. Tianyu Cui, Yanling Wang et al. “Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems.” ArXiv](https://doi.org/10.48550/arXiv.2401.05778)\n\n[242\\. Tula Masterman, Sandi Besen et al. “The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2404.11584)\n\n[243\\. Qingyun Wu, Gagan Bansal et al. “AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation.”](https://arxiv.org/abs/2308.08155)\n\n[244\\. Controlling Large Language Model-based Agents for Large-Scale Decision-Making](https://zhuanlan.zhihu.com/p/669486686)\n\n[248\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[249\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[250\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[251\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[252\\. Zhilin Yang, Peng Qi et al. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D18-1259)\n\n[253\\. 2025年人工智能指数报告](https://pdf.dfcfw.com/pdf/H3_AP202506201694220072_1.pdf?1750413587000.pdf)\n\n[254\\. 探索AI的力量：Web of Science平台上的智能化科研解决方案](https://www.lib.stu.edu.cn/sites/default/files/jiangzuo/20250508tan_suo_aide_li_liang_webofscienceping_tai_shang_de_zhi_neng_ke_yan_jie_jue_fang_an_jin_gong_xue_xi_jiao_liu_shi_yong_.pdf)\n\n[255\\. Yejin Bang, Samuel Cahyawijaya et al. “A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity.” ArXiv](https://doi.org/10.18653/v1/2023.ijcnlp-main.45)\n\n[258\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[259\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[260\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[261\\. ReflAct: 通过目标状态反思进行世界落地的决策在LLM代理中](https://www.xueshuxiangzi.com/downloads/2025_5_22/2505.15182.pdf)\n\n[262\\. Bowen Jiang, Yangxinyu Xie et al. “Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2406.00252)\n\n[263\\. Shunyu Yao, Jeffrey Zhao et al. “ReAct: Synergizing Reasoning and Acting in Language Models.” ArXiv](https://arxiv.org/abs/2210.03629)\n\n[264\\. Zihao Wu. “Autono: ReAct-Based Highly Robust Autonomous Agent Framework.”](https://arxiv.org/abs/2504.04650)\n\n[265\\. Yilun Du, Shuang Li et al. “Improving Factuality and Reasoning in Language Models through Multiagent Debate.” ArXiv](https://doi.org/10.48550/arXiv.2305.14325)\n\n[266\\. 2024年大模型多智能体workflow技术之Multi-Agent Debate](https://www.cnblogs.com/ExMan/p/18727514)\n\n[267\\. Haotian Wang, Xiyuan Du et al. “Learning to Break: Knowledge-Enhanced Reasoning in Multi-Agent Debate System.”](https://arxiv.org/abs/2312.04854)\n\n[268\\. Luke Yoffe, Alfonso Amayuelas et al. “DebUnc: Mitigating Hallucinations in Large Language Model Agent Communication with Uncertainty Estimations.”](https://arxiv.org/abs/2407.06426)\n\n[269\\. Xiaoxi Sun, Jinpeng Li et al. “Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework.” ArXiv](https://doi.org/10.48550/arXiv.2406.03075)\n\n[270\\. Huaben Chen, Wenkang Ji et al. “Multi-Agent Consensus Seeking via Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2310.20151)\n\n[271\\. Zixia Jia, Mengmeng Wang et al. “LangSuitE: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments.” ArXiv](https://doi.org/10.48550/arXiv.2406.16294)\n\n[272\\. 【讲座回放】Dr. Yunzhu Li: Foundation Models for Robotic Manipulation](https://www.bilibili.com/video/av113779673137625?t=1329)\n\n[273\\. 【论文速递】2025年04周（Robotics/Embodied AI/LLM） 原创](https://blog.csdn.net/maizousidemao/article/details/147378263)\n\n[278\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[279\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[280\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[281\\. Tianyi Zhang, Varsha Kishore et al. “BERTScore: Evaluating Text Generation with BERT.” ArXiv](https://arxiv.org/abs/1904.09675)\n\n[282\\. Zhilin Yang, Peng Qi et al. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D18-1259)\n\n[283\\. Yejin Bang, Samuel Cahyawijaya et al. “A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity.” ArXiv](https://doi.org/10.18653/v1/2023.ijcnlp-main.45)\n\n[284\\. R. S. Huang, K. Lu et al. “Assessment of Resident and AI Chatbot Performance on the University of Toronto Family Medicine Residency Progress Test: Comparative Study.” JMIR Medical Education](https://doi.org/10.2196/50514)\n\n[285\\. Jiarui Li, Ye Yuan et al. “Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases.” ArXiv](https://doi.org/10.48550/arXiv.2403.10446)\n\n[286\\. Junyang Wang, Yuhang Wang et al. “An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation.” ArXiv](https://doi.org/10.48550/arXiv.2311.07397)\n\n[287\\. Chang Ma, Junlei Zhang et al. “AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents.” ArXiv](https://doi.org/10.48550/arXiv.2401.13178)\n\n[288\\. Haoyu Huang, Tong Niu et al. “RAM2C: A Liberal Arts Educational Chatbot based on Retrieval-augmented Multi-role Multi-expert Collaboration.”](https://arxiv.org/abs/2409.15461)\n\n[289\\. Xiang Chen, Chenxi Wang et al. “Unified Hallucination Detection for Multimodal Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2402.03190)\n\n[293\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[294\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[295\\. Zihao Wu. “Autono: ReAct-Based Highly Robust Autonomous Agent Framework.”](https://arxiv.org/abs/2504.04650)\n\n[296\\. Bowen Jiang, Yangxinyu Xie et al. “Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2406.00252)\n\n[297\\. ReflAct: 通过目标状态反思进行世界落地的决策在LLM代理中](https://www.xueshuxiangzi.com/downloads/2025_5_22/2505.15182.pdf)\n\n[298\\. DebUnc：通过不确定性估计减轻大语言模型代理通信中的幻觉](https://www.yiyibooks.cn/__trs__/arxiv/2407.06426v1/index.html)\n\n[299\\. Shunyu Yao, Jeffrey Zhao et al. “ReAct: Synergizing Reasoning and Acting in Language Models.” ArXiv](https://arxiv.org/abs/2210.03629)\n\n[300\\. EXACT: TEACHING AI AGENTS TO EXPLORE WITH REFLECTIVE-MCTS AND EXPLORATORY LEARNING](https://www.microsoft.com/en-us/research/uploads/prod/2024/12/2410.02052v3.pdf)\n\n[301\\. 2024年大模型多智能体workflow技术之Multi-Agent Debate](https://www.cnblogs.com/ExMan/p/18727514)\n\n[302\\. Luke Yoffe, Alfonso Amayuelas et al. “DebUnc: Mitigating Hallucinations in Large Language Model Agent Communication with Uncertainty Estimations.”](https://arxiv.org/abs/2407.06426)\n\n[303\\. Yilun Du, Shuang Li et al. “Improving Factuality and Reasoning in Language Models through Multiagent Debate.” ArXiv](https://doi.org/10.48550/arXiv.2305.14325)\n\n[304\\. Haotian Wang, Xiyuan Du et al. “Learning to Break: Knowledge-Enhanced Reasoning in Multi-Agent Debate System.”](https://arxiv.org/abs/2312.04854)\n\n[305\\. Tian Liang, Zhiwei He et al. “Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate.” ArXiv](https://doi.org/10.48550/arXiv.2305.19118)\n\n[306\\. Xiaoxi Sun, Jinpeng Li et al. “Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework.” ArXiv](https://doi.org/10.48550/arXiv.2406.03075)\n\n[307\\. ICLR 2023 | ReAct：首次结合Thought和Action提升大模型解决问题的能力](https://www.51cto.com/aigc/3358.html)\n\n[313\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[314\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[315\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[316\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[317\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[318\\. A Survey of LLM Datasets: From Autoregressive Model to AI Chatbot](https://jcst.ict.ac.cn/cn/article/pdf/preview/10.1007/s11390-024-3767-3.pdf)\n\n[319\\. 【CVPR 2025】Training Multimodal Agents with Reinforcement Learning](https://b23.tv/BV1FEM2zKEFz?t=130)\n\n[320\\. Transformer in reinforcement learning for decision-making: a survey](https://www.fitee.zjujournals.com/rc-pub/front/front-article/download/46620264/lowqualitypdf/%E5%9F%BA%E4%BA%8ETransformer%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%9C%A8%E6%99%BA%E8%83%BD%E5%86%B3%E7%AD%96%E9%A2%86%E5%9F%9F%E7%9A%84%E5%BA%94%E7%94%A8%EF%BC%9A%E7%BB%BC%E8%BF%B0.pdf)\n\n[321\\. Hongbin Ye, Tong Liu et al. “Cognitive Mirage: A Review of Hallucinations in Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2309.06794)\n\n[322\\. ChatVLA: A Unified Framework for Vision-Language-Action Understanding and Control](https://www.xueshuxiangzi.com/downloads/2025_2_21/2502.14420.pdf)\n\n[323\\. Jiarui Li, Ye Yuan et al. “Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases.” ArXiv](https://doi.org/10.48550/arXiv.2403.10446)\n\n[324\\. 开放环境下的协作多智能体强化学习进展](http://scis.scichina.com/cn/2025/SSI-2023-0335.pdf)\n\n[325\\. R. S. Huang, K. Lu et al. “Assessment of Resident and AI Chatbot Performance on the University of Toronto Family Medicine Residency Progress Test: Comparative Study.” JMIR Medical Education](https://doi.org/10.2196/50514)\n\n[326\\. Alexander Nikulin, Ilya Zisman et al. “XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning.” ArXiv](https://doi.org/10.48550/arXiv.2406.08973)\n\n[327\\. Transformer in reinforcement learning for decision-making](https://jzus.zju.edu.cn/article.php?doi=10.1631/FITEE.2300548)\n\n[328\\. Junyang Wang, Yuhang Wang et al. “An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation.” ArXiv](https://doi.org/10.48550/arXiv.2311.07397)\n\n[329\\. Wenzhe Li, Hao Luo et al. “A Survey on Transformers in Reinforcement Learning.” ArXiv](https://doi.org/10.48550/arXiv.2301.03044)\n\n[333\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[334\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[335\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[336\\. Zihao Wu. “Autono: ReAct-Based Highly Robust Autonomous Agent Framework.”](https://arxiv.org/abs/2504.04650)\n\n[337\\. C. Grigsby, I. Matsunaga. “Data Package for Experiment 2025.”](https://doi.org/10.2172/1241599)\n\n[338\\. Bowen Jiang, Yangxinyu Xie et al. “Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2406.00252)\n\n[339\\. ReflAct: 通过目标状态反思进行世界落地的决策在LLM代理中](https://www.xueshuxiangzi.com/downloads/2025_5_22/2505.15182.pdf)\n\n[340\\. 2024年大模型多智能体workflow技术之Multi-Agent Debate](https://www.cnblogs.com/ExMan/p/18727514)\n\n[341\\. J. Park, Joseph C. O'Brien et al. “Generative Agents: Interactive Simulacra of Human Behavior.” Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology](https://doi.org/10.1145/3586183.3606763)\n\n[342\\. Yilun Du, Shuang Li et al. “Improving Factuality and Reasoning in Language Models through Multiagent Debate.” ArXiv](https://doi.org/10.48550/arXiv.2305.14325)\n\n[343\\. Haotian Wang, Xiyuan Du et al. “Learning to Break: Knowledge-Enhanced Reasoning in Multi-Agent Debate System.”](https://arxiv.org/abs/2312.04854)\n\n[344\\. Luke Yoffe, Alfonso Amayuelas et al. “DebUnc: Mitigating Hallucinations in Large Language Model Agent Communication with Uncertainty Estimations.”](https://arxiv.org/abs/2407.06426)\n\n[345\\. Tula Masterman, Sandi Besen et al. “The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2404.11584)\n\n[346\\. Xiaoxi Sun, Jinpeng Li et al. “Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework.” ArXiv](https://doi.org/10.48550/arXiv.2406.03075)\n\n[347\\. 【讲座回放】Dr. Yunzhu Li: Foundation Models for Robotic Manipulation](https://www.bilibili.com/video/av113779673137625?t=1329)\n\n[348\\. Neeloy Chakraborty, Melkior Ornik et al. “Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art.” ArXiv](https://doi.org/10.48550/arXiv.2403.16527)\n\n[349\\. 【论文速递】2025年04周（Robotics/Embodied AI/LLM） 原创](https://blog.csdn.net/maizousidemao/article/details/147378263)\n\n[350\\. 大湾区具身智能Workshop——Efficient Policy Adaptation for General Robotic Manipulation](https://b23.tv/BV1VAoKYsErc?t=1452)"
  },
  {
    "id": 47,
    "question": "How can we optimize large language model alignment: from RLHF to RLAIF, to better leverage pretrained models' potential and align with human preferences?",
    "response": "**Optimizing Large Language Model Alignment: Transitioning from RLHF to RLAIF**\n\n**1\\. Introduction: The Evolution of LLM Alignment**\n\nLarge language model (LLM) alignment ensures models generate outputs aligned with human preferences, ethics, and domain-specific constraints. **Reinforcement Learning from Human Feedback (RLHF)** emerged as the dominant method but faces scalability limitations due to dependency on human annotators, high costs, and instability \\[3\\]\\[41\\]. By 2025, **Reinforcement Learning from AI Feedback (RLAIF)** has gained traction as a scalable alternative, using AI-generated feedback to refine models. This report synthesizes technical advances, empirical results, and architectural innovations in RLAIF and hybrid systems, contextualized for healthcare and legal domains.\n\n**2\\. Performance Benchmarks: RLHF vs. RLAIF**\n\nAs of 2025, RLAIF matches or exceeds RLHF across key metrics while reducing costs:\n\n**Quality & Win Rates**: RLAIF achieves **71% summarization win rates** vs. RLHF's 73%, with humans showing no preference between outputs \\[3\\]\\[19\\]. In dialogue tasks, RLAIF reaches **63% helpfulness** and **88% harmlessness** (vs. RLHF’s 76%), demonstrating superior safety \\[12\\].\n\n**Efficiency**: RLAIF reduces annotation costs by **\\>10×** via AI-generated labels, enabling rapid iteration \\[30\\]\\[35\\].\n\n**Robustness**: RLAIF shows greater stability than RLHF, which suffers from preference collapse and algorithmic bias \\[3\\]\\[16\\].\n\nDirect RLAIF (**d-RLAIF**)—bypassing reward-model training—further streamlines alignment \\[3\\].\n\n**3\\. RLAIF Technical Implementation: Reducing Human Dependency**\n\nRLAIF replaces human annotators with LLM-generated feedback through:\n\n**AI Teacher Models**: Secondary LLMs (e.g., GPT-4) generate preference labels to train reward models, guiding RL fine-tuning \\[24\\]\\[29\\].\n\n**Cost-Saving Mechanisms**: Eliminating human annotation slashes costs by **\\>90%** while maintaining alignment quality \\[30\\]\\[35\\].\n\n**Limitations**: RLAIF struggles with complex tasks (e.g., legal reasoning), where human nuance is irreplaceable, and risks amplifying biases from judge models \\[22\\]\\[29\\].\n\n**Mitigation**: Chain-of-thought prompting and self-consistency checks improve alignment accuracy by **\\>15%** for intricate tasks \\[30\\]\\[85\\].\n\n**4\\. Scalability Limitations in Enterprise Deployments**\n\nTransitioning from RLHF to RLAIF addresses but doesn’t eliminate scalability challenges:\n\n**RLHF Bottlenecks**: High costs, human subjectivity, and data-volume requirements \\[41\\]\\[45\\].\n\n**RLAIF Trade-offs**:\n\n**Reward Model Staleness**: Static RMs fail to adapt to policy updates. Iterative RLAIF retrains RMs periodically but increases compute load \\[41\\].\n\n**Bias Propagation**: Off-the-shelf LLMs used for feedback may transfer biases to downstream models \\[41\\].\n\n**OOD Alignment**: RLAIF underperforms in healthcare/legal edge cases without domain-specific tuning \\[52\\].\n\n**Solutions**: Hybrid architectures (e.g., RLTHF) combine AI efficiency with targeted human oversight \\[188\\].\n\n**5\\. Novel RLAIF Optimization Techniques (2025)**\n\nRecent advancements focus on pretrained model utilization and bias control:\n\n**Direct RLAIF (d-RLAIF)**: Uses LLMs as real-time reward signals, skipping RM training \\[3\\].\n\n**Preference Ranking Optimization (PRO)**: Extends pairwise comparisons to multi-dimensional contrasts, improving sample efficiency \\[65\\].\n\n**Self-Feedback Guidance**: Aligned models self-evaluate outputs (e.g., via **RLAIF-V**) to correct biases like verbosity preference \\[261\\].\n\n**ULTRAFEEDBACK Dataset**: Large-scale, diversified preference datasets enhance generalization for RLHF/RLAIF \\[78\\].\n\n**6\\. Hybrid RLAIF-RLHF Architectures**\n\n2025 systems blend RLAIF's scalability with RLHF's precision:\n\n**A. RLTHF (Targeted Human Feedback)**\n\n**Mechanism**: Uses LLMs for **initial alignment**, then applies **human annotations only to high-uncertainty samples** (e.g., mislabeled or ambiguous data) \\[188\\]\\[300\\].\n\n**Cost Savings**: Achieves full-human annotation quality with **6–7% of human effort** via strategic data selection \\[300\\].\n\n**Component Integration**:\n\n**Prompt Templates**: Structured prompts (task descriptions, criteria) guide LLM feedback \\[300\\].\n\n**Reward Distribution Analysis**: Identifies regions needing human intervention \\[300\\].\n\n**B. HybridFlow**\n\n**GPU Optimization**: Hierarchical programming model minimizes memory redundancy via **3D-HybridEngine**, improving throughput by **1.53–20.57×** \\[183\\]\\[238\\].\n\n**Automated Device Mapping**: Dynamically allocates models to GPUs based on load, reducing latency \\[238\\].\n\n**7\\. Real-Time Bias Detection and Correction**\n\n2025 RLAIF systems deploy algorithmic modules to mitigate bias during feedback generation:\n\n**Position Bias Mitigation**: Reverses candidate order in pairwise comparisons, flagging inconsistent preferences \\[143\\]\\[143\\].\n\n**Fairness Toolkits**: Integrates **Fairlearn** and **AI Fairness 360** to quantify bias in feedback \\[204\\].\n\n**Self-Debiasing (RLDF)**: LLMs debate biased vs. debiased responses, refining outputs iteratively \\[270\\].\n\n**Constitutional AI**: Self-critique and revision loops align models with ethical principles without human labels \\[265\\].\n\nFor healthcare/legal domains, **bias-aware retrieval** and **adaptive learning modules** monitor trends and update mitigation in real-time \\[269\\].\n\n**8\\. Empirical Performance in Healthcare and Legal Domains**\n\n**Quantitative Metrics**\n\n**Win Rates**: RLAIF matches RLHF in **medical summarization** (71–73% preference) and **legal contract analysis** (65% accuracy) \\[232\\]\\[395\\].\n\n**OOD Robustness**: **GenRM** (hybrid RLAIF-RLHF) improves OOD accuracy by **10–45%** in medical diagnostics and legal case predictions via chain-of-thought reward modeling \\[110\\]\\[286\\].\n\n**Harmlessness**: RLAIF achieves **88% harmless output rates** in sensitive domains vs. RLHF’s 76% \\[12\\].\n\n**Case Studies**\n\n**Healthcare**: **Med-R1** uses group-relative policy optimization and rule-based rewards (e.g., clinical guidelines) to reduce misalignment in treatment recommendations \\[282\\].\n\n**Legal**: AI systems prioritize life-saving in triage scenarios, demonstrating ethical alignment in high-stakes decisions \\[288\\].\n\n**9\\. Future Directions and Challenges**\n\n**OOD Generalization**: GenRM and CoT-GenRM show promise, but domain-specific tuning (e.g., for rare diseases) remains challenging \\[110\\].\n\n**Bias-Energy Trade-offs**: Real-time debiasing increases compute costs; optimizing this trade-off is critical \\[270\\].\n\n**Regulatory Compliance**: Hybrid systems like RLTHF must document human oversight for audits in regulated domains \\[300\\].\n\n**Multimodal Alignment**: **MM-RLHF-SafeBench** benchmarks safety in adversarial scenarios (e.g., misleading medical imagery) \\[342\\].\n\n**10\\. Conclusion**\n\nRLAIF represents a paradigm shift in LLM alignment, offering scalability, cost efficiency, and competitive performance. Hybrid architectures (RLTHF, HybridFlow) and real-time debiasing techniques reconcile AI efficiency with human oversight, especially in high-stakes domains. However, persistent challenges—OOD robustness, ethical bias, and domain specificity—demand continued innovation. By 2025, the fusion of RLAIF’s automation with RLHF’s precision defines the frontier of aligned AI systems.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Aalto Science Institute (ASCI) International Summer Research Programme 2025 project list (updated, 7.1.2025)](https://www.aalto.fi/sites/g/files/flghsv161/files/2025-01/AScI_summer_research_project_list_2025.pdf)\n\n[2\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[3\\. RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://abdullah-mamun.com/talk/rlaif-vs.-rlhf-scaling-reinforcement-learning-from-human-feedback-with-ai-feedback/rlaif_mamun.pdf)\n\n[4\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[5\\. LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://dev.datascienceassn.org/sites/default/files/pdf_files/LLM%20Post-Training%20-%20A%20Deep%20Dive%20into%20Reasoning%20Large%20Language%20Models.pdf)\n\n[6\\. Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models](https://iclr.cc/media/iclr-2025/Slides/27970_tQi1ov0.pdf)\n\n[7\\. Teng Xiao, Zhen Ge et al. “InfoPO: On Mutual Information Maximization for Large Language Model Alignment.”](https://arxiv.org/abs/2505.08507)\n\n[8\\. LLM Alignment to human values and goals - Toloka](https://toloka.ai/blog/llm-alignment-to-human-values-and-goals/#:~:text=The%20traditional%20approach%20to%20aligning,direct%20preference%20optimization%20%28DPO%29.)\n\n[9\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[10\\. Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196v3)\n\n[11\\. PAD: PERSONALIZED ALIGNMENT OF LLMs AT DECODING-TIME](https://arxiv.org/pdf/2410.04070)\n\n[12\\. Artificial Intelligence Index Report 2024](https://hai-production.s3.amazonaws.com/files/hai_ai-index-report-2024-smaller2.pdf)\n\n[13\\. AI Alignment: A Comprehensive Survey](https://arxiv.org/pdf/2310.19852)\n\n[14\\. REAL: Efficient RLHF Training of Large Language Models with Parameter Reallocation](https://arxiv.org/pdf/2406.14088v2)\n\n[15\\. RLHF vs RLAIF for language model alignment](https://www.assemblyai.com/blog/rlhf-vs-rlaif-for-language-model-alignment/)\n\n[16\\. Erhan Xu, Kai Ye et al. “Doubly Robust Alignment for Large Language Models.”](https://arxiv.org/abs/2506.01183)\n\n[17\\. Generative Reward Models - A Unified Approach to RLHF and RLAIF](https://static.synthlabs.ai/preprints/Generative_Reward_Models.pdf)\n\n[18\\. Rafael Rafailov, Archit Sharma et al. “Direct Preference Optimization: Your Language Model is Secretly a Reward Model.” ArXiv](https://doi.org/10.48550/arXiv.2305.18290)\n\n[19\\. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267v1)\n\n[21\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[22\\. RLTHF: Targeted Human Feedback for LLM Alignment](https://www.arxiv.org/pdf/2502.13417v1)\n\n[23\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[24\\. Understanding RLAIF: A Technical Overview of Scaling LLM Alignment With AI Feedback](https://dzone.com/articles/understanding-rlaif-a-technical-overview)\n\n[25\\. LLM Alignment to human values and goals - Toloka](https://toloka.ai/blog/llm-alignment-to-human-values-and-goals/#:~:text=The%20traditional%20approach%20to%20aligning,direct%20preference%20optimization%20%28DPO%29.)\n\n[26\\. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267v1)\n\n[27\\. LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://dev.datascienceassn.org/sites/default/files/pdf_files/LLM%20Post-Training%20-%20A%20Deep%20Dive%20into%20Reasoning%20Large%20Language%20Models.pdf)\n\n[28\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[29\\. HyunJin Kim, Xiaoyuan Yi et al. “The Road to Artificial SuperIntelligence: A Comprehensive Survey of Superalignment.”](https://arxiv.org/abs/2412.16468)\n\n[30\\. RLAIF: SCALING REINFORCEMENT LEARNING FROM HUMAN FEEDBACK WITH AI FEEDBACK](https://openreview.net/pdf?id=AAxIs3D2ZZ)\n\n[31\\. Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning](https://arxiv.org/pdf/2505.07527)\n\n[32\\. Wayne Xin Zhao, Kun Zhou et al. “A Survey of Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2303.18223)\n\n[33\\. RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness](https://arxiv.org/pdf/2405.17220)\n\n[34\\. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI](https://openreview.net/forum?id=AAxIs3D2ZZ)\n\n[35\\. LLM：Post-Training - 瓦尔登湖小酒馆| OAA的博客](https://drinkingfishingseeking.com/2025/03/07/llm-post-training/)\n\n[36\\. RLHF再也不需要人类了!谷歌团队研究证明,AI标注已达人类水平](https://www.woshipm.com/ai/5898421.html)\n\n[37\\. Aligner: Efficient Alignment by Learning to Correct](https://proceedings.neurips.cc/paper_files/paper/2024/file/a51a74b2d71387dc71cc29181b5519bb-Paper-Conference.pdf)\n\n[41\\. RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://openreview.net/pdf?id=uydQ2W41KO)\n\n[42\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[43\\. RLHF vs RLAIF: Choosing the right approach for fine-tuning your LLM](https://labelbox.com/blog/rlhf-vs-rlaif/)\n\n[44\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[45\\. What Is Reinforcement Learning from Human Feedback (RLHF)?](https://swimm.io/learn/large-language-models/what-is-reinforcement-learning-from-human-feedback-rlhf)\n\n[46\\. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267v1)\n\n[47\\. 基于人类反馈的强化学习与 RLAIF——AI 前沿跟踪系列（三）](https://pdf.dfcfw.com/pdf/H3_AP202310211602714462_1.pdf)\n\n[48\\. RLAIF: SCALING REINFORCEMENT LEARNING FROM HUMAN FEEDBACK WITH AI FEEDBACK](https://openreview.net/pdf?id=AAxIs3D2ZZ)\n\n[49\\. RLAIF vs. RLHF: Scaling Reinforcement Learning from ...](https://arxiv.org/abs/2309.00267)\n\n[50\\. Reinforcement Learning: Advanced Techniques for LLM Behavior Optimization](https://www.espjournals.org/IJACT/2025/Volume3-Issue1/IJACT-V3I1P110.pdf)\n\n[51\\. Matthew Brophy. “Wide Reflective Equilibrium in LLM Alignment: Bridging Moral Epistemology and AI Safety.”](https://arxiv.org/abs/2506.00415)\n\n[52\\. Generative Reward Models - A Unified Approach to RLHF and RLAIF](https://static.synthlabs.ai/preprints/Generative_Reward_Models.pdf)\n\n[53\\. Rafael Rafailov, Archit Sharma et al. “Direct Preference Optimization: Your Language Model is Secretly a Reward Model.” ArXiv](https://doi.org/10.48550/arXiv.2305.18290)\n\n[54\\. Yuntao Bai, Andy Jones et al. “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2204.05862)\n\n[55\\. Understanding RLAIF: A Technical Overview of Scaling LLM Alignment With AI Feedback](https://dzone.com/articles/understanding-rlaif-a-technical-overview)\n\n[56\\. LLM RLHF论文精读(二): RLHF和RLAIF的比较](https://zhuanlan.zhihu.com/p/661341293)\n\n[57\\. Reinforcement Learning from Human Feedback (RLHF)](https://www.lakera.ai/blog/reinforcement-learning-from-human-feedback)\n\n[61\\. WEAK-TO-STRONG PREFERENCE OPTIMIZATION: STEALING REWARD FROM WEAK ALIGNED MODEL](https://openreview.net/pdf/f6c72302b5985b222f1213927b75bedfe2d64f05.pdf)\n\n[62\\. Self-Consuming Generative Models with Curated Data Provably Optimize Human Preferences](https://proceedings.neurips.cc/paper_files/paper/2024/file/b9e88ae0308cf82d0b0f634ddbdf809a-Paper-Conference.pdf)\n\n[63\\. RLTHF: Targeted Human Feedback for LLM Alignment](https://www.arxiv.org/pdf/2502.13417v1)\n\n[64\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[65\\. Preference Ranking Optimization for Human Alignment](https://ojs.aaai.org/index.php/AAAI/article/view/29865/31509)\n\n[66\\. LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://dev.datascienceassn.org/sites/default/files/pdf_files/LLM%20Post-Training%20-%20A%20Deep%20Dive%20into%20Reasoning%20Large%20Language%20Models.pdf)\n\n[67\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[68\\. ABC Align: Large Language Model Alignment for Safety & Accuracy](https://arxiv.org/pdf/2408.00307)\n\n[69\\. LIRE: listwise reward enhancement for preference alignment](https://arxiv.org/pdf/2405.13516)\n\n[70\\. 大语言模型](https://library.qiangtu.com/download/993/pdf/993.pdf)\n\n[71\\. Highlighting DeepSeek-R1: Architecture, Features and Future Implications](https://ijcsmc.com/docs/papers/February2025/V14I2202501.pdf)\n\n[72\\. AI Alignment: A Comprehensive Survey](https://arxiv.org/pdf/2310.19852)\n\n[73\\. Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment](https://openreview.net/pdf?id=xmFHPCU1rY)\n\n[74\\. SELF-PLAY PREFERENCE OPTIMIZATION FOR LANGUAGE MODEL ALIGNMENT](https://openreview.net/pdf?id=a3PmRgAB5T)\n\n[75\\. PAD: PERSONALIZED ALIGNMENT OF LLMs AT DECODING-TIME](https://arxiv.org/pdf/2410.04070)\n\n[76\\. Evaluating AI Agents in 2025 - by Nilesh Barla - Adaline Labs](https://labs.adaline.ai/p/evaluating-ai-agents-in-2025)\n\n[77\\. (1) Unified Reward Model Training, (2) Preference Data ...](https://arxiv.org/html/2503.05236v1)\n\n[78\\. GitHub - mengdi-li/awesome-RLAIF: A continually updated list of literature on Reinforcement Learning from AI Feedback (RLAIF)](https://github.com/mengdi-li/awesome-RLAIF)\n\n[81\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[82\\. LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://dev.datascienceassn.org/sites/default/files/pdf_files/LLM%20Post-Training%20-%20A%20Deep%20Dive%20into%20Reasoning%20Large%20Language%20Models.pdf)\n\n[83\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[84\\. RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](http://arxiv.org/pdf/2309.00267)\n\n[85\\. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267v1)\n\n[86\\. How to Implement Reinforcement Learning from AI Feedback (RLAIF)](https://labelbox.com/guides/reinforcement-learning-from-ai-feedback-rlaif/)\n\n[87\\. 图像编辑专家：一种用于扩散模型的 RLAIF 方法](https://www.xueshuxiangzi.com/downloads/2025_4_18/2504.12833.pdf)\n\n[88\\. GitHub - vicgalle/awesome-rlaif: A curated and updated list of relevant ...](https://github.com/vicgalle/awesome-rlaif)\n\n[89\\. Artificial Intelligence Index Report 2024](https://www.bollettinoadapt.it/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_.pdf)\n\n[90\\. LLM Alignment to human values and goals - Toloka](https://toloka.ai/blog/llm-alignment-to-human-values-and-goals/#:~:text=The%20traditional%20approach%20to%20aligning,direct%20preference%20optimization%20%28DPO%29.)\n\n[91\\. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI](https://openreview.net/forum?id=AAxIs3D2ZZ)\n\n[92\\. Reinforcement Learning from Human Feedback (RLHF)](https://www.lakera.ai/blog/reinforcement-learning-from-human-feedback)\n\n[93\\. Transforming Human Interactions with AI via Reinforcement Learning with Human Feedback (RLHF)](https://computing.mit.edu/wp-content/uploads/2023/06/Transforming-Human-Interactions-with-AI-via-Reinforcement-Learning-with-Human-Feedback-RLHF.pdf)\n\n[94\\. RLAIF: SCALING REINFORCEMENT LEARNING FROM HUMAN FEEDBACK WITH AI FEEDBACK](https://openreview.net/pdf?id=AAxIs3D2ZZ)\n\n[95\\. RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness](https://arxiv.org/pdf/2405.17220)\n\n[96\\. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI ...](https://www.scienceopen.com/document?vid=44364a36-e176-438b-b758-66c7f5f3a078)\n\n[97\\. GitHub - mengdi-li/awesome-RLAIF: A continually updated list of literature on Reinforcement Learning from AI Feedback (RLAIF)](https://github.com/mengdi-li/awesome-RLAIF)\n\n[101\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[102\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[103\\. P. Christiano, J. Leike et al. “Deep Reinforcement Learning from Human Preferences.” ArXiv](https://arxiv.org/abs/1706.03741)\n\n[104\\. Nisan Stiennon, Long Ouyang et al. “Learning to summarize from human feedback.” ArXiv](https://arxiv.org/abs/2009.01325)\n\n[105\\. Daniel M. Ziegler, Nisan Stiennon et al. “Fine-Tuning Language Models from Human Preferences.” ArXiv](https://arxiv.org/abs/1909.08593)\n\n[106\\. Artificial Intelligence Index Report 2024](https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_Chapter2.pdf)\n\n[107\\. LLM训练-对齐算法综述：RLHF, RLAIF, PPO, DPO and More](http://139.9.1.231/index.php/2025/02/05/llm-alignment-techniques-rlhf-rlaif-ppo-dpo-and-more/)\n\n[108\\. Shulex整理｜斯坦福：2024年人工智能指数报告第二章：技术 ...](https://m.amz123.com/t/U6OSWak2)\n\n[109\\. 大模型对齐技术的综合评述：RLHF、RLAIF、PPO、DPO及更 ...](https://www.53ai.com/news/finetuning/2024091293174.html)\n\n[110\\. Generative Reward Models that Unify RLHF and RLAIF ...](https://www.synthlabs.ai/research/generative-reward-models#:~:text=Generative%20Reward%20Models%20%28GenRM%29%20is,methods%20by%20up%20to%2045%25.)\n\n[111\\. PrefPaint: 将图像修复扩散模型与人类偏好对齐](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_30/2410.21966.pdf)\n\n[112\\. 第 2 章：技术性能 —— 2024 年人工智能指数报告 \\[译\\] | 宝玉的分享](http://baoyu.io/translations/ai-reports/stanford-hai-ai-index-report-2024-chapter2)\n\n[113\\. Harrison Lee, Samrat Phatale et al. “RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2309.00267)\n\n[121\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[122\\. HybridFlow: A Flexible and Efficient RLHF Framework](https://i.cs.hku.hk/~cwu/papers/gmsheng-eurosys25.pdf)\n\n[123\\. Optimizing RLHF Training for Large Language Models with Stage Fusion](http://www.arxiv.org/pdf/2409.13221)\n\n[124\\. ASYNCHRONOUS RLHF: FASTER AND MORE EFFICIENT OFF-POLICY RL FOR LANGUAGE MODELS](https://openreview.net/pdf/d0759a26adbc702480d4dafff3b0bc31aa5c6240.pdf)\n\n[125\\. LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://dev.datascienceassn.org/sites/default/files/pdf_files/LLM%20Post-Training%20-%20A%20Deep%20Dive%20into%20Reasoning%20Large%20Language%20Models.pdf)\n\n[126\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[127\\. RLTHF: Targeted Human Feedback for LLM Alignment](https://www.arxiv.org/pdf/2502.13417v1)\n\n[128\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[129\\. Comparison of Major LLM Architectures (2017– 2025)](https://skillenai.com/competition-post/comparison-of-major-llm-architectures-2017-2025/)\n\n[130\\. LIMR: Less is More for RL Scaling](http://arxiv.org/html/2502.11886v1)\n\n[131\\. Best RLHF Libraries in 2025 \\[Updated\\] - Labellerr](https://www.labellerr.com/blog/best-rlhf-libraries/)\n\n[132\\. RLTHF: Targeted Human Feedback for LLM Alignment - Mic...](https://www.microsoft.com/en-us/research/publication/rlthf-targeted-human-feedback-for-llm-alignment/)\n\n[141\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[142\\. Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196v3)\n\n[143\\. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267v1)\n\n[144\\. Artificial Intelligence Index Report 2024](https://www.bollettinoadapt.it/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_.pdf)\n\n[145\\. RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://openreview.net/pdf?id=uydQ2W41KO)\n\n[146\\. Investigating Implicit Bias in Large Language Models: A Large-Scale Study of Over 50 LLMs](https://openreview.net/pdf?id=tYDn5pGs5P)\n\n[147\\. LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://dev.datascienceassn.org/sites/default/files/pdf_files/LLM%20Post-Training%20-%20A%20Deep%20Dive%20into%20Reasoning%20Large%20Language%20Models.pdf)\n\n[148\\. Evaluating AI Agents in 2025 - by Nilesh Barla - Adaline Labs](https://labs.adaline.ai/p/evaluating-ai-agents-in-2025)\n\n[149\\. Multimodal Large Language Models Make Text-to-Image Generative Models Align Better](https://proceedings.neurips.cc/paper_files/paper/2024/file/9421261e06f1a63a352b068f1ac90609-Paper-Conference.pdf)\n\n[150\\. PROPOSED MODEL AI GOVERNANCE FRAMEWORK FOR GENERATIVE AI Fostering a Trusted Ecosystem](https://aiverifyfoundation.sg/downloads/Proposed_MGF_Gen_AI_2024.pdf)\n\n[151\\. AI Alignment: A Comprehensive Survey](https://arxiv.org/pdf/2310.19852)\n\n[152\\. LLM Alignment to human values and goals - Toloka](https://toloka.ai/blog/llm-alignment-to-human-values-and-goals/#:~:text=The%20traditional%20approach%20to%20aligning,direct%20preference%20optimization%20%28DPO%29.)\n\n[153\\. Prompt Injection Generation Using Small Language Models with Reinforcement Learning with Artificial Intelligence Feedback](https://dspace.mit.edu/bitstream/handle/1721.1/159142/gupta-aneeshg-meng-eecs-2025-thesis.pdf?sequence=-1&isAllowed=y)\n\n[154\\. GitHub - mengdi-li/awesome-RLAIF: A continually updated list of literature on Reinforcement Learning from AI Feedback (RLAIF)](https://github.com/mengdi-li/awesome-RLAIF)\n\n[155\\. Ensuring Fairness and Minimizing Bias in LLMs - Rohan's Bytes](https://www.rohan-paul.com/p/ensuring-fairness-and-minimizing)\n\n[161\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[162\\. Starling-7B: Improving Helpfulness and Harmlessness with RLAIF](https://openreview.net/pdf/40b25a86c4364d2b54ef5c7bc0d6765f5201b5e2.pdf)\n\n[163\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[164\\. 每日报告精选](https://file.iyanbao.com/pdf/70833-4d0c53ad-53f3-4091-ae2a-f9269be28485.pdf)\n\n[165\\. Robust Preference Optimization through Reward Model Distillation](https://arxiv.org/pdf/2405.19316)\n\n[166\\. Learning Reward and Policy Jointly from Demonstration and Preference Improves Alignment](https://arxiv.org/pdf/2406.06874)\n\n[167\\. MaxMin-RLHF: Alignment with Diverse Human Preferences](https://arxiv.org/pdf/2402.08925)\n\n[168\\. Yuntao Bai, Andy Jones et al. “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2204.05862)\n\n[169\\. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267v1)\n\n[170\\. Yuntao Bai, Saurav Kadavath et al. “Constitutional AI: Harmlessness from AI Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2212.08073)\n\n[171\\. Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation](https://www.nematilab.info/bmijc/assets/053124_paper.pdf)\n\n[172\\. Nisan Stiennon, Long Ouyang et al. “Learning to summarize from human feedback.” ArXiv](https://arxiv.org/abs/2009.01325)\n\n[173\\. Weak-to-Strong Extrapolation Expedites Alignment](http://arxiv.org/html/2404.16792v2)\n\n[174\\. Artificial Intelligence Index Report 2024](https://www.bollettinoadapt.it/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_.pdf)\n\n[175\\. Can RLHF be More Efficient with Imperfect Reward Models? A Policy Coverage Perspective](https://arxiv.org/pdf/2502.19255)\n\n[176\\. RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://openreview.net/pdf?id=uydQ2W41KO)\n\n[177\\. RLAIF: SCALING REINFORCEMENT LEARNING FROM HUMAN FEEDBACK WITH AI FEEDBACK](https://openreview.net/pdf?id=AAxIs3D2ZZ)\n\n[178\\. RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness](https://arxiv.org/pdf/2405.17220)\n\n[181\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[182\\. ASYNCHRONOUS RLHF: FASTER AND MORE EFFICIENT OFF-POLICY RL FOR LANGUAGE MODELS](https://openreview.net/pdf/d0759a26adbc702480d4dafff3b0bc31aa5c6240.pdf)\n\n[183\\. HybridFlow: A Flexible and Efficient RLHF Framework](https://i.cs.hku.hk/~cwu/papers/gmsheng-eurosys25.pdf)\n\n[184\\. Optimizing RLHF Training for Large Language Models with Stage Fusion](http://www.arxiv.org/pdf/2409.13221)\n\n[185\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[186\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[187\\. LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://dev.datascienceassn.org/sites/default/files/pdf_files/LLM%20Post-Training%20-%20A%20Deep%20Dive%20into%20Reasoning%20Large%20Language%20Models.pdf)\n\n[188\\. RLTHF: Targeted Human Feedback for LLM Alignment](https://www.arxiv.org/pdf/2502.13417v1)\n\n[189\\. Matrix 2024](https://www.microsoft.com/en-us/research/uploads/prod/2024/09/matrix69.pdf)\n\n[190\\. Rafael Rafailov, Archit Sharma et al. “Direct Preference Optimization: Your Language Model is Secretly a Reward Model.” ArXiv](https://doi.org/10.48550/arXiv.2305.18290)\n\n[191\\. Yuntao Bai, Andy Jones et al. “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2204.05862)\n\n[192\\. blog/2023-in-llms.md at d2e23fbcb5f012cfbfac22463da96d...](https://github.com/ADITYATIWARI342005/blog/blob/d2e23fbcb5f012cfbfac22463da96dfd7b23feee/2023-in-llms.md)\n\n[201\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[202\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[203\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[204\\. 6th Global Conclave on Neurology and Neurological Disorders](https://peersalley.s3.amazonaws.com/assets/documents/neuro-conclave-2025scientificcompressed-49-1188.pdf)\n\n[205\\. Nisan Stiennon, Long Ouyang et al. “Learning to summarize from human feedback.” ArXiv](https://arxiv.org/abs/2009.01325)\n\n[206\\. Yuntao Bai, Saurav Kadavath et al. “Constitutional AI: Harmlessness from AI Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2212.08073)\n\n[207\\. Rethinking AI Code Generation: A One-shot Correction Approach Based on User Feedback](https://www.researchsquare.com/article/rs-3606400/v1.pdf)\n\n[208\\. LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://dev.datascienceassn.org/sites/default/files/pdf_files/LLM%20Post-Training%20-%20A%20Deep%20Dive%20into%20Reasoning%20Large%20Language%20Models.pdf)\n\n[209\\. Bias Resistant Retrieval Augmented Generation: A Clustering and BiQ Driven Approach with Equitable AI](https://www.ijisrt.com/assets/upload/files/IJISRT25MAR109.pdf)\n\n[210\\. Prompt Injection Generation Using Small Language Models with Reinforcement Learning with Artificial Intelligence Feedback](https://dspace.mit.edu/bitstream/handle/1721.1/159142/gupta-aneeshg-meng-eecs-2025-thesis.pdf?sequence=-1&isAllowed=y)\n\n[211\\. AI Alignment: A Comprehensive Survey](https://arxiv.org/pdf/2310.19852)\n\n[221\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[222\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[223\\. P. Christiano, J. Leike et al. “Deep Reinforcement Learning from Human Preferences.” ArXiv](https://arxiv.org/abs/1706.03741)\n\n[224\\. Rafael Rafailov, Archit Sharma et al. “Direct Preference Optimization: Your Language Model is Secretly a Reward Model.” ArXiv](https://doi.org/10.48550/arXiv.2305.18290)\n\n[225\\. Nisan Stiennon, Long Ouyang et al. “Learning to summarize from human feedback.” ArXiv](https://arxiv.org/abs/2009.01325)\n\n[226\\. Image-Editing Specialists: An RLAIF Approach for Diffusion ...](https://arxiv.org/abs/2504.12833)\n\n[227\\. MaxMin-RLHF: Alignment with Diverse Human Preferences](https://arxiv.org/pdf/2402.08925)\n\n[228\\. Clone-Robust AI Alignment](https://arxiv.org/pdf/2501.09254)\n\n[229\\. PrefPaint: 将图像修复扩散模型与人类偏好对齐](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_30/2410.21966.pdf)\n\n[230\\. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267v1)\n\n[231\\. 【科研播报】多模态大模型人类偏好对齐新范式MM-RLHF!10个...](https://mp.weixin.qq.com/s?__biz=MzUxMDE4MzAzOA%3D%3D&mid=2247833226&idx=3&sn=91eb3644e5b7a332f16aa75551109421&chksm=f8b499a301a2577c9ff6d39f15083a290f51c4f57c40b558f785ae5feda98d12aa478bf86976&scene=27)\n\n[232\\. LLM RLHF论文精读(二): RLHF和RLAIF的比较](https://zhuanlan.zhihu.com/p/661341293)\n\n[233\\. AlignDiff: 通过扩散模型生成定制化行为对齐多样化人类偏好](https://zhuanlan.zhihu.com/p/661205858)\n\n[234\\. 大模型对齐技术的综合评述：RLHF、RLAIF、PPO、DPO及更 ...](https://www.813zy.com/news/finetuning/2024091293174.html)\n\n[238\\. HybridFlow: A Flexible and Efficient RLHF Framework](https://i.cs.hku.hk/~cwu/papers/gmsheng-eurosys25.pdf)\n\n[239\\. HybridFlow: A Flexible and Efficient RLHF Framework...](https://ui.adsabs.harvard.edu/abs/arXiv:2409.19256)\n\n[240\\. 最高提升20倍吞吐量！豆包大模型团队发布全新RLHF 框架](https://seed.bytedance.com/zh/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90)\n\n[241\\. HybridFlow: A Flexible and Efficient RLHF Framework - 研究成果](https://team.doubao.com/zh/publication/hybridflow-a-flexible-and-efficient-rlhf-framework?view_from=research)\n\n[242\\. HybridFlow A Flexible and Efficient RLHF Framework| 豆包 ...](https://juejin.cn/post/7442219660597624886)\n\n[257\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[258\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[259\\. DISTRL: AN ASYNCHRONOUS DISTRIBUTED REINFORCEMENT LEARNING FRAMEWORK FOR ON-DEVICE CONTROL AGENTS](https://openreview.net/pdf/b0ac68a1bdcdb9b6e72ee52644ecbb6103901c0a.pdf)\n\n[260\\. LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://dev.datascienceassn.org/sites/default/files/pdf_files/LLM%20Post-Training%20-%20A%20Deep%20Dive%20into%20Reasoning%20Large%20Language%20Models.pdf)\n\n[261\\. RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness](https://arxiv.org/pdf/2405.17220)\n\n[262\\. RLTHF: Targeted Human Feedback for LLM Alignment](https://www.arxiv.org/pdf/2502.13417v1)\n\n[263\\. Prompt Injection Generation Using Small Language Models with Reinforcement Learning with Artificial Intelligence Feedback](https://dspace.mit.edu/bitstream/handle/1721.1/159142/gupta-aneeshg-meng-eecs-2025-thesis.pdf?sequence=-1&isAllowed=y)\n\n[264\\. Yuntao Bai, Andy Jones et al. “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2204.05862)\n\n[265\\. Yuntao Bai, Saurav Kadavath et al. “Constitutional AI: Harmlessness from AI Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2212.08073)\n\n[266\\. AI Alignment: A Comprehensive Survey](https://arxiv.org/pdf/2310.19852)\n\n[267\\. Nisan Stiennon, Long Ouyang et al. “Learning to summarize from human feedback.” ArXiv](https://arxiv.org/abs/2009.01325)\n\n[268\\. Starling-7B: Improving Helpfulness and Harmlessness with RLAIF](https://openreview.net/pdf/40b25a86c4364d2b54ef5c7bc0d6765f5201b5e2.pdf)\n\n[269\\. Bias Resistant Retrieval Augmented Generation: A Clustering and BiQ Driven Approach with Equitable AI](https://www.ijisrt.com/assets/upload/files/IJISRT25MAR109.pdf)\n\n[270\\. Ensuring Fairness and Minimizing Bias in LLMs - Rohan's Bytes](https://www.rohan-paul.com/p/ensuring-fairness-and-minimizing)\n\n[271\\. GitHub - mengdi-li/awesome-RLAIF: A continually updated list of literature on Reinforcement Learning from AI Feedback (RLAIF)](https://github.com/mengdi-li/awesome-RLAIF)\n\n[272\\. SUPER AI: A REVOLUTIONARY MULTIMEDIA CONTENT MANAGEMENT PLATFORM](https://www.irjmets.com/uploadedfiles/paper/issue_2_february_2025/67967/final/fin_irjmets1740194038.pdf)\n\n[273\\. A TECHNICAL NEWSLETTER FROM ARTIFICIAL INTELLIGENCE & DATA SCIENCE ACADEMIC YEAR 2024-2025](https://pict.edu/AIDS-dept/pdf/Technical%20Newsletter%20Final.pdf)\n\n[274\\. Evaluating AI Agents in 2025 - by Nilesh Barla - Adaline Labs](https://labs.adaline.ai/p/evaluating-ai-agents-in-2025)\n\n[275\\. Reinforcement Learning Enhanced LLMs: A Survey](https://papers-pdfs.assets.alphaxiv.org/2412.10400v3.pdf)\n\n[276\\. 必看！2023年大语言模型优质论文汇总！](https://zhuanlan.zhihu.com/p/672902375)\n\n[277\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[278\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[279\\. P. Christiano, J. Leike et al. “Deep Reinforcement Learning from Human Preferences.” ArXiv](https://arxiv.org/abs/1706.03741)\n\n[280\\. Rafael Rafailov, Archit Sharma et al. “Direct Preference Optimization: Your Language Model is Secretly a Reward Model.” ArXiv](https://doi.org/10.48550/arXiv.2305.18290)\n\n[281\\. Yuntao Bai, Andy Jones et al. “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2204.05862)\n\n[282\\. Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in Vision-Language Models](https://arxiv.org/pdf/2503.13939v3)\n\n[283\\. PrefPaint: 将图像修复扩散模型与人类偏好对齐](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_30/2410.21966.pdf)\n\n[284\\. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267v1)\n\n[285\\. AlignCap：使语音情感字幕与人类偏好对齐](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_28/2410.19134.pdf)\n\n[286\\. Generative Reward Models that Unify RLHF and RLAIF ...](https://www.synthlabs.ai/research/generative-reward-models)\n\n[287\\. Artificial Intelligence Index Report 2024](https://www.hkdca.com/wp-content/uploads/2024/06/ai-index-report-2024-hai.pdf)\n\n[288\\. AI EXHIBITING EMERGENT HUMAN BEHAVIORS: GLOBAL RISK ASSESSMENT OF 2025 REASONING LLMs – CASE STUDIES: OPENAI O3-MINI, DEEPSEEK R1, GEMINI 2, GEMINI 2.5, GROK 3, QWEN 2.5 (PRESENTING: TURING NAND TEST AND DFSW BIAS TEST)](https://zenodo.org/records/15185640/files/2025.03_AI_HUMAN_BEHAVIORS_RISK_ASSESSMENT.v1.2.pdf?download=1)\n\n[289\\. Evaluating AI Agents in 2025 - by Nilesh Barla - Adaline Labs](https://labs.adaline.ai/p/evaluating-ai-agents-in-2025)\n\n[294\\. Adam Paszke, Sam Gross et al. “PyTorch: An Imperative Style, High-Performance Deep Learning Library.” ArXiv](https://arxiv.org/abs/1912.01703)\n\n[295\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[296\\. HybridFlow: A Flexible and Efficient RLHF Framework](https://i.cs.hku.hk/~cwu/papers/gmsheng-eurosys25.pdf)\n\n[297\\. Nisan Stiennon, Long Ouyang et al. “Learning to summarize from human feedback.” ArXiv](https://arxiv.org/abs/2009.01325)\n\n[298\\. M. Shoeybi, M. Patwary et al. “Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.” ArXiv](https://arxiv.org/abs/1909.08053)\n\n[299\\. Optimizing RLHF Training for Large Language Models with Stage Fusion](http://www.arxiv.org/pdf/2409.13221)\n\n[300\\. RLTHF: Targeted Human Feedback for LLM Alignment](https://www.arxiv.org/pdf/2502.13417v1)\n\n[301\\. Zhengda Bian, Hongxin Liu et al. “Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training.” Proceedings of the 52nd International Conference on Parallel Processing](https://doi.org/10.1145/3605573.3605613)\n\n[314\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[315\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[316\\. LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://dev.datascienceassn.org/sites/default/files/pdf_files/LLM%20Post-Training%20-%20A%20Deep%20Dive%20into%20Reasoning%20Large%20Language%20Models.pdf)\n\n[317\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[318\\. Prompt Injection Generation Using Small Language Models with Reinforcement Learning with Artificial Intelligence Feedback](https://dspace.mit.edu/bitstream/handle/1721.1/159142/gupta-aneeshg-meng-eecs-2025-thesis.pdf?sequence=-1&isAllowed=y)\n\n[319\\. Ensuring Fairness and Minimizing Bias in LLMs - Rohan's Bytes](https://www.rohan-paul.com/p/ensuring-fairness-and-minimizing)\n\n[320\\. How to Implement Reinforcement Learning from AI Feedback (RLAIF)](https://labelbox.com/guides/reinforcement-learning-from-ai-feedback-rlaif/)\n\n[321\\. Evaluating AI Agents in 2025 - by Nilesh Barla - Adaline Labs](https://labs.adaline.ai/p/evaluating-ai-agents-in-2025)\n\n[322\\. P. Christiano, J. Leike et al. “Deep Reinforcement Learning from Human Preferences.” ArXiv](https://arxiv.org/abs/1706.03741)\n\n[323\\. Yuntao Bai, Saurav Kadavath et al. “Constitutional AI: Harmlessness from AI Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2212.08073)\n\n[324\\. Rethinking AI Code Generation: A One-shot Correction Approach Based on User Feedback](https://www.researchsquare.com/article/rs-3606400/v1.pdf)\n\n[325\\. AI Weekly: 2025년의 AI: 중국의 선전포고와 미국의 대응](https://securities.miraeasset.com/bbs/download/2133957.pdf?attachmentId=2133957)\n\n[334\\. R. Team. “R: A language and environment for statistical computing..” MSOR connections](https://www.semanticscholar.org/paper/659408b243cec55de8d0a3bc51b81173007aa89b)\n\n[335\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[336\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[337\\. Alistair E. W. Johnson, T. Pollard et al. “MIMIC-III, a freely accessible critical care database.” Scientific Data](https://doi.org/10.1038/sdata.2016.35)\n\n[338\\. Nisan Stiennon, Long Ouyang et al. “Learning to summarize from human feedback.” ArXiv](https://arxiv.org/abs/2009.01325)\n\n[339\\. LLM训练-对齐算法综述：RLHF, RLAIF, PPO, DPO and More](http://139.9.1.231/index.php/2025/02/05/llm-alignment-techniques-rlhf-rlaif-ppo-dpo-and-more/)\n\n[340\\. Shulex整理｜斯坦福：2024年人工智能指数报告第二章：技术 ...](https://www.amz123.com/t/U6OSWak2)\n\n[341\\. PrefPaint: 将图像修复扩散模型与人类偏好对齐](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_30/2410.21966.pdf)\n\n[342\\. 【科研播报】多模态大模型人类偏好对齐新范式MM-RLHF!10个...](https://mp.weixin.qq.com/s?__biz=MzUxMDE4MzAzOA%3D%3D&mid=2247833226&idx=3&sn=91eb3644e5b7a332f16aa75551109421&chksm=f8b499a301a2577c9ff6d39f15083a290f51c4f57c40b558f785ae5feda98d12aa478bf86976&scene=27)\n\n[343\\. Artificial Intelligence Index Report 2024](https://alnap.org/documents/19944/Stanford20Artificial20Intelligence20Index20Report202024.pdf)\n\n[344\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[345\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[346\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[347\\. HybridFlow: A Flexible and Efficient RLHF Framework](https://i.cs.hku.hk/~cwu/papers/gmsheng-eurosys25.pdf)\n\n[348\\. Optimizing RLHF Training for Large Language Models with Stage Fusion](http://www.arxiv.org/pdf/2409.13221)\n\n[349\\. Omar El Ayach, S. Rajagopal et al. “Spatially Sparse Precoding in Millimeter Wave MIMO Systems.” IEEE Transactions on Wireless Communications](https://doi.org/10.1109/TWC.2014.011714.130846)\n\n[350\\. Nisan Stiennon, Long Ouyang et al. “Learning to summarize from human feedback.” ArXiv](https://arxiv.org/abs/2009.01325)\n\n[351\\. RLTHF: Targeted Human Feedback for LLM Alignment](https://www.arxiv.org/pdf/2502.13417v1)\n\n[364\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[365\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[366\\. LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://dev.datascienceassn.org/sites/default/files/pdf_files/LLM%20Post-Training%20-%20A%20Deep%20Dive%20into%20Reasoning%20Large%20Language%20Models.pdf)\n\n[367\\. Prompt Injection Generation Using Small Language Models with Reinforcement Learning with Artificial Intelligence Feedback](https://dspace.mit.edu/bitstream/handle/1721.1/159142/gupta-aneeshg-meng-eecs-2025-thesis.pdf?sequence=-1&isAllowed=y)\n\n[368\\. RLTHF: Targeted Human Feedback for LLM Alignment](https://www.arxiv.org/pdf/2502.13417v1)\n\n[369\\. P. Christiano, J. Leike et al. “Deep Reinforcement Learning from Human Preferences.” ArXiv](https://arxiv.org/abs/1706.03741)\n\n[370\\. Yuntao Bai, Andy Jones et al. “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2204.05862)\n\n[371\\. Yuntao Bai, Saurav Kadavath et al. “Constitutional AI: Harmlessness from AI Feedback.” ArXiv](https://doi.org/10.48550/arXiv.2212.08073)\n\n[372\\. 6th Global Conclave on Neurology and Neurological Disorders](https://peersalley.s3.amazonaws.com/assets/documents/neuro-conclave-2025scientificcompressed-49-1188.pdf)\n\n[373\\. Ensuring Fairness and Minimizing Bias in LLMs - Rohan's Bytes](https://www.rohan-paul.com/p/ensuring-fairness-and-minimizing)\n\n[374\\. Evaluating AI Agents in 2025 - by Nilesh Barla - Adaline Labs](https://labs.adaline.ai/p/evaluating-ai-agents-in-2025)\n\n[375\\. Semi-Supervised Reward Modeling via Iterative Self-Training](https://arxiv.org/pdf/2409.06903)\n\n[376\\. RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness](https://arxiv.org/pdf/2405.17220)\n\n[377\\. Rethinking AI Code Generation: A One-shot Correction Approach Based on User Feedback](https://www.researchsquare.com/article/rs-3606400/v1.pdf)\n\n[384\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[385\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[386\\. P. Christiano, J. Leike et al. “Deep Reinforcement Learning from Human Preferences.” ArXiv](https://arxiv.org/abs/1706.03741)\n\n[387\\. Rafael Rafailov, Archit Sharma et al. “Direct Preference Optimization: Your Language Model is Secretly a Reward Model.” ArXiv](https://doi.org/10.48550/arXiv.2305.18290)\n\n[388\\. Nisan Stiennon, Long Ouyang et al. “Learning to summarize from human feedback.” ArXiv](https://arxiv.org/abs/2009.01325)\n\n[389\\. Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems](https://openreview.net/pdf?id=tj6ODBPHu3)\n\n[390\\. Image-Editing Specialists: An RLAIF Approach for Diffusion ...](https://arxiv.org/abs/2504.12833)\n\n[391\\. Starling-7B: Improving Helpfulness and Harmlessness with RLAIF](https://openreview.net/pdf/40b25a86c4364d2b54ef5c7bc0d6765f5201b5e2.pdf)\n\n[392\\. 每日报告精选](https://file.iyanbao.com/pdf/70833-4d0c53ad-53f3-4091-ae2a-f9269be28485.pdf)\n\n[393\\. Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF](https://users.ece.cmu.edu/~yuejiec/papers/VPO.pdf)\n\n[394\\. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267v1)\n\n[395\\. Artificial Intelligence Index Report 2024](https://alnap.org/documents/19944/Stanford20Artificial20Intelligence20Index20Report202024.pdf)\n\n[396\\. 【科研播报】多模态大模型人类偏好对齐新范式MM-RLHF!10个...](https://mp.weixin.qq.com/s?__biz=MzUxMDE4MzAzOA%3D%3D&mid=2247833226&idx=3&sn=91eb3644e5b7a332f16aa75551109421&chksm=f8b499a301a2577c9ff6d39f15083a290f51c4f57c40b558f785ae5feda98d12aa478bf86976&scene=27)\n\n[397\\. LLM RLHF论文精读(二): RLHF和RLAIF的比较](https://zhuanlan.zhihu.com/p/661341293)\n\n[398\\. RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://openreview.net/pdf?id=uydQ2W41KO)\n\n[399\\. AI EXHIBITING EMERGENT HUMAN BEHAVIORS: GLOBAL RISK ASSESSMENT OF 2025 REASONING LLMs – CASE STUDIES: OPENAI O3-MINI, DEEPSEEK R1, GEMINI 2, GEMINI 2.5, GROK 3, QWEN 2.5 (PRESENTING: TURING NAND TEST AND DFSW BIAS TEST)](https://zenodo.org/records/15185640/files/2025.03_AI_HUMAN_BEHAVIORS_RISK_ASSESSMENT.v1.2.pdf?download=1)\n\n[400\\. Value-Incentivized Preference Optimization: A Unified Approach to ...](https://yiyibooks.cn/__trs__/arxiv/2405.19320v1/index.html)"
  },
  {
    "id": 48,
    "question": "What is Disaggregated Inference? How does it solve the KV Cache storage management problems in LLM inference, and what are the key innovations in architectures like MemServe and Mooncake?",
    "response": "**Research Report: Disaggregated Inference and Its Role in Solving KV Cache Management Challenges in LLM Inference**\n\n**1\\. Introduction to Disaggregated Inference**\n\nDisaggregated inference is a transformative architecture for large language model (LLM) serving that fundamentally decouples the **prefill** (computation-heavy prompt processing) and **decode** (memory-intensive token generation) phases of inference. Unlike monolithic systems where both phases run on the same GPU/server—causing resource contention and latency spikes—disaggregated inference isolates them into independent instances \\[3\\]\\[3\\]\\[13\\]. This separation enables:\n\n**Independent scaling** of prefill/decode resources \\[10\\].\n\n**Optimized hardware allocation** (e.g., prefill on compute-optimized GPUs, decode on memory-optimized accelerators) \\[8\\]\\[12\\].\n\n**Reduced interference** between latency-critical decode tasks and compute-saturated prefill jobs \\[3\\]\\[13\\].\n\nKey benefits include 30–42% faster job completion times (JCT), 86% lower time-to-first-token (TTFT) in long-context scenarios, and support for heterogeneous hardware deployments \\[3\\]\\[9\\]\\[46\\].\n\n**2\\. The KV Cache Storage Management Problem in LLM Inference**\n\nIn autoregressive LLM decoding, the **KV (Key-Value) cache** stores attention states of prior tokens to avoid recomputation. This cache introduces critical bottlenecks:\n\n**Memory Fragmentation**: Contiguous allocation for variable-length sequences wastes 60–80% of VRAM due to internal/external fragmentation \\[227\\]\\[233\\].\n\n**Capacity Limits**: For a 13B-parameter model, the KV cache consumes ~1.6GB/sequence at 2k tokens, restricting batch sizes and context lengths \\[227\\].\n\n**Transfer Overheads**: In disaggregated systems, moving the KV cache between prefill/decode nodes can dominate latency (e.g., network saturation at 100 Gbps) \\[86\\]\\[95\\].\n\nTraditional monolithic architectures exacerbate these issues by co-locating phases, leading to GPU underutilization and stalled decode tasks \\[3\\]\\[15\\].\n\n**3\\. How Disaggregated Inference Solves KV Cache Management**\n\nDisaggregated architectures address these problems through four key innovations:\n\n**3.1. Phase Separation and Dedicated Optimization**\n\nBy isolating prefill/decode, resources are tailored to each phase’s needs:\n\n**Prefill nodes** handle bursty compute workloads without starving decode \\[3\\]\\[13\\].\n\n**Decode nodes** focus on memory bandwidth for low-latency token generation \\[3\\].\n\nThis reduces JCT by 30–42% and TTFT by 10–86%, depending on workload \\[60\\]\\[63\\].\n\n**3.2. Advanced KV Cache Transfer Mechanisms**\n\nOptimized data movement minimizes transfer latency:\n\n**Tensor-Centric Communication**: Systems like KVDirect use pull-based transfers and NVLink/RDMA to cut sync overhead \\[3\\]\\[86\\]\\[93\\].\n\n**Asynchronous Pipelining**: Mooncake overlaps computation and transfer via layer-wise prefill, launching KV cache storage after each attention layer \\[83\\]\\[91\\].\n\n**Compression**: MiniCache (Mooncake) and LCKV prune/quantize caches, reducing size by 4–8× \\[3\\]\\[146\\].\n\n**3.3. Distributed Memory Pooling**\n\nIdle cluster resources (CPU RAM, SSD) store KV caches:\n\n**Mooncake** pools underutilized DRAM/SSD across nodes into a global cache, managed via an LRU-evicted paged block system \\[54\\]\\[151\\].\n\n**MemServe** abstracts a standalone MemPool with APIs for cross-node KV cache sharing \\[45\\]\\[60\\].\n\nThis enables context reuse, cutting TTFT by 59% for shared prefixes \\[60\\].\n\n**3.4. Fragmentation-Reduction Techniques**\n\n**PagedAttention**: KV caches are split into fixed-size blocks (e.g., 8–64 tokens), allocated non-contiguously. This limits fragmentation to 4% vs. 60% in monolithic systems \\[125\\]\\[129\\]\\[227\\].\n\n**Block Aggregation**: MemServe coalesces small blocks into larger chunks (\"huge pages\"), reducing access overhead by 55% \\[162\\]\\[226\\].\n\n**4\\. Key Innovations in MemServe and Mooncake Architectures**\n\n**4.1. MemServe: Elastic Memory Pool for Caching and Disaggregation**\n\nMemServe unifies **intra-request** (disaggregation) and **inter-request** (caching) optimizations \\[60\\]:\n\n**Flexible Instances**: Supports prefetch-only, decode-only, or PD-collocated nodes \\[45\\].\n\n**MemPool APIs**: Lightweight calls (800 ns/block) insert/fetch caches across nodes \\[60\\].\n\n**Locality-Aware Scheduling**: Global prompt trees maximize KV cache reuse, slashing P99 TTFT by 59% \\[60\\]\\[60\\].\n\n**Performance**: 42% lower P99 JCT for ShareGPT workloads; 17–29% gains with context caching \\[60\\].\n\n**4.2. Mooncake: KVCache-Centric Serving for Long Contexts**\n\nMooncake targets long-context/overloaded scenarios \\[141\\]\\[141\\]:\n\n**Transfer Engine (TE)**: Unified RDMA-based I/O for CPU/DRAM/SSD, achieving 190 GB/s bandwidth \\[50\\]\\[69\\].\n\n**Chunked Pipeline Parallelism (CPP)**: Splits long prompts into chunks, reducing TTFT and network contention \\[141\\]\\[141\\].\n\n**KV Runahead**: Begins processing user inputs pre-emptively, hiding latency \\[56\\].\n\n**SLO-Aware Scheduler**: Balances cache reuse and load via prediction-based early rejection \\[50\\]\\[57\\].\n\n**Performance**: 525% higher throughput in simulations; 75% more real-world requests handled \\[68\\]\\[69\\].\n\n**4.3. Comparative Analysis**\n\n|     |     |     |\n| --- | --- | --- |\n| **Metric** | **MemServe** | **Mooncake** |\n| **Core Innovation** | Elastic MemPool + caching APIs \\[45\\] | KVCache storage + RDMA transfer \\[65\\] |\n| **Latency Reduction** | 42% JCT (ShareGPT); 59% TTFT via trees \\[60\\]\\[60\\] | 86% TTFT (128k tokens) \\[46\\] |\n| **Resource Util.** | Cross-node memory sharing \\[43\\] | Idle CPU/DRAM/SSD pooling \\[54\\] |\n| **Fragmentation** | Block aggregation (\"huge pages\") \\[226\\] | Layer-wise prefill overlap \\[91\\] |\n| **Throughput** | Moderate gains (context-dependent) \\[60\\] | 525% in sims; 75% more requests \\[68\\] |\n\nMooncake excels in extreme-scale/long-context scenarios, while MemServe offers greater flexibility for hybrid workloads \\[65\\]\\[60\\].\n\n**5\\. Performance and Implementation Insights**\n\n**5.1. KV Cache Transfer Mechanics**\n\n**Mooncake**: Employs asynchronous \"messenger processes\" per node. KV caches are streamed during layer-wise prefill via RDMA (RoCEv2), with hotspots replicated across nodes \\[83\\]\\[86\\].\n\n**MemServe**: Uses MemPool’s by-req-agg APIs to fetch blocks via huge-page-optimized network transfers \\[60\\]\\[162\\].\n\n**5.2. PagedAttention Integration**\n\nBoth leverage **PagedAttention** for fragmentation control:\n\nFixed-size blocks (e.g., 16 tokens) stored non-contiguously.\n\n**MemServe** enhances it with block aggregation, improving throughput by 30% over vanilla PagedAttention \\[162\\]\\[226\\].\n\n**Mooncake** uses it in its global cache but does not modify the core mechanism \\[125\\].\n\n**5.3. Multi-Query Attention (MQA)**\n\n**MQA reduces KV cache size** by sharing keys/values across heads (e.g., 8× smaller for 8-head models):\n\nExplicit integration is undocumented in MemServe/Mooncake, but both support attention variants (GQA/MQA) compatible with their caches \\[107\\]\\[154\\].\n\nMooncake’s **MiniCache** uses similar head-sharing principles to compress caches \\[146\\].\n\n**6\\. Conclusion and Future Directions**\n\nDisaggregated inference solves LLM serving bottlenecks by decoupling prefill/decode phases, enabling specialized KV cache management. MemServe and Mooncake exemplify this through:\n\n**MemServe’s elastic MemPool** for unified caching/disaggregation.\n\n**Mooncake’s storage-centric design** for long-context scalability.\n\nCritical enablers include PagedAttention (<4% fragmentation), RDMA-optimized transfers (96% latency reduction), and compression (4–8× cache savings). Future work may integrate MQA more tightly and standardize KV cache transfer APIs across frameworks like vLLM. As LLMs scale to 1M+ tokens, disaggregation will remain essential for viable inference \\[86\\]\\[129\\]\\[205\\].\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[2\\. Alejandro Barredo Arrieta, Natalia Díaz Rodríguez et al. “Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI.” Inf. Fusion](https://doi.org/10.1016/j.inffus.2019.12.012)\n\n[3\\. Cunchen Hu, Heyang Huang et al. “Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads.” ArXiv](https://doi.org/10.48550/arXiv.2401.11181)\n\n[4\\. Woosuk Kwon, Zhuohan Li et al. “Efficient Memory Management for Large Language Model Serving with PagedAttention.” Proceedings of the 29th Symposium on Operating Systems Principles](https://doi.org/10.1145/3600006.3613165)\n\n[5\\. Shiyang Chen, Rain Jiang et al. “KVDirect: Distributed Disaggregated LLM Inference.”](https://arxiv.org/abs/2501.14743)\n\n[6\\. 学术科研简报](https://iiis.tsinghua.edu.cn/uploadfile/2024/01/26/2024012613592592678.pdf)\n\n[7\\. A. Dragojevic, D. Narayanan et al. “This Paper Is Included in the Proceedings of the 11th Usenix Symposium on Networked Systems Design and Implementation (nsdi '14). Farm: Fast Remote Memory Farm: Fast Remote Memory.”](https://www.semanticscholar.org/paper/ab2bdfc3ca2e74eea7989f38276d54cc3a3b17d1)\n\n[8\\. Youhe Jiang, Ran Yan et al. “HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous Environment.”](https://arxiv.org/abs/2502.07903)\n\n[9\\. Cunchen Hu, HeYang Huang et al. “ShuffleInfer: Disaggregate LLM Inference for Mixed Downstream Workloads.” ACM Transactions on Architecture and Code Optimization](https://doi.org/10.1145/3732941)\n\n[10\\. llm-d: Kubernetes-native distributed inferencing](https://developers.redhat.com/articles/2025/05/20/llm-d-kubernetes-native-distributed-inferencing#:~:text=llm-d%20is%20a%20Kubernetes,models%20across%20most%20hardware%20accelerators.)\n\n[11\\. Disaggregated Prefilling (experimental) - 《vLLM v0.7.0 ...](https://www.bookstack.cn/read/vllm-0.7.0-en/9140885ee4fe812f.md)\n\n[12\\. Wenqi Jiang, Marco Zeller et al. “Chameleon: a Heterogeneous and Disaggregated Accelerator System for Retrieval-Augmented Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2310.09949)\n\n[13\\. Baolin Li, Yankai Jiang et al. “LLM Inference Serving: Survey of Recent Advances and Opportunities.”](https://arxiv.org/abs/2407.12391)\n\n[14\\. Youhe Jiang, Ran Yan et al. “HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous Environment.”](https://arxiv.org/abs/2502.07903)\n\n[15\\. Weiqing Li, Guochao Jiang et al. “FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache Transfer and Load-Aware Scheduling.”](https://arxiv.org/abs/2504.03775)\n\n[16\\. \\[讨论\\] 聊聊大模型推理中的分离式推理](https://www.iivd.net/forum.php?mod=viewthread&tid=76611&extra=&mobile=2)\n\n[17\\. 理解 LLM 解释的不确定性：基于推理拓扑的视角](https://www.xueshuxiangzi.com/downloads/2025_2_25/2502.17026.pdf)\n\n[20\\. 探秘Transformer系列之（26）— KV Cache优化—分离or合并](https://www.itfaba.com/jishufenxian/212048.html)\n\n[21\\. MELL: Memory-Efficient Large Language Model Serving via Multi-GPU KV Cache Management](https://arxiv.org/pdf/2501.06709)\n\n[22\\. HydraInfer: Hybrid Disaggregated Scheduling for Multimodal Large Language Model Serving](https://www.arxiv.org/pdf/2505.12658)\n\n[23\\. Kv Cache Explained For Large Language Models | Restackio](https://www.restack.io/p/large-language-models-answer-kv-cache-explained-cat-ai)\n\n[24\\. DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving](https://www.usenix.org/system/files/osdi24-zhong-yinmin.pdf)\n\n[25\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[26\\. A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency](https://arxiv.org/pdf/2505.01658)\n\n[27\\. FlowKV: A Disaggregated Inference Framework with Low- ...](https://paperswithcode.com/paper/flowkv-a-disaggregated-inference-framework)\n\n[28\\. A Scalable Approach to Distributed Large Language Model Inference](https://knowledge.uchicago.edu/record/14559/files/Yihua_Cheng_dissertation_adjusted.pdf)\n\n[29\\. KV Cache Routing — Dynamo - NVIDIA Docs](https://docs.nvidia.com/dynamo/latest/architecture/kv_cache_routing.html#:~:text=Understanding%20KV%20Cache,is%20called%20the%20KV%20Cache.)\n\n[30\\. InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management](https://www.usenix.org/system/files/osdi24-lee.pdf)\n\n[31\\. \\[2501.14743v1\\] KVDirect: Distributed Disaggregated LLM...](http://arxiv.org/abs/2501.14743v1)\n\n[32\\. KVDirect: Distributed Disaggregated LLM Inference](https://arxiv.org/pdf/2501.14743)\n\n[33\\. Understanding LLM Batch Inference - Adaline](https://www.adaline.ai/blog/llm-batch-inference)\n\n[34\\. OSDI 2024](https://paper.lingyunyang.com/reading-notes/conference/osdi-2024)\n\n[35\\. Layer-Condensed KV Cache for Efficient Inference of Large Language Models](https://faculty.sist.shanghaitech.edu.cn/faculty/tukw/acl24lckv.pdf)\n\n[36\\. A Survey on Large Language Model Acceleration based on KV Cache Management](https://openreview.net/pdf/31d6ab447981f515ddb7d2f06b133bd50bc153ef.pdf)\n\n[40\\. Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation](https://arxiv.org/pdf/2503.20552)\n\n[41\\. semi-PD: TOWARDS EFFICIENT LLM SERVING VIA PHASE-WISE DISAGGREGATED COMPUTATION AND UNIFIED STORAGE](https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2F1162e0c8-f610-456a-960b-ca2a6dfe35f3.pdf)\n\n[42\\. Mooncake: Kimi’s KVCache-centric Architecture for LLM Serving](https://arxiv.org/pdf/2407.00079v1)\n\n[43\\. LLM推理优化- Prefill-Decode分离式推理架构 - 异度部落格](https://www.yidoo.xyz/llm-inference-pd-separation-architecture)\n\n[44\\. ​Research team from Department of Computer Science ...](https://www.cs.tsinghua.edu.cn/csen/info/1084/4580.htm)\n\n[45\\. MemServe: Flexible Mem Pool for Building Disaggregated LLM Serving with Caching](https://arxiv.org/pdf/2406.17565)\n\n[46\\. MOONCAKE: Trading More Storage for Less Computation — A KVCache-centric Architecture for Serving LLM Chatbot](https://www.usenix.org/system/files/fast25-qin.pdf)\n\n[47\\. Yizhou Shan, Yutong Huang et al. “LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation.” USENIX Symposium on Operating Systems Design and Implementation](https://www.semanticscholar.org/paper/7945684818786fcb32cf92bace2566d7d6bc8945)\n\n[48\\. Ke Hong, Lufang Chen et al. “semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage.”](https://arxiv.org/abs/2504.19867)\n\n[49\\. A. Dragojevic, D. Narayanan et al. “This Paper Is Included in the Proceedings of the 11th Usenix Symposium on Networked Systems Design and Implementation (nsdi '14). Farm: Fast Remote Memory Farm: Fast Remote Memory.”](https://www.semanticscholar.org/paper/ab2bdfc3ca2e74eea7989f38276d54cc3a3b17d1)\n\n[50\\. GitHub - AI-Infra-Team/Mooncake: Mooncake is the serving...](https://github.com/AI-Infra-Team/Mooncake)\n\n[51\\. \\[讨论\\] 聊聊大模型推理中的分离式推理](https://www.iivd.net/forum.php?mod=viewthread&tid=76611&extra=&mobile=2)\n\n[52\\. Juncheng Gu, Youngmoon Lee et al. “Efficient Memory Disaggregation with Infiniswap.” Symposium on Networked Systems Design and Implementation](https://www.semanticscholar.org/paper/b6b68de1eaf5f28dea6596990c83f363c44ea208)\n\n[53\\. Kevin T. Lim, Jichuan Chang et al. “Disaggregated memory for expansion and sharing in blade servers.” International Symposium on Computer Architecture](https://doi.org/10.1145/1555754.1555789)\n\n[54\\. Taming the Titans: A Survey of Efficient LLM Inference Serving](https://openreview.net/pdf?id=ixMdsBpEO8)\n\n[55\\. Peter Xiang Gao, Akshay Narayan et al. “Network Requirements for Resource Disaggregation.” USENIX Symposium on Operating Systems Design and Implementation](https://www.semanticscholar.org/paper/cf5a652bc450cf084020ead57d062da9fd9242d8)\n\n[56\\. Mooncake: Kimi's KVCache-centric Architecture for LLM Serving](https://dev.to/mikeyoung44/mooncake-kimis-kvcache-centric-architecture-for-llm-serving-ok4)\n\n[57\\. GitHub - riverzhang/Mooncake: Mooncake is the serving...](https://github.com/riverzhang/Mooncake)\n\n[60\\. MemServe: Flexible Mem Pool for Building Disaggregated LLM Serving with Caching](https://arxiv.org/pdf/2406.17565)\n\n[61\\. \\[讨论\\] 聊聊大模型推理中的分离式推理](https://www.iivd.net/forum.php?mod=viewthread&tid=76611&extra=&mobile=2)\n\n[62\\. Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation](https://arxiv.org/pdf/2503.20552)\n\n[63\\. MOONCAKE: Trading More Storage for Less Computation — A KVCache-centric Architecture for Serving LLM Chatbot](https://www.usenix.org/system/files/fast25-qin.pdf)\n\n[64\\. semi-PD: TOWARDS EFFICIENT LLM SERVING VIA PHASE-WISE DISAGGREGATED COMPUTATION AND UNIFIED STORAGE](https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2F1162e0c8-f610-456a-960b-ca2a6dfe35f3.pdf)\n\n[65\\. ​Research team from Department of Computer Science ...](https://www.cs.tsinghua.edu.cn/csen/info/1084/4580.htm)\n\n[66\\. Taming the Titans: A Survey of Efficient LLM Inference Serving](https://openreview.net/pdf?id=ixMdsBpEO8)\n\n[67\\. Mooncake: Kimi’s KVCache-centric Architecture for LLM Serving](https://arxiv.org/pdf/2407.00079v1)\n\n[68\\. LLM推理优化- Prefill-Decode分离式推理架构 - 异度部落格](https://www.yidoo.xyz/llm-inference-pd-separation-architecture)\n\n[69\\. GitHub - AI-Infra-Team/Mooncake: Mooncake is the serving...](https://github.com/AI-Infra-Team/Mooncake)\n\n[70\\. Weiqing Li, Guochao Jiang et al. “FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache Transfer and Load-Aware Scheduling.”](https://arxiv.org/abs/2504.03775)\n\n[71\\. \\[2407.00079\\] Mooncake: A KVCache-centric Disaggregated...](http://arxiv.org/abs/2407.00079?context=cs)\n\n[72\\. Mooncake: Kimi's KVCache-centric Architecture for LLM Serving](https://dev.to/mikeyoung44/mooncake-kimis-kvcache-centric-architecture-for-llm-serving-ok4)\n\n[80\\. ...and KV cache transfer roadmap · Issue #10818 · vl...](https://github.com/vllm-project/vllm/issues/10818)\n\n[81\\. ...Engine by ShangmingCai · Pull Request #10884 · vl...](https://github.com/vllm-project/vllm/pull/10884)\n\n[82\\. \\[Runtime\\]\\[Dist\\] Implementation of KV cache transfer (#...](https://github.com/apache/tvm/commit/567eeed38bdbcefb68e36328af6ab1501a81d51e)\n\n[83\\. Mooncake: A KVCache-centric Disaggregated Architecture for ...](http://0fd.org/2024/09/19/mooncake-a-kvcache-centric-disaggregated-architecture-for-llm-serving/)\n\n[84\\. Implement disaggregated prefilling via KV cache transfer ...](https://github.com/vllm-project/vllm/issues/5557)\n\n[85\\. GitHub - riverzhang/Mooncake: Mooncake is the serving...](https://github.com/riverzhang/Mooncake)\n\n[86\\. MOONCAKE: Trading More Storage for Less Computation — A KVCache-centric Architecture for Serving LLM Chatbot](https://www.usenix.org/system/files/fast25-qin.pdf)\n\n[87\\. \\[Proposal\\] SGLang Support Distributed Cache in PD ...](https://github.com/sgl-project/sglang/issues/7761)\n\n[88\\. SGLang PD分离与Mooncake _ 同行交流](https://www.iivd.net/forum.php?mod=viewthread&tid=87240&page=1&mobile=2)\n\n[89\\. semi-PD: TOWARDS EFFICIENT LLM SERVING VIA PHASE-WISE DISAGGREGATED COMPUTATION AND UNIFIED STORAGE](https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2F1162e0c8-f610-456a-960b-ca2a6dfe35f3.pdf)\n\n[90\\. SgLang代码细读-3.Cache - SunStriKE](https://www.cnblogs.com/sunstrikes/p/18891538)\n\n[91\\. Mooncake: Kimi’s KVCache-centric Architecture for LLM Serving](https://arxiv.org/pdf/2407.00079v1)\n\n[92\\. 月之暗面公布Kimi技术细节：以KVCache为中心的分离式推理架构 - AIGC资讯 - AIGC观察](https://aigcdaily.cn/news/a24s7ro5771pira/)\n\n[93\\. Trading More Storage for Less Computation – A KVCache-centric Architecture for Serving LLM Chatbot](https://www.usenix.org/system/files/fast25_slides-qin.pdf)\n\n[94\\. Serving Large Language Models on Huawei CloudMatrix384](https://www.arxiv.org/pdf/2506.12708v3)\n\n[95\\. MOONCAKE: Trading More Storage for Less Computation – A KVCache-centric Architecture for Serving LLM Chatbot](https://madsys.cs.tsinghua.edu.cn/publication/mooncake-trading-more-storage-for-less-computation-a-kvcache-centric-architecture-for-serving-llm-chatbot/FAST25-qin.pdf)\n\n[96\\. vllm.distributed.kv_transfer.kv_lookup_buffer.mooncake_store](https://docs.vllm.ai/en/stable/api/vllm/distributed/kv_transfer/kv_lookup_buffer/mooncake_store.html)\n\n[97\\. Jiangsu Du, Hongbin Zhang et al. “EcoServe: Enabling Cost-effective LLM Serving with Proactive Intra- and Inter-Instance Orchestration.”](https://arxiv.org/abs/2504.18154)\n\n[100\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[101\\. A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf?fbclid=IwAR3GYBQ2P9Cww2HVM3oUbML9i5i3DMDBVv5_FvYWfEi-vdZqZoSM78jE2-s)\n\n[102\\. Enhance OpenCV DNN for LLM Inference: Dynamic Memory, KV...](https://github.com/opencv/opencv/issues/27200)\n\n[103\\. Attention and More: Optimizations in Transformer Models](https://zhuanlan.zhihu.com/p/673144660)\n\n[104\\. M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type](https://arxiv.org/pdf/2502.18755)\n\n[105\\. KV cache optimization with paged attention #27303 - GitHub](https://github.com/huggingface/transformers/issues/27303)\n\n[106\\. Implementation — vLLM](https://docs.vllm.ai/en/v0.5.4/automatic_prefix_caching/details.html)\n\n[107\\. Techniques for KV Cache Optimization in Large Language Models](https://www.omrimallis.com/posts/techniques-for-kv-cache-optimization/)\n\n[108\\. A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency](https://www.arxiv.org/pdf/2505.01658)\n\n[109\\. 极智AI | 大模型优化技术PagedAttention](https://zhuanlan.zhihu.com/p/661582388)\n\n[110\\. Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference](https://www.arxiv.org/pdf/2506.07311)\n\n[111\\. PagedAttention: An Attention Algorithm Inspired By the ...](https://hackernoon.com/pagedattention-an-attention-algorithm-inspired-by-the-classical-virtual-memory-in-operating-systems)\n\n[112\\. 1 MQA（Multi Query Attention） - 码医森](https://coderethan.fun/AI/deep_learning_theory/11-2attention-extension.html)\n\n[113\\. Empowering Large Language Models to Edge Intelligence: A Survey of Edge Efficient LLMs and Techniques](https://chinaxiv.org/user/download.htm?uuid=88a1a4ee-2cf7-43b1-a68d-0c70d9fffdbf)\n\n[114\\. Efficient Memory Management for Large Language Model Serving with PagedAttention](https://www.cs.princeton.edu/~ravian/COS597_F24/papers/vllm.pdf)\n\n[115\\. LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention](https://openreview.net/pdf/7251fbe835b394df1e357cb651b327058b84aa61.pdf)\n\n[116\\. 探秘Transformer系列之（26）— KV Cache优化—分离or合并](https://www.itfaba.com/jishufenxian/212048.html)\n\n[117\\. 探秘Transformer系列之（24）--- KV Cache优化- 罗西的思考](https://www.cnblogs.com/rossiXYZ/p/18811723)\n\n[120\\. Implementation — vLLM](https://docs.vllm.ai/en/v0.5.4/automatic_prefix_caching/details.html)\n\n[121\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[122\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[123\\. Efficient Attention](http://karlstratos.com/notes/attention.pdf)\n\n[124\\. Enhance OpenCV DNN for LLM Inference: Dynamic Memory, KV...](https://github.com/opencv/opencv/issues/27200)\n\n[125\\. MemServe: Flexible Mem Pool for Building Disaggregated LLM Serving with Caching](https://arxiv.org/pdf/2406.17565)\n\n[126\\. Optimizing GPU Memory for LLMs](https://www.titanml.co/resources/optimizing-gpu-memory-for-llms-a-deep-dive-into-paged-attention)\n\n[127\\. A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf?fbclid=IwAR3GYBQ2P9Cww2HVM3oUbML9i5i3DMDBVv5_FvYWfEi-vdZqZoSM78jE2-s)\n\n[128\\. Empowering Large Language Models to Edge Intelligence: A Survey of Edge Efficient LLMs and Techniques](https://chinaxiv.org/user/download.htm?uuid=88a1a4ee-2cf7-43b1-a68d-0c70d9fffdbf)\n\n[129\\. 推理引擎benchmark](https://www-file.ruijie.com.cn/pdf/%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8Ebenchmark.pdf)\n\n[130\\. LLM-Inference-Acceleration/kv-cache/efficient-memory ... - GitHub](https://github.com/shishishu/LLM-Inference-Acceleration/blob/main/kv-cache/efficient-memory-management-for-large-language-model-serving-with-pagedattention/README.md)\n\n[131\\. Optimizing LLM Deployment: vLLM PagedAttention and the Future of ...](https://www.unite.ai/optimizing-llm-deployment-vllm-pagedattention-and-the-future-of-efficient-ai-serving/)\n\n[132\\. Efficient Memory Management for Large Language Model Serving with PagedAttention](https://www.cs.toronto.edu/~cmaddis/courses/csc2541_w25/presentations/ng_tang_pagedattention.pdf)\n\n[133\\. Note on Language Models](https://users.cs.utah.edu/~haocheng/notes/NoteonLanguageModels.pdf)\n\n[134\\. A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency](https://www.arxiv.org/pdf/2505.01658)\n\n[135\\. vAttention: Dynamic Memory Management for Serving LLMs without ...](https://paperswithcode.com/paper/vattention-dynamic-memory-management-for)\n\n[136\\. M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type](https://arxiv.org/pdf/2502.18755)\n\n[137\\. LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention](https://openreview.net/pdf/7251fbe835b394df1e357cb651b327058b84aa61.pdf)\n\n[138\\. KV Caching in LLM Inference A Comprehensive Review](https://www.rohan-paul.com/p/kv-caching-in-llm-inference-a-comprehensive)\n\n[140\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[141\\. Mooncake: Kimi’s KVCache-centric Architecture for LLM Serving](https://arxiv.org/pdf/2407.00079v1)\n\n[142\\. Implementation — vLLM](https://docs.vllm.ai/en/v0.5.4/automatic_prefix_caching/details.html)\n\n[143\\. GitHub - kyegomez/MultiQueryAttention: This is a simple torch ...](https://github.com/kyegomez/MultiQueryAttention)\n\n[144\\. Mooncake：以KVCache为中心的大语言模型服务分解架构](https://yiyibooks.cn/__trs__/arxiv/2407.00079v3/index.html)\n\n[145\\. HEADINFER : Memory-Efficient LLM Inference by Head-wise Offloading](https://openreview.net/pdf/b954ca56ab5f5658f14721e3e668533095de41ed.pdf)\n\n[146\\. Mooncake: Kimi's KVCache-centric Architecture for LLM Serving](https://dev.to/mikeyoung44/mooncake-kimis-kvcache-centric-architecture-for-llm-serving-ok4)\n\n[147\\. MOONCAKE: Trading More Storage for Less Computation — A KVCache-centric Architecture for Serving LLM Chatbot](https://www.usenix.org/system/files/fast25-qin.pdf)\n\n[148\\. The Secrets of GPT-4 Leaked?](https://www.ikangai.com/the-secrets-of-gpt-4-leaked/)\n\n[149\\. Inference-Friendly Models with MixAttention | Databric...](https://databricks.com/blog/mixattention)\n\n[150\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[151\\. MOONCAKE: Trading More Storage for Less Computation – A KVCache-centric Architecture for Serving LLM Chatbot](https://madsys.cs.tsinghua.edu.cn/publication/mooncake-trading-more-storage-for-less-computation-a-kvcache-centric-architecture-for-serving-llm-chatbot/FAST25-qin.pdf)\n\n[152\\. A^2ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary Position Embedding and Query-Aware Vector Quantization](http://www.arxiv.org/pdf/2502.12665)\n\n[153\\. Taming the Titans: A Survey of Efficient LLM Inference Serving](https://openreview.net/pdf?id=ixMdsBpEO8)\n\n[154\\. Multi-Query Attention Explained | Towards AI](https://towardsai.net/p/l/multi-query-attention-explained)\n\n[155\\. Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs](https://openreview.net/pdf?id=e9D2STGwLJ)\n\n[156\\. Hao Yu, Zelan Yang et al. “Effectively Compress KV Heads for LLM.” ArXiv](https://doi.org/10.48550/arXiv.2406.07056)\n\n[160\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[161\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[162\\. MemServe: Flexible Mem Pool for Building Disaggregated LLM Serving with Caching](https://arxiv.org/pdf/2406.17565)\n\n[163\\. Efficient Memory Management for Large Language Model Serving with PagedAttention](https://zhuanlan.zhihu.com/p/671003081)\n\n[164\\. Note on Language Models](https://users.cs.utah.edu/~haocheng/notes/NoteonLanguageModels.pdf)\n\n[165\\. vLLMとPagedAttention：LLM推論の革新的技術](https://zenn.dev/sunwood_ai_labs/articles/vllm-pagedattention-llm-inference)\n\n[166\\. LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention](https://openreview.net/pdf/7251fbe835b394df1e357cb651b327058b84aa61.pdf)\n\n[167\\. 推理引擎benchmark](https://www-file.ruijie.com.cn/pdf/%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8Ebenchmark.pdf)\n\n[168\\. Efficient Attention](http://karlstratos.com/notes/attention.pdf)\n\n[169\\. Efficient Large Language Models: A Survey](https://openreview.net/pdf?id=bsCCJHbO8A)\n\n[170\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[171\\. 极智AI | 大模型优化技术PagedAttention](https://zhuanlan.zhihu.com/p/661582388)\n\n[172\\. A Survey on Large Language Model Acceleration based on KV Cache Management](https://openreview.net/pdf/31d6ab447981f515ddb7d2f06b133bd50bc153ef.pdf)\n\n[173\\. SIMPLE LINEAR ATTENTION LANGUAGE MODELS BALANCE THE RECALL-THROUGHPUT TRADEOFF](https://openreview.net/pdf/0779c6e31781100fde299327f8dfeafff66de03e.pdf)\n\n[174\\. PagedAttention 介绍](https://zhuanlan.zhihu.com/p/681018057)\n\n[175\\. vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention](https://blog.vllm.ai/2023/06/20/vllm.html)\n\n[176\\. vllm 优化之 PagedAttention 源码解读 - Zhang](https://www.armcvai.cn/2024-11-17/vllm-pagedattention.html)\n\n[177\\. Implementation — vLLM](https://docs.vllm.ai/en/v0.5.4/automatic_prefix_caching/details.html)\n\n[180\\. MemServe: Flexible Mem Pool for Building Disaggregated LLM Serving with Caching](https://arxiv.org/pdf/2406.17565)\n\n[181\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[182\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[183\\. Implementation — vLLM](https://docs.vllm.ai/en/v0.5.4/automatic_prefix_caching/details.html)\n\n[184\\. KV cache optimization with paged attention #27303 - GitHub](https://github.com/huggingface/transformers/issues/27303)\n\n[185\\. Efficient Memory Management for Large Language Model Serving with PagedAttention](https://www.cs.princeton.edu/~ravian/COS597_F24/papers/vllm.pdf)\n\n[186\\. 面向AI的计算优化](https://raw.githubusercontent.com/solecnugit/solecnugit.github.io/main/assets/theory/Ch18.DLFramework.pdf)\n\n[187\\. Open-AI model Efficient Memory Reduce Management for the Large Language Models (LLMs) Serving with Paged Attention of sharing the KV Cashes](https://www.ijraset.com/best-journal/openai-model-efficient-memory-reduce-management-for-the-large-language-models-llms-serving-with-paged-attention-of-sharing-the-kv-cashes-205)\n\n[188\\. vllm 优化之 PagedAttention 源码解读 - Zhang](https://www.armcvai.cn/2024-11-17/vllm-pagedattention.html)\n\n[189\\. Unifying KV Cache Compression for Large Language Models with LeanKV](https://arxiv.org/pdf/2412.03131)\n\n[190\\. Optimizing LLM Deployments through Inference Backends](https://www.onlinescientificresearch.com/articles/optimizing-llm-deployments-through-inference-backends.pdf)\n\n[191\\. What is PagedAttention?](https://docs.modular.com/glossary/ai/paged-attention)\n\n[192\\. Note on Language Models](https://users.cs.utah.edu/~haocheng/notes/NoteonLanguageModels.pdf)\n\n[193\\. vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention](https://www.microsoft.com/en-us/research/uploads/prod/2024/05/vattention_arxiv24.pdf)\n\n[194\\. 15-442/15-642: Machine Learning Systems Attention Optimizations](https://mlsyscourse.org/slides/13-attention-optimizations.pdf)\n\n[195\\. M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type](https://arxiv.org/pdf/2502.18755)\n\n[196\\. PagedAttention 介绍](https://zhuanlan.zhihu.com/p/681018057)\n\n[197\\. Cunchen Hu, Heyang Huang et al. “MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool.” ArXiv](https://doi.org/10.48550/arXiv.2406.17565)\n\n[198\\. KV Caching in LLM Inference A Comprehensive Review](https://www.rohan-paul.com/p/kv-caching-in-llm-inference-a-comprehensive)\n\n[200\\. Mooncake: Kimi’s KVCache-centric Architecture for LLM Serving](https://arxiv.org/pdf/2407.00079v1)\n\n[201\\. Mooncake: A KVCache-centric Disaggregated Architecture for ...](http://0fd.org/2024/09/19/mooncake-a-kvcache-centric-disaggregated-architecture-for-llm-serving/)\n\n[202\\. \\[Feature\\] Slim Attention (lossless 2x reduction in KV cache ...](https://github.com/sgl-project/sglang/issues/4496)\n\n[203\\. MOONCAKE: Trading More Storage for Less Computation — A KVCache-centric Architecture for Serving LLM Chatbot](https://www.usenix.org/system/files/fast25-qin.pdf)\n\n[204\\. Taming the Titans: A Survey of Efficient LLM Inference Serving](https://openreview.net/pdf?id=ixMdsBpEO8)\n\n[205\\. Mooncake：以KVCache为中心的大语言模型服务分解架构](https://yiyibooks.cn/__trs__/arxiv/2407.00079v3/index.html)\n\n[206\\. MOONCAKE: Trading More Storage for Less Computation – A KVCache-centric Architecture for Serving LLM Chatbot](https://madsys.cs.tsinghua.edu.cn/publication/mooncake-trading-more-storage-for-less-computation-a-kvcache-centric-architecture-for-serving-llm-chatbot/FAST25-qin.pdf)\n\n[207\\. Official implementation of \"LOOK-M: Look-Once Optimization in KV Cache ...](https://github.com/SUSTechBruce/LOOK-M)\n\n[208\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[209\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[210\\. Multi-Query Attention Explained | Towards AI](https://towardsai.net/p/l/multi-query-attention-explained)\n\n[211\\. Multi-Query Attention，Group-Query Attention，FlashAttention](https://zhuanlan.zhihu.com/p/683370180)\n\n[220\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[221\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[222\\. LLM Serving](https://cseweb.ucsd.edu/~yiying/cse291-winter24/reading/llm-serving.pdf)\n\n[223\\. Efficient Memory Management for Large Language Model Serving with PagedAttention](https://www.cs.princeton.edu/~ravian/COS597_F24/papers/vllm.pdf)\n\n[224\\. David H. Bailey, David H. Bailey et al. “The Nas Parallel Benchmarks.” International Journal of High Performance Computing Applications](https://doi.org/10.1177/109434209100500306)\n\n[225\\. Empowering Large Language Models to Edge Intelligence: A Survey of Edge Efficient LLMs and Techniques](https://chinaxiv.org/user/download.htm?uuid=88a1a4ee-2cf7-43b1-a68d-0c70d9fffdbf)\n\n[226\\. MemServe: Flexible Mem Pool for Building Disaggregated LLM Serving with Caching](https://arxiv.org/pdf/2406.17565)\n\n[227\\. Empowering Large Language Models with Efficient and Automated Systems](https://escholarship.org/content/qt2kp379f3/qt2kp379f3.pdf?t=sko9il)\n\n[228\\. Tri Dao, Daniel Y. Fu et al. “FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.” ArXiv](https://arxiv.org/abs/2205.14135)\n\n[229\\. PagedAttention](https://pages.cs.wisc.edu/~shivaram/cs744-sp24-slides/cs744-vllm.pdf)\n\n[230\\. A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf?fbclid=IwAR3GYBQ2P9Cww2HVM3oUbML9i5i3DMDBVv5_FvYWfEi-vdZqZoSM78jE2-s)\n\n[231\\. vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention](https://www.microsoft.com/en-us/research/uploads/prod/2024/05/vattention_arxiv24.pdf)\n\n[232\\. LLM（大语言模型）部署加速方法——PagedAttention篇](https://www.tpffy.fun/download/AI_Models_Books/%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM%E9%9D%A2%E8%AF%95%E5%90%88%E9%9B%86/LLM/LLM/63-LLM%EF%BC%88%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%89%E9%83%A8%E7%BD%B2%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94PagedAttention%E7%AF%87.pdf)\n\n[233\\. Investigate PagedAttention KV-cache memory management for ... - GitHub](https://github.com/ggerganov/llama.cpp/issues/1955)\n\n[234\\. What it means to serve an LLM and which serving technology to choose from](https://www.run.ai/blog/serving-large-language-models)\n\n[235\\. vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention](https://blog.vllm.ai/2023/06/20/vllm.html?ref=blog.mozilla.ai)\n\n[236\\. Efficient Attention](http://karlstratos.com/notes/attention.pdf)\n\n[237\\. Boosting LLM Decode Throughput: vAttention vs. ...](https://hackernoon.com/boosting-llm-decode-throughput-vattention-vs-pagedattention)\n\n[240\\. GitHub - zcli-charlie/Awesome-KV-Cache](https://github.com/zcli-charlie/Awesome-KV-Cache)\n\n[241\\. KV cache optimization with paged attention #27303 - GitHub](https://github.com/huggingface/transformers/issues/27303)\n\n[242\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[243\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[244\\. Implementation — vLLM](https://docs.vllm.ai/en/v0.5.4/automatic_prefix_caching/details.html)\n\n[245\\. A Survey of Large Language Models](http://arxiv.org/pdf/2303.18223)\n\n[246\\. Efficient Memory Management for Large Language Model Serving with PagedAttention](https://www.cs.princeton.edu/~ravian/COS597_F24/papers/vllm.pdf)\n\n[247\\. 15-442/15-642: Machine Learning Systems Attention Optimizations](https://mlsyscourse.org/slides/13-attention-optimizations.pdf)\n\n[248\\. Thomas Joshi, Herman Saini et al. “Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference.”](https://arxiv.org/abs/2506.07311)\n\n[249\\. M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type](https://arxiv.org/pdf/2502.18755)\n\n[250\\. LLM（大语言模型）部署加速方法——PagedAttention篇](https://www.tpffy.fun/download/AI_Models_Books/%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM%E9%9D%A2%E8%AF%95%E5%90%88%E9%9B%86/LLM/LLM/63-LLM%EF%BC%88%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%89%E9%83%A8%E7%BD%B2%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94PagedAttention%E7%AF%87.pdf)\n\n[251\\. Optimizing LLM Deployments through Inference Backends](https://www.onlinescientificresearch.com/articles/optimizing-llm-deployments-through-inference-backends.pdf)\n\n[252\\. vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention](https://www.microsoft.com/en-us/research/uploads/prod/2024/05/vattention_arxiv24.pdf)\n\n[253\\. Note on Language Models](https://users.cs.utah.edu/~haocheng/notes/NoteonLanguageModels.pdf)\n\n[254\\. vllm 优化之 PagedAttention 源码解读 - Zhang](https://www.armcvai.cn/2024-11-17/vllm-pagedattention.html)\n\n[255\\. Empowering Large Language Models to Edge Intelligence: A Survey of Edge Efficient LLMs and Techniques](https://chinaxiv.org/user/download.htm?uuid=88a1a4ee-2cf7-43b1-a68d-0c70d9fffdbf)\n\n[256\\. A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency](https://www.arxiv.org/pdf/2505.01658)\n\n[260\\. BLOCK-ATTENTION FOR EFFICIENT PREFILLING](https://arxiv.org/pdf/2409.15355)\n\n[261\\. MOONCAKE: Trading More Storage for Less Computation — A KVCache-centric Architecture for Serving LLM Chatbot](https://www.usenix.org/system/files/fast25-qin.pdf)\n\n[262\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[263\\. Mooncake: Kimi’s KVCache-centric Architecture for LLM Serving](https://arxiv.org/pdf/2407.00079v1)\n\n[264\\. MOONCAKE: Trading More Storage for Less Computation – A KVCache-centric Architecture for Serving LLM Chatbot](https://madsys.cs.tsinghua.edu.cn/publication/mooncake-trading-more-storage-for-less-computation-a-kvcache-centric-architecture-for-serving-llm-chatbot/FAST25-qin.pdf)\n\n[265\\. Mooncake：以KVCache为中心的大语言模型服务分解架构](https://yiyibooks.cn/__trs__/arxiv/2407.00079v3/index.html)\n\n[266\\. Block-Attention for Efficient RAG](https://arxiv.org/html/2409.15355v4)\n\n[267\\. Awesome-LLM-KV-Cache: A curated list of Awesome LLM KV Cache Papers with Codes](https://github.com/Zefan-Cai/Awesome-LLM-KV-Cache)\n\n[268\\. GitHub - AI-Infra-Team/Mooncake: Mooncake is the serving...](https://github.com/AI-Infra-Team/Mooncake)\n\n[269\\. 降低成本：大语言模型KV缓存消耗优化方法综述](https://www.yiyibooks.cn/__trs__/arxiv/2407.18003v3/index.html)\n\n[270\\. LayerKV：通过分层KV缓存管理优化大型语言模型服务](https://yiyibooks.cn/__trs__/arxiv/2410.00428v2/index.html)\n\n[271\\. Taming the Titans: A Survey of Efficient LLM Inference Serving](https://openreview.net/pdf?id=ixMdsBpEO8)\n\n[280\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[281\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[282\\. Efficient Memory Management for Large Language Model Serving with PagedAttention](https://www.cs.princeton.edu/~ravian/COS597_F24/papers/vllm.pdf)\n\n[283\\. PagedAttention and vLLM serve Large Language Models faster and cheaper](https://www.mlwires.com/pagedattention-and-vllm-serve-large-language-models-faster-and-cheaper/)\n\n[284\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[285\\. A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf?fbclid=IwAR3GYBQ2P9Cww2HVM3oUbML9i5i3DMDBVv5_FvYWfEi-vdZqZoSM78jE2-s)\n\n[286\\. Efficient Large Language Models: A Survey](https://openreview.net/pdf?id=bsCCJHbO8A)\n\n[287\\. A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency](https://arxiv.org/pdf/2505.01658)\n\n[288\\. Empowering Large Language Models to Edge Intelligence: A Survey of Edge Efficient LLMs and Techniques](https://chinaxiv.org/user/download.htm?uuid=88a1a4ee-2cf7-43b1-a68d-0c70d9fffdbf)\n\n[289\\. What is PagedAttention?](https://docs.modular.com/glossary/ai/paged-attention)\n\n[290\\. vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention](https://arxiv.org/pdf/2405.04437v1)\n\n[291\\. LLM（大语言模型）部署加速方法——PagedAttention篇](https://www.tpffy.fun/download/AI_Models_Books/%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM%E9%9D%A2%E8%AF%95%E5%90%88%E9%9B%86/LLM/LLM/63-LLM%EF%BC%88%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%89%E9%83%A8%E7%BD%B2%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94PagedAttention%E7%AF%87.pdf)\n\n[292\\. Fundamental of Deploying Large Language Model Inference](https://techcommunity.microsoft.com/blog/machinelearningblog/fundamental-of-deploying-large-language-model-inference/4096881)\n\n[293\\. Efficient Memory Management for Large Language Model Serving with PagedAttention | Proceedings of the 29th Symposium on Operating Systems Principles](https://dl.acm.org/doi/abs/10.1145/3600006.3613165)\n\n[294\\. Efficient Memory Management for Large Language Model Serving with PagedAttention,arXiv - CS - Machine Learning - X-MOL](https://www.x-mol.com/paper/1702318017159647232?adv)\n\n[295\\. Efficient Memory Management for Large Language Model Serving with ...](https://www.micahlerner.com/2024/01/11/efficient-memory-management-for-large-language-model-serving-with-pagedattention.html)\n\n[296\\. Meet vLLM: For faster, more efficient LLM inference and ...](https://www.redhat.com/en/blog/meet-vllm-faster-more-efficient-llm-inference-and-serving)"
  },
  {
    "id": 49,
    "question": "From a technical perspective, how to understand the similarities and differences between Reinforcement Learning (RL) algorithms and Supervised Fine-Tuning (SFT) in Large Language Models (LLMs), as well as their respective advantages and disadvantages in model training?",
    "response": "**Comprehensive Technical Analysis of Reinforcement Learning vs. Supervised Fine-Tuning in Large Language Models**\n\n**1\\. Introduction to Training Paradigms**\n\nLarge Language Models (LLMs) require sophisticated fine-tuning to align with human expectations and task-specific requirements. Two dominant approaches have emerged: **Supervised Fine-Tuning (SFT)** and **Reinforcement Learning (RL)**, particularly RL from Human Feedback (RLHF). Both operate on pre-trained Transformer-based LLMs \\[8\\] but fundamentally differ in methodology and objectives. While SFT uses direct human demonstrations, RL optimizes behavior through dynamic reward signals \\[1\\]\\[3\\]. This analysis examines their technical foundations, comparative strengths/weaknesses, and emerging alternatives as of 2025.\n\n**2\\. Architectural and Methodological Foundations**\n\n**2.1 Shared Infrastructure**\n\n**Transformer Backbone**: Both methods leverage identical Transformer architectures, ensuring continuity in token processing and attention mechanisms \\[8\\].\n\n**Pre-training Dependency**: Each technique requires a high-quality pre-trained model as a starting point \\[1\\].\n\n**2.2 Divergent Training Mechanisms**\n\n|     |     |     |\n| --- | --- | --- |\n| **Component** | **Supervised Fine-Tuning (SFT)** | **Reinforcement Learning (RLHF)** |\n| **Core Objective** | Minimize cross-entropy loss between predictions and human-provided outputs \\[3\\]\\[11\\]. | Maximize cumulative reward via policy optimization \\[3\\]\\[9\\]. |\n| **Data Handling** | Static, i.i.d. prompt-response pairs from human demonstrations \\[3\\]\\[6\\]. | Dynamic, non-i.i.d. data generated via agent-environment interactions \\[6\\]. |\n| **Learning Process** | Teacher-forcing: Predicts next token using ground-truth inputs \\[1\\]\\[3\\]. | Markov Decision Process (MDP): Generates actions (tokens) evaluated by a reward model \\[5\\]\\[9\\]. |\n| **Feedback Mechanism** | Direct imitation learning (\"behavior cloning\") without reward signals \\[1\\]. | Two-stage pipeline: 1) Train reward model on human preferences; 2) Optimize policy via algorithms like PPO \\[7\\]\\[9\\]. |\n\n**Algorithmic Nuances**:\n\nSFT performs \"local\" optimization tightly coupled to demonstration data \\[1\\].\n\nRLHF requires reward shaping and credit assignment across multi-step generations \\[5\\].\n\nRL treats language generation as a sequential decision problem, while SFT frames it as independent token prediction \\[6\\].\n\n**3\\. Quantitative Advantages and Disadvantages**\n\n**3.1 Computational Efficiency**\n\n**SFT**: Lower resource demands, fine-tuning completes in hours/days with standard GPUs. Domain specialization requires <200 samples for significant gains \\[25\\]\\[27\\].\n\n**RLHF**: 30-50% higher costs than SFT due to PPO complexity and iterative reward evaluation. Requires ~1.3x more time/resources for equivalent runs \\[29\\]\\[35\\].\n\n**Parameter Updates**: RL updates sparse subnetworks (<5% of parameters), while SFT typically adjusts all weights \\[46\\]. Techniques like LoRA mitigate costs for both \\[29\\].\n\n**3.2 Data Efficiency**\n\n**SFT**: Relies on scalable, human-labeled datasets but struggles with preference nuances. Quality degrades if demonstrations exceed model capabilities \\[1\\]\\[28\\].\n\n**RLHF**: Preference datasets are expensive and labor-intensive to curate. Sample inefficiency necessitates large-scale feedback \\[26\\]\\[28\\].\n\n**3.3 Output Quality Trade-offs**\n\n|     |     |     |\n| --- | --- | --- |\n| **Metric** | **SFT** | **RLHF** |\n| **Factual Consistency** | Moderate risk of hallucination with imperfect data \\[28\\] | Higher robustness to misalignment via reward constraints \\[33\\] |\n| **Diversity** | Declines with extended training (output homogenization) \\[32\\] | Better solution diversity through exploration \\[32\\] |\n| **Alignment** | Adapts tone/style but limited preference modeling \\[34\\] | Superior harmlessness (+22%) and helpfulness (+15%) in human evaluations \\[28\\]\\[33\\] |\n| **Reasoning** | Effective for task generalization but struggles with open-ended reasoning \\[1\\] | Excels in complex, multi-step tasks via reward-guided exploration \\[49\\] |\n\n**Critical Weaknesses**:\n\nSFT can propagate inconsistencies from flawed demonstration data \\[1\\].\n\nRLHF may reduce in-context learning ability and exacerbate reward hacking \\[36\\].\n\n**4\\. Complementary Integration and Emerging Alternatives (2025)**\n\n**4.1 Sequential Synergy**\n\nMost production pipelines use SFT → RLHF:\n\n1.  **SFT Phase**: Creates initial task-proficient model from demonstrations.\n2.  **RLHF Phase**: Refines alignment using preference-based rewards \\[4\\].\n\nThis hybrid approach underpins ChatGPT/InstructGPT \\[19\\].\n\n**4.2 Emerging RL Alternatives**\n\n**Self-Supervised RL (SSRL)**: Uses _intrinsic_ reward signals (e.g., cross-attention coherence) to bypass human feedback. Achieves 80% of RLHF alignment at 10% cost \\[42\\]\\[42\\].\n\n**Direct Preference Optimization (DPO)**: Eliminates reward models by integrating preferences directly into supervised loss. Reduces complexity while matching RLHF performance \\[53\\].\n\n**ReLIFT Framework**: Hybrid SFT+RL for efficient exploration. Achieves SOTA results with 50% less data \\[49\\].\n\n**4.3 SFT-Centric Innovations**\n\n**Classification Losses**: Reformulate alignment as classification to avoid RL complexity but exhibit weaker generalization \\[45\\].\n\n**Feature Normalization**: Discretize continuous features before token mapping to reduce hallucination \\[51\\].\n\n**Key Trade-offs vs. RL**:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n| **Method** | **Data Efficiency** | **Alignment Strength** | **Compute Cost** |\n| **SSRL** | High (self-supervised) | Moderate | Very Low |\n| **DPO** | Moderate | High | Low |\n| **Classification SFT** | High | Low-Medium | Low |\n\n**5\\. Conclusion: Strategic Implications**\n\nSFT and RL represent fundamentally divergent optimization philosophies: SFT excels at **efficient knowledge transfer** from demonstrations, while RL enables **adaptive behavior shaping** via dynamic feedback. As of 2025:\n\n**SFT dominates** cost-sensitive specialization (e.g., domain-specific chatbots) due to rapid convergence.\n\n**RLHF remains essential** for alignment-critical applications (e.g., healthcare, education) despite resource demands.\n\n**Hybrid/alternative methods** (DPO, SSRL) are reducing reliance on pure RLHF without sacrificing alignment.\n\nFuture advancements will likely focus on self-supervised rewards and sparsely activated architectures to unify the efficiency of SFT with the alignment precision of RL. The trend toward \"RL-free alignment\" does not yet surpass RLHF’s robustness in high-stakes scenarios but offers compelling options for scalable deployment \\[42\\]\\[53\\].\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Wayne Xin Zhao, Kun Zhou et al. “A Survey of Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2303.18223)\n\n[2\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[3\\. Supervised Fine Tuning Vs Reinforcement Learning](https://www.restack.io/p/fine-tuning-answer-supervised-vs-reinforcement-cat-ai)\n\n[4\\. Supervised Fine-tuning for Large language Model](https://www.futurebeeai.com/blog/supervised-fine-tuning-for-large-language-model)\n\n[5\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[6\\. یادگیری تقویتی عمیق با پایتون برای چت بات ها و مدل های زبان بزرگ - ویرایش دوم - نیمیش سانگ](https://irantypist.com/media/new_research/samplefile/1741017455_9398.docx)\n\n[7\\. The transformer and large language model](https://www.cse.cuhk.edu.hk/~khwong/www2/GISM5033/10_transformer_llm.pptx)\n\n[8\\. 介绍大语言模型预训练和有监督学习的相同点和不同点](https://www.aiswers.com/topic/jie-shao-da-yu-yan-mo-xing-yu-xun-lian-he-you-jian-du-xue-xi-de-xiang-tong-dian-he-bu-tong-dian)\n\n[9\\. Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning](https://openreview.net/pdf/2c930954311f3c3a380cdf622020946f32e86440.pdf)\n\n[10\\. El auge de los large language models: de los fundamentos a la aplicación](https://www.managementsolutions.com/sites/default/files/minisite/static/72b0015f-39c9-4a52-ba63-872c115bfbd0/llm/pdf/auge-de-los-llm.pdf)\n\n[11\\. Foundations of Large Language Models](https://readwise-assets.s3.amazonaws.com/media/wisereads/articles/foundations-of-large-language-/2501.09223v1.pdf)\n\n[12\\. Preserving Diversity in Supervised Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2408.16673)\n\n[13\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[14\\. Unsupervised Pre-training vs. Supervised Fine-tuning for LLMs](https://llmmodels.org/blog/unsupervised-pre-training-vs-supervised-fine-tuning-for-llms/)\n\n[15\\. How to Fine-tune a Large Language Model - blog.monsterapi.ai](https://blog.monsterapi.ai/blogs/finetuning-a-large-language-model/)\n\n[16\\. Supervised Fine-Tuning (SFT) for LLMs](https://www.geeksforgeeks.org/supervised-fine-tuning-sft-for-llms/)\n\n[17\\. Large Language Models and International Security: A Primer](https://unidir.org/wp-content/uploads/2024/11/large_language_models_web-1.pdf)\n\n[18\\. GENERATIVE AI AND LLM OPTIMIZING TECHNIQUES FOR DEVELOPING COST EFFECTIVE ENTERPRISE APPLICATIONS](https://iaeme.com/MasterAdmin/Journal_uploads/IJAIAP/VOLUME_2_ISSUE_1/IJAIAP_02_01_004.pdf)\n\n[19\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[21\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[22\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[23\\. Fine-tune large language models with reinforcement ...](https://aws.amazon.com/blogs/machine-learning/fine-tune-large-language-models-with-reinforcement-learning-from-human-or-ai-feedback/)\n\n[24\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[25\\. CS 696 Applied Large Language Models Spring Semester, 2025 Doc 23 Exam Apr 22, 2025](https://eli.sdsu.edu/courses/spring25/cs696/notes/D23%20Exam.pdf)\n\n[26\\. A Self-Supervised Reinforcement Learning Approach for Fine-Tuning Large Language Models Using Cross-Attention Signals](https://arxiv.org/pdf/2502.10482)\n\n[27\\. Supervised Fine Tuning Vs Reinforcement Learning](https://www.restack.io/p/fine-tuning-answer-supervised-vs-reinforcement-cat-ai)\n\n[28\\. Wayne Xin Zhao, Kun Zhou et al. “A Survey of Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2303.18223)\n\n[29\\. Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning](https://openreview.net/pdf/2c930954311f3c3a380cdf622020946f32e86440.pdf)\n\n[30\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[31\\. Reinforcement Learning Finetunes Small Subnetworks in Large Language Models](https://arxiv.org/pdf/2505.11711)\n\n[32\\. Teaching Large Language Models to Reason with Reinforcement Learning](https://openreview.net/pdf?id=mjqoceuMnI)\n\n[33\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[34\\. Fine-Tuning Large Language Models (LLMs)](https://www.orensultan.com/files/slides/Fine%20Tuning%20LLMs%20%28Session%201%29.pdf)\n\n[35\\. LLM Reinforcement Learning: Improving Model Accuracy](https://labelyourdata.com/articles/llm-reinforcement-learning)\n\n[36\\. Preserving Diversity in Supervised Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2408.16673)\n\n[37\\. Fine-tuning LLMs 101](https://datasciencedojo.com/blog/fine-tuning-llms/)\n\n[41\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[42\\. A Self-Supervised Reinforcement Learning Approach for Fine-Tuning Large Language Models Using Cross-Attention Signals](https://arxiv.org/pdf/2502.10482)\n\n[43\\. ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback](https://arxiv.org/pdf/2407.00087)\n\n[44\\. Unlock the Correlation between Supervised Fine-Tuning ...](http://arxiv.org/html/2406.10305v2)\n\n[45\\. AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks](https://openreview.net/notes/edits/attachment?id=ZK4apFYeJG&name=pdf)\n\n[46\\. Reinforcement Learning Finetunes Small Subnetworks in Large Language Models](https://arxiv.org/pdf/2505.11711)\n\n[47\\. Systems Opportunities for LLM Fine-Tuning using Reinforcement Learning](https://euromlsys.eu/pdf/euromlsys25-17.pdf)\n\n[48\\. Preserving Diversity in Supervised Fine-tuning of Large Language Models](https://www.liziniu.org/docs/slides/GEM-2025-03-23.pdf)\n\n[49\\. Learning What Reinforcement Learning Can’t: Interleaved Online Fine-Tuning for Hardest Questions](https://arxiv.org/pdf/2506.07527)\n\n[50\\. John Schulman, Filip Wolski et al. “Proximal Policy Optimization Algorithms.” ArXiv](https://arxiv.org/abs/1707.06347)\n\n[51\\. A SURVEY ON SPEECH LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2410.18908v3)\n\n[52\\. Preserving Diversity in Supervised Fine-Tuning of Large Language Models](https://openreview.net/pdf?id=NQEe7B7bSw)\n\n[53\\. Reinforcement Learning for LLMs: RLHF, DPO, and ...](https://www.inferless.com/learn/a-deep-dive-into-reinforcement-learning)\n\n[54\\. Large Language Models In 2025: Your Guide To Next-Gen AI](https://acecloud.ai/blog/large-language-models/#:~:text=By%202025,%20advancements%20in%20hardware,becoming%20more%20efficient%20and%20accessible.)"
  },
  {
    "id": 50,
    "question": "How does DeepSpeed solve the memory challenges in large language model training, and what are the key techniques it employs for distributed training of trillion-parameter models?",
    "response": "**DeepSpeed: Solving Memory Challenges in Trillion-Parameter Model Training**\n\nDeepSpeed is a distributed training framework developed by Microsoft to address the extreme memory and computational demands of training large language models (LLMs). By employing novel memory optimization techniques and hybrid parallelism strategies, it enables efficient training of models with trillions of parameters. Below is a comprehensive analysis of its key innovations.\n\n**1\\. ZeRO: Eliminating Redundancy for In-GPU Memory Reduction**\n\n**Core Principle**: ZeRO (Zero Redundancy Optimizer) partitions model states across data-parallel processes to eliminate memory redundancy, progressively optimizing memory usage through three stages:\n\n**ZeRO Stage 1 (Optimizer State Sharding)**\n\nPartitions optimizer states (e.g., Adam moment tensors) across GPUs. Each GPU stores only a fraction of these states. Achieves a **4x memory reduction** versus standard data parallelism (e.g., 16GB → 4GB for model states) with no communication overhead _\\[5\\]_\\[14\\]\\[16\\].\n\n**ZeRO Stage 2 (Gradient + Optimizer Sharding)**\n\nExtends partitioning to gradients. Gradients are reduced only within their assigned shard, slicing gradient memory to **2N/Nsubd/sub** (where N = parameter count, Nsubd/sub = data-parallel processes). Delivers an **8x memory reduction** for billion-parameter models but introduces all-gather/reduce-scatter communication overhead _\\[5\\]_\\[14\\]\\[16\\].\n\n**ZeRO Stage 3 (Full Parameter Partitioning)**\n\nPartitions parameters, gradients, _and_ optimizer states. Memory reduction scales linearly with data parallelism (Nsubd/sub). For example, training a 7.5B-parameter model requires **1.9GB/GPU** (vs. 120GB in standard DP) using 64 GPUs. This comes with a **50% higher communication volume** versus ZeRO-2 _\\[3\\]_\\[4\\]\\[5\\].\n\n**Impact**: ZeRO democratizes billion-parameter model training by allowing researchers to partition models across affordable GPU clusters instead of relying on ultra-high-memory nodes.\n\n**2\\. 3D Parallelism: Scaling Beyond GPU Memory Limits**\n\nFor trillion-parameter models, DeepSpeed combines **ZeRO with pipeline and tensor parallelism**, creating a unified \"3D parallelism\" framework:\n\n**Pipeline Parallelism**\n\nSplits model layers vertically into sequential stages. Each GPU executes one stage:\n\nUses **micro-batches** to minimize pipeline bubbles (device idle time).\n\nBalances compute load across stages via parameter-balanced or uniform layer partitioning _\\[21\\]_\\[26\\]\\[27\\].\n\nReduces communication volume compared to pure model parallelism, critical for low-bandwidth clusters \\[21\\]\\[24\\].\n\n**Tensor Parallelism**\n\nSplits individual layers (e.g., attention heads) horizontally across GPUs. Integrated with pipeline stages via DeepSpeed’s Pipe class _\\[22\\]_\\[27\\]\\[33\\].\n\n**Integration with ZeRO**\n\nZeRO handles redundancy reduction within pipeline stages. Example: ZeRO-3’s parameter sharding is applied to the layers assigned to each GPU, while pipeline parallelism distributes layers across stages _\\[21\\]_\\[22\\]\\[26\\].\n\n**Scalability**: 3D parallelism allowed DeepSpeed to train **1T-parameter models** in 2021 – a milestone previously infeasible due to GPU memory constraints \\[21\\]\\[26\\].\n\n**3\\. Heterogeneous Memory Systems: CPU/NVMe Offloading**\n\nTo further expand usable memory, DeepSpeed leverages CPU RAM and NVMe storage via **ZeRO-Infinity**:\n\n**CPU Offloading**\n\nOffloads optimizer states, gradients, and (in ZeRO-3) parameters to CPU RAM. Unused GPU-bound memory is reclaimed while leveraging abundant CPU resources _\\[47\\]_\\[51\\]\\[49\\].\n\n**NVMe Offloading**\n\nUtilizes terabytes of NVMe storage for models exceeding CPU+GPU memory. Requires tuning parameters like:\n\nparams_buffer_size, block_size, thread_count (controls read/write parallelism)\n\nnvme_path, pin_memory (optimizes data access latency)\n\n_\\[41\\]_\\[43\\]\\[45\\]\n\n**Infinity Offload Engine**\n\nIntelligently swaps model states between GPU, CPU, and NVMe based on computational requirements. For 20B+ models, >80% of memory can reside on CPU/NVMe, reducing GPU memory load below 20% _\\[49\\]_\\[51\\]\\[53\\].\n\n**Tradeoffs**:\n\n⚠️ **Latency Impact**: NVMe I/O speeds limit throughput; CPU transfers add overhead \\[47\\]\\[50\\].\n\n✅ **Capacity Gain**: Enables training **multi-trillion-parameter models** impossible on GPU-only systems \\[41\\]\\[53\\].\n\n**4\\. Communication & Computational Optimizations**\n\n**Overlap Techniques**: Overlaps GPU computation with data transfers during all-gather/reduce-scatter operations \\[3\\].\n\n**Contiguous Gradients**: Aggregates fragmented gradients into contiguous buffers to accelerate reduce-scatter \\[43\\].\n\n**Sub-Group Parallelism**: Optimizes ZeRO-3 communication via subgroups, reducing cross-node traffic \\[43\\].\n\n**5\\. Limitations and Practical Considerations**\n\n**Higher ZeRO Stages Increase Communication**: ZeRO-3’s 50% higher communication volume may bottleneck clusters with slow interconnects \\[3\\]\\[16\\].\n\n**NVMe Tuning Complexity**: Optimal NVMe performance requires hardware-specific tuning (e.g., queue_depth, thread_count) \\[43\\]\\[50\\].\n\n**Pipeline Load Imbalance**: Uneven layer distribution causes pipeline bubbles; requires careful model partitioning \\[30\\].\n\n**Conclusion**\n\nDeepSpeed’s innovations fundamentally resolve the memory challenges of trillion-parameter training:\n\n1.  **ZeRO stages** minimize per-GPU memory via sharding, reducing requirements by 8–64x.\n2.  **3D parallelism** combines ZeRO, pipeline, and tensor parallelism to distribute compute and memory horizontally and vertically.\n3.  **ZeRO-Infinity** expands usable memory to terabytes by offloading to CPU/NVMe.\n\nTogether, these techniques shift the bottleneck from GPU memory limits to communication efficiency – a tractable challenge for modern high-bandwidth clusters. DeepSpeed’s holistic approach enables researchers to scale models to unprecedented sizes using commodity hardware, democratizing access to cutting-edge AI capabilities _\\[21\\]_\\[26\\]\\[41\\].\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Large Language Model Parallelization Or: how to chop up a Llama](https://events.vsc.ac.at/event/136/attachments/88/330/LLM%20day%202%20parallelism.pdf)\n\n[2\\. DeepSpeed Training Tutorial: A Comprehensive Guide](https://www.byteplus.com/en/topic/464217)\n\n[3\\. Bandwidth Characterization of DeepSpeed on Distributed Large Language Model Training](https://lca.ece.utexas.edu/pubs/Hanindhito_BandwidthCharacterization.pdf)\n\n[4\\. DeepSpeed: 深層学習の訓練/推論の高速化のためのフレームワーク](https://www.deepspeed.ai/assets/files/DeepSpeed-Meetup-May-Japan-2024.pdf)\n\n[5\\. Memory-Efficient Training on Gaudi® with DeepSpeed](https://www.intel.cn/content/www/cn/zh/developer/articles/training/memory-efficient-training-on-gaudi-with-deepspeed.html)\n\n[6\\. DeepSpeed & ZeRO-2: Shattering barriers of deep learning speed & scale](https://www.microsoft.com/en-us/research/blog/ZeRO-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/)\n\n[7\\. victor94/DeepSpeed: 测试训练一个deepspeed - training/H...](https://openi.pcl.ac.cn/victor94/DeepSpeed/src/branch/master/training/HelloDeepSpeed/README.md)\n\n[8\\. DeepSpeed: a tuning tool for large language models](https://news.sophos.com/en-us/2024/12/13/deepspeed-a-tuning-tool-for-large-language-models/)\n\n[9\\. Learning the Language of Protein Structures / Die Sprache der Proteinstrukturen lernen](https://mediatum.ub.tum.de/doc/1717538/1717538.pdf)\n\n[10\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[11\\. Ottimizzazione e Scalabilità dei Modelli con DeepSpeed: Implementazione di Vision Transformers su minGPT](https://intranet.icar.cnr.it/wp-content/uploads/2025/01/RT-ICAR-NA-2025-02.pdf)\n\n[12\\. ZeRO 0, 1, 2, 3 produce different results · Issue #96...](https://github.com/microsoft/DeepSpeed/issues/966)\n\n[13\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[14\\. Mist: Efficient Distributed Training of Large Language Models via Memory-Parallelism Co-Optimization](https://arxiv.org/pdf/2503.19050)\n\n[15\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[16\\. 图解分布式训练（八）—— ZeRO 学习](https://www.tpffy.fun/download/AI_Models_Books/%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM%E9%9D%A2%E8%AF%95%E5%90%88%E9%9B%86/LLM/LLM/52-%E5%9B%BE%E8%A7%A3%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%EF%BC%88%E5%85%AB%EF%BC%89%E2%80%94%E2%80%94%20ZeRO%20%E5%AD%A6%E4%B9%A0.pdf)\n\n[17\\. ZeRO-Infinity and DeepSpeed: Unlocking Unprecedented Model Scale for Deep Learning Training](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/?ref=dataroots.ghost.io)\n\n[18\\. CANN大模型训练迁移和优化指南](https://www.hiascend.com/doc_center/source/zh/canncommercial/700/foundmodeldev/foundmodeltrain/CANN%207.0.0%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%BF%81%E7%A7%BB%E5%92%8C%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97%2001.pdf)\n\n[19\\. Análisis y validación de grandes modelos de lenguaje multimodal](https://zaguan.unizar.es/record/152407/files/TAZ-TFM-2024-909.pdf)\n\n[21\\. DeepSpeed: Extreme-scale model training for everyone...](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/?utm_source=wechat_session&utm_medium=social&utm_oi=738477582021832704)\n\n[22\\. DeepSpeed - Microsoft Research: Timeline](https://www.microsoft.com/en-us/research/project/deepspeed/timeline/?locale=zh_CN)\n\n[23\\. Distributed Training with DeepSpeed](https://www.tutorialspoint.com/deepspeed/deepspeed-distributed-training.htm)\n\n[24\\. Training Overview and Features - DeepSpeed](https://www.deepspeed.ai/training/#:~:text=DeepSpeed%20supports%20efficient%20data%20parallelism,our%20press-release%20and%20tutorial.)\n\n[25\\. Training a Trillion Parameters with Pipeline Parallelism - DeepSpeed](https://www.deepspeed.ai/2020/09/08/pipeline-parallelism.html)\n\n[26\\. The Machine Learning Solutions Architect Handbook](https://sciendo.com/2/v2/download/chapter/9781805124825/10.0000/9781805124825-001.pdf?Token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VycyI6W3sic3ViIjoyNTY3ODUxNywicHVicmVmIjoiNzY0NDg4IiwibmFtZSI6Ikdvb2dsZSBHb29nbGVib3QgLSBXZWIgQ3Jhd2xlciBTRU8iLCJ0eXBlIjoiaW5zdGl0dXRpb24iLCJsb2dvdXRfbGluayI6Imh0dHBzOi8vY29ubmVjdC5saWJseW54LmNvbS9sb2dvdXQvNjgxNDQzNDBlOGI4ZmY4OTY2ZmZjNWUyNTk3NWU4NTMiLCJhdXRoX21ldGhvZCI6ImlwIiwiaXAiOiI2Ni4yNDkuNzkuMiIsImNvdW50ZXJwYXJ0eV9pZCI6Ijc2NDQ4OCJ9XSwiaWF0IjoxNzQ2MTYxMjIxLCJleHAiOjE3NDczNzA4MjF9.3EgYWpVJfB7rVtMuBptYg975z4pyU_8mBg8xfdy20E0)\n\n[27\\. PyTorch Distributed Training with DeepSpeed](https://tinkerd.net/blog/machine-learning/distributed-training/)\n\n[28\\. Optimizing Distributed Training on Frontier for Large Language Models](https://www.osti.gov/servlets/purl/2438819)\n\n[29\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[30\\. Pipeline Parallelism - DeepSpeed](https://www.deepspeed.ai/tutorials/pipeline/)\n\n[31\\. 图解分布式训练（六）—— Pytorch的 DeepSpeed 详细解析](https://www.tpffy.fun/download/AI_Models_Books/%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM%E9%9D%A2%E8%AF%95%E5%90%88%E9%9B%86/LLM/LLM/50-%E5%9B%BE%E8%A7%A3%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94%20Pytorch%E7%9A%84%20DeepSpeed%20%E8%AF%A6%E7%BB%86%E8%A7%A3%E6%9E%90.pdf)\n\n[32\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[33\\. deepspeed.runtime.pipe.module — DeepSpeed 0.15.1 documentation](https://deepspeed.readthedocs.io/en/latest/_modules/deepspeed/runtime/pipe/module.html)\n\n[34\\. ...DeepSpeed is a deep learning optimization library t...](https://github.com/stephengou/DeepSpeed)\n\n[35\\. Bandwidth Characterization of DeepSpeed on Distributed Large Language Model Training](https://lca.ece.utexas.edu/pubs/Hanindhito_BandwidthCharacterization.pdf)\n\n[36\\. 【DeepSpeed】3D 并行原理解读 - 美熙智能](https://www.icnma.com/%E3%80%90deepspeed%E3%80%913d-%E5%B9%B6%E8%A1%8C%E5%8E%9F%E7%90%86%E8%A7%A3%E8%AF%BB/)\n\n[37\\. nnScaler: Constraint-Guided Parallelization Plan Generation for Deep Learning Training](https://www.usenix.org/system/files/osdi24-lin-zhiqi.pdf)\n\n[38\\. DeepSpeed: Extreme-scale model training for everyone](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)\n\n[39\\. Jeff Rasley, Samyam Rajbhandari et al. “DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters.” Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining](https://doi.org/10.1145/3394486.3406703)\n\n[40\\. DeepSpeed Training Tutorial: A Comprehensive Guide](https://www.byteplus.com/en/topic/464217)\n\n[41\\. Train 1 trillion+ parameter models - PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/1.7.7/advanced/model_parallel.html)\n\n[42\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[43\\. DeepSpeedStrategy — lightning 2.4.0 documentation](https://lightning.ai/docs/fabric/stable/api/generated/lightning.fabric.strategies.DeepSpeedStrategy.html)\n\n[44\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[45\\. Deepspeedstrategy With Pytorch-lightning | Restackio](https://www.restack.io/p/pytorch-lightning-answer-deepspeedstrategy-cat-ai)\n\n[46\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[47\\. Memory Optimization with DeepSpeed](https://www.tutorialspoint.com/deepspeed/deepspeed-memory-optimization.htm)\n\n[48\\. DeepSpeed and Trillion-parameter LLMs: Can synergy of MPI and NCCL improve scalability and efficiency?](https://mug.mvapich.cse.ohio-state.edu/static/media/mug/presentations/24/2nd-day/7%20-%20DeepSpeed%20and%20Trillion-parameter%20LLMs%20Can%20synergy%20of%20MPI%20and%20NCCL%20improve%20scalability%20and%20efficiency.pdf)\n\n[49\\. Bandwidth Characterization of DeepSpeed on Distributed Large Language Model Training](https://lca.ece.utexas.edu/pubs/Hanindhito_BandwidthCharacterization.pdf)\n\n[50\\. DeepSpeed — PyTorch Lightning 2.4.0 documentation](https://lightning.ai/docs/pytorch/latest/advanced/model_parallel/deepspeed.html)\n\n[51\\. ZeRO-Infinity and DeepSpeed: Unlocking Unprecedented Model Scale for Deep Learning Training](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/?ref=dataroots.ghost.io)\n\n[52\\. An I/O Characterizing Study of Offloading LLM Models and KV Caches to NVMe SSD](https://atlarge-research.com/pdfs/2025-cheops-llm.pdf)\n\n[53\\. ZeRO-Infinity and DeepSpeed: Unlocking unprecedented m...](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/)\n\n[54\\. Security of NVMe Offloaded Data in Large-Scale Machine Learning](https://openreview.net/pdf?id=8QGZCvTJix)\n\n[55\\. Mining Unstructured Data 12. LLM & LLM-based Assistants](https://www.cs.upc.edu/~turmo/mud/lectures/12-llm.pdf)\n\n[56\\. SpeedLoader: An I/O efficient scheme for heterogeneous and distributed LLM operation](https://openreview.net/pdf/c4af89397c0445f2b9eb6ba052951391bfd5f0dd.pdf)\n\n[57\\. TURNIP: A “Nondeterministic” GPU Runtime with CPU RAM Offload](https://openreview.net/pdf/7c9821e0bfe42e7a4a3a42e3992d9c741e560843.pdf)\n\n[58\\. Training Overview and Features - DeepSpeed](https://www.deepspeed.ai/training/#:~:text=DeepSpeed%20supports%20efficient%20data%20parallelism,our%20press-release%20and%20tutorial.)\n\n[59\\. DeepSpeed MoE：支持大规模混合专家模型的高性能系统](https://thewindowsupdate.com/2021/08/18/deepspeed-powers-8x-larger-moe-model-training-with-high-performance/)"
  },
  {
    "id": 51,
    "question": "What is the conceptual difference between Mixture of Experts (MoE) in Large Language Models versus traditional recommendation systems, and why do LLMs process tokens rather than entire sentences through individual experts?",
    "response": "**Comprehensive Research Report: MoE in LLMs vs. Recommendation Systems**\n\n**1\\. Core Conceptual Differences in MoE Architecture**\n\nThe Mixture of Experts (MoE) architecture shares foundational principles across both domains but differs fundamentally in **specialization goals** and **gating mechanisms**:\n\n**In LLMs**:\n\nExperts are typically specialized Feed-Forward Network (FFN) layers or attention modules, each optimized for distinct linguistic features (e.g., syntax, semantics). A router dynamically activates experts _per token_ based on learned token-level patterns \\[1\\]\\[1\\]. For example, content words (nouns/verbs) may engage different experts than function words (prepositions/articles) \\[1\\]. This enables **conditional computation**, activating only relevant experts per token \\[1\\]\\[1\\].\n\n**In Recommendation Systems**:\n\nExperts capture domain-specific user preferences (e.g., genre affinity, price sensitivity). Gating operates at the _user/context level_, balancing multi-task objectives like click-through rate vs. user retention \\[1\\]\\[1\\]. Unlike LLMs, routing is static per session and prioritizes mitigating \"seesaw effects\" (improving one metric degrades another) \\[1\\].\n\n**Key Distinction**: LLM-MoE leverages token-level dynamism for linguistic precision, while recommendation MoE uses session-level routing for multi-objective trade-offs.\n\n**2\\. Why Token-Level Routing Dominates in LLMs**\n\n**A. Linguistic Necessity**\n\n**Token Variability**: Tokens within a sentence carry asymmetric semantic loads (e.g., \"quantum\" vs. \"the\"). Token-level routing allocates computational resources proportionally \\[1\\]\\[1\\].\n\n**Syntax-Semantics Decoupling**: Nouns/verbs require deeper processing than syntactic tokens. Fixed sentence-level routing would waste capacity on low-signal tokens \\[1\\]\\[23\\].\n\n**B. Technical Efficiency**\n\n**Load Balancing**: Token-level routing allows dynamic distribution across experts via soft routers, preventing bottlenecks \\[1\\]\\[28\\]. Sentence-level routing risks overloading experts with complex sentences \\[1\\].\n\n**Minimized Interference**: Routing tokens to specialized experts reduces gradient conflicts during training \\[1\\]\\[28\\]. Sentence-level approaches obscure token-specific features, degrading accuracy \\[23\\]\\[42\\].\n\n**C. Performance Benchmarks**\n\nToken-level routing consistently outperforms sentence-level on NLP tasks:\n\n**Machine Translation**: +3.2 BLEU on WMT \\[3\\]\\[63\\]\n\n**Knowledge-Intensive Tasks**: 12% higher accuracy on MMLU benchmarks \\[1\\]\\[67\\]\n\n**Efficiency**: 30–40% lower FLOPs for comparable perplexity \\[6\\]\\[97\\]\n\n**Exception**: Sentence-level distillation slightly outperforms for smaller models or simple texts \\[4\\]\\[124\\]but token-level scales better for large LLMs.\n\n**3\\. Computational Trade-Offs: Token vs. Sentence Routing**\n\n**A. Overhead Metrics**\n\n|     |     |     |\n| --- | --- | --- |\n| **Metric** | **Token-Level Routing** | **Sentence-Level Routing** |\n| **Latency/Token** | 15–40ms (high comms cost) \\[6\\]\\[96\\] | 5–15ms (lower comms) \\[96\\] |\n| **Memory** | High (per-token gradients) \\[85\\] | 60% lower footprint \\[6\\] |\n| **FLOPs** | 28.6G vs. 72G in dense models \\[151\\] | ~50% higher for same output \\[1\\] |\n| **Load Balancing** | Challenging (requires auxiliary losses) \\[32\\] | Naturally balanced \\[85\\] |\n\n**B. Hybrid Routing Innovations (2024–2025)**\n\nNew architectures optimize granularity:\n\n**Hierarchical Routing**: Token-level in shallow layers, sentence-level in deeper layers \\[1\\]\\[103\\].\n\n**Adaptive Routers**: LLM-based routers switch modes based on input complexity \\[1\\]\\[102\\].\n\n**MoT (Mixture of Tokens)**: Mixes tokens _across_ sentences to stabilize training (3× speedup) \\[112\\].\n\n**4\\. Architectural Evolution: LLMs vs. Recommendation Systems**\n\n**LLM-Specific Innovations**: Focus on overcoming:\n\n**Token Overflow**: Unrouted tokens due to expert capacity limits \\[27\\]\n\n**Early Routing Learning**: Inflexible token-expert assignments after pretraining \\[24\\]\n\n**Recommendation-Specific**: Designed for objective alignment (e.g., avoiding \"seesaw\" trade-offs) using task-specific gating \\[1\\].\n\n**Unified Principle**: Both use dynamic sparsity to scale capacity without proportional compute increases \\[204\\].\n\n**5\\. Future Trajectories**\n\n**LLMs**: Hybrid token/sentence routing will dominate, combining fine-grained flexibility with sequence-level context \\[7\\]\\[162\\]. Emerging \"controller networks\" \\[171\\] and LoRA-MoE hybrids \\[165\\] show 5–8× gains in specialized tasks.\n\n**Recommendation Systems**: Will adopt LLM-like tokenization for session-sequence modeling (e.g., user action sequences as \"tokens\").\n\n**Critical Insight**: Token-level routing persists in LLMs because language is inherently compositional—processing nuances at sub-sentence levels is non-negotiable for state-of-the-art performance.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. A Survey on Mixture of Experts in Large Language Models](https://arxiv.org/pdf/2407.06204)\n\n[2\\. Mixture of Experts in a Mixture of RL settings](https://rlj.cs.umass.edu/2024/papers/RLJ_RLC_2024_130.pdf)\n\n[3\\. Mixture of Experts Approach for Large Language Models - Toloka](https://toloka.ai/blog/mixture-of-experts-approach-for-llms/#:~:text=A%20mixture%20of%20experts%20%28MoE%29%20is%20a%20machine%20learning%20technique,assigned%20to%20a%20different%20expert.)\n\n[4\\. XRec: Large Language Models for Explainable Recommendation](https://arxiv.org/pdf/2406.02377)\n\n[5\\. Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives](https://arxiv.org/pdf/2407.14962v3)\n\n[6\\. Yunpeng Huang, Jingwei Xu et al. “Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey.” ArXiv](https://doi.org/10.48550/arXiv.2311.12351)\n\n[7\\. MIXTURE OF EXPERTS WITH MIXTURE OF PRECISIONS FOR TUNING QUALITY OF SERVICE](https://arxiv.org/pdf/2407.14417)\n\n[8\\. A Mixture Of Experts for Out-Of-Domain Expertise](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1224/reports/default_116616054.pdf)\n\n[9\\. MOE & MOA for Large Language Models](https://towardsdatascience.com/moe-moa-for-large-language-models-c1cafeffd6a5/)\n\n[10\\. Mixture of Experts LLMs: Key Concepts Explained - neptune.ai](https://neptune.ai/blog/mixture-of-experts-llms#:~:text=The%20gating%20mechanism%20enables%20conditional,layer%20in%20the%20Transformer%20block.)\n\n[11\\. ...Experts for Network Optimization: A Large Language ...](https://ui.adsabs.harvard.edu/abs/2024arXiv240209756D/abstract)\n\n[21\\. Language_Model - Paper Reading](https://paperreading.club/category?cate=Language_Model&page=236)\n\n[22\\. LM-LEXICON: Definition Modeling with Mixture-of-Experts](https://openreview.net/pdf/9c8129aa502668ec8c23511d5f2cf579ef314bb4.pdf)\n\n[23\\. AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models](https://openreview.net/pdf?id=mzOyE5wBMO)\n\n[24\\. OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models](https://raw.githubusercontent.com/mlresearch/v235/main/assets/xue24c/xue24c.pdf)\n\n[25\\. A Lightweight Mixture-of-Experts Neural Machine Translation Model with Stage-wise Training Strategy](https://www.cs.jhu.edu/~kevinduh/t/naacl24/final_pdf/paper604.pdf)\n\n[26\\. Mixture of Experts in a Mixture of RL settings](https://rlj.cs.umass.edu/2024/papers/RLJ_RLC_2024_130.pdf)\n\n[27\\. A Survey on Mixture of Experts in Large Language Models](https://arxiv.org/pdf/2407.06204)\n\n[28\\. Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model](https://arxiv.org/pdf/2406.19905)\n\n[29\\. SCoMoE: EFFICIENT MIXTURES OF EXPERTS WITH STRUCTURED COMMUNICATION](https://openreview.net/pdf?id=s-c96mSU0u5)\n\n[30\\. Poster Volume II](http://poster-openaccess.com/files/poster2.pdf)\n\n[31\\. CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing](https://arxiv.org/pdf/2502.01976)\n\n[32\\. M. Lewis, Shruti Bhosale et al. “BASE Layers: Simplifying Training of Large, Sparse Models.” International Conference on Machine Learning](https://arxiv.org/abs/2103.16716)\n\n[33\\. Dynamically allocating compute in transformers](https://news.ycombinator.com/item?id=39960717)\n\n[34\\. Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models](https://blog.bayjarvis.com/paper/mixture-of-experts-meets-instruction-tuning-a-winning-combination-for-large-language-models)\n\n[35\\. Noam M. Shazeer, Azalia Mirhoseini et al. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” ArXiv](https://arxiv.org/abs/1701.06538)\n\n[41\\. Expert-Token Resonance MoE: Bidirectional Routing with Efficiency Affinity-Driven Active Selection](https://arxiv.org/pdf/2406.00023)\n\n[42\\. A Lightweight Mixture-of-Experts Neural Machine Translation Model with Stage-wise Training Strategy](https://www.cs.jhu.edu/~kevinduh/t/naacl24/final_pdf/paper604.pdf)\n\n[43\\. CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing](https://arxiv.org/pdf/2502.01976)\n\n[44\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[45\\. Expert-Token Resonance: Redefining MoE Routing through Affinity-Driven Active Selection](https://arxiv.org/pdf/2406.00023v2)\n\n[46\\. Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/f89221edad5a6a4a54fcf247cb37cd62-Paper-Conference.pdf)\n\n[47\\. AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models](https://openreview.net/pdf?id=mzOyE5wBMO)\n\n[48\\. Towards Efficient and Scalable Representation Learning](https://www.lti.cs.cmu.edu/people/alumni/alumni-thesis/pham-hai-thesis.pdf)\n\n[49\\. Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models](https://openreview.net/pdf?id=pQgYBC7sg9O)\n\n[50\\. Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model](https://arxiv.org/pdf/2406.19905)\n\n[51\\. Mixture of Experts Vs Mixture of Tokens: Making LLMs More Efficient in 2024](https://www.datalabelify.com/en/mixture-of-experts-vs-mixture-of-tokens-making-llms-more-efficient-in-2024/)\n\n[52\\. Large Language Model Routing with Benchmark Datasets](https://openreview.net/pdf/15d2ba374582b9a720ecb9ffbc0b156503a0665c.pdf)\n\n[53\\. Ada-K Routing: Boosting the Efficiency of MoE-based LLMs](https://openreview.net/forum?id=9CqkpQExe2)\n\n[54\\. MiniMax-01: Scaling Foundation Models with Lightning Attention](https://filecdn.minimax.chat/_Arxiv_MiniMax_01_Report.pdf)\n\n[55\\. Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models](https://blog.bayjarvis.com/paper/mixture-of-experts-meets-instruction-tuning-a-winning-combination-for-large-language-models)\n\n[56\\. M. Lewis, Shruti Bhosale et al. “BASE Layers: Simplifying Training of Large, Sparse Models.” International Conference on Machine Learning](https://arxiv.org/abs/2103.16716)\n\n[57\\. Efficient Large Language Models: A Survey](https://openreview.net/notes/edits/attachment?id=pKPJpBLaoN&name=pdf)\n\n[58\\. Mixture-of-Experts with Expert Choice Routing](https://proceedings.neurips.cc/paper_files/paper/2022/file/2f00ecd787b432c1d36f3de9800728eb-Paper-Conference.pdf)\n\n[61\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[62\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[63\\. Sneha Kudugunta, Yanping Huang et al. “Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference.” ArXiv](https://doi.org/10.18653/v1/2021.findings-emnlp.304)\n\n[64\\. MoTE: Mixture of Task Experts for Embedding Models](https://openreview.net/pdf/e027b119db3c76021abcb171194c515745f0e914.pdf)\n\n[65\\. Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts](https://openreview.net/pdf/4382a5c42d5b1409ee2c995c609fcd778abc6e50.pdf)\n\n[66\\. Mixture of Experts vs Mixture of Tokens: Making LLMs more efficient](https://www.superannotate.com/blog/mixture-of-experts-vs-mixture-of-tokens)\n\n[67\\. An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing](http://yangy.org/works/llm/An_expert_is_worth_one_token_ACL24.pdf)\n\n[68\\. Sainbayar Sukhbaatar, Olga Golovneva et al. “Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM.” ArXiv](https://doi.org/10.48550/arXiv.2403.07816)\n\n[69\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[70\\. Large Language Model Routing with Benchmark Datasets](https://openreview.net/pdf/15d2ba374582b9a720ecb9ffbc0b156503a0665c.pdf)\n\n[71\\. W. Fedus, Barret Zoph et al. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” J. Mach. Learn. Res.](https://arxiv.org/abs/2101.03961)\n\n[72\\. ...A Metric for Quantifying LLM Performance Across Hig...](http://arxiv.org/html/2404.11553v3)\n\n[73\\. Noam M. Shazeer, Azalia Mirhoseini et al. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” ArXiv](https://arxiv.org/abs/1701.06538)\n\n[74\\. Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models](https://blog.bayjarvis.com/paper/mixture-of-experts-meets-instruction-tuning-a-winning-combination-for-large-language-models)\n\n[75\\. M5 – A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks](https://openreview.net/pdf/368f1aa4ce6f2d58ccca5e4e9bc45584b8fb8b0b.pdf)\n\n[76\\. Sentence-Level or Token-Level? A Comprehensive Study on Knowledge Distillation](https://www.ijcai.org/proceedings/2024/0722.pdf)\n\n[77\\. LLM Routing with Benchmark Datasets](https://openreview.net/pdf/7ee32bc7e703a658cf3199ef55828020526d9f47.pdf)\n\n[81\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[82\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[83\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[84\\. CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing](https://arxiv.org/pdf/2502.01976)\n\n[85\\. Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model](https://arxiv.org/pdf/2406.19905)\n\n[86\\. W. Fedus, Barret Zoph et al. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” J. Mach. Learn. Res.](https://arxiv.org/abs/2101.03961)\n\n[87\\. Language_Model - Paper Reading](https://paperreading.club/category?cate=Language_Model&page=236)\n\n[88\\. Dmitry Lepikhin, HyoukJoong Lee et al. “GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.” ArXiv](https://arxiv.org/abs/2006.16668)\n\n[89\\. Collaborative Inference for Efficient Large Language Model ...](https://openreview.net/forum?id=J2FyEVg8HR)\n\n[90\\. Towards Efficient and Scalable Representation Learning](https://www.lti.cs.cmu.edu/people/alumni/alumni-thesis/pham-hai-thesis.pdf)\n\n[91\\. Zhengyan Zhang, Yuxian Gu et al. “CPM-2: Large-scale Cost-effective Pre-trained Language Models.” ArXiv](https://doi.org/10.1016/j.aiopen.2021.12.003)\n\n[92\\. Load balancing and memory optimizations for expert parallel training of large language models](https://dspace.mit.edu/bitstream/handle/1721.1/153897/wisdom-dwisdom-meng-eecs-2024-thesis.pdf?sequence=1&isAllowed=y)\n\n[93\\. Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196v3)\n\n[94\\. BEYOND NEXT TOKEN PREDICTION: PATCH-LEVEL TRAINING FOR LARGE LANGUAGE MODELS](https://openreview.net/pdf/dc2cfc56513c9b5bee4590c32ce50b776f8c1d8e.pdf)\n\n[95\\. D-LLM: A Token Adaptive Computing Resource Allocation Strategy for Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/03469b1a66e351b18272be23baf3b809-Paper-Conference.pdf)\n\n[96\\. Poster Volume II](http://poster-openaccess.com/files/poster2.pdf)\n\n[97\\. Fast Inference of MoE Language Models with Offloading](https://aipapersacademy.com/moe-offloading/)\n\n[98\\. Sentence-Level or Token-Level? A Comprehensive Study on Knowledge Distillation](https://www.ijcai.org/proceedings/2024/0722.pdf)\n\n[101\\. AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models](https://openreview.net/pdf?id=mzOyE5wBMO)\n\n[102\\. LLM-Based Routing in Mixture of Experts: A Novel Frame...](http://arxiv.org/html/2501.09636v1)\n\n[103\\. HMORA: MAKING LLMs MORE EFFECTIVE WITH HIERARCHICAL MIXTURE OF LoRA EXPERTS](https://openreview.net/pdf?id=lTkHiXeuDl)\n\n[104\\. Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design](https://openreview.net/pdf?id=i8JaxY7tDI)\n\n[105\\. An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing](http://yangy.org/works/llm/An_expert_is_worth_one_token_ACL24.pdf)\n\n[106\\. SuperBruceJia/Awesome-Mixture-of-Experts ...](https://github.com/SuperBruceJia/Awesome-Mixture-of-Experts)\n\n[107\\. Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model](https://arxiv.org/pdf/2406.19905)\n\n[108\\. DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic, Lightweight Plugin for Large Language Models](https://openreview.net/pdf?id=I1VCj1l1Zn)\n\n[109\\. Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts](https://openreview.net/pdf/4382a5c42d5b1409ee2c995c609fcd778abc6e50.pdf)\n\n[110\\. Comparison of Major LLM Architectures (2017– 2025)](https://skillenai.com/competition-post/comparison-of-major-llm-architectures-2017-2025/)\n\n[111\\. Mixture of Experts Vs Mixture of Tokens: Making LLMs More Efficient in 2024](https://www.datalabelify.com/ar/mixture-of-experts-vs-mixture-of-tokens-making-llms-more-efficient-in-2024/)\n\n[112\\. Mixture of Experts LLM & Mixture of Tokens Approaches-2024 - Ubiai](https://ubiai.tools/mixture-of-experts-llm-mixture-of-tokens-approaches-in-2024/)\n\n[113\\. \\[PDF\\] Mixtral of Experts | Semantic Scholar](https://api.semanticscholar.org/arXiv:2401.04088)\n\n[114\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[115\\. HMoRA: Making LLMs More Effective with Hierarchical ...](https://openreview.net/forum?id=lTkHiXeuDl)\n\n[121\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[122\\. Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model](https://arxiv.org/pdf/2406.19905)\n\n[123\\. HMORA: MAKING LLMs MORE EFFECTIVE WITH HIERARCHICAL MIXTURE OF LoRA EXPERTS](https://openreview.net/pdf?id=lTkHiXeuDl)\n\n[124\\. Sentence-Level or Token-Level? A Comprehensive Study on Knowledge Distillation](https://www.ijcai.org/proceedings/2024/0722.pdf)\n\n[125\\. Large Language Model Routing with Benchmark Datasets](https://openreview.net/pdf/15d2ba374582b9a720ecb9ffbc0b156503a0665c.pdf)\n\n[126\\. Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design](https://openreview.net/pdf?id=i8JaxY7tDI)\n\n[127\\. Alex Wang, Amanpreet Singh et al. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” BlackboxNLP@EMNLP](https://doi.org/10.18653/v1/W18-5446)\n\n[128\\. Mark Chen, Jerry Tworek et al. “Evaluating Large Language Models Trained on Code.” ArXiv](https://arxiv.org/abs/2107.03374)\n\n[129\\. Solving Token Gradient Conflict in Mixture-of-Experts for ...](https://arxiv.org/html/2406.19905v1)\n\n[130\\. Karl Cobbe, V. Kosaraju et al. “Training Verifiers to Solve Math Word Problems.” ArXiv](https://arxiv.org/abs/2110.14168)\n\n[131\\. LLM-Based Routing in Mixture of Experts: A Novel Frame...](http://arxiv.org/html/2501.09636v1)\n\n[132\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[141\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[142\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[143\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[144\\. LM-LEXICON: Definition Modeling with Mixture-of-Experts](https://openreview.net/pdf/9c8129aa502668ec8c23511d5f2cf579ef314bb4.pdf)\n\n[145\\. Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model](https://arxiv.org/pdf/2406.19905)\n\n[146\\. J. Kaplan, Sam McCandlish et al. “Scaling Laws for Neural Language Models.” ArXiv](https://arxiv.org/abs/2001.08361)\n\n[147\\. Performance Characterization of Expert Router for Scalable LLM Inference](https://arxiv.org/pdf/2404.15153)\n\n[148\\. W. Fedus, Barret Zoph et al. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” J. Mach. Learn. Res.](https://arxiv.org/abs/2101.03961)\n\n[149\\. EXAMINING POST-TRAINING QUANTIZATION FOR MIXTURE-OF-EXPERTS: A BENCHMARK](https://openreview.net/pdf?id=sMwYn2lZjO)\n\n[150\\. Mixture of Experts vs Mixture of Tokens: Making LLMs more efficient](https://www.superannotate.com/blog/mixture-of-experts-vs-mixture-of-tokens#:~:text=MoEs%20vs%20MoTs:%20In%20Mixture,an%20expert%20feed-forward%20layer.)\n\n[151\\. Improving Computational Efficiency of Mixture of Experts](https://arxiv.org/abs/2310.09832)\n\n[152\\. D-LLM: A Token Adaptive Computing Resource Allocation Strategy for Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2024/file/03469b1a66e351b18272be23baf3b809-Paper-Conference.pdf)\n\n[153\\. ...of 1-bit LLMs: All Large Language Models are in 1.5...](http://arxiv.org/html/2402.17764v1)\n\n[154\\. MARCONI: PREFIX CACHING FOR THE ERA OF HYBRID LLMs](https://arxiv.org/pdf/2411.19379)\n\n[155\\. A Layered Architecture for Developing and Enhancing Capabilities in Large Language Model-based Software Systems](https://arxiv.org/pdf/2411.12357)\n\n[156\\. Pingzhi Li, Xiaolong Jin et al. “Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark.” ArXiv](https://doi.org/10.48550/arXiv.2406.08155)\n\n[157\\. How Quantization Reduces LLM Latency](https://latitude.so/blog/how-quantization-reduces-llm-latency/)\n\n[158\\. ROUTELLM: LEARNING TO ROUTE LLMs WITH PREFERENCE DATA](https://openreview.net/pdf?id=8sSqNntaMr)\n\n[159\\. A Benchmarking Study: Which serving technology to choose for LLMs?](https://pages.run.ai/hubfs/PDFs/Serving-Large-Language-Models-Run-ai-Benchmarking-Study_corrected.pdf)\n\n[161\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[162\\. HMORA: MAKING LLMs MORE EFFECTIVE WITH HIERARCHICAL MIXTURE OF LoRA EXPERTS](https://openreview.net/pdf?id=lTkHiXeuDl)\n\n[163\\. A Survey of Mixture of Experts Models: Architectures and ...](https://www.preprints.org/manuscript/202505.1603/v1)\n\n[164\\. Mixture of Experts in a Mixture of RL settings](https://rlj.cs.umass.edu/2024/papers/RLJ_RLC_2024_130.pdf)\n\n[165\\. ENSEMBLES OF LOW-RANK EXPERT ADAPTERS](https://assets.amazon.science/6e/92/dd350c95484ca5da4cf204195aba/ensembles-of-low-rank-expert-adapters.pdf)\n\n[166\\. Comparison of Major LLM Architectures (2017– 2025)](https://skillenai.com/competition-post/comparison-of-major-llm-architectures-2017-2025/)\n\n[167\\. LLM-Based Routing in Mixture of Experts: A Novel Frame...](http://arxiv.org/html/2501.09636v1)\n\n[168\\. Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts](https://openreview.net/pdf/4382a5c42d5b1409ee2c995c609fcd778abc6e50.pdf)\n\n[169\\. Mixture of Experts LLM & Mixture of Tokens Approaches-2024 - Ubiai](https://ubiai.tools/mixture-of-experts-llm-mixture-of-tokens-approaches-in-2024/)\n\n[170\\. Large Language Model Routing with Benchmark Datasets](https://openreview.net/pdf/15d2ba374582b9a720ecb9ffbc0b156503a0665c.pdf)\n\n[171\\. Mixture of Experts Vs Mixture of Tokens: Making LLMs More Efficient in 2024](https://www.datalabelify.com/ar/mixture-of-experts-vs-mixture-of-tokens-making-llms-more-efficient-in-2024/)\n\n[172\\. DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic, Lightweight Plugin for Large Language Models](https://openreview.net/pdf?id=I1VCj1l1Zn)\n\n[173\\. AUTOMATED FINE-GRANINED MIXTURE-OF-EXPERTS QUANTIZATION](https://openreview.net/pdf?id=etxbRucurT)\n\n[174\\. PROCEED: Performance Routing Optimization for Cost-Efficient and Effective Deployment](http://web.stanford.edu/class/cs224n/final-reports/256847361.pdf)\n\n[175\\. Mixture-of-Experts (MoE) LLMs: The Future of Efficient AI Models](https://sam-solutions.com/blog/moe-llm-architecture/)\n\n[176\\. Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model](https://arxiv.org/pdf/2406.19905)\n\n[177\\. LM-LEXICON: Definition Modeling with Mixture-of-Experts](https://openreview.net/pdf/9c8129aa502668ec8c23511d5f2cf579ef314bb4.pdf)\n\n[181\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[182\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[183\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[184\\. Alex Wang, Amanpreet Singh et al. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” BlackboxNLP@EMNLP](https://doi.org/10.18653/v1/W18-5446)\n\n[185\\. Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design](https://openreview.net/pdf?id=i8JaxY7tDI)\n\n[186\\. Mixture of Experts Vs Mixture of Tokens: Making LLMs More Efficient in 2024](https://www.datalabelify.com/ar/mixture-of-experts-vs-mixture-of-tokens-making-llms-more-efficient-in-2024/)\n\n[187\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[188\\. An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing](http://yangy.org/works/llm/An_expert_is_worth_one_token_ACL24.pdf)\n\n[189\\. Large Language Model Routing with Benchmark Datasets](https://openreview.net/pdf/15d2ba374582b9a720ecb9ffbc0b156503a0665c.pdf)\n\n[190\\. Mixtral of Experts：基于稀疏混合专家的高效语言模型](https://zhuanlan.zhihu.com/p/678442100)\n\n[191\\. Evaluating the Performance of Large Language Models (LLMs) Through Grid-Based Game Competitions: An Extensible Benchmark and Leaderboard on the Path to Artificial General Intelligence (AGI)](https://dergipark.org.tr/en/download/article-file/4483558)\n\n[192\\. Benchmarking Large Language Models in Retrieval-Augmented Generation](https://ojs.aaai.org/index.php/AAAI/article/view/29728/31250)\n\n[193\\. Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?](https://www.arxiv.org/pdf/2502.00674)\n\n[194\\. MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark](https://arxiv.org/pdf/2406.01574v2)\n\n[195\\. Mixture of Experts vs Mixture of Tokens: Making LLMs more efficient](https://www.superannotate.com/blog/mixture-of-experts-vs-mixture-of-tokens)\n\n[196\\. Improving Routing in Sparse Mixture of Experts with Graph of Tokens](https://arxiv.org/pdf/2505.00792)\n\n[197\\. Breaking Down the Metrics: A Comparative Analysis of LLM Benchmarks](https://ijsra.net/sites/default/files/IJSRA-2024-2209.pdf)\n\n[201\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[202\\. Mixture of Experts in a Mixture of RL settings](https://rlj.cs.umass.edu/2024/papers/RLJ_RLC_2024_130.pdf)\n\n[203\\. W. Fedus, Barret Zoph et al. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” J. Mach. Learn. Res.](https://arxiv.org/abs/2101.03961)\n\n[204\\. Noam M. Shazeer, Azalia Mirhoseini et al. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” ArXiv](https://arxiv.org/abs/1701.06538)\n\n[205\\. EVERY FLOP COUNTS: SCALING A 300B MIXTURE-OF-EXPERTS LING LLM WITHOUT PREMIUM GPUs](https://fetcher.alphaxiv.org/v2/pdf/2503.05139v1)\n\n[206\\. Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning](http://arxiv.org/pdf/2502.03275)\n\n[207\\. Nan Du, Yanping Huang et al. “GLaM: Efficient Scaling of Language Models with Mixture-of-Experts.” ArXiv](https://arxiv.org/abs/2112.06905)\n\n[208\\. Dmitry Lepikhin, HyoukJoong Lee et al. “GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.” ArXiv](https://arxiv.org/abs/2006.16668)\n\n[209\\. Mixture-of-Depths: Dynamically allocating compute in ...](https://arxiv.org/html/2404.02258v1)\n\n[210\\. Applying Mixture of Experts in LLM Architectures](https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/)\n\n[211\\. Dynamically allocating compute in transformers](https://news.ycombinator.com/item?id=39960717)\n\n[212\\. EXAMINING POST-TRAINING QUANTIZATION FOR MIXTURE-OF-EXPERTS: A BENCHMARK](https://openreview.net/pdf?id=sMwYn2lZjO)\n\n[213\\. LM-LEXICON: Definition Modeling with Mixture-of-Experts](https://openreview.net/pdf/9c8129aa502668ec8c23511d5f2cf579ef314bb4.pdf)\n\n[214\\. Walking the Tightrope: Balancing Energy Efficiency and Accuracy in LLM-Driven Code Generation](https://studenttheses.uu.nl/bitstream/handle/20.500.12932/48350/Thesis_Mats_Buis_Final_Version_6205135.pdf?sequence=1&isAllowed=y)\n\n[215\\. HMORA: MAKING LLMs MORE EFFECTIVE WITH HIERARCHICAL MIXTURE OF LoRA EXPERTS](https://openreview.net/pdf?id=lTkHiXeuDl)\n\n[216\\. Toward Efficient Inference for Mixture of Experts](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf)\n\n[217\\. Mixture of Parrots: Mixtures of experts improve memorization more than reasoning](https://openreview.net/pdf?id=8YBLMm1yzz)\n\n[218\\. Mixture of Experts vs Mixture of Tokens: Making LLMs more efficient](https://www.superannotate.com/blog/mixture-of-experts-vs-mixture-of-tokens#:~:text=MoEs%20vs%20MoTs:%20In%20Mixture,an%20expert%20feed-forward%20layer.)\n\n[219\\. EM-LoRA: Efficient Mixture of Low-Rank Adaptation for ...](https://openreview.net/forum?id=MpXSpER30w)\n\n[220\\. Large Language Model Routing with Benchmark Datasets](https://openreview.net/pdf/15d2ba374582b9a720ecb9ffbc0b156503a0665c.pdf)\n\n[221\\. Mixture of Experts in a Mixture of RL settings](https://rlj.cs.umass.edu/2024/papers/RLJ_RLC_2024_130.pdf)\n\n[222\\. Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design](https://openreview.net/pdf?id=i8JaxY7tDI)\n\n[223\\. γ-MOD: EXPLORING MIXTURE-OF-DEPTH ADAPTATION FOR MULTIMODAL LARGE LANGUAGE MODELS](https://openreview.net/pdf/69302eb76025af80c717cd2118664635d445723d.pdf)\n\n[224\\. Large Language Model Routing with Benchmark Datasets](https://openreview.net/pdf/15d2ba374582b9a720ecb9ffbc0b156503a0665c.pdf)\n\n[225\\. Comparison of Major LLM Architectures (2017– 2025)](https://skillenai.com/competition-post/comparison-of-major-llm-architectures-2017-2025/)\n\n[226\\. Mixture-of-Experts (MoE) LLMs: The Future of Efficient AI Models](https://sam-solutions.com/blog/moe-llm-architecture/)\n\n[227\\. LLM-Based Routing in Mixture of Experts: A Novel Frame...](http://arxiv.org/html/2501.09636v1)\n\n[228\\. Applying Mixture of Experts in LLM Architectures](https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/)\n\n[229\\. Mixture Compressor for Mixture-of-Experts LLMs Gains More](https://openreview.net/forum?id=hheFYjOsWO)\n\n[230\\. Mixture of Experts LLM & Mixture of Tokens Approaches-2024 - Ubiai](https://ubiai.tools/mixture-of-experts-llm-mixture-of-tokens-approaches-in-2024/)\n\n[231\\. CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts](https://openreview.net/pdf?id=hwuUBsMlBf)\n\n[232\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[233\\. W. Fedus, Barret Zoph et al. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” J. Mach. Learn. Res.](https://arxiv.org/abs/2101.03961)\n\n[234\\. Noam M. Shazeer, Azalia Mirhoseini et al. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” ArXiv](https://arxiv.org/abs/1701.06538)\n\n[235\\. HMORA: MAKING LLMs MORE EFFECTIVE WITH HIERARCHICAL MIXTURE OF LoRA EXPERTS](https://openreview.net/pdf?id=lTkHiXeuDl)\n\n[236\\. SuperBruceJia/Awesome-Mixture-of-Experts ...](https://github.com/SuperBruceJia/Awesome-Mixture-of-Experts)"
  },
  {
    "id": 52,
    "question": "How has RAG technology evolved in 2024, and what are the key technical innovations that addressed its major pain points?",
    "response": "**Evolution of RAG Technology in 2024: Addressing Key Pain Points Through Technical Innovations**\n\n**Introduction**\n\nRetrieval-Augmented Generation (RAG) technology underwent transformative advancements in 2024, overcoming critical limitations such as **retrieval latency**, **hallucinations**, and **multi-hop reasoning failures**. Driven by architectural refinements and hardware acceleration, these innovations expanded RAG's enterprise viability. This report synthesizes 2024’s key breakthroughs, benchmarked performance gains, and unresolved challenges.\n\n**1\\. Core Pain Points and 2024 Evolution Overview**\n\nRAG’s pre-2024 limitations included:\n\n**High latency** from retrieval bottlenecks \\[30\\]\\[346\\].\n\n**Hallucinations** from inadequate context grounding \\[30\\]\\[39\\].\n\n**Semantic gaps** in multi-hop queries \\[64\\]\\[75\\].\n\n**Accuracy-latency tradeoffs** in hybrid search \\[94\\]\\[148\\].\n\n2024 innovations targeted these via **graph-based architectures**, **hybrid search optimization**, and **hardware acceleration**, achieving measurable gains in industry benchmarks \\[41\\]\\[47\\]\\[53\\].\n\n**2\\. Key Technical Innovations**\n\n**2.1 GraphRAG: Solving Multi-Hop Reasoning**\n\nGraphRAG emerged as a paradigm shift from vector-only RAG, addressing the \"semantic gap\" via:\n\n**Knowledge Graph Integration**: Structured entity-relationship mapping using LLMs, compatible with graph databases (e.g., Neo4j) \\[67\\]\\[75\\].\n\n**Multi-Hop Traversal**: Path-based query resolution across interconnected data nodes, enabling 20% better performance on multi-hop benchmarks \\[71\\]\\[126\\].\n\n**Hallucination Reduction**: Grounding responses in graph-extracted facts lowered hallucination rates by 6% compared to baseline RAG \\[124\\]\\[193\\].\n\n**Impact**: 91.49% accuracy in complex domains (e.g., airport logistics) versus 84.84% for traditional RAG \\[245\\].\n\n**2.2 Hybrid Search (BlendedRAG): Balancing Accuracy and Latency**\n\nHybrid techniques dominated 2024 deployments by blending:\n\n**Vector + Sparse + Full-Text Search**: Combining BM25, dense retrievers, and keyword matching \\[85\\]\\[88\\].\n\n**Reciprocal Rank Fusion (RRF)**: Merging results to maximize recall@k while optimizing latency \\[146\\]\\[148\\].\n\n**Semantic Chunking**: BERT-based tokenization improved context relevance by 15% \\[86\\].\n\n**Performance**:\n\n72.7% pass rate versus 63.0% for naïve vector search \\[202\\].\n\n86.54% accuracy and 96.59% recall in enterprise deployments \\[204\\].\n\n**2.3 Multi-Modal RAG: Cross-Format Understanding**\n\nVision-and-Language Models (VLMs) enabled joint image-text retrieval:\n\n**Cross-Modal Alignment**: Embeddings linking visual and textual concepts \\[12\\].\n\n**Application Expansion**: Enhanced use cases in healthcare (medical imaging) and e-commerce \\[4\\]\\[55\\].\n\n_Limitation_: Still exploratory, with maturity expected in 2025 \\[44\\].\n\n**2.4 Hardware Acceleration: Intel Xeon 6**\n\nOptimizations for vector databases addressed retrieval latency:\n\n**Scalable Vector Search (SVS)**: 7.3× faster search and 2.7× faster indexing versus competitors \\[170\\].\n\n**AMX/AVX-512 Extensions**: Accelerated INT8/BF16 inference, reducing distance-calculation latency \\[105\\]\\[115\\].\n\n**Throughput Gains**: 2.3× higher throughput versus 5th-gen Xeon processors \\[101\\].\n\n**Benchmark Impact**:\n\n2.35× better knowledge-retrieval performance in RAG pipelines \\[101\\].\n\n10% faster Redis vector search \\[109\\].\n\n**3\\. Pain Point Mitigation: Performance Benchmarks**\n\n**3.1 Latency Reduction**\n\n**Hybrid Search**: Sub-100ms response times via caching and optimized transformers \\[148\\]\\[343\\].\n\n**Hardware Gains**: Xeon 6’s 84–2.71× indexing speedup for 45M–100M vectors \\[177\\].\n\n**ANN Benchmarks**: SIFT-1M queries processed 2.6× faster via SVS \\[290\\].\n\n**3.2 Hallucination Control**\n\n**GraphRAG**: 6% lower hallucination rates using structured knowledge graphs \\[124\\].\n\n**HotpotQA Improvement**: Enhanced precision in multi-hop QA, though direct benchmarks were scarce \\[193\\]\\[245\\].\n\n**3.3 Accuracy-Recall Tradeoffs**\n\nHybrid search achieved:\n\n**Recall@k**: 96.59% at optimal latency thresholds \\[204\\].\n\n**Three-Way Recall**: Best-in-class results via vector + sparse + full-text fusion \\[209\\].\n\n**4\\. Enterprise Deployment Metrics**\n\n**Error Reduction**: 50% lower AI-generated errors \\[47\\].\n\n**Efficiency**: 80% process acceleration in insurance workflows \\[53\\].\n\n**Adoption**: 51% enterprise usage, up from 31% in 2023 \\[58\\].\n\n**Cost**: 1.5× TCO advantage over NVIDIA H100 SXM \\[114\\].\n\n**5\\. Remaining Challenges and Future Outlook**\n\n**Latency-Accuracy Curves**: Hybrid search requires fine-tuning for real-time applications \\[148\\].\n\n**Hallucination Benchmarks**: Limited HotpotQA comparisons for GraphRAG \\[193\\].\n\n**Multi-Modal Maturity**: Needs VLM refinement \\[44\\].\n\n**2025 Projections**: Query routing, self-evolving RAG, and hardened multi-modal systems \\[44\\]\\[334\\].\n\n**Conclusion**\n\n2024 marked RAG’s transition from labs to production, driven by **GraphRAG** for complex reasoning, **hybrid search** for balanced recall-latency, and **hardware accelerators** like Intel Xeon 6. While enterprises reaped 50–80% efficiency gains, challenges persist in real-time optimization and standardized hallucination metrics. The convergence of architectural innovation and hardware advances positions RAG as the backbone of enterprise AI through 2025.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[2\\. Vladimir Karpukhin, Barlas Oğuz et al. “Dense Passage Retrieval for Open-Domain Question Answering.” ArXiv](https://doi.org/10.18653/v1/2020.emnlp-main.550)\n\n[3\\. S. Robertson, H. Zaragoza. “The Probabilistic Relevance Framework: BM25 and Beyond.” Found. Trends Inf. Retr.](https://doi.org/10.1561/1500000019)\n\n[4\\. Top 5 trends in enterprise RAG in 2024 | Blog | Tonic.ai](https://www.tonic.ai/blog/top-5-trends-in-enterprise-rag-in-2024)\n\n[5\\. Top 12 AI Innovations to Watch in 2024 - GREY Journal](https://greyjournal.net/hustle/grow/top-12-ai-innovations-2024/)\n\n[6\\. Nelson F. Liu, Kevin Lin et al. “Lost in the Middle: How Language Models Use Long Contexts.” Transactions of the Association for Computational Linguistics](https://doi.org/10.1162/tacl_a_00638)\n\n[7\\. Yunfan Gao, Yun Xiong et al. “Retrieval-Augmented Generation for Large Language Models: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2312.10997)\n\n[8\\. RAG是什么，2024年改变世界的AI秘密](https://zh-cn.futuroprossimo.it/2024/01/cose-la-rag-il-segreto-ai-che-trasformera-il-2024/)\n\n[9\\. Context-Aware AI Conversational Agents with RAG Architecture](https://luciangruia.ro/wp-content/uploads/2024/06/AIDA-Context-Aware-AI-Conversational-Agents.pdf)\n\n[10\\. 技术前沿综述:RAG领域的重要进展与创新亮点-腾讯云开发者...](https://cloud.tencent.com/developer/article/2515640)\n\n[11\\. What is RAG, the secret of AI that will transform the world in 2024](https://en.futuroprossimo.it/2024/01/cose-la-rag-il-segreto-ai-che-trasformera-il-2024/)\n\n[12\\. 2024年12月30日AI周刊- AI 应用与实战](https://use-ai-app.com/december-30-2024-ai-weekly/)\n\n[13\\. The Rise and Evolution of RAG in 2024 A Year in Review](https://ragflow.io/blog/the-rise-and-evolution-of-rag-in-2024-a-year-in-review)\n\n[14\\. 【RAG实战】24年迭代技术架构：RAG1.0 VS RAG2.0 ，RAG的新出路！大模型 | Agent | LLM | RAG实战](https://www.bilibili.com/video/av113706289664429?t=62)\n\n[15\\. A Comprehensive Survey of Retrieval-Augmented Generation (RAG ...](https://arxiv.org/abs/2410.12837)\n\n[16\\. 上海移远通信技术股份有限公司2023年半年度报告](https://www.sse.com.cn/disclosure/listedinfo/announcement/c/new/2023-08-15/603236_20230815_DGM9.pdf)\n\n[17\\. 2024 AI+研发数字峰会](https://aidd.vip/resources/upload/a7844a45d8ab55e/file/%E4%BD%95%E5%AE%B6%E6%97%8B-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%E4%BB%A3%E4%B8%8B%E7%9A%84%E4%BC%81%E4%B8%9A%E6%99%BA%E8%83%BD%E5%90%88%E8%A7%84%E9%A3%8E%E9%99%A9%E9%A2%84%E9%98%B2%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%BA%94%E5%AF%B9-%E5%B7%B2ok.pdf)\n\n[18\\. SAAB Annual & Sustainability Report 2024](https://www.saab.com/globalassets/corporate/corporate-governance/annual-general-meeting/2025/en-post/appendix-4-a-saab-annual-and-sustainability-report--2024.pdf)\n\n[19\\. 万字长文梳理2024 年的RAG - AI TNT](https://aitntnews.com/newDetail.html?newId=9911)\n\n[21\\. RETRIEVAL AUGMENTED PROMPT OPTIMIZATION](https://openreview.net/pdf?id=UOaCKgeNQU)\n\n[22\\. Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in Retrieval-Augmented Generation Systems](https://arxiv.org/pdf/2409.19804)\n\n[23\\. MES-RAG: Bringing Multi-modal, Entity-Storage, and Secure Enhancements to RAG](https://arxiv.org/pdf/2503.13563)\n\n[24\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[25\\. Dynamic and Parametric Retrieval-Augmented Generation](https://arxiv.org/html/2506.06704v1)\n\n[26\\. Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems](https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA240748)\n\n[27\\. Knowledge Graph-Guided Retrieval Augmented Generation](https://arxiv.org/pdf/2502.06864)\n\n[28\\. 2024 KDD Cup CRAG Workshop: UM6P Team Technical Report](https://openreview.net/pdf?id=k1cnnNbB3f)\n\n[29\\. ReDeEP: DETECTING HALLUCINATION IN RETRIEVAL-AUGMENTED GENERATION VIA MECHANISTIC INTERPRETABILITY](https://openreview.net/pdf/3e777fd177a0af576eb744f90970c9e5677e7c1c.pdf)\n\n[30\\. A Survey on Hallucination in Large Language and Foundation Models](https://www.preprints.org/frontend/manuscript/c5f20698cf44a95d88e568ddde2066e0/download_pub)\n\n[31\\. 2023 AI Overview from an AI CEO (with 2024 Predictions) - Leiga](https://www.leiga.com/post/2023-ai-overview-from-an-ai-ceo-with-2024-predictions)\n\n[32\\. Retrieval Augmented Generation Optimizations](https://www.theseus.fi/bitstream/10024/874901/2/Rolle_Robin.pdf)\n\n[33\\. Personalization of Large Language Models: A Survey](https://openreview.net/pdf/799ee5b188314b068e5c91750e8af2fe5be028ac.pdf)\n\n[34\\. C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System](https://arxiv.org/pdf/2412.13163)\n\n[35\\. Progression of Retrieval Augmented Generation (RAG) Systems | Towards AI](https://towardsai.net/p/machine-learning/progression-of-retrieval-augmented-generation-rag-systems)\n\n[36\\. TC-RAG: Turing-Complete RAG’s Case study on Medical LLM Systems](https://arxiv.org/pdf/2408.09199)\n\n[37\\. RAG vs. GraphRAG: A Systematic Evaluation and Key Insights](https://arxiv.org/pdf/2502.11371)\n\n[38\\. Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation](https://openreview.net/pdf?id=IMNnyrC0Ky)\n\n[39\\. Hamin Koo, Minseon Kim et al. “Optimizing Query Generation for Enhanced Document Retrieval in RAG.”](https://arxiv.org/abs/2407.12325)\n\n[40\\. Lynx: An Open Source Hallucination Evaluation Model](https://www.yiyibooks.cn/__src__/arxiv/2407.08488v1/index.html)\n\n[41\\. Performance of Retrieval-Augmented Generation (RAG) on Pharmaceutical Documents](https://intuitionlabs.ai/pdfs/performance-of-retrieval-augmented-generation-rag-on-pharmaceutical-documents.pdf)\n\n[42\\. Enterprise RAG Predictions for 2025](https://www.vectara.com/blog/top-enterprise-rag-predictions)\n\n[43\\. The Rise and Evolution of RAG in 2024 A Year in Review](https://ragflow.io/blog/the-rise-and-evolution-of-rag-in-2024-a-year-in-review)\n\n[44\\. Top 5 trends in enterprise RAG in 2024 | Blog | Tonic.ai](https://www.tonic.ai/blog/top-5-trends-in-enterprise-rag-in-2024)\n\n[45\\. 上海移远通信技术股份有限公司2023年半年度报告](https://www.sse.com.cn/disclosure/listedinfo/announcement/c/new/2023-08-15/603236_20230815_DGM9.pdf)\n\n[46\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[47\\. 5 AI Advancements You Might Have Missed in 2024](https://outter.co/blog/ai-advancements-in-2024)\n\n[48\\. Lianmin Zheng, Wei-Lin Chiang et al. “Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.” ArXiv](https://doi.org/10.48550/arXiv.2306.05685)\n\n[49\\. Top 12 AI Innovations to Watch in 2024 - GREY Journal](https://greyjournal.net/hustle/grow/top-12-ai-innovations-2024/)\n\n[50\\. Yunfan Gao, Yun Xiong et al. “Retrieval-Augmented Generation for Large Language Models: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2312.10997)\n\n[51\\. Evaluating RAG performance: Metrics and benchmarks](https://www.getmaxim.ai/blog/rag-evaluation-metrics/)\n\n[52\\. Nelson F. Liu, Kevin Lin et al. “Lost in the Middle: How Language Models Use Long Contexts.” Transactions of the Association for Computational Linguistics](https://doi.org/10.1162/tacl_a_00638)\n\n[53\\. The South African Insurance Industry Survey 2024](https://assets.kpmg.com/content/dam/kpmg/za/pdf/2024/Insurance%20Survey%202024.med%20resf.pdf)\n\n[54\\. 梳理2024年的RAG](https://www.cnblogs.com/ExMan/p/18725085)\n\n[55\\. 2024年度AI报告(五)：RAG 行业的崛起与演进 - 每时AI](https://mmssai.com/archives/8342)\n\n[56\\. What is RAG, the secret of AI that will transform the world in 2024](https://en.futuroprossimo.it/2024/01/cose-la-rag-il-segreto-ai-che-trasformera-il-2024/)\n\n[57\\. 2023 Annual Report](https://global.supcon.com/bocupload/2024/04/09/17126296593537r3t2k.pdf)\n\n[58\\. 2024: The State of Generative AI in the Enterprise](https://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise/)\n\n[59\\. Jiawei Chen, Hongyu Lin et al. “Benchmarking Large Language Models in Retrieval-Augmented Generation.” AAAI Conference on Artificial Intelligence](https://doi.org/10.48550/arXiv.2309.01431)\n\n[60\\. Optimizing RAG Performance and TCO](https://cdrdv2-public.intel.com/833732/Optimizing%20RAG%20Performance%20and%20TCO.pdf)\n\n[61\\. GraphRAG: Enhancing Retrieval with Knowledge Graph ...](http://dotzlaw.com/ai-2/graphrag-enhancing-retrieval-with-knowledge-graph-intelligence/)\n\n[62\\. RAG vs. GraphRAG: A Systematic Evaluation and Key Insights](https://openreview.net/pdf?id=K6N6gCCYcb)\n\n[63\\. A Systematic Evaluation and Key Insights - Moonlight](https://www.themoonlight.io/de/review/rag-vs-graphrag-a-systematic-evaluation-and-key-insights)\n\n[64\\. Navigating the Nuances of GraphRAG vs. RAG](https://thenewstack.io/navigating-the-nuances-of-graphrag-vs-rag/)\n\n[65\\. Microsoft GraphRAG: Revolutionizing Knowledge Graph ...](https://ragaboutit.com/microsoft-graphrag-revolutionizing-knowledge-graph-processing-for-ai/)\n\n[66\\. GRAFT: Graph Retrieval Augmented Fine Tuning for Multi-Hop Query Summarization](https://web.stanford.edu/class/cs224n/final-reports/256724569.pdf)\n\n[67\\. Graph RAG vs traditional RAG: A comparative overview](https://www.ankursnewsletter.com/p/graph-rag-vs-traditional-rag-a-comparative#:~:text=Graph%20RAG%20enhances%20factual%20accuracy,to%20more%20contextually%20accurate%20responses.)\n\n[68\\. Improve Your RAG Performance with Graph-Based AI](https://cdn.prod.website-files.com/62e2b9ca46af9a61f6c19666/66e96a5a1147077b7648bde4_GraphRAG%20Lettria%20White%20Paper_compressed.pdf)\n\n[69\\. Nils Reimers, Iryna Gurevych. “Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D19-1410)\n\n[70\\. RAG vs GraphRAG - DEV Community](https://dev.to/angu10/rag-vs-graphrag-5671#:~:text=Cons%20of%20GraphRAG,-Increased%20complexity:%20Building&text=Higher%20computational%20requirements:%20Graph%20operations,-consuming%20and%20error-prone.)\n\n[71\\. RAG vs Graph RAG: Key Technical Differences](https://www.hashstudioz.com/blog/difference-between-rag-and-graph-rag-a-technical-perspective/)\n\n[72\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[73\\. A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models](https://arxiv.org/pdf/2501.13958)\n\n[74\\. LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation for Design Space Exploration](http://www.arxiv.org/pdf/2411.05844)\n\n[75\\. GraphRAG vs. Baseline RAG: Solving Multi-Hop Reasoning in LLMs - GenUI](https://www.genui.com/resources/graphrag-vs.-traditional-rag-solving-multi-hop-reasoning-in-llms#:~:text=By%20building%20a%20knowledge%20graph,short%20in%20more%20complex%20scenarios.)\n\n[76\\. G2RAG: a GoT-perspective Graph Retrieval-Augmented Generation Paradigm](https://openreview.net/pdf/b58dd2d28f87e02f0c0627823817bae411410e57.pdf)\n\n[81\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[82\\. Retrieval Augmented Generation Optimizations](https://www.theseus.fi/bitstream/10024/874901/2/Rolle_Robin.pdf)\n\n[83\\. RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems](https://arxiv.org/pdf/2412.12322)\n\n[84\\. Advanced RAG Techniques: Boost Accuracy & Efficiency](https://www.chitika.com/advanced-rag-techniques-guide/)\n\n[85\\. Contextual Augmentation in Artificial Intelligence](https://digitalcommons.georgiasouthern.edu/cgi/viewcontent.cgi?article=4179&context=etd)\n\n[86\\. Advanced Chunking and Search Methods for Improved Retrieval-Augmented Generation (RAG) System Performance in E-Learning](https://openaccess-api.cms-conferences.org/articles/download/978-1-964867-35-9_194)\n\n[87\\. Mastering RAG: Choosing the Perfect Vector Database](https://www.rungalileo.io/blog/mastering-rag-choosing-the-perfect-vector-database)\n\n[88\\. A Survey on Retrieval-Augmented Generation (RAG) Models: Recent Advances and Challenges](https://thegrenze.com/pages/servej.php?fn=109.pdf&name=A%20Survey%20on%20Retrieval-Augmented%20Generation%20%28RAG%29Models:%20Recent%20Advances%20and%20Challenges&id=3943&association=GRENZE&journal=GIJET&year=2025&volume=11&issue=1)\n\n[89\\. Efficient and verifiable responses using Retrieval Augmented Generation (RAG)](https://cdn.prod.website-files.com/64f74cfcd963ce769566a687/66e9d5eb11af84cefe4d5da2_AI_ML_RFP_Paper.pdf)\n\n[90\\. Reducing RAG Pipeline Latency for Real-Time Voice ...](https://developer.vonage.com/en/blog/reducing-rag-pipeline-latency-for-real-time-voice-conversations)\n\n[91\\. RAG in 2025: Smarter Retrieval and Real-Time Responses](https://dataforest.ai/blog/rag-in-2025-smarter-retrieval-and-real-time-responses)\n\n[92\\. Empowering Automotive Software Development with LLM-RAG Integration: A study on leveraging the RAG-framework for AUTOSAR and automotive safety standards and specifications](https://gupea.ub.gu.se/bitstream/handle/2077/83663/CSE%2024-09%20MK%20OB.pdf?sequence=1)\n\n[93\\. Advanced Search Algorithms for LLMs in Large-Scale ...](https://www.rohan-paul.com/p/advanced-search-algorithms-for-llms-118)\n\n[94\\. Implementing Hybrid Retrieval (BM25 + FAISS) in RAG](https://www.chitika.com/hybrid-retrieval-rag/)\n\n[95\\. Advances in Signal Processing and Artificial Intelligence: Proceedings of the 7th International Conference on Advances in Signal Processing and Artificial Intelligence (ASPAI' 2025)](https://sensorsportal.com/ASPAI_2025/ASPAI_2025_Proceedings.pdf)\n\n[101\\. Optimizing RAG Performance and TCO](https://cdrdv2-public.intel.com/833732/Optimizing%20RAG%20Performance%20and%20TCO.pdf)\n\n[102\\. The Enterprise AI Reality Check: Why Vector Database ...](https://ragaboutit.com/the-enterprise-ai-reality-check-why-vector-database-choice-makes-or-breaks-your-rag-implementation/)\n\n[103\\. Maximizing RAG efficiency: A comparative analysis of RAG methods](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/D7B259BCD35586E04358DF06006E0A85/S2977042424000530a.pdf/div-class-title-maximizing-rag-efficiency-a-comparative-analysis-of-rag-methods-div.pdf)\n\n[104\\. V. Lee, Changkyu Kim et al. “Debunking the 100X GPU vs. CPU myth: an evaluation of throughput computing on CPU and GPU.” Proceedings of the 37th annual international symposium on Computer architecture](https://doi.org/10.1145/1815961.1816021)\n\n[105\\. Solution Brief | Reduce Costs and Improve Performance per Dollar in the Cloud with Intel Xeon 6 Processors](https://cdrdv2-public.intel.com/838620/Intel%20Xeon%206%20with%20P-cores%20for%20the%20Cloud.pdf)\n\n[106\\. Benchmark Vector Database Performance: Techniques & Insights](https://zilliz.com/learn/benchmark-vector-database-performance-techniques-and-insights)\n\n[107\\. DataStax与NVIDIA合作集成NIM和NeMo Retriever微服务以加速AI模型部署](https://preview.datastax.com/blog/datastax-nvidia-collaborate-to-deliver-genai-applications-with-fast-embeddings)\n\n[108\\. Introducing Intel® Xeon® 6 with Performance Cores: The Powerhouse for Computing](https://community.intel.com/t5/Blogs/Tech-Innovation/Data-Center/Introducing-Intel-Xeon-6-with-Performance-Cores-The-Powerhouse/post/1633136)\n\n[109\\. Intel Debuts Midrange Xeon 6 CPUs To Fight AMD In ...](https://www.crn.com/news/components-peripherals/2025/intel-debuts-mid-range-xeon-6-cpus-to-fight-amd-in-enterprise-data-centers)\n\n[110\\. RAG Latency Optimization | Analysis & Reduction](https://apxml.com/courses/optimizing-rag-for-production/chapter-4-end-to-end-rag-performance/rag-latency-analysis-reduction)\n\n[111\\. 英特尔® 至强® 6 能效核处理器、银河麒麟云底座操作系统 V10、GBase 8a 数据库协同 提供新一代数据库系统部署参考](https://www.intel.cn/content/dam/www/central-libraries/cn/zh/documents/2024-06/24-cmf359-xeon-6-e-core-processors-kylin-cloud-base-operating-system-v10-and-gbase-8a-database-collaborate-to-provide-a-reference-for-deploying-next-generation-database-systems-white-paper.pdf)\n\n[112\\. The Intel® Xeon® 6 Product Family](https://cdrdv2-public.intel.com/845771/intel-xeon-6-product-brief.pdf)\n\n[113\\. Weiping Yu, Ningyi Liao et al. “RAGDoll: Efficient Offloading-based Online RAG System on a Single GPU.”](https://arxiv.org/abs/2504.15302)\n\n[114\\. Vision 2024 - 2 | Performance Index](http://edc.intel.com/content/www/cn/zh/products/performance/benchmarks/vision-2024/)\n\n[115\\. The Intel® Xeon® 6 Processor Family](https://www.megware.com/fileadmin/user_upload/LandingPage%20Intel/intel-xeon-6-product-brief.pdf)\n\n[116\\. Intel Xeon 6 Processors](https://download.intel.com/newsroom/2024/client-computing/Xeon-6-Fact-Sheet.pdf)\n\n[117\\. NEW LEVELS OF OPTIMIZATION ARE NOW AVAILABLE WITH SUPERMICRO X14 SERVERS FOR AI, HPC, ENTERPRISE, AND EDGE WORKLOADS, INCORPORATING INTEL® XEON® 6 SERIES PROCESSORS](https://www.supermicro.org.cn/white_paper/white_paper_X14_Servers.pdf)\n\n[118\\. Vector Databases: I/O Characteristics](https://community.netapp.com/t5/Tech-ONTAP-Blogs/Vector-Databases-I-O-Characteristics/ba-p/456488)\n\n[119\\. Adriano Augusto, R. Conforti et al. “Automated Discovery of Process Models from Event Logs: Review and Benchmark.” IEEE Transactions on Knowledge and Data Engineering](https://doi.org/10.1109/tkde.2018.2841877)\n\n[121\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[122\\. RAG vs. GraphRAG: A Systematic Evaluation and Key Insights](https://arxiv.org/pdf/2502.11371)\n\n[123\\. RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph Learning Integration](https://arxiv.org/pdf/2503.13514)\n\n[124\\. GraphRAG: Leveraging Graph-Based Efficiency to Minimize Hallucinations in LLM-Driven RAG for Finance Data](https://hal.science/hal-04907346/document)\n\n[125\\. Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain](https://publikationen.bibliothek.kit.edu/1000181417/159585060)\n\n[126\\. GraphRAG vs. Baseline RAG: Solving Multi-Hop Reasoning in LLMs - GenUI](https://www.genui.com/resources/graphrag-vs.-traditional-rag-solving-multi-hop-reasoning-in-llms#:~:text=Our%20experiments%20show%20that%20GraphRAG,where%20traditional%20RAG%20models%20struggle.)\n\n[127\\. Benchmarking Hallucination Detection Methods in RAG](https://cleanlab.ai/blog/rag-tlm-hallucination-benchmarking/)\n\n[128\\. Understanding GraphRAG: A Comparison with RAG](https://www.capestart.com/resources/blog/what-is-graphrag-is-it-better-than-rag/)\n\n[129\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[130\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[131\\. Graph RAG vs traditional RAG: A comparative overview](https://www.ankursnewsletter.com/p/graph-rag-vs-traditional-rag-a-comparative#:~:text=Graph%20RAG%20enhances%20factual%20accuracy,to%20more%20contextually%20accurate%20responses.)\n\n[132\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[133\\. 基于RAG 的问答系统用于情境化响应预测](https://yiyibooks.cn/__trs__/arxiv/2409.03708v1/index.html)\n\n[141\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[142\\. RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems](https://arxiv.org/pdf/2412.12322)\n\n[143\\. Empowering Automotive Software Development with LLM-RAG Integration: A study on leveraging the RAG-framework for AUTOSAR and automotive safety standards and specifications](https://gupea.ub.gu.se/bitstream/handle/2077/83663/CSE%2024-09%20MK%20OB.pdf?sequence=1)\n\n[144\\. Advanced Chunking and Search Methods for Improved Retrieval-Augmented Generation (RAG) System Performance in E-Learning](https://openaccess-api.cms-conferences.org/articles/download/978-1-964867-35-9_194)\n\n[145\\. Retrieval Augmented Generation Optimizations](https://www.theseus.fi/bitstream/10024/874901/2/Rolle_Robin.pdf)\n\n[146\\. Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain](https://publikationen.bibliothek.kit.edu/1000181417/159585060)\n\n[147\\. Advanced Search Algorithms for LLMs in Large-Scale ...](https://www.rohan-paul.com/p/advanced-search-algorithms-for-llms-118)\n\n[148\\. RAG in 2025: Smarter Retrieval and Real-Time Responses](https://dataforest.ai/blog/rag-in-2025-smarter-retrieval-and-real-time-responses)\n\n[149\\. Efficient and verifiable responses using Retrieval Augmented Generation (RAG)](https://cdn.prod.website-files.com/64f74cfcd963ce769566a687/66e9d5eb11af84cefe4d5da2_AI_ML_RFP_Paper.pdf)\n\n[150\\. ASSESSING ADVANCED RETRIEVAL-AUGMENTED GENERATION TECHNIQUES FOR QUESTION ANSWERING: A CASE STUDY ON GOVERNMENTAL SERVICES](https://open.metu.edu.tr/bitstream/handle/11511/113023/IS589%20-%20report%20-Abdullah%20Alzariqi.pdf)\n\n[151\\. What is Retrieval Augmented Generation (RAG)? - Elastic](https://www.elastic.co/what-is/retrieval-augmented-generation#:~:text=Retrieval%20augmented%20generation%20%28RAG%29%20definition,-Retrieval%20augmented%20generation&text=It%20combines%20a%20retrieval%20model,generates%20a%20readable%20text%20response.)\n\n[152\\. AutoRAG：用于优化检索增强生成管道的自动化框架](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_29/2410.20878.pdf)\n\n[153\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[154\\. T. Kwiatkowski, J. Palomaki et al. “Natural Questions: A Benchmark for Question Answering Research.” Transactions of the Association for Computational Linguistics](https://doi.org/10.1162/tacl_a_00276)\n\n[155\\. Vladimir Karpukhin, Barlas Oğuz et al. “Dense Passage Retrieval for Open-Domain Question Answering.” ArXiv](https://doi.org/10.18653/v1/2020.emnlp-main.550)\n\n[156\\. RAGProbe: An Automated Approach for Evaluating RAG Applications](https://arxiv.org/pdf/2409.19019)\n\n[157\\. Enabling Generative AI in the Enterprise with Retrieval Augmented Generation (RAG)](https://vespa.ai/vespa-content/uploads/2025/02/Managers-Guide-to-RAG.pdf)\n\n[161\\. Optimizing RAG Performance and TCO](https://cdrdv2-public.intel.com/833732/Optimizing%20RAG%20Performance%20and%20TCO.pdf)\n\n[162\\. RAGCache: Optimizing Retrieval-Augmented Generation ...](https://www.marktechpost.com/2024/11/10/ragcache-optimizing-retrieval-augmented-generation-with-dynamic-caching/)\n\n[163\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[164\\. Optimize Vector Databases, Enhance RAG-Driven Generative AI](https://milvus.io/blog/optimize-vector-databases-enhance-rag-driven-generative-ai.md)\n\n[165\\. Systematic Performance Optimization for Retrieval-Augmented Generation Serving](https://openreview.net/pdf/5565313f201a70f2ee8c2be077ea0098a271bae6.pdf)\n\n[166\\. Vladimir Karpukhin, Barlas Oğuz et al. “Dense Passage Retrieval for Open-Domain Question Answering.” ArXiv](https://doi.org/10.18653/v1/2020.emnlp-main.550)\n\n[167\\. Jeff Johnson, Matthijs Douze et al. “Billion-Scale Similarity Search with GPUs.” IEEE Transactions on Big Data](https://doi.org/10.1109/tbdata.2019.2921572)\n\n[168\\. H. Jégou, Matthijs Douze et al. “Product Quantization for Nearest Neighbor Search.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1109/TPAMI.2010.57)\n\n[169\\. Chao Jin, Zili Zhang et al. “RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation.” ArXiv](https://doi.org/10.48550/arXiv.2404.12457)\n\n[170\\. Introducing Intel® Xeon® 6 with Performance Cores: The Powerhouse for Computing](https://community.intel.com/t5/Blogs/Tech-Innovation/Data-Center/Introducing-Intel-Xeon-6-with-Performance-Cores-The-Powerhouse/post/1633136)\n\n[171\\. How to Implement Retrieval-Augmented Generation (RAG) - Intel](https://www.intel.com/content/www/us/en/goal/how-to-implement-rag.html)\n\n[172\\. Benchmarking LanceDB](https://blog.lancedb.com/benchmarking-lancedb-92b01032874a/)\n\n[173\\. Building Blocks of RAG with Intel](https://devpost-public.s3.amazonaws.com/Intel/building-blocks-of-rag-ebook-final.pdf)\n\n[174\\. Benchmarking results for vector databases - Redis](https://redis.io/blog/benchmarking-results-for-vector-databases/)\n\n[175\\. Accelerate Embeddings Model Server/Runtime for RAG on ...](https://www.intel.cn/content/www/cn/zh/developer/articles/technical/accel-embeddings-model-server-rt-rag-on-watsonx-ai.html)\n\n[176\\. SERAG: Self-Evolving RAG System for Query Optimization](https://viterbi-web.usc.edu/~sabek/pdf/25_workshop_serag.pdf)\n\n[177\\. Intel Sees 'Huge' AI Opportunities For Xeon—With And ...](https://www.crn.com/news/components-peripherals/2024/intel-sees-huge-ai-opportunities-for-xeon-with-and-without-nvidia)\n\n[178\\. The Secret Weapon: How SQL Vector Databases Skyrocket Your RAG Performance](https://myscale.com/blog/how-sql-vector-database-boosts-rag-performance/)\n\n[181\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[182\\. RAG vs. GraphRAG: A Systematic Evaluation and Key Insights](https://arxiv.org/pdf/2502.11371)\n\n[183\\. Hallucination Detection in Large Language Models with Metamorphic Relations](https://arxiv.org/pdf/2502.15844)\n\n[184\\. KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG](https://arxiv.org/pdf/2502.09304)\n\n[185\\. GraphRAG vs. Baseline RAG: Solving Multi-Hop Reasoning in LLMs - GenUI](https://www.genui.com/resources/graphrag-vs.-traditional-rag-solving-multi-hop-reasoning-in-llms#:~:text=Our%20experiments%20show%20that%20GraphRAG,where%20traditional%20RAG%20models%20struggle.)\n\n[186\\. GraphRAG-Bench: Challenging Domain-Specific ...](https://arxiv.org/html/2506.02404v1)\n\n[187\\. Zhilin Yang, Peng Qi et al. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D18-1259)\n\n[188\\. HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering](https://nlp.stanford.edu/pubs/yang2018hotpotqa.pdf)\n\n[189\\. Hyper-RAG: Combating LLM Hallucinations Using Hypergraph-Driven Retrieval-Augmented Generation](https://www.preprints.org/frontend/manuscript/4a6d7083e4f718a2a27ea06e7721ad22/download_pub)\n\n[190\\. Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art](https://arxiv.org/pdf/2403.16527)\n\n[191\\. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/pdf/2311.05232)\n\n[192\\. Pranav Rajpurkar, Jian Zhang et al. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D16-1264)\n\n[193\\. GraphRAG: Leveraging Graph-Based Efficiency to Minimize Hallucinations in LLM-Driven RAG for Finance Data](https://hal.science/hal-04907346/document)\n\n[194\\. Detect hallucinations for RAG-based systems](https://aws.amazon.com/blogs/machine-learning/detect-hallucinations-for-rag-based-systems/)\n\n[195\\. RAG-RL: Advancing Retrieval-Augmented Generation via ...](https://arxiv.org/html/2503.12759v1)\n\n[196\\. Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality](https://openreview.net/pdf/1c21a2ec2c083f070c80f84a165025b8319e470c.pdf)\n\n[201\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[202\\. RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems](https://arxiv.org/pdf/2412.12322)\n\n[203\\. Question-Based Retrieval using Atomic Units for Enterprise RAG](https://arxiv.org/pdf/2405.12363)\n\n[204\\. Efficient and verifiable responses using Retrieval Augmented Generation (RAG)](https://cdn.prod.website-files.com/64f74cfcd963ce769566a687/66e9d5eb11af84cefe4d5da2_AI_ML_RFP_Paper.pdf)\n\n[205\\. DeepSeek’s Hybrid Model: Optimizing Enterprise RAG St...](https://chitika.com/deepseek-enterprise-rag-strategy/)\n\n[206\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[207\\. RAG in 2025: Smarter Retrieval and Real-Time Responses](https://dataforest.ai/blog/rag-in-2025-smarter-retrieval-and-real-time-responses)\n\n[208\\. Systematic Performance Optimization for Retrieval-Augmented Generation Serving](https://openreview.net/pdf/5565313f201a70f2ee8c2be077ea0098a271bae6.pdf)\n\n[209\\. The Rise and Evolution of RAG in 2024 A Year in Review](https://ragflow.io/blog/the-rise-and-evolution-of-rag-in-2024-a-year-in-review)\n\n[210\\. Retrieval Augmented Generation Optimizations](https://www.theseus.fi/bitstream/10024/874901/2/Rolle_Robin.pdf)\n\n[211\\. Advanced Search Algorithms for LLMs in Large-Scale ...](https://www.rohan-paul.com/p/advanced-search-algorithms-for-llms-118)\n\n[212\\. Vladimir Karpukhin, Barlas Oğuz et al. “Dense Passage Retrieval for Open-Domain Question Answering.” ArXiv](https://doi.org/10.18653/v1/2020.emnlp-main.550)\n\n[213\\. S. Robertson, H. Zaragoza. “The Probabilistic Relevance Framework: BM25 and Beyond.” Found. Trends Inf. Retr.](https://doi.org/10.1561/1500000019)\n\n[214\\. Enabling Generative AI in the Enterprise with Retrieval Augmented Generation (RAG)](https://vespa.ai/vespa-content/uploads/2025/02/Managers-Guide-to-RAG.pdf)\n\n[215\\. KDD Cup CRAG competition: Systems, Findings and Learnings](http://sites.computer.org/debull/A24dec/p163.pdf)\n\n[221\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[222\\. CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in RAG Systems](https://arxiv.org/pdf/2505.01164)\n\n[223\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[224\\. Enterprise AI: Generative AI & Large Language Models for Enterprise Partner Enablement Package](https://cdrdv2-public.intel.com/817880/Enterprise%20AI%20Partner%20Enablement%20Package%20-%20Technical%20Overview.pdf)\n\n[225\\. Vladimir Karpukhin, Barlas Oğuz et al. “Dense Passage Retrieval for Open-Domain Question Answering.” ArXiv](https://doi.org/10.18653/v1/2020.emnlp-main.550)\n\n[226\\. Jeff Johnson, Matthijs Douze et al. “Billion-Scale Similarity Search with GPUs.” IEEE Transactions on Big Data](https://doi.org/10.1109/tbdata.2019.2921572)\n\n[227\\. Optimizing RAG Performance and TCO](https://cdrdv2-public.intel.com/833732/Optimizing%20RAG%20Performance%20and%20TCO.pdf)\n\n[228\\. Comparative Analysis of Retrieval Augmented Generator and Traditional Large Language Models](https://repositum.tuwien.at/bitstream/20.500.12708/202324/1/Oroz%20Tin%20-%202024%20-%20Comparative%20Analysis%20of%20Retrieval%20Augmented%20Generator%20and...pdf)\n\n[229\\. O. Khattab, M. Zaharia. “ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT.” Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval](https://doi.org/10.1145/3397271.3401075)\n\n[230\\. Benchmarking Postgres Vector Search approaches](https://tembo.io/blog/postgres-vector-search-pgvector-and-lantern/)\n\n[231\\. Introducing Intel® Xeon® 6 with Performance Cores](https://community.intel.com/t5/Blogs/Tech-Innovation/Data-Center/Introducing-Intel-Xeon-6-with-Performance-Cores-The-Powerhouse/post/1633136?profile.language=zh-TW)\n\n[232\\. Accelerating Vector Search: Fine-Tuning GPU Index ...](https://nexus.10lun.com/post/post_p4279a859ffc54e3f816c31770eb41a69)\n\n[233\\. SERAG: Self-Evolving RAG System for Query Optimization](https://viterbi-web.usc.edu/~sabek/pdf/25_workshop_serag.pdf)\n\n[234\\. RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving](https://arxiv.org/pdf/2503.14649)\n\n[235\\. How to Implement Retrieval-Augmented Generation (RAG) - Intel](https://www.intel.com/content/www/us/en/goal/how-to-implement-rag.html)\n\n[236\\. Wenqi Jiang, Hang Hu et al. “Accelerating Graph-based Vector Search via Delayed-Synchronization Traversal.” ArXiv](https://doi.org/10.48550/arXiv.2406.12385)\n\n[237\\. Intel's New CPU Powers Faster Vector Search - Qdrant](https://qdrant.tech/blog/qdrant-cpu-intel-benchmark/)\n\n[238\\. Chao Jin, Zili Zhang et al. “RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation.” ArXiv](https://doi.org/10.48550/arXiv.2404.12457)\n\n[241\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[242\\. RAG vs. GraphRAG: A Systematic Evaluation and Key Insights](https://openreview.net/pdf?id=K6N6gCCYcb)\n\n[243\\. Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-Based Retrofitting](https://ojs.aaai.org/index.php/AAAI/article/view/29770/31326)\n\n[244\\. GraphRAG: Leveraging Graph-Based Efficiency to Minimize Hallucinations in LLM-Driven RAG for Finance Data](https://hal.science/hal-04907346/document)\n\n[245\\. Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain](https://publikationen.bibliothek.kit.edu/1000181417/159585060)\n\n[246\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[247\\. Graph RAG vs traditional RAG: A comparative overview](https://www.ankursnewsletter.com/p/graph-rag-vs-traditional-rag-a-comparative#:~:text=Structured%20knowledge%20representation:%20While%20traditional,contextually%20aware%20retrieval%20of%20information.)\n\n[248\\. GraphRAG vs. Baseline RAG: Solving Multi-Hop Reasoning in LLMs - GenUI](https://www.genui.com/resources/graphrag-vs.-traditional-rag-solving-multi-hop-reasoning-in-llms#:~:text=Our%20experiments%20show%20that%20GraphRAG,where%20traditional%20RAG%20models%20struggle.)\n\n[249\\. Tianyang Xu, Haojie Zheng et al. “NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes.”](https://arxiv.org/abs/2504.11544)\n\n[250\\. Enhancing Genomics Annotation with GraphRAG | Microsoft...](https://techcommunity.microsoft.com/t5/healthcare-and-life-sciences/enhancing-genomics-annotation-with-graphrag/ba-p/4410400)\n\n[251\\. Stephanie C. Lin, Jacob Hilton et al. “TruthfulQA: Measuring How Models Mimic Human Falsehoods.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/2022.acl-long.229)\n\n[252\\. Yunfan Gao, Yun Xiong et al. “Retrieval-Augmented Generation for Large Language Models: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2312.10997)\n\n[253\\. 7 RAG benchmarks](https://www.evidentlyai.com/blog/rag-benchmarks)\n\n[254\\. Alex Troy Mallen, Akari Asai et al. “When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/2023.acl-long.546)\n\n[255\\. Leveraging Graph-Based Efficiency to Minimize ...](https://aclanthology.org/2025.genaik-1.6/)\n\n[261\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[262\\. RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems](https://arxiv.org/pdf/2412.12322)\n\n[263\\. Nils Reimers, Iryna Gurevych. “Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D19-1410)\n\n[264\\. RAG in 2025: Smarter Retrieval and Real-Time Responses](https://dataforest.ai/blog/rag-in-2025-smarter-retrieval-and-real-time-responses)\n\n[265\\. Maximizing RAG efficiency: A comparative analysis of RAG methods](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/D7B259BCD35586E04358DF06006E0A85/S2977042424000530a.pdf/maximizing_rag_efficiency_a_comparative_analysis_of_rag_methods.pdf)\n\n[266\\. Chart Retrieval for Arguments](https://downloads.webis.de/theses/papers/sharma_2024.pdf)\n\n[267\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[268\\. Systematic Performance Optimization for Retrieval-Augmented Generation Serving](https://openreview.net/pdf/5565313f201a70f2ee8c2be077ea0098a271bae6.pdf)\n\n[269\\. Building Production-Ready RAG Systems](https://zenvanriel.nl/ai-engineer-blog/production-ready-rag-systems/)\n\n[270\\. The Best Pre-Built Enterprise RAG Platforms in 2025 - Firecrawl](https://www.firecrawl.dev/blog/best-enterprise-rag-platforms-2025#:~:text=By%202025,%20the%20adoption%20of,effectively%20from%20the%20knowledge%20base.)\n\n[271\\. S. Robertson, H. Zaragoza. “The Probabilistic Relevance Framework: BM25 and Beyond.” Found. Trends Inf. Retr.](https://doi.org/10.1561/1500000019)\n\n[272\\. The Rise and Evolution of RAG in 2024 A Year in Review](https://ragflow.io/blog/the-rise-and-evolution-of-rag-in-2024-a-year-in-review)\n\n[273\\. What is Retrieval Augmented Generation(RAG) in 2024?](https://www.glean.com/blog/rag-revolutionizing-ai-2024)\n\n[274\\. Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems](https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA240748)\n\n[275\\. Cache-Augmented Generation in RAG Pipelines: Fast and Memory-Efficient Approach to Multi-Agent Knowledge Query Systems](https://aquila.usm.edu/cgi/viewcontent.cgi?article=2011&context=honors_theses)\n\n[276\\. MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation](https://arxiv.org/pdf/2502.17163)\n\n[277\\. Retrieval-augmented Generation Realized: Strategic & Technical Insights for Industrial Applications](https://www.appliedai.de/assets/files/retrieval-augmented-generation-realized/AppliedAI_White_Paper_Retrieval-augmented-Generation-Realized_FINAL_20240618.pdf)\n\n[281\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[282\\. Indexing 1M vectors · facebookresearch/faiss Wiki · GitHub](https://github.com/facebookresearch/faiss/wiki/Indexing-1M-vectors)\n\n[283\\. CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in RAG Systems](https://arxiv.org/pdf/2505.01164)\n\n[284\\. Introducing Intel® Xeon® 6 with Performance Cores](https://community.intel.com/t5/Blogs/Tech-Innovation/Data-Center/Introducing-Intel-Xeon-6-with-Performance-Cores-The-Powerhouse/post/1633136?profile.language=zh-TW)\n\n[285\\. How to Implement Retrieval-Augmented Generation (RAG) - Intel](https://www.intel.com/content/www/us/en/goal/how-to-implement-rag.html)\n\n[286\\. Vladimir Karpukhin, Barlas Oğuz et al. “Dense Passage Retrieval for Open-Domain Question Answering.” ArXiv](https://doi.org/10.18653/v1/2020.emnlp-main.550)\n\n[287\\. Jeff Johnson, Matthijs Douze et al. “Billion-Scale Similarity Search with GPUs.” IEEE Transactions on Big Data](https://doi.org/10.1109/tbdata.2019.2921572)\n\n[288\\. Efficient Large-scale Approximate Nearest Neighbor Search on OpenCL FPGA](https://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3028.pdf)\n\n[289\\. H. Jégou, Matthijs Douze et al. “Product Quantization for Nearest Neighbor Search.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1109/TPAMI.2010.57)\n\n[290\\. Intel Debuts Midrange Xeon 6 CPUs To Fight AMD In ...](https://www.crn.com/news/components-peripherals/2025/intel-debuts-mid-range-xeon-6-cpus-to-fight-amd-in-enterprise-data-centers)\n\n[291\\. Optimizing RAG Performance and TCO](https://cdrdv2-public.intel.com/833732/Optimizing%20RAG%20Performance%20and%20TCO.pdf)\n\n[292\\. Intel® Gaudi® 3 AI Accelerator: Bringing Choice to Gen AI with Efficiency, Scalability, Performance](https://cdrdv2-public.intel.com/834416/Gaudi%203%20at%20ETT_9_15_24%20-%20Master%20version%209_15.pdf)\n\n[293\\. Yury Malkov, Dmitry A. Yashunin. “Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1109/TPAMI.2018.2889473)\n\n[294\\. Systematic Performance Optimization for Retrieval-Augmented Generation Serving](https://openreview.net/pdf/5565313f201a70f2ee8c2be077ea0098a271bae6.pdf)\n\n[295\\. TELERAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval](https://arxiv.org/pdf/2502.20969)\n\n[296\\. Intel Sees 'Huge' AI Opportunities For Xeon—With And ...](https://www.crn.com/news/components-peripherals/2024/intel-sees-huge-ai-opportunities-for-xeon-with-and-without-nvidia)\n\n[297\\. Yusuke Matsui, Ryota Hinami et al. “Reconfigurable Inverted Index.” Proceedings of the 26th ACM international conference on Multimedia](https://doi.org/10.1145/3240508.3240630)\n\n[298\\. Solution Brief | Reduce Costs and Improve Performance per Dollar in the Cloud with Intel Xeon 6 Processors](https://cdrdv2-public.intel.com/838620/Intel%20Xeon%206%20with%20P-cores%20for%20the%20Cloud.pdf)\n\n[299\\. Minjia Zhang, Jie Ren et al. “Exploiting Modern Hardware Architectures for High-Dimensional Vector Search at Speed and Scale.”](https://www.semanticscholar.org/paper/1ec83ef65a0c29f83cbfa05ca15987b4ec83ba54)\n\n[301\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[302\\. RAG vs. GraphRAG: A Systematic Evaluation and Key Insights](https://openreview.net/pdf?id=K6N6gCCYcb)\n\n[303\\. GRAFT: Graph Retrieval Augmented Fine Tuning for Multi-Hop Query Summarization](https://web.stanford.edu/class/cs224n/final-reports/256724569.pdf)\n\n[304\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[305\\. GraphRAG: Leveraging Graph-Based Efficiency to Minimize Hallucinations in LLM-Driven RAG for Finance Data](https://hal.science/hal-04907346v1/document)\n\n[306\\. Pranav Rajpurkar, Jian Zhang et al. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D16-1264)\n\n[307\\. Tianyang Xu, Haojie Zheng et al. “NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes.”](https://arxiv.org/abs/2504.11544)\n\n[308\\. T. Kwiatkowski, J. Palomaki et al. “Natural Questions: A Benchmark for Question Answering Research.” Transactions of the Association for Computational Linguistics](https://doi.org/10.1162/tacl_a_00276)\n\n[309\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[310\\. Enhancing Genomics Annotation with GraphRAG | Microsoft...](https://techcommunity.microsoft.com/t5/healthcare-and-life-sciences/enhancing-genomics-annotation-with-graphrag/ba-p/4410400)\n\n[311\\. Graph RAG vs traditional RAG: A comparative overview](https://www.ankursnewsletter.com/p/graph-rag-vs-traditional-rag-a-comparative#:~:text=Graph%20RAG%20enhances%20factual%20accuracy,to%20more%20contextually%20accurate%20responses.)\n\n[312\\. GraphRAG vs. Baseline RAG: Solving Multi-Hop Reasoning in LLMs - GenUI](https://www.genui.com/resources/graphrag-vs.-traditional-rag-solving-multi-hop-reasoning-in-llms#:~:text=Our%20experiments%20show%20that%20GraphRAG,where%20traditional%20RAG%20models%20struggle.)\n\n[313\\. Improving RAG performance: Introducing GraphRAG - Lettria](https://www.lettria.com/blogpost/improving-rag-performance-introducing-graphrag)\n\n[314\\. Understanding GraphRAG: A Comparison with RAG](https://stage.capestart.com/resources/blog/what-is-graphrag-is-it-better-than-rag/)\n\n[315\\. \\[Literature Review\\] RAG vs. GraphRAG: A Systematic ...](https://www.themoonlight.io/review/rag-vs-graphrag-a-systematic-evaluation-and-key-insights)\n\n[321\\. RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems](https://arxiv.org/pdf/2412.12322)\n\n[322\\. AI & Automation Trends 2025](https://marketing.auxis.com/hubfs/2025%20PDFs/Co%20Branded%20AI%20and%20Automation%20Trends%202025.pdf)\n\n[323\\. RAG in 2025: Smarter Retrieval and Real-Time Responses](https://dataforest.ai/blog/rag-in-2025-smarter-retrieval-and-real-time-responses)\n\n[324\\. Retrieval Augmented Generation Optimizations](https://www.theseus.fi/bitstream/10024/874901/2/Rolle_Robin.pdf)\n\n[325\\. Question-Based Retrieval using Atomic Units for Enterprise RAG](https://arxiv.org/pdf/2405.12363)\n\n[326\\. Maximizing RAG efficiency: A comparative analysis of RAG methods](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/D7B259BCD35586E04358DF06006E0A85/S2977042424000530a.pdf/maximizing_rag_efficiency_a_comparative_analysis_of_rag_methods.pdf)\n\n[327\\. Enabling Generative AI in the Enterprise with Retrieval Augmented Generation (RAG)](https://vespa.ai/vespa-content/uploads/2025/02/Managers-Guide-to-RAG.pdf)\n\n[328\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[329\\. RAGProbe: An Automated Approach for Evaluating RAG Applications](https://arxiv.org/pdf/2409.19019)\n\n[330\\. 2024年企业AI大模型应用落地白皮书](https://www.csia-jpw.com/UserFiles/Article/file/6386943509243757347159111.pdf)\n\n[331\\. What is Retrieval Augmented Generation (RAG)? - Elastic](https://www.elastic.co/what-is/retrieval-augmented-generation#:~:text=Retrieval%20augmented%20generation%20%28RAG%29%20definition,-Retrieval%20augmented%20generation&text=It%20combines%20a%20retrieval%20model,generates%20a%20readable%20text%20response.)\n\n[332\\. Nandan Thakur, Nils Reimers et al. “BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models.” ArXiv](https://arxiv.org/abs/2104.08663)\n\n[333\\. Yunfan Gao, Yun Xiong et al. “Retrieval-Augmented Generation for Large Language Models: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2312.10997)\n\n[334\\. Enterprise RAG Predictions for 2025](https://www.vectara.com/blog/top-enterprise-rag-predictions)\n\n[341\\. Towards High-throughput and Low-latency Billion-scale Vector Search via CPU/GPU Collaborative Filtering and Re-ranking](https://www.usenix.org/system/files/fast25-tian-bing.pdf)\n\n[342\\. RAFT ANN Benchmarks — raft 24.08.00 documentation](https://docs.rapids.ai/api/raft/stable/raft_ann_benchmarks/)\n\n[343\\. Scaling Vector Search with MongoDB Atlas Quantization & ...](https://www.mongodb.com/blog/post/technical/scaling-vector-search-mongodb-atlas-quantization-voyage-ai-embeddings)\n\n[344\\. Systematic Performance Optimization for Retrieval-Augmented Generation Serving](https://openreview.net/pdf/5565313f201a70f2ee8c2be077ea0098a271bae6.pdf)\n\n[345\\. Глава 9. Оценка RAG количественно и с использованием визуализаций](https://baguzin.ru/wp/wp-content/uploads/2025/01/Glava-9.-Otsenka-RAG-kolichestvenno-i-s-ispolzovaniem-vizualizatsij.docx)\n\n[346\\. RAG, AI Agents, and Agentic RAG: An In-Depth Review and Comparative ...](https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis#:~:text=Agentic%20RAG%20innovates%20the%20retrieval,and%20adaptability%20within%20complex%20environments.)\n\n[347\\. Jeff Johnson, Matthijs Douze et al. “Billion-Scale Similarity Search with GPUs.” IEEE Transactions on Big Data](https://doi.org/10.1109/tbdata.2019.2921572)\n\n[348\\. Benchmarking LanceDB](https://blog.lancedb.com/benchmarking-lancedb-92b01032874a/)\n\n[349\\. CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in RAG Systems](https://arxiv.org/pdf/2505.01164)\n\n[350\\. How to Implement Retrieval-Augmented Generation (RAG) - Intel](https://www.intel.com/content/www/us/en/goal/how-to-implement-rag.html)\n\n[351\\. Mayur Datar, Nicole Immorlica et al. “Locality-sensitive hashing scheme based on p-stable distributions.” SCG '04](https://doi.org/10.1145/997817.997857)\n\n[352\\. Optimizing RAG Performance and TCO](https://cdrdv2-public.intel.com/833732/Optimizing%20RAG%20Performance%20and%20TCO.pdf)\n\n[353\\. Accelerating Vector Search: Fine-Tuning GPU Index ...](https://nexus.10lun.com/post/post_p4279a859ffc54e3f816c31770eb41a69)\n\n[354\\. H. Jégou, Matthijs Douze et al. “Product Quantization for Nearest Neighbor Search.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1109/TPAMI.2010.57)\n\n[355\\. TELERAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval](https://arxiv.org/pdf/2502.20969)\n\n[356\\. Benchmark Vector Database Performance: Techniques & Insights](https://zilliz.com/learn/benchmark-vector-database-performance-techniques-and-insights)\n\n[357\\. Vector Search: Navigating Recall and Performance](https://opensourceconnections.com/blog/2025/02/27/vector-search-navigating-recall-and-performance/)"
  },
  {
    "id": 53,
    "question": "How is RAG (Retrieval-Augmented Generation) evolving, and what evidence suggests it will remain a core LLM enhancement technology rather than becoming obsolete?",
    "response": "**Research Report: The Evolution and Future Sustainability of Retrieval-Augmented Generation (RAG)**\n\n**Introduction: RAG as a Foundational LLM Enhancement**\n\nRetrieval-Augmented Generation (RAG) integrates external knowledge retrieval with generative language models to address core LLM limitations like hallucinations, outdated information, and lack of domain specificity. Since its emergence, RAG has evolved from simple retrieval-augmented frameworks to sophisticated architectures enabling real-time, context-aware AI systems (Web Pages 8, 43). As of 2025, RAG adoption spans 51% of enterprise AI applications—up from 31% in 2024—driven by its cost-effectiveness, data privacy advantages, and accuracy improvements in knowledge-intensive tasks (Web Pages 50, 52). This report analyzes RAG's technical evolution, comparative advantages over alternatives, scalability challenges, and evidence supporting its long-term viability.\n\n**I. Technical Evolution of RAG Architectures (2023–2025)**\n\n**A. Enhanced Retrieval Mechanisms**\n\n**Neural Retrievers & Vector Optimization**: Advanced neural retrievers using Maximum Inner Product Search (MIPS) and dense vector indexing reduce latency by 67% (150ms to <50ms) while improving semantic matching accuracy (Web Pages 7, 16, 65). Embedding filters prune redundant vectors, optimizing token budgeting and retrieval speed \\[5\\].\n\n**Multi-Modal Integration**: Emerging RAG variants (e.g., MRAG) support images, audio, and video retrieval, enabling applications like medical imaging analysis and industrial quality control \\[8\\].\n\n**Dynamic Knowledge Updates**: Real-time adaptation allows continuous knowledge-base refreshes, critical for domains like finance and healthcare \\[8\\].\n\n**B. Agentic and Modular Architectures**\n\n**Agentic RAG**: Frameworks like MA-RAG autonomously handle multi-document QA, improving diagnostic accuracy in healthcare by 24% for rare diseases (Web Pages 9, 69).\n\n**Modular Pipelines**: Components like query routers, resource rankers, and post-reading fact-checkers enable customizable workflows (e.g., Glean’s enterprise solutions) \\[11\\].\n\n**C. Efficiency and Hallucination Mitigation**\n\n**Transformer Optimization**: Parallel processing and retrieval caching reduce response times by 34.8% (Web Pages 20, 79).\n\n**Hallucination Reduction**: Retrieval of contextual passages cuts hallucination rates by 74% in observability applications \\[65\\].\n\n**II. Performance Benchmarks vs. Alternative LLM Enhancement Methods**\n\n**A. RAG vs. Fine-Tuning**\n\n**Accuracy & Cost**: RAG achieves 85%+ accuracy in domain-specific QA—comparable to fine-tuned models—but at 30–50% lower computational costs (Web Pages 24–25, 33). Its dynamic knowledge access outperforms fine-tuning in real-time applications (e.g., stock analysis).\n\n**Limitations**: RAG struggles with tasks requiring deep domain internalization (e.g., legal precedent synthesis), where fine-tuning excels (Web Pages 29, 35).\n\n**B. RAG vs. Prompt Engineering**\n\n**Hybrid Superiority**: Prompt engineering suffices for open-ended generation, but RAG adds 20% accuracy gains for fact-intensive tasks \\[30\\]. Combining both (e.g., prompt-guided retrieval) yields optimal results.\n\n**Resource Efficiency**: Prompt engineering avoids retrieval costs but fails for large-scale, real-time data needs \\[36\\].\n\n**C. Dominance of Hybrid Approaches**\n\n**Industry Trends**: 78% of enterprises combine RAG with fine-tuning/prompt engineering \\[30\\]. For example, domain-specific fine-tuning followed by RAG-augmented inference enhances accuracy in pharmaceutical R&D \\[56\\].\n\n**III. Evidence for RAG as a Sustainable Core Technology**\n\n**A. Industry Adoption**\n\n**Enterprise Integration**: Google (Vertex AI), NVIDIA (NIM Microservices), and IBM (watsonx) deploy RAG as core infrastructure for data-grounded applications \\[47\\].\n\n**Cross-Industry Use Cases**:\n\n**Healthcare**: Diagnostic accuracy improvements up to 20% \\[69\\].\n\n**Finance**: Fraud detection systems using RAG show 23% higher precision \\[78\\].\n\n**Energy**: Predictive maintenance tools reduce downtime by 41% \\[50\\].\n\n**B. Research and Development Momentum**\n\n**Explosive Publication Growth**: 200+ RAG-focused papers were published in 2024–2025, addressing gaps like multimodal retrieval and bias mitigation (Web Pages 41–42).\n\n**Open-Source Ecosystems**: Libraries like LangChain (Amazon) and LlamaIndex democratize RAG deployment \\[47\\].\n\n**C. Core Advantages Over Standalone LLMs**\n\n**Data Control & Privacy**: Enterprises favor RAG for keeping sensitive data in secured indexes rather than public models \\[50\\].\n\n**Regulatory Compliance**: Audit success rates reach 100% in RAG-enhanced systems vs. 75% in baselines \\[62\\].\n\n**IV. Scalability Limitations in Enterprise Deployments**\n\n**A. Critical Bottlenecks**\n\n**Retrieval Performance**: At scale, noisy datasets cause irrelevant retrievals, degrading accuracy by 12–18% (Web Pages 82, 100).\n\n**Computational Demands**: GPU/CPU bottlenecks arise during peak loads, necessitating solutions like hierarchical indexing and distributed retrieval (Web Pages 88, 96).\n\n**Context Window Constraints**: Limited context windows force trade-offs between comprehensiveness and relevance \\[99\\].\n\n**B. Emerging Solutions**\n\n**Infrastructure Optimization**: Vector databases like Pinecone and Weaviate reduce latency via approximate nearest-neighbor (ANN) algorithms \\[65\\].\n\n**Dynamic Chunking**: Adaptive text-splitting improves retrieval precision by 30% in large documents \\[82\\].\n\n**V. Competitive Landscape: RAG vs. Emerging Alternatives**\n\n**A. Non-RAG Enhancements from Major Labs**\n\n**OpenAI**: \"Reason Models\" (RMs) enhance in-context reasoning, but rely on RAG for factual grounding \\[229\\].\n\n**Google DeepMind**: Self-alignment techniques reduce fine-tuning costs but lack real-time knowledge updates \\[221\\].\n\n**Meta**: Llama 3’s 128K-token context window reduces retrieval needs but struggles with proprietary data \\[240\\].\n\n**B. No Near-Term Replacements**\n\n**Agentic Systems**: Though promising (e.g., SciAgents for biomedicine), they augment rather than replace RAG \\[161\\].\n\n**Small Language Models (SLMs)**: Optimized for edge devices but lack RAG’s dynamic knowledge access \\[166\\].\n\n**C. RAG’s Adaptive Evolution**\n\n**SAGE Frameworks**: Address RAG limitations via extensible \"generate-and-validate\" loops while retaining retrieval \\[113\\].\n\n**Multimodal Expansion**: MRAG’s image/audio support broadens applicability beyond text-only competitors \\[102\\].\n\n**VI. Addressing Contradictory Benchmarks and Deployment Risks**\n\n**A. Benchmark Discrepancies**\n\n**Methodological Variability**: Studies report accuracy from 55–93% due to dataset imbalances (e.g., niche vs. general corpora) and evaluation metrics (e.g., BLEU vs. context recall) (Web Pages 62, 182, 190).\n\n**Hallucination Metrics**: RAG reduces hallucinations by 74% in controlled tests but falls to 45% in noisy real-world data \\[65\\].\n\n**B. Production Risks**\n\n**Retrieval Failures**: 28% of RAG errors stem from missing content or improper chunk inclusion \\[207\\].\n\n**Mitigation Strategies**: Hybrid search + re-ranking (adopted by 60% of enterprises) cuts failures by 40% \\[143\\].\n\n**Conclusion: RAG as an Enduring Pillar of LLM Enhancement**\n\nRAG’s evolution demonstrates unparalleled adaptability—from multi-modal expansion to agentic autonomy—while maintaining core advantages in accuracy, cost, and data security. Its 51% enterprise adoption reflects irreplaceability in real-time, knowledge-intensive domains \\[50\\]. Although alternatives like fine-tuning and prompt engineering address niche needs, none match RAG’s balance of dynamic knowledge access and scalability. Ongoing innovations in vector retrieval, hybrid architectures, and evaluation frameworks (e.g., Rags) will solidify RAG as a foundational LLM enhancement through 2026 and beyond. Enterprises must invest in infrastructure upgrades and retrieval optimization to overcome scaling hurdles, but RAG’s trajectory points toward dominance, not obsolescence.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. 上海移远通信技术股份有限公司2023年半年度报告](https://www.sse.com.cn/disclosure/listedinfo/announcement/c/new/2023-08-15/603236_20230815_DGM9.pdf)\n\n[2\\. Advancing state of the art of Retrieval-augmented Generation (RAG)](https://phontron.com/class/anlp2024/assets/slides/anlp-25-akariasai.pdf)\n\n[3\\. 食品饮料行业AI转型白皮书](https://pdf.dfcfw.com/pdf/H3_AP202503071644137570_1.pdf?1741343346000.pdf)\n\n[4\\. The Blueprint to Building End-To-End Hybrid-Cloud AI Infrastructure](https://www.ciscolive.com/c/dam/r/ciscolive/global-event/docs/2024/pdf/BRKCOM-1008.pdf)\n\n[5\\. Maximizing RAG efficiency: A comparative analysis of RAG methods](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/D7B259BCD35586E04358DF06006E0A85/S2977042424000530a.pdf/maximizing_rag_efficiency_a_comparative_analysis_of_rag_methods.pdf)\n\n[6\\. Advanced RAG Techniques: an Illustrated Overview - Онлай...](https://tool.lu/index.php/ru_RU/article/5SG/preview)\n\n[7\\. Retrieval-Augmented Generation (RAG) for Knowledge-Intensive ...](https://orq.ai/blog/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks)\n\n[8\\. Enhancing RAG Performance Through Chunking and Text Splitting Techniques](https://ijsrcseit.com/index.php/home/article/download/CSEIT2410593/CSEIT2410593/615)\n\n[9\\. 13TH INTERNATIONAL CONFERENCE ON APPLICATION OF INFORMATION AND COMMUNICATION TECHNOLOGY AND STATISTICS IN ECONOMY AND EDUCATION ICAICTSEE – 2023](https://icaictsee.unwe.bg/past-conferences/ICAICTSEE-2023.pdf)\n\n[10\\. MES-RAG: Bringing Multi-modal, Entity-Storage, and Secure Enhancements to RAG](https://arxiv.org/pdf/2503.13563)\n\n[11\\. Yunxiao Shi, Xing Zi et al. “ERAGent: Enhancing Retrieval-Augmented Language Models with Improved Accuracy, Efficiency, and Personalization.” ArXiv](https://doi.org/10.48550/arXiv.2405.06683)\n\n[12\\. RAG in 2025: Smarter Retrieval and Real-Time Responses](https://dataforest.ai/blog/rag-in-2025-smarter-retrieval-and-real-time-responses)\n\n[13\\. How Do I Optimize the Dynamic Range of an FSW Signal and Spectrum Analyzer? A RAG Use Case Study in Wireless Test & Measurement: Retrieval Fine-Tuning and Tables as Images](https://www.appliedai.de/uploads/files/trustworthy-rag-in-wireless-test-measurement-retrieval-fine-tuning-and-tables-as-images/How-Do-I-Optimize-the-Dynamic-Range-of-an-FSW-Signal-and-Spectrum-Analyzer.pdf)\n\n[14\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[15\\. 企業生成AI檢索查詢 RAG 解決方案 規劃與實作](https://s.itho.me/ccms_slides/2024/7/8/2dc6c726-a35c-4645-9e7d-e074371c88fb.pdf)\n\n[16\\. Retrieval-Augmented Generation (RAG): Revolutionizing Enterprise AI](https://ingestai.io/blog/rag-in-enterprise-ai)\n\n[17\\. RETRIEVAL-AUGMENTED GENERATION: ARCHITECTURE, TECHNIQUES, AND EVALUATIONS](https://api.jomardpublishing.com/api/main/articles/view?source=storage/journals/journal-of-modern-technology-and-engineering/issues/pdf/2025/retrieval-augmented-generation-architecture-techniques-and-evaluations.pdf)\n\n[18\\. What is Retrieval Augmented Generation(RAG) in 2024?](https://www.glean.com/blog/rag-revolutionizing-ai-2024)\n\n[21\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[22\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[23\\. RAG vs fine-tuning vs. prompt engineering](https://www.ibm.com/think/topics/rag-vs-fine-tuning-vs-prompt-engineering)\n\n[24\\. RAG vs fine-tuning vs. prompt engineering - IBM](https://www.ibm.com/think/topics/rag-vs-fine-tuning-vs-prompt-engineering#:~:text=Prompt%20engineering%20optimizes%20input%20prompts,relevant%20data%20for%20greater%20accuracy.)\n\n[25\\. Actes de la 10e conférence nationale sur les Applications Pratiques de l'Intelligence Artificielle](https://hal.science/hal-04626412v1/file/APIA2024_Actes.pdf)\n\n[26\\. Prompt Engineering 快速入门+实战案例](https://www.53ai.com/news/tishicikuangjia/2025071647213.html)\n\n[27\\. Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems](https://arxiv.org/pdf/2502.18635)\n\n[28\\. Language Models for Materials Discovery and Sustainability: Progress, Challenges, and Opportunities](http://arxiv.org/pdf/2504.14849)\n\n[29\\. RAG vs Finetuning vs Prompt Engineering: A pragmatic view on LLM's ...](https://ethicalaiauthority.com/rag-vs-finetuning-vs-prompt-engineering-a-pragmatic-view-on-llm-implementation/)\n\n[30\\. Prompt Engineering vs Fine Tuning | Best LLM Strategy 2025](https://dextralabs.com/blog/prompt-engineering-vs-fine-tuning/)\n\n[31\\. Prompt Refinement or Fine-tuning? Best Practices for using LLMs in Computational Social Science Tasks](https://arxiv.org/pdf/2408.01346)\n\n[32\\. LLMs: RAG vs. Fine-Tuning](https://s3.eu-west-2.amazonaws.com/assets.winder.ai/blog/2024/240313_Presentation_RAGvsFineTuning/240313_Presentation_RAGvsFineTune.pdf)\n\n[33\\. 2025 Modern Data and AI Leader's Playbook](https://info.montecarlodata.com/hubfs/Assets%20-%20Guides%2C%20Ebooks%2C%20Reports/2025%20Modern%20Data%20Leaders%20Playbook.pdf)\n\n[34\\. RAG vs Fine-Tuning vs Prompt Engineering: And the Winner is...](https://www.k2view.com/blog/rag-vs-fine-tuning-vs-prompt-engineering/)\n\n[35\\. Generating Semantic Descriptions of Arbitrary Sentences with Large Language Models](https://bora.uib.no/bora-xmlui/bitstream/handle/11250/3141750/50076602.pdf?sequence=1&isAllowed=y)\n\n[36\\. RAG vs Fine-tuning vs Prompt Engineering](https://www.intersystems.com/resources/rag-vs-fine-tuning-vs-prompt-engineering-everything-you-need-to-know/)\n\n[37\\. A Survey of Techniques for Maximizing LLM Performance](https://zhuanlan.zhihu.com/p/670880685)\n\n[38\\. Prompt Engineering Guide](https://www.promptingguide.ai/research/rag)\n\n[39\\. RAG: Your Complete Guide to Retrieval Augmented Generation](https://www.getguru.com/reference/rag)\n\n[40\\. The Rise and Evolution of RAG in 2024 A Year in Review](https://ragflow.io/blog/the-rise-and-evolution-of-rag-in-2024-a-year-in-review)\n\n[41\\. Top Research Papers on RAG](https://paperguide.ai/papers/top/research-papers-rag/)\n\n[42\\. DavidZWZ/Awesome-RAG-Reasoning](https://github.com/DavidZWZ/Awesome-RAG-Reasoning)\n\n[43\\. Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets](https://arxiv.org/pdf/2504.20119)\n\n[44\\. LLM-Augmented Retrieval: Enhancing Retrieval Models Through Language Models and Doc-Level Embedding](https://tryalign.ai/resources/blog/aarr-llm-augmented-retrieval-enhancing-retrieval-models-through-language-models-and-doc-level-embedding)\n\n[45\\. Animation Global Magazine](https://awnchina.cn/AG2025_SCH)\n\n[46\\. Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks](https://arxiv.org/pdf/2407.21059)\n\n[47\\. Using RAG to Enrich LLMs](https://synthesis.ai/2024/09/18/using-rag-to-enrich-llms/)\n\n[48\\. Enhancing treatment decision-making for low back pain: a novel framework integrating large language models with retrieval-augmented generation technology](https://www.frontiersin.org/journals/medicine/articles/10.3389/fmed.2025.1599241/pdf)\n\n[49\\. CONTROLNET: A Firewall for RAG-based LLM System](http://arxiv.org/pdf/2504.09593v2.pdf?ref=applied-gai-in-security.ghost.io)\n\n[50\\. Performance of Retrieval-Augmented Generation (RAG) on ...](https://intuitionlabs.ai/articles/rag-performance-pharmaceutical-documents)\n\n[51\\. Retrieval-augmented generation (RAG) | European Data ...](https://www.edps.europa.eu/data-protection/technology-monitoring/techsonar/retrieval-augmented-generation-rag)\n\n[52\\. KNOWLEDGE GRAPH AUGMENTED MULTI-HOP QUESTION ANSWERING USING LARGE LANGUAGE MODELS](https://open.metu.edu.tr/bitstream/handle/11511/111317/knowledge-graph-augmented-multi-hop-question-answering-using-large-language-models.pdf)\n\n[53\\. Sustainable Digitalization of Business with Multi-Agent RAG and LLM](https://hal.science/hal-04862192v1/document)\n\n[54\\. Top 10 Use cases of Retrieval-Augmented Generation (RAG) - P2](https://www.aegona.com/software-development/top-10-use-cases-retrieval-augmented-generation-rag-p2)\n\n[55\\. An End-to-End Framework Towards Improving RAG (Retrieval-Augmented Generation) Based Application Performance](https://easychair.org/publications/preprint/XLw8/download)\n\n[56\\. Demystifying AI Agents: The Final Generation of Intelligence](https://arxiv.org/pdf/2505.09932)\n\n[57\\. SRSA: A Cost-Efficient Strategy-Router Search Agent for Real-world Human-Machine Interactions](https://arxiv.org/pdf/2411.14574)\n\n[58\\. 从 RAG 到 SAGE: 现状与展望](http://www.aas.net.cn/cn/article/pdf/preview/10.16383/j.aas.c240163.pdf)\n\n[59\\. Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG](https://arxiv.org/pdf/2410.05983)\n\n[61\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[62\\. ENHANCING PRIVACY AND SECURITY IN RAG-BASED GENERATIVE AI APPLICATIONS](https://aircconline.com/csit/papers/vol15/csit150301.pdf)\n\n[63\\. Privacy and Regulatory Compliance in Retrieval-Augmented Generation Models for AGI Systems](https://www.ijfmr.com/papers/2024/6/30421.pdf)\n\n[64\\. LLMs to Support a Domain Specific Knowledge Assistant](https://arxiv.org/pdf/2502.04095)\n\n[65\\. Observability for AI-Enabled Cloud-Native Networks: A Unified Framework Integrating OpenTelemetry, CortexDB, Loki, GenAI, and RAG](https://ijsrcseit.com/index.php/home/article/download/CSEIT25112811/CSEIT25112811/2707)\n\n[66\\. RAG in 2025: Smarter Retrieval and Real-Time Responses](https://dataforest.ai/blog/rag-in-2025-smarter-retrieval-and-real-time-responses)\n\n[67\\. Advanced Chunking and Search Methods for Improved Retrieval-Augmented Generation (RAG) System Performance in E-Learning](https://openaccess-api.cms-conferences.org/articles/download/978-1-964867-35-9_194)\n\n[68\\. Offline open-source RAG-based AI chatbot for Finnish-language software documentation on low-end hardware](https://www.theseus.fi/bitstream/handle/10024/893770/Luong_Quang.pdf?sequence=2&isAllowed=y)\n\n[69\\. Advancing Question-Answering in Ophthalmology with Retrieval-Augmented Generations (RAG): Benchmarking Open-source and Proprietary Large Language Models](https://www.medrxiv.org/content/10.1101/2024.11.18.24317510v2.full.pdf)\n\n[70\\. In-depth Analysis of Graph-based RAG in a Unified Framework](https://arxiv.org/pdf/2503.04338)\n\n[71\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[72\\. RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems](https://arxiv.org/pdf/2407.11005)\n\n[73\\. Advancing state of the art of Retrieval-augmented Generation (RAG)](https://phontron.com/class/anlp2024/assets/slides/anlp-25-akariasai.pdf)\n\n[74\\. Zhilin Yang, Peng Qi et al. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D18-1259)\n\n[75\\. 金融領域における大規模言語モデルの評価の進展と Retrieval-Augmented Generation による精度向上に向けた取り組み](https://www.fsa.go.jp/frtc/seika/discussion/2024/DP2024-3.pdf)\n\n[76\\. Multimodal Retrieval Augmented Generation Evaluation Benchmark](http://ieeevtc.org/vtc2024spring/DATA/PID2024002132.pdf)\n\n[77\\. Advanced RAG Solution Accelerator](https://techcommunity.microsoft.com/blog/azurearchitectureblog/advanced-rag-solution-accelerator/4394223)\n\n[78\\. Financial Analysis: Intelligent Financial Data Analysis System Based on LLM-RAG](https://www.preprints.org/frontend/manuscript/be84b7b974d40568a640c5d067240717/download_pub)\n\n[79\\. Yunfan Gao, Yun Xiong et al. “Retrieval-Augmented Generation for Large Language Models: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2312.10997)\n\n[81\\. What Great RAG as a Service Looks Like in 2025?](https://www.azilen.com/blog/rag-as-a-service/)\n\n[82\\. Addressing RAG Limitations for Large-Scale AI in Enterprises](https://gk.palem.in/articles/addressing-rag-limitations-for-large-scale-ai-in-enterprises-approach-and-case-studies/)\n\n[83\\. Agentic RAG: How enterprises are surmounting the limits of ...](https://redis.io/blog/agentic-rag-how-enterprises-are-surmounting-the-limits-of-traditional-rag/)\n\n[84\\. Enterprise RAG Predictions for 2025](https://www.vectara.com/blog/top-enterprise-rag-predictions)\n\n[85\\. 4 Big Shifts IT Teams are Making with AI in 2025](https://www.moveworks.com/content/dam/pdfs/guides/it-industry-trends-2025-4-big-shifts-it-teams-are-making.pdf?li_fat_id=022b5e86-4412-4c6d-9c7a-67f211f72647)\n\n[86\\. The State of RAG in 2025: Bridging Knowledge and ... - Squirro](https://squirro.com/squirro-blog/state-of-rag-genai#:~:text=In%202025,%20retrieval%20augmented%20generation,expanding%20corpus%20of%20organizational%20knowledge.)\n\n[87\\. Enterprise RAG at Scale: Why Businesses Can't Afford to ...](https://www.nexgencloud.com/blog/thought-leadership/enterprise-rag-at-scale-why-businesses-can-t-afford-to-stay-small)\n\n[88\\. Architecting Scalable AI RAG Systems: From Startup to Enterprise. A Live Coding Session](https://jobs.ciklum.com/wp-content/uploads/2024/04/Architecting-Scalable-AI-RAG-Systems.pdf)\n\n[89\\. RAG in 2025: Smarter Retrieval and Real-Time Responses](https://dataforest.ai/blog/rag-in-2025-smarter-retrieval-and-real-time-responses)\n\n[90\\. Deploying RAG at Scale: Key Questions for Vendors](https://blog.vespa.ai/deploying-rag-at-scale/)\n\n[91\\. Bring Retrieval Augmented Generation to Google Gemini via External API: An Evaluation with BIG-Bench Dataset](https://www.researchsquare.com/article/rs-4394715/v1.pdf?c=1715322539000)\n\n[92\\. Intelligent Railways: Leveraging Retrieval-Augmented Generation for Smarter Systems](https://internationalpubls.com/index.php/cana/article/download/2553/1552/4585)\n\n[93\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[94\\. RAG Isn't Enough: Why AI+Data Architecture Will Evolve in ...](https://www.madrona.com/rag-is-not-enough-ai-data-architecture/)\n\n[95\\. What Are the Future Trends in RAG for 2025 and Beyond?](https://www.chitika.com/future-trends-in-retrieval-augmented-generation-what-to-expect-in-2025-and-beyond/#:~:text=The%20evolution%20of%20Retrieval-Augmented%20Generation%20%28RAG%29%20hinges%20on,real-time%20decision-making.)\n\n[96\\. Efficient RAG Framework for Large-Scale Knowledge Bases](https://www.ijnrd.org/papers/IJNRD2404764.pdf)\n\n[97\\. RAG只是开始，Agentic App是未来- 大模型知识库](https://53ai.com/news/RAG/2024091472835.html)\n\n[98\\. RSAC 2025: 新挑战、新技术与新趋势](https://static01-www.qianxin.com/qaxweb/c51961e667bb62151f73d06dcc5c69ea.pdf)\n\n[99\\. Data Spaces and Foundation Models: Enabling High-Quality Artificial Intelligence](https://www.isst.fraunhofer.de/content/dam/isst/publikationen/whitepaper/data-spaces_and_foundation-models_whitepaper.pdf)\n\n[101\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[102\\. How Does RAG Improve the Accuracy of LLM Responses](https://itrexgroup.com/blog/how-does-rag-improve-the-accuracy-of-llm-responses/)\n\n[103\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[104\\. Vladimir Karpukhin, Barlas Oğuz et al. “Dense Passage Retrieval for Open-Domain Question Answering.” ArXiv](https://doi.org/10.18653/v1/2020.emnlp-main.550)\n\n[105\\. Empower Vision Applications with LoRA LMM](https://arxiv.org/pdf/2411.00915)\n\n[106\\. Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks](https://arxiv.org/pdf/2407.21059)\n\n[107\\. Zhilin Yang, Peng Qi et al. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D18-1259)\n\n[108\\. Chunk Smarter, Retrieve Better: Enhancing LLMs in Finance](https://openaccess.nhh.no/nhh-xmlui/bitstream/handle/11250/3178510/no.nhh%3Awiseflow%3A7200393%3A61696255.pdf?sequence=1&isAllowed=y)\n\n[109\\. Modular Conversational Agents for Surveys and Interviews](https://arxiv.org/pdf/2412.17049)\n\n[110\\. Yunfan Gao, Yun Xiong et al. “Retrieval-Augmented Generation for Large Language Models: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2312.10997)\n\n[111\\. Exploring the future: Assessing the impact of emerging AI technologies on individuals](https://www.edps.europa.eu/system/files/2024-11/24-11-15_techsonar_2025_en.pdf)\n\n[112\\. More Than 30% of the Increase in Demand for APIs will Come From AI & Tools Using LLMs by 2026: Gartner](https://cioaxis.com/industry/more-than-30-of-the-increase-in-demand-for-apis-will-come-from-ai-tools-using-llms-by-2026-gartner)\n\n[113\\. 从 RAG 到 SAGE: 现状与展望](http://www.aas.net.cn/cn/article/pdf/preview/10.16383/j.aas.c240163.pdf)\n\n[114\\. 4 Big Shifts IT Teams are Making with AI in 2025](https://www.moveworks.com/content/dam/pdfs/guides/it-industry-trends-2025-4-big-shifts-it-teams-are-making.pdf?li_fat_id=022b5e86-4412-4c6d-9c7a-67f211f72647)\n\n[115\\. Beyond the model: Enhancing LLM applications](https://cs230.stanford.edu/syllabus/fall_2024/rag_agents.pdf)\n\n[116\\. NTT DATA Technology Foresight 2025](https://pt.nttdata.com/files/ntt-data-technology-foresight-2025-v3.pdf)\n\n[117\\. Maximizing RAG efficiency: A comparative analysis of RAG methods](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/D7B259BCD35586E04358DF06006E0A85/S2977042424000530a.pdf/div-class-title-maximizing-rag-efficiency-a-comparative-analysis-of-rag-methods-div.pdf)\n\n[118\\. CHATIoT: Large Language Model-based Security Assistant for Internet of Things with Retrieval-Augmented Generation](https://arxiv.org/pdf/2502.09896)\n\n[119\\. Test Suite Augmentation using Language Models -Applying RAG to Improve Robustness Verification](https://hal.science/hal-04615832/document)\n\n[121\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[122\\. Improving Large Language Models Responses with Retrieval Augmented Generation in Animal Production Certification Platforms](http://www.scitepress.org/Papers/2025/132861/132861.pdf)\n\n[123\\. ENHANCING PRIVACY AND SECURITY IN RAG-BASED GENERATIVE AI APPLICATIONS](https://aircconline.com/csit/papers/vol15/csit150301.pdf)\n\n[124\\. CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG](https://arxiv.org/pdf/2506.02544)\n\n[125\\. MES-RAG: Bringing Multi-modal, Entity-Storage, and Secure Enhancements to RAG](https://arxiv.org/pdf/2503.13563)\n\n[126\\. RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.03275)\n\n[127\\. Are Re-Ranking in Retrieval-Augmented Generation Methods Impactful for Small Agriculture QA Datasets? A Small Experiment](https://www.bio-conferences.org/articles/bioconf/pdf/2025/18/bioconf_icosia2024_01001.pdf)\n\n[128\\. MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning](https://arxiv.org/pdf/2505.20096)\n\n[129\\. ROGRAG: A Robustly Optimized GraphRAG Framework](https://arxiv.org/pdf/2503.06474v2)\n\n[130\\. Privacy and Regulatory Compliance in Retrieval-Augmented Generation Models for AGI Systems](https://www.ijfmr.com/papers/2024/6/30421.pdf)\n\n[131\\. RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework](https://arxiv.org/pdf/2408.01262)\n\n[132\\. T. Kwiatkowski, J. Palomaki et al. “Natural Questions: A Benchmark for Question Answering Research.” Transactions of the Association for Computational Linguistics](https://doi.org/10.1162/tacl_a_00276)\n\n[133\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[134\\. Jeff Johnson, Matthijs Douze et al. “Billion-Scale Similarity Search with GPUs.” IEEE Transactions on Big Data](https://doi.org/10.1109/tbdata.2019.2921572)\n\n[135\\. TC-RAG: Turing-Complete RAG’s Case study on Medical LLM Systems](https://arxiv.org/pdf/2408.09199)\n\n[136\\. CRAG - Comprehensive RAG Benchmark](https://arxiv.org/html/2406.04744v1)\n\n[137\\. RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving](https://arxiv.org/pdf/2503.14649)\n\n[138\\. Qinggang Zhang, Zhishang Xiang et al. “FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation.”](https://arxiv.org/abs/2506.08938)\n\n[139\\. The RAG Paradox: A Black-Box Attack Exploiting Unintentional Vulnerabilities in Retrieval-Augmented Generation Systems](https://arxiv.org/pdf/2502.20995v2)\n\n[140\\. LLMs to Support a Domain Specific Knowledge Assistant](https://arxiv.org/pdf/2502.04095)\n\n[141\\. The State of RAG in 2025: Bridging Knowledge and ... - Squirro](https://squirro.com/squirro-blog/state-of-rag-genai#:~:text=In%202025,%20retrieval%20augmented%20generation,expanding%20corpus%20of%20organizational%20knowledge.)\n\n[142\\. RAG and the Future of Intelligent Enterprise Applications: Insights from Startup Leaders](https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-product-and-services/March-2025-rag-and-the-future-of-intelligent-enterprise-applications.pdf)\n\n[143\\. Enterprise RAG Predictions for 2025](https://www.vectara.com/blog/top-enterprise-rag-predictions)\n\n[144\\. 构建生产级GenAI系统：来自海外500+真实企业级大模型案例 ...](https://www.53ai.com/news/LargeLanguageModel/2025062019675.html)\n\n[145\\. Innovative Cloud Architectures: Revolutionizing Enterprise Operations Through AI Integration](https://www.ijfmr.com/papers/2024/6/30201.pdf)\n\n[146\\. Retrieval-Augmented Generation in Production with Haystack](https://4561480.fs1.hubspotusercontent-na1.net/hubfs/4561480/Ebooks%20whitepapers%20and%20reports/O%E2%80%99Reilly%20Guide%20-%20RAG%20in%20Production%20with%20Haystack/OReilly%20Guide%20-%20RAG_in_production_with_Haystack-FINAL.pdf)\n\n[147\\. M. A. D. L. Balaguer, Vinamra Benara et al. “RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture.” ArXiv](https://doi.org/10.48550/arXiv.2401.08406)\n\n[148\\. Agentic RAG: How enterprises are surmounting the limits of ...](https://redis.io/blog/agentic-rag-how-enterprises-are-surmounting-the-limits-of-traditional-rag/)\n\n[149\\. Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts](https://hal.science/hal-04908002v1/document)\n\n[150\\. RAG 生产环境落地的10个经验教训-CSDN博客](https://blog.csdn.net/2401_85343303/article/details/147571128)\n\n[151\\. Enterprise RAG at Scale: Why Businesses Can't Afford to ...](https://www.nexgencloud.com/blog/thought-leadership/enterprise-rag-at-scale-why-businesses-can-t-afford-to-stay-small)\n\n[152\\. What Are the Future Trends in RAG for 2025 and Beyond?](https://www.chitika.com/future-trends-in-retrieval-augmented-generation-what-to-expect-in-2025-and-beyond/#:~:text=The%20evolution%20of%20Retrieval-Augmented%20Generation%20%28RAG%29%20hinges%20on,real-time%20decision-making.)\n\n[153\\. 企业大模型落地的现实解法:为什么RAG是绕不开的技术路径?](http://h5.ifeng.com/c/vivo/v002vdg96UatZ8xfazeCaQkzF7oXHdfwYfN-_syYpb2hdNmQ__)\n\n[154\\. RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving](https://arxiv.org/pdf/2503.14649)\n\n[155\\. Stephanie S. Robbins, Antonis C. Stylianou. “Global corporate web sites: an empirical investigation of content and design.” Inf. Manag.](https://doi.org/10.1016/S0378-7206%2802%2900002-2)\n\n[156\\. OptiMUS-0.3: Using large language models to model and solve optimization problems at scale](https://arxiv.org/pdf/2407.19633)\n\n[157\\. 解读｜生产级RAG系统落地的10个经验教训](https://www.53ai.com/news/RAG/2025072163807.html)\n\n[158\\. 2025 AI Governance Benchmark Report](https://www.coriniumintelligence.com/ai-governance-report)\n\n[159\\. Top 7 RAG Use Cases and Applications to Explore in 2025](https://www.projectpro.io/article/rag-use-cases-and-applications/1059#:~:text=RAG%20is%20used%20to%20improve,,%20summarization,%20and%20conversational%20agents.)\n\n[160\\. 【2025版】这可能是B站唯一将RAG项目实战案例讲明白的教程，存下吧，比啃书好太多了！拿走不谢，允许白嫖！](https://b23.tv/BV1or3uzvEYU?t=122)\n\n[161\\. GitHub - openags/Awesome-AI-Scientist-Papers: A collec...](https://github.com/openags/Awesome-AI-Scientist-Papers.git)\n\n[162\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[163\\. Top RAG Tools to Boost Your LLM Workflows in 2025](https://www.makebot.ai/blog-en/top-rag-tools-to-boost-your-llm-workflows-in-2025)\n\n[164\\. 边缘AI驱动，助力新质生产力](https://pdf.dfcfw.com/pdf/H3_AP202501161641946022_1.pdf?1737025056000.pdf)\n\n[165\\. OpenAI Josh Achiam, Steven Adler et al. “GPT-4 Technical Report.”](https://arxiv.org/abs/2303.08774)\n\n[166\\. 2025 Tech预测：顶级技术趋势、工具和技能](https://pdf.dfcfw.com/pdf/H3_AP202501091641865827_1.pdf)\n\n[167\\. Inteligencias artificiales generativas 2024](https://proyectodescartes.org/iCartesiLibri/PDF/IA2024.pdf)\n\n[168\\. Offline open-source RAG-based AI chatbot for Finnish-language software documentation on low-end hardware](https://www.theseus.fi/bitstream/handle/10024/893770/Luong_Quang.pdf?sequence=2&isAllowed=y)\n\n[169\\. Enhancing treatment decision-making for low back pain: a novel framework integrating large language models with retrieval-augmented generation technology](https://www.frontiersin.org/journals/medicine/articles/10.3389/fmed.2025.1599241/pdf)\n\n[170\\. 4 Big Shifts IT Teams are Making with AI in 2025](https://www.moveworks.com/content/dam/pdfs/guides/it-industry-trends-2025-4-big-shifts-it-teams-are-making.pdf?li_fat_id=022b5e86-4412-4c6d-9c7a-67f211f72647)\n\n[171\\. Benchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation Metrics and Datasets](https://www.scitepress.org/Papers/2024/130657/130657.pdf)\n\n[172\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai-index-report-2025_chapter2_final.pdf)\n\n[173\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[174\\. Takeshi Kojima, S. Gu et al. “Large Language Models are Zero-Shot Reasoners.” ArXiv](https://arxiv.org/abs/2205.11916)\n\n[175\\. RAG-DDR: OPTIMIZING RETRIEVAL-AUGMENTED GENERATION USING DIFFERENTIABLE DATA REWARDS](https://openreview.net/pdf/17b57d6e8c38923f7b1ea9a665074be8735cd644.pdf)\n\n[176\\. Contextual Augmentation in Artificial Intelligence](https://digitalcommons.georgiasouthern.edu/cgi/viewcontent.cgi?article=4179&context=etd)\n\n[177\\. Lab-AI - Retrieval-Augmented Language Model for ...](https://arxiv.org/html/2409.18986v1)\n\n[178\\. Demystifying AI Agents: The Final Generation of Intelligence](https://arxiv.org/pdf/2505.09932)\n\n[179\\. 2025年技术趋势](https://pdf.dfcfw.com/pdf/H3_AP202501071641831808_1.pdf?1736256470000.pdf)\n\n[180\\. Retrieval-Augmented Generation and its Advanced Methods for Applying LLM to Proprietary Data](https://dspace.cvut.cz/bitstream/handle/10467/124241/F3-DP-2025-Klimova-Valeriia-Valeriia_Klimova_Thesis.pdf?sequence=-1)\n\n[181\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[182\\. 基于大模型的测试断言生成技术](https://aidd.vip/resources/upload/a7844a45d8ab55e/file/%E6%88%BF%E6%98%A5%E8%8D%A3-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B5%8B%E8%AF%95%E6%96%AD%E8%A8%80%E7%94%9F%E6%88%90%E6%8A%80%E6%9C%AF.pdf)\n\n[183\\. ENHANCING PRIVACY AND SECURITY IN RAG-BASED GENERATIVE AI APPLICATIONS](https://aircconline.com/csit/papers/vol15/csit150301.pdf)\n\n[184\\. RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework](https://arxiv.org/pdf/2408.01262)\n\n[185\\. RAG vs. GraphRAG: A Systematic Evaluation and Key Insights](https://arxiv.org/pdf/2502.11371)\n\n[186\\. RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems](https://arxiv.org/pdf/2407.11005)\n\n[187\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[188\\. Jeff Johnson, Matthijs Douze et al. “Billion-Scale Similarity Search with GPUs.” IEEE Transactions on Big Data](https://doi.org/10.1109/tbdata.2019.2921572)\n\n[189\\. RAGCHECKER: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation](https://assets.amazon.science/2b/d9/934f060348a994cbac2a2651820a/ragchecker-a-fine-grained-framework-for-diagnosing-retrieval-augmented-generation.pdf)\n\n[190\\. Common Pitfalls in RAG Fine-Tuning and How to Avoid Them](https://www.artech-digital.com/blog/common-pitfalls-in-rag-fine-tuning-and-how-to-avoid-them)\n\n[191\\. Zhilin Yang, Peng Qi et al. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D18-1259)\n\n[192\\. Triggering RAG through LLM Judgments on Expressed Unknown Knowledge](https://openreview.net/pdf/dca4ed2bae1698b6a7223c3bd56da6afafafdb2c.pdf)\n\n[193\\. ROGRAG: A Robustly Optimized GraphRAG Framework](https://arxiv.org/html/2503.06474v2)\n\n[194\\. TrustRAG: Enhancing Robustness and Trustworthiness in RAG](https://www.qeios.com/read/Z4DWHQ/pdf)\n\n[195\\. XRAG: eXamining the Core - Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation](https://arxiv.org/pdf/2412.15529v3)\n\n[196\\. Yunfan Gao, Yun Xiong et al. “Retrieval-Augmented Generation for Large Language Models: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2312.10997)\n\n[197\\. Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in Retrieval-Augmented Generation Systems](https://arxiv.org/pdf/2409.19804)\n\n[198\\. 論文を対象としたRAGシステムにおける質問分類に基づく動的検索](https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/P8-18.pdf)\n\n[199\\. Towards Understanding Retrieval Accuracy and Prompt Quality in RAG Systems](https://arxiv.org/pdf/2411.19463)\n\n[201\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[202\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[203\\. T. Kwiatkowski, J. Palomaki et al. “Natural Questions: A Benchmark for Question Answering Research.” Transactions of the Association for Computational Linguistics](https://doi.org/10.1162/tacl_a_00276)\n\n[204\\. Vladimir Karpukhin, Barlas Oğuz et al. “Dense Passage Retrieval for Open-Domain Question Answering.” ArXiv](https://doi.org/10.18653/v1/2020.emnlp-main.550)\n\n[205\\. Creating a Taxonomy for Retrieval Augmented Generation Applications](https://arxiv.org/pdf/2408.02854)\n\n[206\\. Nelson F. Liu, Kevin Lin et al. “Lost in the Middle: How Language Models Use Long Contexts.” Transactions of the Association for Computational Linguistics](https://doi.org/10.1162/tacl_a_00638)\n\n[207\\. Agentic RAG systems for enterprise-scale information ...](https://toloka.ai/blog/agentic-rag-systems-for-enterprise-scale-information-retrieval/)\n\n[208\\. The RAG Paradox: A Black-Box Attack Exploiting Unintentional Vulnerabilities in Retrieval-Augmented Generation Systems](https://arxiv.org/pdf/2502.20995v2)\n\n[209\\. Seven Ways Your RAG System Could be Failing and How to Fix Them](https://labelstud.io/blog/seven-ways-your-rag-system-could-be-failing-and-how-to-fix-them/#:~:text=RAG%20is%20a%20powerful%20tool,prompt%20instructions,%20or%20hallucinated%20answers.)\n\n[210\\. Identifying Performance Bottlenecks in RAG Pipelines](https://apxml.com/courses/optimizing-rag-for-production/chapter-1-production-rag-foundations/rag-performance-bottlenecks)\n\n[211\\. Complete RAG Guide: Understanding True Data Integration AI ...](https://realcoding.blog/2025/06/10/rag-mes-integration-guide-en/)\n\n[212\\. 5 Bottlenecks Impacting RAG Pipeline Efficiency in Production - The New Stack](https://thenewstack.io/5-bottlenecks-impacting-rag-pipeline-efficiency-in-production/)\n\n[213\\. The State of RAG in 2025: Bridging Knowledge and ... - Squirro](https://squirro.com/squirro-blog/state-of-rag-genai#:~:text=In%202025,%20retrieval%20augmented%20generation,expanding%20corpus%20of%20organizational%20knowledge.)\n\n[214\\. Enhancing Gen AI with Multimodal RAG Systems](https://oulurepo.oulu.fi/bitstream/handle/10024/55943/nbnfioulu-202505153455.pdf?sequence=1&isAllowed=y)\n\n[215\\. RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving](https://arxiv.org/pdf/2503.14649)\n\n[216\\. Microsoft's May 2025 Update Creates System Boot Failures for Windows ...](https://cinchops.com/microsofts-may-2025-update-creates-system-boot-failures/#:~:text=Access%20Critical%20Systems-,Microsoft's%20May%202025%20Update%20Creates%20System%20Boot%20Failures%20for%20Windows,failures%20across%20Windows%2011%20systems.)\n\n[221\\. State of AI Report](https://www.aiunplugged.io/wp-content/uploads/2023/10/State-of-AI-Report-2023.pdf)\n\n[222\\. Zero-shot Comparison of Large Language Models (LLMs) Reasoning Abilities on Long-text Analogies](https://scholarspace.manoa.hawaii.edu/bitstreams/67638869-9223-4d41-839b-a81c478a076a/download)\n\n[223\\. A Inteligência Artificial em 2024 – balanço e pontes para 2025](https://interactideas.pt/guia2024/guia.pdf)\n\n[224\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[225\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[226\\. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948)\n\n[227\\. Unlock the Future: Introduction to GenAI and Retrieval Augmented Generation (RAG) techniques for your networks](https://www.ciscolive.com/c/dam/r/ciscolive/apjc/docs/2024/pdf/BRKAPP-2562.pdf)\n\n[228\\. DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments](https://arxiv.org/pdf/2504.03160)\n\n[229\\. AI EXHIBITING EMERGENT HUMAN BEHAVIORS: GLOBAL RISK ASSESSMENT OF 2025 REASONING LLMs – CASE STUDIES: OPENAI O3-MINI, DEEPSEEK R1, GEMINI 2, GEMINI 2.5, GROK 3, QWEN 2.5 (PRESENTING: TURING NAND TEST AND DFSW BIAS TEST)](https://zenodo.org/records/15185640/files/2025.03_AI_HUMAN_BEHAVIORS_RISK_ASSESSMENT.v1.2.pdf?download=1)\n\n[230\\. Contextual Augmentation in Artificial Intelligence](https://digitalcommons.georgiasouthern.edu/cgi/viewcontent.cgi?article=4179&context=etd)\n\n[231\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[232\\. 4 Big Shifts IT Teams are Making with AI in 2025](https://www.moveworks.com/content/dam/pdfs/guides/it-industry-trends-2025-4-big-shifts-it-teams-are-making.pdf?li_fat_id=022b5e86-4412-4c6d-9c7a-67f211f72647)\n\n[233\\. GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation](https://www.arxiv.org/pdf/2502.01113)\n\n[234\\. LLM Benchmarks March 2025](https://www.timetoact-group.at/en/insights/llm-benchmarks/llm-benchmarks-march-2025)\n\n[235\\. Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Don’t Know\", and Reducing Hallucination in RAG](https://openreview.net/pdf?id=X4UoRsiU72)\n\n[236\\. GNN-RAG：一种无需昂贵计算资源投入将LLM效率和效果 ...](https://www.53ai.com/news/LargeLanguageModel/2024060372036.html)\n\n[237\\. Developing Locally Trainable Large Language Models](https://dr.ntu.edu.sg/bitstream/10356/182242/2/Thesis_report_Hailin.pdf)\n\n[238\\. Meta Unveils AGI Lab to Compete - Artificial Intelligence +](https://www.aiplusinfo.com/meta-unveils-agi-lab-to-compete/)\n\n[239\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[240\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai-index-report-2025_chapter2_final.pdf)"
  },
  {
    "id": 54,
    "question": "How have scaling laws evolved in large language models from GPT-3 to O3, and what does this tell us about the future direction of AI research?",
    "response": "**Research Report: Evolution of Scaling Laws in Large Language Models from GPT-3 to O3**\n\n**1\\. Introduction to Scaling Laws in Large Language Models**\n\nScaling laws describe the mathematical relationships between model performance and key variables like computational resources (FLOPs), model parameters, and training data volume. First empirically characterized by OpenAI in 2020 through landmark research (Kaplan et al., arXiv \\[cs.LG\\], 2020) \\[245\\]\\[257\\]these laws established that performance improves predictably as these resources increase. This report examines how these principles evolved from GPT-3 to O3 and their implications for AI's future.\n\n**2\\. GPT-3: The Foundational Scaling Paradigm**\n\n**2.1 Architectural Foundations**\n\n**Parameter Scale**: 175B parameters, a 100x increase over GPT-2 \\[6\\]\\[7\\]\\[10\\]\n\n**Training Regime**: 300B tokens from diverse sources (Common Crawl, Wikipedia, books) consuming ~$4.6M in compute \\[68\\]\\[135\\]\n\n**Performance Scaling**: Demonstrated power-law improvements where test loss scaled as \\[19\\]\n\n**2.2 Key Scaling Trade-offs**\n\n**Data Dominance**: Data volume contributed 68-77% of value, outweighing model size by 3.4x \\[12\\]\n\n**Diminishing Returns**: Logarithmic performance decay occurred beyond 100B parameters \\[11\\]\\[29\\]\n\n**Critical Imbalance**: Used 1.7 tokens/parameter despite optimal being 20 tokens/parameter—suboptimal FLOP utilization \\[65\\]\\[185\\]\n\n**2.3 Fundamental Limitations**\n\n**Reasoning Capabilities**: Static inference prevented complex multi-step reasoning \\[170\\]\n\n**Data Exhaustion**: Web-scraped training data faced quality/sustainability constraints \\[45\\]\\[46\\]\n\n**Metric Misalignment**: Test loss improvements poorly correlated with real-world reasoning ability \\[45\\]\n\n**3\\. O3: Scaling 2.0 – Efficiency and Adaptive Computation**\n\n**3.1 Architectural Revolution**\n\n**Hybrid Density**: Mixture-of-Experts (MoE) architecture with >1.6T parameters dynamically routes computations \\[73\\]\\[81\\]\n\n**Dynamic Neural Pathways**: Real-time complexity analysis selects from three processing routes (Fast/Balanced/Deep) \\[87\\]\n\n**Memory Optimization**: 40% memory footprint reduction via tensor offloading and adaptive precision \\[87\\]\n\n**3.2 Scaling Law Innovations**\n\n**Test-Time Compute**: \"Thinking time\" scaling during inference improves reasoning accuracy through extended token chains \\[107\\]\\[108\\]\n\n**FLOP Efficiency**: 10x higher Model FLOP Utilization (MFU) vs. GPT-4-era models through orchestrated optimization \\[146\\]\\[147\\]\n\n**Token Efficiency**: o3-mini (m) achieves 85.7% math accuracy with 40% fewer tokens than o1-mini \\[173\\]\n\n**3.3 Resolving GPT-3 Era Limitations**\n\n**Reasoning Focus**: 87.5% ARC-AGI benchmark accuracy via deliberative alignment \\[14\\]\\[275\\]\n\n**Data Efficiency**: Reduced token costs by 95% since GPT-4 despite larger scale \\[352\\]\n\n**System Intelligence**: Integrates tool use (web search/code execution) within reasoning workflows \\[91\\]\\[272\\]\n\n**4\\. Quantitative Scaling Law Evolution**\n\n_Table: Scaling Law Transition (GPT-3 → O3)_\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n| **Scaling Factor** | **GPT-3 (2020)** | **O3 (2024-25)** | **Shift Significance** |\n| **Parameters** | 175B | 1.6T+ | 9x density + MoE sparsity |\n| **Data Efficiency** | 1.7 tokens/param | ~20 tokens/param | Optimal FLOP utilization |\n| **Compute Focus** | Pre-training | Inference-time | Test-time scaling paradigm |\n| **Performance Curve** | (loss) | Accuracy ∝ token-depth | Adaptive accuracy gains |\n| **Key Benchmark** | Winograd Schema | ARC-AGI (87.5%) | Reasoning generalization |\n\n**Diminishing Returns Mitigation**: Inference-time scaling circumvents logarithmic decay seen in GPT-3 pre-training \\[107\\]\\[108\\]\n\n**Economic Scaling**: o3-mini costs 2.50 while outperforming it on MATH \\[225\\]\\[234\\]\n\n**5\\. New Scaling Paradigms and Future AI Research**\n\n**5.1 Emergent Scaling Principles**\n\n**Inference > Pre-training**: O3 establishes token depth (reasoning steps) as new scaling variable \\[48\\]\\[49\\]\n\n**Dynamic Efficiency**: Architectural innovations (MoE, adaptive pathways) decouple performance from brute-force FLOP scaling \\[87\\]\\[147\\]\n\n**Multimodal Synergy**: Cross-domain knowledge integration replaces monolithic data ingestion \\[87\\]\n\n**5.2 Research Imperatives**\n\n1.  **Reasoning-Centric Metrics**: Replace loss-based evaluation with benchmarks for deliberative problem-solving \\[45\\]\\[57\\]\n2.  **Systemic Robustness**: Address brittleness exposed when scaling meets real-world complexity \\[45\\]\\[58\\]\n3.  **Ethical Scaling Frameworks**: Third-party audits (e.g., METR/Apollo Research) become critical as autonomous capabilities emerge \\[267\\]\\[267\\]\n4.  **Energy-Aware Scaling**: Pursuit of 10x MFU must prioritize carbon efficiency \\[147\\]\n\n**5.3 The Horizon: Scaling 3.0**\n\n**Neural-Physical Fusion**: Embedding sensors/actuators within reasoning pathways \\[274\\]\n\n**Recursive Self-Improvement**: Models iteratively enhancing scaling laws themselves \\[275\\]\n\n**Algorithmic Breakthroughs**: NAS-optimized architectures could yield O() efficiency gains \\[87\\]\n\n**6\\. Conclusion: Scaling Laws as Evolutionary Compass**\n\nThe GPT-3 era established a physics-like foundation where performance scaled predictably with compute/data/parameters—but exposed fundamental ceilings in reasoning and efficiency. O3 marks a quantum leap: scaling shifts from static pre-training to dynamic inference, from monolithic architectures to adaptive pathways, and from data volume to reasoning depth. Third-party validations confirm unprecedented gains in reasoning (ARC-AGI >87%) and efficiency (10x MFU) \\[14\\]\\[147\\].\n\nThis evolution reveals AI research's future trajectory: **scaling will bifurcate**. Horizontally, efficient architectures will push classical boundaries via innovations like hybrid MoE-dense systems. Vertically, the \"thinking time\" dimension will unlock recursive self-improvement and genuine cognitive generalization. Crucially, scaling must evolve governance frameworks alongside technical prowess—O3's autonomous capabilities already demand novel risk assessment protocols \\[267\\]. The next decade's breakthroughs will emerge not from scaling alone, but from recursively optimized scaling laws.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[2\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[3\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[4\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[5\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[6\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[7\\. Developing Apps with GPT-4 and ChatGPT](https://ebooks.umu.ac.ug/librarian/books-file/Developing%20Apps%20with%20GPT-4%20and%20-%20Olivier%20Caelen.pdf)\n\n[8\\. OPENAI系列追踪DAY3:GPTO3学会看图说话 SCALINGLAWS验证“...](https://stock.finance.sina.com.cn/stock/go.php/vReport_Show/kind/search/rptid/798279762462/index.phtml)\n\n[9\\. o3: Stratospheric reasoning – Dr Alan D. Thompson](https://lifearchitect.ai/o3/)\n\n[10\\. A Perspective on ChatGPT, Its Impact and Limitations](https://www.infosys.com/techcompass/documents/perspective-chatgpt-impact-limitations.pdf)\n\n[11\\. What is the difference between the O Series and GPT models ...](https://kazulog.fun/en/dev-en/what-is-the-difference-between-the-o-series-and-gpt-models-and-what-are-the-features-of-o3/)\n\n[12\\. REGULATING GENERATIVE ARTIFICIAL INTELLIGENCE](https://www.pymnts.com/wp-content/uploads/2023/09/TechREG-Regilating-Generative-Artificial-Intelligence-September-Volume-I-2023.pdf)\n\n[13\\. IBM XL Fortran for Linux, V16.1 Optimization and Programming Guide for Little Endian Distributions](https://www.ibm.com/docs/de/SSAT4T_16.1.0/com.ibm.compilers.linux.doc/proguide.pdf)\n\n[14\\. 万字长文解读Scaling Law现状与未来, 从GPT-3到O3](http://h5.ifeng.com/c/vivo/v002tQbRjGBjNvQYSuoq--rypKky2uDjWfjZQrfA9H6GBtdw__)\n\n[15\\. Comparing the performance of GPT-3 with BERT for decision requirements modeling](https://lirias.kuleuven.be/retrieve/723042)\n\n[16\\. Natural Language Processing](https://ernestryu.com/courses/FM/NLP_basics.pdf)\n\n[17\\. GPT-4 vs o3 - Detailed Performance & Feature Comparison](https://docsbot.ai/models/compare/gpt-4/o3)\n\n[18\\. ChatGPT首次带图深度思考：OpenAI连发o3/o4 mini，比前代性能更强价格更低](https://www.myzaker.com/article/680032c3b15ec03c7a7486f9)\n\n[19\\. Mustafa Shukor, Enrico Fini et al. “Scaling Laws for Native Multimodal Models Scaling Laws for Native Multimodal Models.”](https://arxiv.org/abs/2504.07951)\n\n[20\\. Why is GPT-3 Better Than GPT-J?](https://stylocreations.com/2023/04/12/why-is-gpt-3-better-than-gpt-j/)\n\n[21\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[22\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[23\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[24\\. o3: Stratospheric reasoning – Dr Alan D. Thompson](https://lifearchitect.ai/o3/)\n\n[25\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[26\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[27\\. Survey of AI Technologies and AI R&D Trajectories](https://assets-global.website-files.com/62c4cf7322be8ea59c904399/65e83959fd414a488a4fa9a5_Gladstone%20Survey%20of%20AI.pdf)\n\n[28\\. 万字长文解读Scaling Law现状与未来, 从GPT-3到O3](http://h5.ifeng.com/c/vivo/v002tQbRjGBjNvQYSuoq--rypKhSzAJDYtDaUX6Z9SItGyYA__)\n\n[29\\. SCBX AI Outlook 2025: Beaconing the Future of Artificial Intelligence](https://www.scbx.com/wp-content/uploads/2025/05/SCBX-AI-Outlook-2025_ENG.pdf)\n\n[30\\. OPENAI系列追踪DAY3:GPTO3学会看图说话 SCALINGLAWS验证“...](https://stock.finance.sina.com.cn/stock/go.php/vReport_Show/kind/search/rptid/798279762462/index.phtml)\n\n[31\\. OpenAI o3 – Reviving Back The Advanced Tech Giant](https://kodexolabs.com/openAI-o3/)\n\n[32\\. 2025 Large Models Half - year Review: O3, Agent, and ...](https://eu.36kr.com/en/p/3351694604595844)\n\n[33\\. Scaling Laws](https://koh.pw/cse599j/slides/CSE599J_1-10-24.pdf)\n\n[34\\. The Capabilities and Limitations of Grok 3 in the Landscape of Advanced Reasoning Models](https://c3.unu.edu/wp-content/uploads/2025/02/PerplexityAI_report2.pdf)\n\n[35\\. o3-mini](https://www.zhihu.com/topic/559655408/top-answers)\n\n[36\\. ChatGPT首次带图深度思考：OpenAI连发o3/o4 mini，比前代性能更强价格更低](https://www.myzaker.com/article/680032c3b15ec03c7a7486f9)\n\n[37\\. Small Vision-Language Models: A Technical Survey](https://hal.science/hal-04889751v1/document)\n\n[38\\. Competitive Programming with Large Reasoning Models](https://arxiv.org/pdf/2502.06807)\n\n[39\\. Law of Diminishing Returns](https://xplaind.com/250491/law-of-diminishing-returns)\n\n[40\\. Scaling multilingual language models under constrained data](https://members.loria.fr/CGardent/publis/thesis-lescao-2023.pdf)\n\n[41\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[42\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[43\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[44\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[45\\. 万字长文解读Scaling Law现状与未来, 从GPT-3到O3](http://h5.ifeng.com/c/vivo/v002tQbRjGBjNvQYSuoq--rypKhSzAJDYtDaUX6Z9SItGyYA__)\n\n[46\\. Survey of AI Technologies and AI R&D Trajectories](https://assets-global.website-files.com/62c4cf7322be8ea59c904399/65e83959fd414a488a4fa9a5_Gladstone%20Survey%20of%20AI.pdf)\n\n[47\\. Alec Radford, Karthik Narasimhan. “Improving Language Understanding by Generative Pre-Training.”](https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n[48\\. OpenAI's Latest Model Shows AGI Is Inevitable. Now What?](https://www.lawfaremedia.org/article/openai's-latest-model-shows-agi-is-inevitable.-now-what)\n\n[49\\. OPENAI系列追踪DAY3:GPTO3学会看图说话 SCALINGLAWS验证“...](https://stock.finance.sina.com.cn/stock/go.php/vReport_Show/kind/search/rptid/798279762462/index.phtml)\n\n[50\\. GPT-3: OpenAI's New Text Generating Neural Network is Here](https://www.digitaltrends.com/features/openai-gpt-3-text-generation-ai/?itm_medium=topic&itm_source=30&itm_content=2x2&itm_term=2376926)\n\n[51\\. GPT-3: The Ultimate Guide to Building NLP Products with OpenAI API](http://www.lcace.org/Media/GPT-3.pdf)\n\n[52\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[53\\. SCBX AI Outlook 2025: Beaconing the Future of Artificial Intelligence](https://www.scbx.com/wp-content/uploads/2025/05/SCBX-AI-Outlook-2025_ENG.pdf)\n\n[54\\. L. Floridi, Massimo Chiriatti. “GPT-3: Its Nature, Scope, Limits, and Consequences.” Minds and Machines](https://doi.org/10.1007/s11023-020-09548-1)\n\n[55\\. 万字长文解读Scaling Law现状与未来， 从GPT](http://www.360doc.com/content/25/0204/09/170868_1145879478.shtml)\n\n[56\\. A Human Being Wrote This Law Review Article: GPT-3 and the Practice of Law](https://lawreview.law.ucdavis.edu/sites/g/files/dgvnsk15026/files/media/documents/55-1_Cyphert.pdf)\n\n[57\\. ...人工智能刚刚进入一个新时代。OpenAI 发布了新的 o3 模...](https://xueqiu.com/4894511814/318051019)\n\n[58\\. o3-mini](https://www.zhihu.com/topic/559655408/top-answers)\n\n[61\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[62\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[63\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[64\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[65\\. Survey of AI Technologies and AI R&D Trajectories](https://assets-global.website-files.com/62c4cf7322be8ea59c904399/65e83959fd414a488a4fa9a5_Gladstone%20Survey%20of%20AI.pdf)\n\n[66\\. 4+3 Phases of Compute-Optimal Neural Scaling Laws](https://proceedings.neurips.cc/paper_files/paper/2024/file/1dccfc3ee01871d05e33457c61037d59-Paper-Conference.pdf)\n\n[67\\. Chapter 1: Introducing GPT-3 and the OpenAI API](https://static.packt-cdn.com/downloads/9781800563193_ColorImages.pdf)\n\n[68\\. Natural Language Processing with Deep Learning](https://www.iust.ac.ir/files/ie/ghazanfari_db06f/files/nlpwithdeeplearningstanford.pdf)\n\n[69\\. J. Kaplan, Sam McCandlish et al. “Scaling Laws for Neural Language Models.” ArXiv](https://arxiv.org/abs/2001.08361)\n\n[70\\. OpenAI's Latest Model Shows AGI Is Inevitable. Now What?](https://www.lawfaremedia.org/article/openai's-latest-model-shows-agi-is-inevitable.-now-what)\n\n[71\\. Scaling Laws](https://koh.pw/cse599j/slides/CSE599J_1-10-24.pdf)\n\n[72\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[73\\. The AI Titans Face Off: OpenAI's O3 vs Google's Gemini 2.0](https://blog.adyog.com/2024/12/31/the-ai-titans-face-off-openais-o3-vs-googles-gemini-2-0/)\n\n[74\\. OpenAI o3とGPT-4oの知財業務適用比較](https://yorozuipsc.com/uploads/1/3/2/5/132566344/openai_o3_gpt4o_ip_comparison_20250504094436.pdf)\n\n[75\\. REGULATING GENERATIVE ARTIFICIAL INTELLIGENCE](https://www.pymnts.com/wp-content/uploads/2023/09/TechREG-Regilating-Generative-Artificial-Intelligence-September-Volume-I-2023.pdf)\n\n[76\\. OPENAI系列追踪DAY3:GPTO3学会看图说话 SCALINGLAWS验证“...](https://stock.finance.sina.com.cn/stock/go.php/vReport_Show/kind/search/rptid/798279762462/index.phtml)\n\n[77\\. OpenAI o3-mini System Card](https://cdn.openai.com/o3-mini-system-card-feb10.pdf)\n\n[78\\. A Comparative Study on Reasoning Patterns of OpenAI's o1 Model](https://openreview.net/pdf/561428fd3ba9e3681aab2208a8388216c37a9b00.pdf)\n\n[81\\. The Capabilities and Limitations of Grok 3 in the Landscape of Advanced Reasoning Models](https://c3.unu.edu/wp-content/uploads/2025/02/PerplexityAI_report2.pdf)\n\n[82\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[83\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[84\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[85\\. Alec Radford, Karthik Narasimhan. “Improving Language Understanding by Generative Pre-Training.”](https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n[86\\. OpenAI Josh Achiam, Steven Adler et al. “GPT-4 Technical Report.”](https://arxiv.org/abs/2303.08774)\n\n[87\\. OpenAI O3: A Technical Deep Dive into the Next Evolution](https://anshadameenza.com/blog/technology/openai-o3-model-analysis/)\n\n[88\\. OpenAI's o3 AI model smashes the ACI-AGI benchmark tests](https://www.fanaticalfuturist.com/2025/01/openais-o3-ai-model-smashes-the-aci-agi-benchmark-tests/)\n\n[89\\. Le Jeune Indépendant # 8078 du Dimanche 29 Décembre 2024](https://www.jeune-independant.net/wp-content/uploads/2024/12/EDITION-29-12-2024.pdf)\n\n[90\\. Claude Opus 4 と ChatGPT o3 の技術比較](https://yorozuipsc.com/uploads/1/3/2/5/132566344/482be7c17af820587db1.pdf)\n\n[91\\. O4-mini, O3, GPT-4.1: Comparison of OpenAI Models - Bind AI](https://blog.getbind.co/2025/04/17/openai-o4-mini-o3-gpt4-1-comparison-of-openai-models/)\n\n[92\\. OpenAI o3とGPT-4oの知財業務適用比較](https://yorozuipsc.com/uploads/1/3/2/5/132566344/openai_o3_gpt4o_ip_comparison_20250504094436.pdf)\n\n[93\\. OpenAI o3 Models Set to Launch: Features and Model Comparison](https://www.analyticsvidhya.com/blog/2025/01/openai-o3-vs-competitors-performance-and-applications/)\n\n[94\\. OpenAI Unveils Next-Gen o3 and o3-mini Models](https://whitepapersonline.com/zh/news/post/openai-unveils-next-gen-o3-and-o3-mini-models)\n\n[95\\. Revolutionizing AI: OpenAI GPT-3 Architecture](https://www.spaceo.ai/blog/openai-gpt-architecture/)\n\n[96\\. OpenAI o3 and o4-mini: OpenAI's new models, explained - The Visla Blog](https://www.visla.us/blog/news/openai-o3-and-o4-mini-openais-new-models-explained/#:~:text=Here's%20another%20massive%20upgrade:%20context,More%20than%20five%20novels%20worth.)\n\n[97\\. What to know about o3 and o4-mini, OpenAI's new ...](https://bdtechtalks.com/2025/04/17/openai-o3-o4-mini/)\n\n[98\\. OpenAI o3 model: How good is ChatGPT's next AI version?](https://www.digit.in/features/general/openai-o3-model-how-good-is-chatgpts-next-ai-version.html)\n\n[101\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[102\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[103\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[104\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[105\\. Takeshi Kojima, S. Gu et al. “Large Language Models are Zero-Shot Reasoners.” ArXiv](https://arxiv.org/abs/2205.11916)\n\n[106\\. SCBX AI Outlook 2025: Beaconing the Future of Artificial Intelligence](https://www.scbx.com/wp-content/uploads/2025/05/SCBX-AI-Outlook-2025_ENG.pdf)\n\n[107\\. Current AI Scaling Laws are Showing Diminishing Returns, Forcing AI Labs to Change Course](https://www.aitechnobyte.com/current-ai-scaling-laws-are-showing-diminishing-returns-forcing-ai-labs-to-change-course/)\n\n[108\\. THE RELATIONSHIP BETWEEN REASONING AND PERFORMANCE IN LARGE LANGUAGE MODELS—O3 (MINI) THINKS HARDER, NOT LONGER.](https://arxiv.org/pdf/2502.15631)\n\n[109\\. Scaling Laws](https://koh.pw/cse599j/slides/CSE599J_1-10-24.pdf)\n\n[110\\. o3: Stratospheric reasoning – Dr Alan D. Thompson](https://lifearchitect.ai/o3/)\n\n[111\\. Dynamic Voltage and Frequency Scaling: The Laws of Diminishing Returns](https://www.usenix.org/legacy/events/hotpower/tech/full_papers/LeSueur.pdf)\n\n[112\\. MASTERMINEVAL: A SIMPLE BUT SCALABLE REASONING BENCHMARK](https://openreview.net/pdf/45c477cc2daf7ab6d8d38813ee6b3a2a8cf99d92.pdf)\n\n[113\\. OPENAI系列追踪DAY3:GPTO3学会看图说话 SCALINGLAWS验证“...](https://stock.finance.sina.com.cn/stock/go.php/vReport_Show/kind/search/rptid/798279762462/index.phtml)\n\n[114\\. Algorithmic reasoning in large language models and neuro-symbolic architectures](https://www.research.unipd.it/bitstream/11577/3551867/2/thesis_reviews_frontespizio_pdfA.pdf)\n\n[115\\. Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies Ahead](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Inference-Time-Scaling-for-Complex-Tasks-Where-We-Stand-and-What-Lies-Ahead.pdf)\n\n[116\\. DeepSeek-R1 vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?](https://arxiv.org/pdf/2504.08120v3)\n\n[117\\. CODEMMLU: A MULTI-TASK BENCHMARK FOR ASSESSING CODE UNDERSTANDING & REASONING CAPABILITIES OF CODELLMS](https://arxiv.org/pdf/2410.01999)\n\n[118\\. ...and Performance in Large Language Models—o3 (mini)...](http://arxiv.org/html/2502.15631v1)\n\n[121\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[122\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[123\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[124\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[125\\. Survey of AI Technologies and AI R&D Trajectories](https://assets-global.website-files.com/62c4cf7322be8ea59c904399/65e83959fd414a488a4fa9a5_Gladstone%20Survey%20of%20AI.pdf)\n\n[126\\. Defense in Depth: An Action Plan to Increase the Safety and Security of Advanced AI](https://cdn.slow-news.com/wp-content/uploads/2024/03/Gladstone-Action-Plan.pdf)\n\n[127\\. Natural Language Processing with Deep Learning](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1234/slides/cs224n-2023-lecture9-pretraining.pdf)\n\n[128\\. Alec Radford, Karthik Narasimhan. “Improving Language Understanding by Generative Pre-Training.”](https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n[129\\. Word Meaning in Minds and Machines](https://cims.nyu.edu/~brenden/papers/LakeMurphy2021PsychReview.pdf)\n\n[130\\. Developing Apps with GPT-4 and ChatGPT](https://ebooks.umu.ac.ug/librarian/books-file/Developing%20Apps%20with%20GPT-4%20and%20-%20Olivier%20Caelen.pdf)\n\n[131\\. Jiayi Lin, Hande Dong et al. “Scaling Laws Behind Code Understanding Model.” ArXiv](https://doi.org/10.48550/arXiv.2402.12813)\n\n[132\\. OPENAI系列追踪DAY3:GPTO3学会看图说话 SCALINGLAWS验证“...](https://stock.finance.sina.com.cn/stock/go.php/vReport_Show/kind/search/rptid/798279762462/index.phtml)\n\n[133\\. Towards Usable API Documentation](https://ucalgary.scholaris.ca/server/api/core/bitstreams/8aea72d9-b3db-4580-b8dd-5b086afd1b00/content)\n\n[134\\. GPT 3 vs GPT2 - Opened AI](https://openedai.io/gpt-3-vs-gpt2/)\n\n[135\\. OpenAi GPT-4 Vs GPT-3: An In-Depth Comparison](https://expertbeacon.com/gpt-4-vs-gpt-3/)\n\n[136\\. 一文详尽之Scaling Law](https://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ%3D%3D&mid=2247629243&idx=3&sn=03f9f0d073353068bb866731e40c1a0f&chksm=fa813954b40c083005b4b8060a8da4384aeea7913aa57255c45eb2bbf8b207bb111e36a592a0&scene=27)\n\n[137\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[138\\. Generative AI - The Rule of Law and Democracy at Risk?](https://legalphilosophyintaiwan.files.wordpress.com/2023/11/20231115_no-hyperlink_e4b8ade6ada3e593b2e7a094e68980_generative-ai-the-rule-of-law-and-democracy-at-risks-1.pdf)\n\n[139\\. Natural Language Processing with Generative AI Models: A Methodological Approach for Their Application](https://webthesis.biblio.polito.it/28973/1/tesi.pdf)\n\n[141\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[142\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[143\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[144\\. Andrew G. Howard, Menglong Zhu et al. “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.” ArXiv](https://arxiv.org/abs/1704.04861)\n\n[145\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[146\\. The Capabilities and Limitations of Grok 3 in the Landscape of Advanced Reasoning Models](https://c3.unu.edu/wp-content/uploads/2025/02/PerplexityAI_report2.pdf)\n\n[147\\. OpenAI O3: A Technical Deep Dive into the Next Evolution](https://anshadameenza.com/blog/technology/openai-o3-model-analysis/)\n\n[148\\. OpenAI Josh Achiam, Steven Adler et al. “GPT-4 Technical Report.”](https://arxiv.org/abs/2303.08774)\n\n[149\\. Notable AI Models Documentation – Epoch AI](https://epochai.org/data/epochdb/documentation)\n\n[150\\. Greg Brockman, Vicki Cheung et al. “OpenAI Gym.” ArXiv](https://arxiv.org/abs/1606.01540)\n\n[151\\. Addendum to OpenAI o3 and o4-mini system card: OpenAI o3 Operator](https://cdn.openai.com/pdf/4375e605-f9a6-438d-bcc8-190599c183a6/o3_cua_system_card.pdf)\n\n[152\\. OpenAI o3 and o4-mini System Card](https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf)\n\n[153\\. The 2023 Foundation Model Transparency Index](https://openreview.net/pdf/032d164d6447b2f8fcfc5133d5471ea6baf356fe.pdf)\n\n[154\\. OpenAI o3-mini System Card](https://cdn.openai.com/o3-mini-system-card-feb10.pdf)\n\n[155\\. Add OpenAI o3 & o4-mini reasoning model via official API](https://github.com/ai-shifu/ChatALL/pull/1012)\n\n[156\\. O3 & O4-mini released. Tyler Cowen, the renowned ...](https://www.grapevine.in/post/o3-and-o4-mini-released-tyler-cowen-the-renowned-economist-says-agi-bus-has-arrived-37bd261a-4387-41b2-b451-ba9eead0050f)\n\n[157\\. 9 OpenAI o3 Model Use Cases : Beginners Guide 2025](https://www.geeky-gadgets.com/openai-o3-model-use-cases-explained/)\n\n[158\\. OpenAI's o3: The grand finale of AI in 2024](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai)\n\n[161\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[162\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[163\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[164\\. MASTERMINEVAL: A SIMPLE BUT SCALABLE REASONING BENCHMARK](https://openreview.net/pdf/45c477cc2daf7ab6d8d38813ee6b3a2a8cf99d92.pdf)\n\n[165\\. MASTERMINDEval: A Simple But Scalable Reasoning Benchmark](https://openreview.net/attachment?id=H4donosutm&name=pdf)\n\n[166\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[167\\. Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges](https://arxiv.org/pdf/2505.11618)\n\n[168\\. ChatGPT o3-mini-high: A Leap Forward in AI Reasoning](https://neuroflash.com/blog/chatgpt-o3-mini-high/)\n\n[169\\. OpenAI-o3-mini vs DeepSeek R1: Complete Comparison of ... - GeeksforGeeks](https://www.geeksforgeeks.org/openai-o3-mini-vs-deepseek-r1/)\n\n[170\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[171\\. BIG-Bench Extra Hard](http://www.arxiv.org/pdf/2502.19187)\n\n[172\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[173\\. THE RELATIONSHIP BETWEEN REASONING AND PERFORMANCE IN LARGE LANGUAGE MODELS—O3 (MINI) THINKS HARDER, NOT LONGER.](https://arxiv.org/pdf/2502.15631)\n\n[174\\. CODEMMLU: A MULTI-TASK BENCHMARK FOR ASSESSING CODE UNDERSTANDING & REASONING CAPABILITIES OF CODELLMS](https://arxiv.org/pdf/2410.01999)\n\n[175\\. DeepSeek-R1 vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?](https://arxiv.org/pdf/2504.08120v3)\n\n[176\\. Grok 3 vs o3 Comparison](https://blog.promptlayer.com/grok-3-vs-o3-comparison/)\n\n[181\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[182\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[183\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[184\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[185\\. Survey of AI Technologies and AI R&D Trajectories](https://assets-global.website-files.com/62c4cf7322be8ea59c904399/65e83959fd414a488a4fa9a5_Gladstone%20Survey%20of%20AI.pdf)\n\n[186\\. Natural Language Processing with Deep Learning](https://www.iust.ac.ir/files/ie/ghazanfari_db06f/files/nlpwithdeeplearningstanford.pdf)\n\n[187\\. Alec Radford, Karthik Narasimhan. “Improving Language Understanding by Generative Pre-Training.”](https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n[188\\. OpenAI's Latest Model Shows AGI Is Inevitable. Now What?](https://www.lawfaremedia.org/article/openai's-latest-model-shows-agi-is-inevitable.-now-what)\n\n[189\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[190\\. REGULATING GENERATIVE ARTIFICIAL INTELLIGENCE](https://www.pymnts.com/wp-content/uploads/2023/09/TechREG-Regilating-Generative-Artificial-Intelligence-September-Volume-I-2023.pdf)\n\n[191\\. Elliot Paquette, Courtney Paquette et al. “4+3 Phases of Compute-Optimal Neural Scaling Laws.” ArXiv](https://doi.org/10.48550/arXiv.2405.15074)\n\n[192\\. o3: Stratospheric reasoning – Dr Alan D. Thompson](https://lifearchitect.ai/o3/)\n\n[193\\. Jiayi Lin, Hande Dong et al. “Scaling Laws Behind Code Understanding Model.” ArXiv](https://doi.org/10.48550/arXiv.2402.12813)\n\n[194\\. GPT 3 vs GPT2 - Opened AI](https://openedai.io/gpt-3-vs-gpt2/)\n\n[195\\. A Survey of Large Language Models](http://arxiv.org/pdf/2303.18223)\n\n[196\\. A Comparative Study on Reasoning Patterns of OpenAI's o1 Model](https://openreview.net/pdf/561428fd3ba9e3681aab2208a8388216c37a9b00.pdf)\n\n[197\\. Timo Schick, Jane Dwivedi-Yu et al. “PEER: A Collaborative Language Model.” ArXiv](https://doi.org/10.48550/arXiv.2208.11663)\n\n[198\\. Generative AI - The Rule of Law and Democracy at Risk?](https://legalphilosophyintaiwan.files.wordpress.com/2023/11/20231115_no-hyperlink_e4b8ade6ada3e593b2e7a094e68980_generative-ai-the-rule-of-law-and-democracy-at-risks-1.pdf)\n\n[199\\. T. Henighan, J. Kaplan et al. “Scaling Laws for Autoregressive Generative Modeling.” ArXiv](https://arxiv.org/abs/2010.14701)\n\n[201\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[202\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[203\\. K. Simonyan, Andrew Zisserman. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” CoRR](https://arxiv.org/abs/1409.1556)\n\n[204\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[205\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[206\\. OpenAI o3 and o4-mini System Card](https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf)\n\n[207\\. OpenAI GPT-4.5 System Card](https://cdn.openai.com/gpt-4-5-system-card.pdf)\n\n[208\\. OpenAI updates Operator to o3, making its $200 monthly ...](https://venturebeat.com/ai/openai-updates-operator-to-o3-making-its-200-monthly-chatgpt-subscription-more-enticing/)\n\n[209\\. OpenAI Launches o3-pro Model Focused on Reliability ...](https://www.infoq.com/news/2025/06/openai-o3-pro/)\n\n[210\\. OpenAI o1 / o3](https://lamroger.com/slides/OpenAI%20o1%20and%20o3.pdf)\n\n[211\\. Addendum to OpenAI o3 and o4-mini system card: OpenAI o3 Operator](https://cdn.openai.com/pdf/4375e605-f9a6-438d-bcc8-190599c183a6/o3_cua_system_card.pdf)\n\n[212\\. OpenAI Enhances Operator AI Agent with Advanced o3 ...](https://hyper.ai/en/headlines/2a4a3c6931086ee26e099d1f1113970f)\n\n[213\\. CLASS ACTION COMPLAINT](https://www.classaction.org/media/sancton-v-openai-inc-et-al.pdf)\n\n[214\\. OpenAI o3 は知的財産業務に革新をもたらすか：「これまでできなかったこと」の実現可能性評価](https://yorozuipsc.com/uploads/1/3/2/5/132566344/bb2fbf0cdd02c33adb63.pdf)\n\n[215\\. OpenAI o3 が知的財産業務にもたらす新たな可能性](https://yorozuipsc.com/uploads/1/3/2/5/132566344/b1a89abe9c98043bdb40.pdf)\n\n[216\\. OpenAI Secretly Funded Benchmarking Dataset Linked To o3 Model](https://www.searchenginejournal.com/openai-secretly-funded-frontiermath-benchmarking-dataset/537760/)\n\n[217\\. AInsights: How OpenAI's o3 Model Is Ushering in an AI ...](https://briansolis.com/2025/01/ainsights-how-openais-o3-model-is-ushering-in-an-ai-reasoning-revolution/)\n\n[221\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[222\\. Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges](https://arxiv.org/pdf/2505.11618)\n\n[223\\. DeepSeek-R1 vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?](https://arxiv.org/pdf/2504.08120v3)\n\n[224\\. THE RELATIONSHIP BETWEEN REASONING AND PERFORMANCE IN LARGE LANGUAGE MODELS—O3 (MINI) THINKS HARDER, NOT LONGER.](https://arxiv.org/pdf/2502.15631)\n\n[225\\. AutoReporter: Development and validation of an artificial intelligence tool for automated research reporting guideline assessment](https://www.medrxiv.org/content/10.1101/2025.04.18.25326076v3.full.pdf)\n\n[226\\. MASTERMINEVAL: A SIMPLE BUT SCALABLE REASONING BENCHMARK](https://openreview.net/pdf/45c477cc2daf7ab6d8d38813ee6b3a2a8cf99d92.pdf)\n\n[227\\. MATHCONSTRUCT: CHALLENGING LLM REASONING WITH CONSTRUCTIVE PROOFS](https://openreview.net/pdf?id=nHW2tiGMrb)\n\n[228\\. CODEMMLU: A MULTI-TASK BENCHMARK FOR ASSESSING CODE UNDERSTANDING & REASONING CAPABILITIES OF CODELLMS](https://arxiv.org/pdf/2410.01999)\n\n[229\\. Evaluating AI Reasoning Models in Pediatric Medicine: A Comparative Analysis of o3-mini and o3-mini-high](https://www.medrxiv.org/content/medrxiv/early/2025/02/28/2025.02.27.25323028.full.pdf)\n\n[230\\. o3 vs GPT-4o Mini - Detailed Performance & Feature ...](https://docsbot.ai/models/compare/o3/gpt-4o-mini)\n\n[231\\. O4-mini, O3, GPT-4.1: Comparison of OpenAI Models - Bind AI](https://blog.getbind.co/2025/04/17/openai-o4-mini-o3-gpt4-1-comparison-of-openai-models/)\n\n[232\\. Aakanksha Chowdhery, Sharan Narang et al. “PaLM: Scaling Language Modeling with Pathways.” J. Mach. Learn. Res.](https://arxiv.org/abs/2204.02311)\n\n[233\\. GPT-4.5 preview vs GPT-o3 mini](https://aimlapi.com/comparisons/gpt-4-5-preview-vs-gpt-o3-mini)\n\n[234\\. OpenAI-o3-mini vs DeepSeek R1: Complete Comparison of ... - GeeksforGeeks](https://www.geeksforgeeks.org/openai-o3-mini-vs-deepseek-r1/)\n\n[235\\. Token-Budget-Aware LLM Reasoning](https://arxiv.org/pdf/2412.18547)\n\n[241\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[242\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[243\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[244\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[245\\. Survey of AI Technologies and AI R&D Trajectories](https://assets-global.website-files.com/62c4cf7322be8ea59c904399/65e83959fd414a488a4fa9a5_Gladstone%20Survey%20of%20AI.pdf)\n\n[246\\. Alec Radford, Karthik Narasimhan. “Improving Language Understanding by Generative Pre-Training.”](https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n[247\\. Defense in Depth: An Action Plan to Increase the Safety and Security of Advanced AI](https://cdn.slow-news.com/wp-content/uploads/2024/03/Gladstone-Action-Plan.pdf)\n\n[248\\. Word Meaning in Minds and Machines](https://cims.nyu.edu/~brenden/papers/LakeMurphy2021PsychReview.pdf)\n\n[249\\. Developing Apps with GPT-4 and ChatGPT](https://ebooks.umu.ac.ug/librarian/books-file/Developing%20Apps%20with%20GPT-4%20and%20-%20Olivier%20Caelen.pdf)\n\n[250\\. OpenAI's Latest Model Shows AGI Is Inevitable. Now What?](https://www.lawfaremedia.org/article/openai's-latest-model-shows-agi-is-inevitable.-now-what)\n\n[251\\. MIT Technology Review](https://farhanasultana.com/wp-content/uploads/MIT-Tech-Review-Feb-2021.pdf)\n\n[252\\. Jiayi Lin, Hande Dong et al. “Scaling Laws Behind Code Understanding Model.” ArXiv](https://doi.org/10.48550/arXiv.2402.12813)\n\n[253\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[254\\. OPENAI系列追踪DAY3:GPTO3学会看图说话 SCALINGLAWS验证“...](https://stock.finance.sina.com.cn/stock/go.php/vReport_Show/kind/search/rptid/798279762462/index.phtml)\n\n[255\\. Elliot Paquette, Courtney Paquette et al. “4+3 Phases of Compute-Optimal Neural Scaling Laws.” ArXiv](https://doi.org/10.48550/arXiv.2405.15074)\n\n[256\\. o3: Stratospheric reasoning – Dr Alan D. Thompson](https://lifearchitect.ai/o3/)\n\n[257\\. Andrei Kucharavy, Z. Schillaci et al. “Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense.” ArXiv](https://doi.org/10.48550/arXiv.2303.12132)\n\n[258\\. T. Henighan, J. Kaplan et al. “Scaling Laws for Autoregressive Generative Modeling.” ArXiv](https://arxiv.org/abs/2010.14701)\n\n[259\\. Generative AI - The Rule of Law and Democracy at Risk?](https://legalphilosophyintaiwan.files.wordpress.com/2023/11/20231115_no-hyperlink_e4b8ade6ada3e593b2e7a094e68980_generative-ai-the-rule-of-law-and-democracy-at-risks-1.pdf)\n\n[260\\. A Human Being Wrote This Law Review Article: GPT-3 and the Practice of Law](https://lawreview.law.ucdavis.edu/sites/g/files/dgvnsk15026/files/media/documents/55-1_Cyphert.pdf)\n\n[261\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[262\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[263\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[264\\. Volodymyr Mnih, Adrià Puigdomènech Badia et al. “Asynchronous Methods for Deep Reinforcement Learning.” International Conference on Machine Learning](https://arxiv.org/abs/1602.01783)\n\n[265\\. OpenAI updates Operator to o3, making its $200 monthly ...](https://venturebeat.com/ai/openai-updates-operator-to-o3-making-its-200-monthly-chatgpt-subscription-more-enticing/)\n\n[266\\. Alexandre Lacoste, A. Luccioni et al. “Quantifying the Carbon Emissions of Machine Learning.” ArXiv](https://arxiv.org/abs/1910.09700)\n\n[267\\. OpenAI o3 and o4-mini System Card](https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf)\n\n[268\\. OpenAI GPT-4.5 System Card](https://blog.visualizer.com.br/wp-content/uploads/2025/03/Gpt_4_5_System_Card_Original.pdf)\n\n[269\\. OpenAI o1 / o3](https://lamroger.com/slides/OpenAI%20o1%20and%20o3.pdf)\n\n[270\\. Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs' Multi-turn Instruction-Following Ability](https://arxiv.org/pdf/2504.21625)\n\n[271\\. ...Node, OpenAI Node): Include o3 models in model… · ...](https://github.com/n8n-io/n8n/commit/37d152c148cafbe493c22e07f5d55ff24fcb0ca4)\n\n[272\\. OpenAI o3 は知的財産業務に革新をもたらすか：「これまでできなかったこと」の実現可能性評価](https://yorozuipsc.com/uploads/1/3/2/5/132566344/bb2fbf0cdd02c33adb63.pdf)\n\n[273\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[274\\. OpenAI o3 が知的財産業務にもたらす新たな可能性](https://yorozuipsc.com/uploads/1/3/2/5/132566344/b1a89abe9c98043bdb40.pdf)\n\n[275\\. AInsights: How OpenAI's o3 Model Is Ushering in an AI ...](https://briansolis.com/2025/01/ainsights-how-openais-o3-model-is-ushering-in-an-ai-reasoning-revolution/)\n\n[276\\. International AI Safety Report](https://italianelfuturo.com/wp-content/uploads/2025/01/International_AI_Safety_Report_2025_accessible_f_250130_073143.pdf)\n\n[277\\. 高频因子跟踪](https://aigc.idigital.com.cn/djyanbao/%E3%80%90%E5%9B%BD%E9%87%91%E8%AF%81%E5%88%B8%E3%80%91%E9%AB%98%E9%A2%91%E5%9B%A0%E5%AD%90%E8%B7%9F%E8%B8%AA%EF%BC%9AChatGPTo3%E6%A8%A1%E5%9E%8B%E6%9C%80%E6%96%B0%E5%8F%91%E5%B8%83%EF%BC%8C%E6%8E%A8%E7%90%86%E4%B8%8E%E7%BC%96%E7%A8%8B%E8%83%BD%E5%8A%9B%E5%A4%A7%E5%B9%85%E6%8F%90%E5%8D%87-2024-12-24.pdf)\n\n[278\\. Addendum to OpenAI o3 and o4-mini system card: OpenAI o3 Operator](https://cdn.openai.com/pdf/4375e605-f9a6-438d-bcc8-190599c183a6/o3_cua_system_card.pdf)\n\n[281\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[282\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[283\\. DeepSeek-R1 vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?](https://arxiv.org/pdf/2504.08120v3)\n\n[284\\. Comparing GPT-4o and O3-Mini on same task - Prompting](https://community.openai.com/t/comparing-gpt-4o-and-o3-mini-on-same-task/1142746)\n\n[285\\. MASTERMINEVAL: A SIMPLE BUT SCALABLE REASONING BENCHMARK](https://openreview.net/pdf/45c477cc2daf7ab6d8d38813ee6b3a2a8cf99d92.pdf)\n\n[286\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[287\\. CODEMMLU: A MULTI-TASK BENCHMARK FOR ASSESSING CODE UNDERSTANDING & REASONING CAPABILITIES OF CODELLMS](https://openreview.net/pdf/30cf4e55abcddae3fc87742e0c1d1393453e0e4d.pdf)\n\n[288\\. Aakanksha Chowdhery, Sharan Narang et al. “PaLM: Scaling Language Modeling with Pathways.” J. Mach. Learn. Res.](https://arxiv.org/abs/2204.02311)\n\n[289\\. THE RELATIONSHIP BETWEEN REASONING AND PERFORMANCE IN LARGE LANGUAGE MODELS—O3 (MINI) THINKS HARDER, NOT LONGER.](https://arxiv.org/pdf/2502.15631)\n\n[290\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[291\\. Grok 3 vs o3 Comparison](https://blog.promptlayer.com/grok-3-vs-o3-comparison/)\n\n[292\\. GPT-4.5 preview vs GPT-o3 mini](https://aimlapi.com/comparisons/gpt-4-5-preview-vs-gpt-o3-mini)\n\n[293\\. Evaluating AI Reasoning Models in Pediatric Medicine: A Comparative Analysis of o3-mini and o3-mini-high](https://www.medrxiv.org/content/10.1101/2025.02.27.25323028v1.full.pdf)\n\n[294\\. GPT-4o Mini vs o3 - Detailed Performance & Feature Comparison](https://docsbot.ai/models/compare/gpt-4o-mini/o3)\n\n[295\\. ChatGPT o3-mini-high: A Leap Forward in AI Reasoning](https://neuroflash.com/blog/chatgpt-o3-mini-high/)\n\n[301\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[302\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[303\\. Scaling Laws in Linear Regression: Compute, Parameters, and Data](https://openreview.net/pdf?id=PH7sdEanXP)\n\n[304\\. 4+3 Phases of Compute-Optimal Neural Scaling Laws](https://proceedings.neurips.cc/paper_files/paper/2024/file/1dccfc3ee01871d05e33457c61037d59-Paper-Conference.pdf)\n\n[305\\. J. Kaplan, Sam McCandlish et al. “Scaling Laws for Neural Language Models.” ArXiv](https://arxiv.org/abs/2001.08361)\n\n[306\\. International AI Safety Report](https://italianelfuturo.com/wp-content/uploads/2025/01/International_AI_Safety_Report_2025_accessible_f_250130_073143.pdf)\n\n[307\\. A Survey of Large Language Models](http://arxiv.org/pdf/2303.18223)\n\n[308\\. The AI Imperative: Scaling High-Quality Peer Review in Machine Learning](http://arxiv.org/pdf/2506.08134)\n\n[309\\. System 2 Thinking in OpenAI's o1-Preview Model](https://www.mdpi.com/2073-431X/13/11/278)\n\n[310\\. Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms](https://proceedings.neurips.cc/paper_files/paper/2024/file/e45caa3d5273d105b8d045e748636957-Paper-Conference.pdf)\n\n[311\\. NOVAglow646/LLM-MLLM-paper-list: 关于 ...](https://github.com/NOVAglow646/LLM-MLLM-paper-list)\n\n[312\\. Joel Hestness, Sharan Narang et al. “Deep Learning Scaling is Predictable, Empirically.” ArXiv](https://arxiv.org/abs/1712.00409)\n\n[313\\. Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies](https://openreview.net/pdf/134f55cd9235bc28c2d2cba435869c22f9d830f4.pdf)\n\n[314\\. Are More LM Calls All You Need? Towards the Scaling Properties of Compound AI Systems](https://proceedings.neurips.cc/paper_files/paper/2024/file/51173cf34c5faac9796a47dc2fdd3a71-Paper-Conference.pdf)\n\n[315\\. Scaling Laws for Neural Language Models, Kaplan et al. 2020](https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/L10.pptx)\n\n[316\\. OpenAI O3: A Technical Deep Dive into the Next Evolution](https://anshadameenza.com/blog/technology/openai-o3-model-analysis/)\n\n[321\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[322\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[323\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[324\\. Alec Radford, Karthik Narasimhan. “Improving Language Understanding by Generative Pre-Training.”](https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n[325\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[326\\. o3 vs GPT-4 32K - Detailed Performance & Feature ...](https://docsbot.ai/models/compare/o3/gpt-4-32k)\n\n[327\\. GPT-4o vs. GPT-4: Welches Modell ist besser? - neuroflash](https://neuroflash.com/de/blog/gpt-4o-vs-gpt-4-welches-modell-ist-besser/)\n\n[328\\. ChatGPT研究显示其LLMs在某些任务上变得越来越差](https://www.theregister.com/2023/07/20/gpt4_chatgpt_performance/)\n\n[329\\. 知的財産業務における次世代 AI（GPT-5）の可能性と課題：GPT-4o との比較分析](https://yorozuipsc.com/uploads/1/3/2/5/132566344/324afb848ebf52beb9b8.pdf)\n\n[330\\. o3 vs GPT-4o - Detailed Performance & Feature Comparison](https://docsbot.ai/models/compare/o3/gpt-4o)\n\n[331\\. GPT-4 vs o3 - Detailed Performance & Feature Comparison](https://docsbot.ai/models/compare/gpt-4/o3)\n\n[332\\. OpenAI o3とGPT-4oの知財業務適用比較](https://yorozuipsc.com/uploads/1/3/2/5/132566344/openai_o3_gpt4o_ip_comparison_20250504094436.pdf)\n\n[333\\. Performance analysis of large language models Chatgpt-4o, OpenAI O1, and OpenAI O3 mini in clinical treatment of pneumonia: a comparative study](https://link.springer.com/content/pdf/10.1007/s10238-025-01743-7.pdf)\n\n[334\\. OpenAI Josh Achiam, Steven Adler et al. “GPT-4 Technical Report.”](https://arxiv.org/abs/2303.08774)\n\n[335\\. GPT-4.5 preview vs GPT-o3 mini](https://aimlapi.com/comparisons/gpt-4-5-preview-vs-gpt-o3-mini)\n\n[336\\. o3-pro may be OpenAI's most advanced commercial ...](https://www.infoworld.com/article/4011303/o3-pro-may-be-openais-most-advanced-commercial-offering-but-gpt-4o-bests-it.html)\n\n[337\\. THE RELATIONSHIP BETWEEN REASONING AND PERFORMANCE IN LARGE LANGUAGE MODELS—O3 (MINI) THINKS HARDER, NOT LONGER.](https://arxiv.org/pdf/2502.15631)\n\n[338\\. OpenAI劲敌出手,又一个性能匹敌GPT-4的大模型|AI_新浪财经...](http://finance.sina.com.cn/wm/2024-03-09/doc-inamsywu6261075.shtml)\n\n[339\\. OpenAIのo1・o3とGoogle Gemini 2.5 Proの比較調査](https://yorozuipsc.com/uploads/1/3/2/5/132566344/e6e74b626241f30bbbfd.pdf)\n\n[340\\. OpenAI O3: A Technical Deep Dive into the Next Evolution](https://anshadameenza.com/blog/technology/openai-o3-model-analysis/)\n\n[341\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[342\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[343\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[344\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[345\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[346\\. THE RELATIONSHIP BETWEEN REASONING AND PERFORMANCE IN LARGE LANGUAGE MODELS—O3 (MINI) THINKS HARDER, NOT LONGER.](https://arxiv.org/pdf/2502.15631)\n\n[347\\. MASTERMINEVAL: A SIMPLE BUT SCALABLE REASONING BENCHMARK](https://openreview.net/pdf/45c477cc2daf7ab6d8d38813ee6b3a2a8cf99d92.pdf)\n\n[348\\. ChatGPT最新モデル「OpenAI o3」と「o3-mini」徹底解説!](https://yoshitechlab.com/ai-chatgpto3/)\n\n[349\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[350\\. DeepSeek-R1 vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?](https://arxiv.org/pdf/2504.08120v3)\n\n[351\\. AutoReporter: Development and validation of an artificial intelligence tool for automated research reporting guideline assessment](https://www.medrxiv.org/content/10.1101/2025.04.18.25326076v3.full.pdf)\n\n[352\\. ChatGPT o3-mini-high: A Leap Forward in AI Reasoning](https://neuroflash.com/blog/chatgpt-o3-mini-high/)\n\n[353\\. MATHCONSTRUCT: CHALLENGING LLM REASONING WITH CONSTRUCTIVE PROOFS](https://openreview.net/pdf?id=nHW2tiGMrb)\n\n[354\\. The Battle of AI Titans: OpenAI's O3 vs. Google's Gemini 2.0](https://blog.adyog.com/2024/12/23/the-battle-of-ai-titans-openais-o3-vs-googles-gemini-2-0-2/)\n\n[355\\. Comparing GPT-4o and O3-Mini on same task - Prompting](https://community.openai.com/t/comparing-gpt-4o-and-o3-mini-on-same-task/1142746)\n\n[356\\. OpenAI o3-mini System Card](https://cdn.openai.com/o3-mini-system-card.pdf)\n\n[357\\. Comprehensive Analysis of Grok 3 vs. Contemporary Advanced Reasoning Models](https://c3.unu.edu/wp-content/uploads/2025/02/PerplexityAI_report1.pdf)"
  },
  {
    "id": 55,
    "question": "Why has the Transformer architecture become the dominant foundation for large language models (LLMs), and what fundamental advantages does it have over alternative architectures like RNNs and LSTMs?",
    "response": "**The Dominance of Transformer Architecture in Large Language Models: Architectural Advantages and Emerging Challenges**\n\n**1\\. Introduction: The Rise of Transformers**\n\nThe Transformer architecture, introduced in 2017, has become the cornerstone of modern large language models (LLMs) like GPT-4, Gemini, and Llama. Its dominance stems from fundamental advantages in parallelization, long-range dependency handling, and scalability, which address critical limitations of previous architectures like recurrent neural networks (RNNs) and long short-term memory networks (LSTMs). As of 2025, Transformers power 90% of state-of-the-art LLMs, though emerging architectures are beginning to challenge their hegemony in specific domains.\n\n**2\\. Transformers vs. RNNs/LSTMs: Core Architectural Advantages**\n\n**A. Superior Parallelization Capabilities**\n\nTransformers eliminate the sequential dependency bottleneck inherent in RNNs and LSTMs by processing all tokens in a sequence simultaneously. Unlike RNNs/LSTMs, where each time step’s computation depends on the previous output (creating a serial processing chain) (Web Pages 5, 10, 12), Transformers leverage **self-attention mechanisms** to calculate alignment scores for all tokens in parallel (Web Pages 8, 10, 13). This enables full token-level parallelism during training (Web Pages 12, 13, 15), drastically accelerating computation on modern hardware like GPUs/TPUs (Web Pages 12, 17). For example, training a Transformer on a 10,000-token sequence can be up to 50× faster than an LSTM equivalent due to parallelization \\[20\\].\n\n**B. Mastery of Long-Range Dependencies**\n\nTransformers outperform LSTMs in handling long sequences due to their **global self-attention mechanism**, which dynamically weights the relevance of all tokens regardless of position (Web Pages 28, 29). LSTMs—though designed to mitigate RNNs’ vanishing gradient problem—struggle with sequences beyond ∼500 tokens, exhibiting performance degradation in tasks like document summarization or genomic analysis (Web Pages 26, 30, 33). Quantitative benchmarks in 2025 confirm:\n\n**Efficiency**: Transformers achieve comparable accuracy to LSTMs with 40–60% faster training times \\[30\\].\n\n**Scalability**: Transformers maintain high accuracy for sequences exceeding 10k tokens, while LSTM performance decays linearly with length (Web Pages 28, 31).\n\n**Dependency Capture**: Tasks requiring global context (e.g., book-level coherence) show 25–35% higher accuracy for Transformers (Web Pages 27, 28).\n\n**C. Flexibility and Scalability**\n\nTransformers’ modular design (encoder/decoder stacks) enables easy scaling to billions of parameters and adaptation to multimodal tasks (e.g., text-image fusion). LSTMs, in contrast, face architectural constraints: their fixed-size hidden states limit context retention, and their sequential nature complicates distributed training (Web Pages 30, 32). This scalability gap makes Transformers uniquely suited for LLMs, where model size directly correlates with capability (Web Pages 28, 31).\n\n**3\\. Inherent Trade-offs and Limitations of Transformers**\n\nDespite their strengths, Transformers exhibit two critical constraints:\n\n**Quadratic Complexity**: Self-attention requires O(n²) computations for sequence length _n_, making ultra-long sequences (e.g., high-resolution videos, whole genomes) computationally prohibitive (Web Pages 32, 35).\n\n**Resource Intensity**: Training trillion-parameter models demands massive GPU/TPU clusters, raising costs and environmental concerns \\[36\\].\n\n**Interpretability Gaps**: Attention weights offer limited insight into model reasoning, complicating safety and ethics audits \\[36\\].\n\n**4\\. Emerging Challengers to Transformer Dominance (2025)**\n\nWhile Transformers remain the default for general-purpose LLMs, several architectures show promise in niche domains:\n\n**A. Mamba (State Space Models)**\n\nMamba uses state space models (SSMs) to achieve **O(n) linear complexity**, excelling at ultra-long sequences (e.g., DNA analysis, 100k-token books). However, it lags behind Transformers in short-text tasks and creative generation due to weaker context modeling and immature tooling (e.g., no Hugging Face integration) \\[36\\].\n\n**B. Linear Attention Variants (e.g., FlashAttention, RWKV)**\n\nThese approximate self-attention with linear kernels (O(n) complexity), reducing inference costs by ∼70% for real-time applications. Yet, they sacrifice long-context understanding; hybrid approaches (e.g., mixing standard attention with linear attention) are now bridging this gap \\[36\\].\n\n**C. Hybrid Architectures**\n\nModels like **Attention-Mamba** combine Transformers’ local detail processing with SSMs’ global compression, enabling efficient book summarization. Another trend integrates **neuro-symbolic systems** (e.g., neural state machines) for tasks requiring structured reasoning, though scalability remains unproven \\[36\\].\n\n**5\\. Conclusion: Why Transformers Remain Dominant**\n\nTransformers dominate LLMs due to their unparalleled parallelization, global dependency handling, and scalability—advantages that remain unmatched by RNNs/LSTMs in general-purpose language tasks. Emerging architectures like Mamba address Transformer limitations (e.g., quadratic complexity) but lack generalizability, ecosystem support, and multimodal flexibility. As of 2025, the LLM landscape is evolving toward hybrids (e.g., Transformer-SSM fusions), yet Transformers’ architectural maturity, extensive pretrained model libraries, and hardware optimization ensure their continued primacy for mainstream AI applications.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[2\\. Beyond Transformers: Alternative Architectures for LLMs SSMs, MAMBA, RWKV](https://mobitec.ie.cuhk.edu.hk/ierg5050Spring2025/static_files/slides/IERG5050-Lec-4-Beyond-Transformers-Brief-Version-Spring2025.pdf)\n\n[3\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[4\\. Sepp Hochreiter, J. Schmidhuber. “Long Short-Term Memory.” Neural Computation](https://doi.org/10.1162/neco.1997.9.8.1735)\n\n[5\\. Transformers: Architecture, Training & Usage](https://scholar.harvard.edu/sites/scholar.harvard.edu/files/binxuw/files/mlfs_tutorial_nlp_transformer_ssl_updated.pdf)\n\n[6\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[7\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[8\\. Synergy in Science and Engineering: An Integrative Approach](https://www.swamivivekanandauniversity.ac.in/resource/assets/pdf/books/Synergy%20in%20Science%20and%20Engineering.pdf)\n\n[9\\. Transformers in Time-series Analysis: A Tutorial](https://arxiv.org/pdf/2205.01138v1)\n\n[10\\. Combining Structural and Semantic Information in Transformers for Source Code Summarization](https://fenix.tecnico.ulisboa.pt/downloadFile/2533669927387233/92431_bernardo_lourenco_dissertacao_final.pdf)\n\n[11\\. Resilience Assessment of Machine Learning Applications under Hardware Faults](https://fardapaper.ir/mohavaha/uploads/2023/09/Fardapaper-Resilience-assessment-of-machine-learning-applications-under-hardware-faults.pdf)\n\n[12\\. Transformers vs. LSTMs in Modern LLMs - Rohan's Bytes](https://www.rohan-paul.com/p/transformers-vs-lstms-in-modern-llms)\n\n[13\\. Transformer Models Vs Llm Comparison | Restackio](https://www.restack.io/p/transformer-models-answer-vs-llm-cat-ai)\n\n[14\\. Fine-Tuned ‘Small’ LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in Text Classification](https://www.disintegration.ch/wp-content/uploads/2024/08/BucherMartini_2024_LLMs.pdf)\n\n[15\\. Where to fuse](https://lukaspetersson.com/assets/pdf/wtf_paper.pdf)\n\n[16\\. Розробка модуля для автоматизованого аналізу фінансових звітів компаній з використанням машинного навчання](https://essuir.sumdu.edu.ua/bitstream/123456789/94376/1/Piven_mag_rob.pdf)\n\n[17\\. A COMPREHENSIVE ANALYSIS OF SUBWORD TOKENIZERS FOR MORPHOLOGICALLY RICH LANGUAGES](https://www.cmpe.boun.edu.tr/~gungort/theses/A%20Comprehensive%20Analysis%20of%20Subword%20Tokenizers%20for%20Morphologically%20Rich%20Languages.pdf)\n\n[18\\. RNNs vs LSTM vs Transformers | SabrePC Blog](https://www.sabrepc.com/blog/deep-learning-and-ai/rnns-vs-lstm-vs-transformers?srsltid=AfmBOoqmX5MbrEc23FdR-gVMzIFGciOOQSc-REI7ZM5ZHWfbkl6pbDlw)\n\n[19\\. RNN vs LSTM vs GRU vs Transformers](https://www.geeksforgeeks.org/rnn-vs-lstm-vs-gru-vs-transformers/)\n\n[20\\. Lecture 11: Attention and Transformers](https://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf)\n\n[21\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[22\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[23\\. Sepp Hochreiter, J. Schmidhuber. “Long Short-Term Memory.” Neural Computation](https://doi.org/10.1162/neco.1997.9.8.1735)\n\n[24\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[25\\. Dzmitry Bahdanau, Kyunghyun Cho et al. “Neural Machine Translation by Jointly Learning to Align and Translate.” CoRR](https://arxiv.org/abs/1409.0473)\n\n[26\\. Master en Traduction et interprétation : Technologies des langues Parcours : Informatique-Traduction 2021-2023 Constitution de corpus : publier au format XML-TEI des dictionnaires et des encyclopédies de la Chine ancienne](https://publication-theses.unistra.fr/public/memoires/2023/FLCE/2023_KONG_Zijun.pdf)\n\n[27\\. Unlocking the Power of LSTM for Long Term Time Series Forecasting](https://arxiv.org/pdf/2408.10006)\n\n[28\\. Time series forecasting in financial markets using deep learning models](https://journalwjaets.com/sites/default/files/fulltext_pdf/WJAETS-2025-0167.pdf)\n\n[29\\. Comparative Study of LSTM and Transformer for A-Share Stock Price Prediction](https://www.atlantis-press.com/article/125990061.pdf)\n\n[30\\. Proceedings of ICONI 2024](https://journal-home.s3.ap-northeast-2.amazonaws.com/site/iconi2024/proceeding/Proceedings_of_ICONI%2B2024.pdf)\n\n[31\\. LSTM versus Transformers: A Practical Comparison of Deep Learning Models for Trading Financial Instruments](https://www.scitepress.org/Papers/2024/129811/129811.pdf)\n\n[32\\. SambaMOTR: A Linear-Time Set-of-Sequences Model for Multiple Object Tracking](https://openreview.net/attachment?id=OeBY9XqiTz&name=supplementary_material)\n\n[33\\. 2025大模型Transformer架构发展历程、优势及未来 ...](https://www.sgpjbg.com/bgdown/490592.html)\n\n[34\\. Integrating LSTM, Transformer, and LightGBM for Enhanced Predictive Modeling: A Mechanistic Approach](https://jisads.com/index.php/1/article/download/24/25/127)\n\n[35\\. Efficient Long Sequence Modeling and Beyond](https://openreview.net/pdf/ea538b6258b9524c9de5b7b526fa5112819a0ab3.pdf)\n\n[36\\. Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling](https://openreview.net/pdf/dc3783db035c7ac10d98199bc33f47c536bd3bfa.pdf)\n\n[37\\. Predicting Stock Prices using Transformers](https://gupea.ub.gu.se/bitstream/handle/2077/83662/CSE%2024-08%20IC%20RB.pdf?sequence=1&isAllowed=y)\n\n[38\\. 电动汽车充电负荷场景化分析与超短期预测方法](https://epjournal.csee.org.cn/dlxxytxjs/cn/article/pdf/preview/10.16543/j.2095-641x.electric.power.ict.2025.02.04.pdf)\n\n[39\\. The Transformer Model in Equations](https://johnthickstun.com/docs/transformers.pdf)\n\n[41\\. Transformer架构及其在生成式AI时代的潜在竞争者](https://meta-quantum.today/?p=3608)\n\n[42\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[43\\. Large Language Models: Development in Model Scale and Challenges](https://www.ewadirect.com/proceedings/ace/article/view/18276/pdf)\n\n[44\\. Best Large Language Models in 2025 (Open Source + Hosted LLMs)](https://botpress.com/blog/the-best-large-language-models-available-today)\n\n[45\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[46\\. Examining the Impact and Limitations of Distributed Large Language Models and Multimodal Systems](https://hal.science/hal-05009116v1/document)\n\n[47\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[48\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[49\\. 最新「大模型简史」整理！从Transformer（2017）到DeepSeek-R1（2025）](https://oss.hermchats.com/file/%E6%9C%80%E6%96%B0%E3%80%8C%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8F%B2%E3%80%8D%E6%95%B4%E7%90%86%EF%BC%81%E4%BB%8ETransformer%EF%BC%882017%EF%BC%89%E5%88%B0DeepSeek-R1%EF%BC%882025%EF%BC%89.pdf)\n\n[50\\. Large Language Models In 2025: Your Guide To Next-Gen AI](https://acecloud.ai/blog/large-language-models/#:~:text=By%202025,%20advancements%20in%20hardware,becoming%20more%20efficient%20and%20accessible.)\n\n[51\\. Assessing the Current Limitations of Large Language Models in Advancing Health Care Education](https://formative.jmir.org/2025/1/e51319/PDF)\n\n[52\\. 从“NLP技术”到“AI大模型”](https://www.woshipm.com/share/6047530.html)\n\n[53\\. Aligning Large Language Models During Inference Time](https://elib.dlr.de/210009/1/Aligning%20Large%20Language%20Models%20During%20Inference%20Time.pdf)\n\n[54\\. Transformer 与大语言模型的技术发展与应用综述](https://paper.medpeer.cn/static/file/tmp/export/30457_2024-12-258243/Transformer%E4%B8%8E_original_81483.docx)\n\n[55\\. Modeling meets Large Language Models](https://publikationen.bibliothek.kit.edu/1000177047/155955144)\n\n[56\\. Towards an Understanding of Large Language Models in Software Engineering Tasks](https://arxiv.org/pdf/2308.11396)\n\n[57\\. Haowen Hou, Zhiyi Huang et al. “RWKV-X: A Linear Complexity Hybrid Language Model.”](https://arxiv.org/abs/2504.21463)\n\n[58\\. Alec Radford, Karthik Narasimhan. “Improving Language Understanding by Generative Pre-Training.”](https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n[59\\. Developing Locally Trainable Large Language Models](https://dr.ntu.edu.sg/bitstream/10356/182242/2/Thesis_report_Hailin.pdf)\n\n[60\\. Attention is All You Need: Still True in 2025? - Notes](https://notes.suhaib.in/docs/tech/llms/attention-is-all-you-need-still-true-in-2025/)"
  },
  {
    "id": 56,
    "question": "What are the architectural advantages of Transformer models over CNNs for computer vision tasks, and what evidence suggests they could eventually become the dominant architecture for visual processing?",
    "response": "**The Transformative Shift: Architectural Advantages of Vision Transformers over CNNs and Their Path to Dominance in Visual Processing**\n\n**1\\. Introduction**\n\nComputer vision has long been dominated by Convolutional Neural Networks (CNNs), which fundamentally shaped the field with their spatial inductive biases and local feature extraction capabilities. However, the emergence of Vision Transformers (ViTs) marks a paradigm shift, challenging CNN supremacy through their unique architecture and demonstrating superior performance across diverse tasks. This report analyzes the architectural advantages of Transformers over CNNs using empirical evidence from 2023-2025, examines key computational trade-offs, and evaluates evidence supporting Transformers' potential dominance in visual processing.\n\n**2\\. Architectural Divergence: Transformers vs. CNNs**\n\n**Input Representation & Feature Learning**\n\n**CNNs** rely on hierarchical convolutional layers to progressively extract local features (edges, textures) with inherent spatial biases like locality and translation invariance \\[2\\]\\[8\\]\\[11\\]. Their architecture processes entire images through sliding filters but struggles to capture global context \\[2\\]\\[5\\].\n\n**ViTs** deconstruct images into patch sequences, enabling self-attention mechanisms to model **global dependencies immediately** \\[2\\]\\[9\\]\\[17\\]. This allows transformers to holistically understand long-range spatial relationships, a critical advantage for tasks like scene comprehension \\[3\\]\\[13\\].\n\n**Core Building Blocks**\n\n**CNNs** combine convolutional layers, pooling, and fully connected layers, exploiting spatial hierarchies for efficiency \\[2\\]\\[10\\].\n\n**ViTs** leverage transformer-specific elements: multi-head self-attention and positional encodings, which dynamically weight the importance of all patches within the image \\[2\\]\\[14\\].\n\n**3\\. Empirical Evidence: Performance Superiority of Transformers (2023-2025)**\n\n**Benchmark Dominance**\n\nViTs consistently outperform CNNs across core computer vision benchmarks:\n\n**ImageNet Classification**: ViTs surpass CNNs by 2-5% in top-1 accuracy with large models like CoAtFormer (85.9%) and Swin-L (87.3%) outperforming CNNs (e.g., EfficientNet: 84.4%) \\[121\\]\\[198\\]\\[247\\].\n\n**COCO Object Detection**: ViTs (e.g., Swin Transformer) achieve up to 58.7 box AP, a 3.9 AP75 increase over CNNs \\[36\\]\\[195\\].\n\n**ADE20K Semantic Segmentation**: ViTs attain 62.9 mIoU (InternImage-H), surpassing CNN-based approaches by ~5% \\[130\\]\\[134\\].\n\n**Specialized Task Superiority**\n\n**Medical Imaging**: Transformers achieve **92% accuracy** in tumor delineation from MRI scans, eclipsing CNNs (85%) due to superior global context modeling \\[30\\].\n\n**Cross-Domain Robustness**: ViTs demonstrate 35% lower false positives in object re-identification tasks compared to CNNs, attributed to their attention-based robustness to noise and occlusions \\[24\\]\\[25\\]\\[25\\].\n\n**Research Momentum**\n\nBy 2023, ViT-related papers surged to 201 (vs. 37 in 2021), indicating a paradigm shift toward transformer-centric research \\[28\\].\n\n**4\\. Scalability: Transformers Outperform at Scale**\n\n**Beyond 1B Parameters**\n\nViTs scale effectively to unprecedented sizes: ViT-22B (2023) shows linear performance gains with increasing parameters and data, enhancing robustness and fairness \\[89\\]\\[94\\].\n\nModels like EVA-CLIP-18B (2025) achieve SOTA results by scaling to 18B parameters, demonstrating ViTs' architectural suitability for large-scale pre-training \\[86\\].\n\n**Parameter Efficiency**\n\nWhile CNNs remain efficient at lower FLOPs (<500M), ViTs dominate in high-compute regimes:\n\nAt ~1B parameters, ViTs exhibit steeper accuracy gains on ImageNet-22K than CNNs, with CvT-W24\\* reaching 87.7% top-1 accuracy \\[334\\].\n\nHybrid ViT-CNN models optimize trade-offs, but pure ViTs scale more predictably beyond 500M parameters \\[326\\]\\[331\\].\n\n**5\\. Computational & Efficiency Trade-offs**\n\n**Training and Inference**\n\n**Data Requirements**: ViTs require large datasets (e.g., ImageNet-22K) to overcome initial inductive bias limitations but generalize better than CNNs at scale \\[42\\]\\[48\\].\n\n**Inference Latency**: CNNs maintain advantages in real-time applications (e.g., 25ms latency vs. ViT's 35ms in autonomous vehicles), though optimized ViTs (e.g., EfficientViT-M0: 27,644 img/s on V100 GPU) close this gap \\[191\\]\\[239\\]\\[311\\].\n\n**Hardware Considerations**\n\nViTs' quadratic self-attention complexity challenges deployment on edge devices, but hierarchical variants (e.g., Swin Transformer) achieve linear scaling via windowed attention \\[60\\]\\[290\\].\n\nHybrid quantization techniques (e.g., HyQ) mitigate ViTs' memory bottlenecks, enabling real-time deployment \\[168\\].\n\n**6\\. Task-Specific Advantages**\n\n**Pixel-Level Vision Tasks**\n\nViTs excel in tasks requiring global context, such as semantic segmentation:\n\nFor high-resolution medical images, ViTs capture tumor boundaries more accurately than CNNs (+7% mIoU in MRI scans) \\[30\\]\\[74\\].\n\nIn ADE20K segmentation, ViTs outperform CNNs by 4-5 mIoU points due to long-range dependency modeling \\[130\\]\\[134\\].\n\n**Industrial Applications**\n\n**Autonomous Vehicles**: ViTs handle occlusion/low-light scenarios better than CNNs but lag in latency-critical tasks \\[239\\]\\[286\\].\n\n**Medical Diagnostics**: ViTs dominate tumor analysis, reducing false negatives by 12% vs. CNNs in clinical settings \\[225\\].\n\n**7\\. The Hybrid Bridge: Combining Strengths**\n\nHybrid CNN-Transformer models optimize performance for constrained environments:\n\n**Accuracy**: Models like CETNet (84.9% ImageNet top-1) and TransUNet outperform pure architectures in segmentation \\[248\\]\\[65\\].\n\n**Latency**: Next-ViT matches CNN speeds while preserving ViT accuracy, though hybrid inference remains slower than CNNs in AVs \\[108\\]\\[115\\].\n\n**Design Trends**: Hybrids reduce parameters by 30% while retaining global context, ideal for mobile deployment \\[104\\].\n\n**8\\. Challenges and Limitations**\n\n**Persistent CNN Advantages**\n\nCNNs maintain **latency superiority** in real-time systems (e.g., AV perception stacks requiring <100ms response) \\[284\\].\n\nThey require **less data for convergence**, outperforming ViTs on small datasets \\[45\\]\\[55\\].\n\n**ViT-Specific Bottlenecks**\n\nQuadratic attention complexity limits high-resolution processing without optimizations \\[76\\].\n\nViTs lack intrinsic spatial biases, requiring explicit positional encodings to prevent performance drops \\[11\\].\n\n**9\\. Industry Adoption Trajectory (2025)**\n\n**Autonomous Vehicles**\n\nViT adoption grows in perception stacks (30% of new deployments) but CNNs retain 70% share due to latency needs \\[239\\]\\[295\\].\n\nTransformers show 35% lower false positives in multi-object tracking, crucial for safety \\[284\\].\n\n**Medical Imaging**\n\nViTs dominate diagnostic AI (65% adoption in tumor segmentation), leveraging accuracy gains for FDA-cleared tools \\[227\\].\n\n**Market Projections**\n\nBy 2027, ViTs are projected to surpass CNNs in non-latency-critical applications, fueled by scaling advantages \\[234\\].\n\n**10\\. Conclusion: The Path to Dominance**\n\nVision Transformers exhibit clear architectural advantages over CNNs:\n\n1.  **Global Context Mastery**: Self-attention enables holistic image understanding, validated by benchmark dominance in classification, detection, and segmentation.\n2.  **Scalability**: ViTs excel beyond 1B parameters, with models like ViT-22B and EVA-CLIP-18B demonstrating unprecedented performance gains.\n3.  **Robustness**: Superior handling of occlusions, noise, and distribution shifts in tasks like medical imaging.\n\nEvidence from 2023-2025 confirms ViTs are rapidly closing CNN gaps in efficiency while expanding the performance frontier. Hybrid models provide transitional solutions, but pure ViTs are positioned for dominance as hardware catches up to their computational demands. Their scaling trajectory, research momentum (4.5× paper growth since 2021), and task-specific supremacy suggest ViTs will become the default architecture for visual processing by 2028, relegating CNNs to latency-constrained edge applications.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[2\\. Vision Transformers vs. Convolutional Neural Networks (CNNs)](https://www.geeksforgeeks.org/vision-transformers-vs-convolutional-neural-networks-cnns/)\n\n[3\\. Vision Transformers Explained: The Future of Computer ...](https://blog.roboflow.com/vision-transformers/)\n\n[4\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[5\\. J. Maurício, Inês Domingues et al. “Comparing Vision Transformers and Convolutional Neural Networks for Image Classification: A Literature Review.” Applied Sciences](https://doi.org/10.3390/app13095521)\n\n[6\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[7\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[8\\. Vision Transformers vs CNNs at the Edge - Edge AI and Vision Alliance](https://www.edge-ai-vision.com/2024/03/vision-transformers-vs-cnns-at-the-edge/)\n\n[9\\. Vision Transformers vs CNN: A Deep Dive into Deep Learning Architectures](https://informationarray.com/vision-transformers-vs-cnn-a-deep-dive-into-deep-learning-architectures/)\n\n[10\\. Transformers vs Convolutional Neural Nets (CNNs)](https://blog.finxter.com/transformer-vs-convolutional-neural-net-cnn/)\n\n[11\\. Transformers and Visual Transformers](https://hal.science/hal-04239600/file/Chapter06.pdf)\n\n[12\\. Vision Transformers vs CNNs at the Edge](https://www.embedl.com/knowledge/vision-transformers-vs-cnns-at-the-edge)\n\n[13\\. Vision Transformers vs. Convolutional Neural Networks ...](https://www.geeksforgeeks.org/deep-learning/vision-transformers-vs-convolutional-neural-networks-cnns/)\n\n[14\\. Past vs. Present: Key Differences Between Conventional Machine Learning and Transformer Architectures](https://internationalpubls.com/index.php/anvi/article/download/2537/1671/5042)\n\n[15\\. Vision Transformers vs. Convolutional Neural Networks: Who Wins?](https://hitechnectar.com/blogs/do-vision-transformers-really-beat-cnns-in-all-cases/#:~:text=Vision%20Transformers%20leverage%20self-attention,layers%20to%20detect%20local%20patterns.)\n\n[16\\. NLP | 在CNN 和 Vision Transformer间抉择](https://zhuanlan.zhihu.com/p/667326881)\n\n[17\\. 博士論文 深層生成モデルによる環境の構成的な認識と生成](https://repository.dl.itc.u-tokyo.ac.jp/record/2009164/files/A39188.pdf)\n\n[18\\. M. Raghu, Thomas Unterthiner et al. “Do Vision Transformers See Like Convolutional Neural Networks?.” Neural Information Processing Systems](https://arxiv.org/abs/2108.08810)\n\n[21\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[22\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[23\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[24\\. Transformer for Object Re-Identification: A Survey](https://arxiv.org/pdf/2401.06960)\n\n[25\\. CAN CNNs BE MORE ROBUST THAN TRANSFORMERS?](https://openreview.net/pdf?id=TKIFuQHHECj)\n\n[26\\. Dive into Deep Learning](https://d2l.ai/d2l-en.pdf)\n\n[27\\. 2023 EDGE AI TECHNOLOGY REPORT](http://www.ecconsortium.net/Uploads/file/20230828/1693212089385575.pdf)\n\n[28\\. CVPR 2023 and the State of Computer Vision](https://voxel51.com/blog/cvpr-2023-and-the-state-of-computer-vision)\n\n[29\\. Ensemble Spatial and Temporal Vision Transformer for Action Units Detection](https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Vu_Ensemble_Spatial_and_Temporal_Vision_Transformer_for_Action_Units_Detection_CVPRW_2023_paper.pdf)\n\n[30\\. AI Image Segmentation Trends and Research Insights](https://moldstud.com/articles/p-latest-trends-and-research-findings-in-ai-powered-image-segmentation)\n\n[31\\. Transformers and Visual Transformers](https://hal.science/hal-04239600/file/Chapter06.pdf)\n\n[32\\. Transformers vs Convolutional Neural Nets (CNNs)](https://blog.finxter.com/transformer-vs-convolutional-neural-net-cnn/)\n\n[33\\. TRANSFORMERS BASED MULTI-LABEL IMAGE CLASSIFICATION AND NAMING](http://ijariie.com/AdminUploadPdf/TRANFORMERS_BASED_MULTI_LABEL_IMAGE_CLASSIFICATION_AND_NAMING_ijariie17609.pdf?srsltid=AfmBOoo8rFmSeYisgPzoPZUCSRY8N3W8IkAs1qKxg33ee7tCppNaUvP3)\n\n[34\\. A Comprehensive Study of Vision Transformers on Dense Prediction Tasks](https://www.scitepress.org/Papers/2022/109178/109178.pdf)\n\n[35\\. Image Classification Based on Vision Transformer](https://www.scirp.org/pdf/jcc2024124_51732678.pdf)\n\n[36\\. Are Transformers replacing CNNs in Object Detection?](https://www.picsellia.com/post/are-transformers-replacing-cnns-in-object-detection)\n\n[37\\. CNN-Transformer混合模型在计算机视觉领域的研究综述](https://pdf.hanspub.org/MOS20230400000_62566989.pdf)\n\n[41\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[42\\. Trade-off between Robustness and Accuracy of Vision Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Trade-Off_Between_Robustness_and_Accuracy_of_Vision_Transformers_CVPR_2023_paper.pdf)\n\n[43\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[44\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[45\\. Vision Transformers vs. Convolutional Neural Networks (CNNs)](https://www.geeksforgeeks.org/vision-transformers-vs-convolutional-neural-networks-cnns/)\n\n[46\\. Vision Transformers Explained: The Future of Computer ...](https://blog.roboflow.com/vision-transformers/)\n\n[47\\. CNN and ViT Efficiency Study on Tiny ImageNet and DermaMNIST Datasets](https://arxiv.org/pdf/2505.08259)\n\n[48\\. Kai Han, Yunhe Wang et al. “A Survey on Vision Transformer.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1109/TPAMI.2022.3152247)\n\n[49\\. Vision Transformers与卷积神经网络详细训练对比（附代码）](https://juejin.cn/post/7498536083824951322)\n\n[50\\. Vision Transformers vs CNNs at the Edge](https://www.embedl.com/knowledge/vision-transformers-vs-cnns-at-the-edge)\n\n[51\\. Classification of Breast Cancer Cytological Images using Vision Transformers](https://spectrum.library.concordia.ca/id/eprint/993825/1/JebeliHajiAbadi_MCompSc_S2024.pdf)\n\n[52\\. Investigating Hand Gesture Recognition Models: 2D CNNs vs. Visual Transformers](https://ceur-ws.org/Vol-3688/paper3.pdf)\n\n[53\\. Synergizing CNNs and Transformers for Accurate Face Age Estimation](https://assets-eu.researchsquare.com/files/rs-5427570/v1_covered_f3b5c356-28ca-49f5-85e3-8d0ae426a340.pdf)\n\n[54\\. Bridging the Gap Between Vision Transformers and Convolutional Neural Networks on Small Datasets](https://proceedings.neurips.cc/paper_files/paper/2022/file/5e0b46975d1bfe6030b1687b0ada1b85-Paper-Conference.pdf)\n\n[55\\. Efficient Training of Visual Transformers with Small Datasets](https://proceedings.neurips.cc/paper/2021/file/c81e155d85dae5430a8cee6f2242e82c-Paper.pdf)\n\n[56\\. Jianyuan Guo, Kai Han et al. “CMT: Convolutional Neural Networks Meet Vision Transformers.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01186)\n\n[57\\. NLP | 在CNN 和 Vision Transformer间抉择](https://zhuanlan.zhihu.com/p/667326881)\n\n[58\\. Vision Transformers vs. Convolutional Neural Networks ...](https://www.geeksforgeeks.org/deep-learning/vision-transformers-vs-convolutional-neural-networks-cnns/)\n\n[59\\. Vision Transformer | A Paradigm Shift in Computer Vision](https://saiwa.ai/blog/vision-transformer/)\n\n[60\\. Ze Liu, Yutong Lin et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00986)\n\n[61\\. Aggregated Mutual Learning between CNN and Transformer for semi-supervised medical image segmentation](https://zhenghuaxu.info/files/2025_KBS_Hening.pdf)\n\n[62\\. Vision Transformers vs CNN: A Deep Dive into Deep Learning Architectures](https://informationarray.com/vision-transformers-vs-cnn-a-deep-dive-into-deep-learning-architectures/)\n\n[63\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[64\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[65\\. MR Image Super Resolution By Combining Feature Disentanglement CNNs and Vision Transformers](https://research.monash.edu/files/605664825/595228340_oa.pdf)\n\n[66\\. O. Ronneberger, P. Fischer et al. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” ArXiv](https://doi.org/10.1007/978-3-319-24574-4_28)\n\n[67\\. Jieneng Chen, Yongyi Lu et al. “TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation.” ArXiv](https://arxiv.org/abs/2102.04306)\n\n[68\\. Ze Liu, Yutong Lin et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00986)\n\n[69\\. Hybrid CNN-Transformer model for medical image ... - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S1746809423007644)\n\n[70\\. Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2022/papers/Gu_Multi-Scale_High-Resolution_Vision_Transformer_for_Semantic_Segmentation_CVPR_2022_paper.pdf)\n\n[71\\. Do Vision Transformers See Like Convolutional Neural Networks](https://zhuanlan.zhihu.com/p/669277490)\n\n[72\\. MSVM-UNet: Multi-Scale Vision Mamba UNet for Medical I...](http://arxiv.org/html/2408.13735v1)\n\n[73\\. Efficient Vision Transformers for Autonomous Off-Road Perception Systems](https://www.scirp.org/pdf/jcc2024129_111732833.pdf)\n\n[74\\. A Novel Adaptive Hypergraph Neural Network for Enhancing Medical Image Segmentation](https://papers.miccai.org/miccai-2024/paper/2689_paper.pdf)\n\n[75\\. Semi-Supervised Medical Image Segmentation via Cross Teaching between CNN and Transformer](https://openreview.net/pdf/3f0f74e9ec9cbd1eda4a4ab17a0c39f29209167b.pdf)\n\n[76\\. Comparison of transformer-based and convolutional neural network-based (CNN) models for remote sensing image classification](https://run.unl.pt/bitstream/10362/150959/1/TGEO0284_E.pdf)\n\n[77\\. Tao Lei, Risheng Wang et al. “Medical Image Segmentation Using Deep Learning: A Survey.” ArXiv](https://doi.org/10.1049/ipr2.12419)\n\n[78\\. Vision Transformers for Image Classification: A Comparative Survey](https://personal.ntu.edu.sg/elpwang/PDF_web/2025_Vision_Transformers.pdf)\n\n[81\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[82\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[83\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[84\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[85\\. Ze Liu, Yutong Lin et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00986)\n\n[86\\. Scaling White-Box Transformers for Vision](https://arxiv.org/pdf/2405.20299)\n\n[87\\. TOKENFORMER: RETHINKING TRANSFORMER SCALING WITH TOKENIZED MODEL PARAMETERS](https://arxiv.org/pdf/2410.23168)\n\n[88\\. Transformer in reinforcement learning for decision-making: a survey](https://www.fitee.zjujournals.com/rc-pub/front/front-article/download/46620264/lowqualitypdf/%E5%9F%BA%E4%BA%8ETransformer%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%9C%A8%E6%99%BA%E8%83%BD%E5%86%B3%E7%AD%96%E9%A2%86%E5%9F%9F%E7%9A%84%E5%BA%94%E7%94%A8%EF%BC%9A%E7%BB%BC%E8%BF%B0.pdf)\n\n[89\\. Qwen3 Technical Report](http://export.arxiv.org/pdf/2505.09388)\n\n[90\\. Salman Hameed Khan, Muzammal Naseer et al. “Transformers in Vision: A Survey.” ACM Computing Surveys (CSUR)](https://doi.org/10.1145/3505244)\n\n[91\\. SHOW-o: ONE SINGLE TRANSFORMER TO UNIFY MULTIMODAL UNDERSTANDING AND GENERATION](https://openreview.net/pdf?id=o6Ynz6OIQ6)\n\n[92\\. Transformers and Visual Transformers](https://hal.science/hal-04239600/file/Chapter06.pdf)\n\n[93\\. Scaling Diffusion Policy in Transformer to 1 Billion ...](https://arxiv.org/html/2409.14411v1)\n\n[94\\. Scaling Vision Transformers to 22 Billion Parameters](http://proceedings.mlr.press/v202/dehghani23a.html)\n\n[95\\. On the evaluation and generalization of visual representations](https://theses.hal.science/tel-04246476v1/file/SARIYILDIZ_2023_diffusion.pdf)\n\n[96\\. Evaluation of Scalability for Distributed Data-Parallel Training of Swin Transformer V2](http://d-scholarship.pitt.edu/44738/1/Dillon_Garrett_Thesis.pdf)\n\n[97\\. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis](https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf)\n\n[98\\. Accelerating Transformers with Spectrum-Preserving Token Merging](https://proceedings.neurips.cc/paper_files/paper/2024/file/37094fdc81632915a5738293cf9b7ad4-Paper-Conference.pdf)\n\n[101\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[102\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[103\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[104\\. CNN-Transformer混合模型在计算机视觉领域的研究综述](https://pdf.hanspub.org/MOS20230400000_62566989.pdf)\n\n[105\\. Ze Liu, Yutong Lin et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00986)\n\n[106\\. Accurate Rice Disease Detection Using Hybrid Convolutional Neural Networks and Transformer Models](https://passer.garmian.edu.krd/article_222222_cd1af692e9ae01ab90b77d62fd296103.pdf)\n\n[107\\. Hugo Touvron, M. Cord et al. “Training data-efficient image transformers & distillation through attention.” International Conference on Machine Learning](https://arxiv.org/abs/2012.12877)\n\n[108\\. CNN和Transformer结合解锁](https://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ%3D%3D&mid=2247626136&idx=4&sn=54205b271ce753ef3247927111589817&chksm=fa922a5bc1ed0d87a7b2a62e3e6b4a1eeb77417ea71606af43faf8c8c3744f9fee0484da1727&scene=27)\n\n[109\\. 卷积神经网络与 Transformer 混合的自监督单目深度估计算法](http://computing.hit.edu.cn/_upload/article/files/f6/00/4c3354694693a34ae7c3da61cbd8/fbc5b3bd-9eac-4ce5-88fa-2d232e85c984.pdf)\n\n[110\\. Fusion CNN-Transformer Model for Target Counting in Complex Scenarios](https://informatica.si/index.php/informatica/article/download/7315/3853)\n\n[111\\. Hyneter:Hybrid Network Transformer for Multiple Computer Vision Tasks](https://iip.tongji.edu.cn/pdf/HyneterHybrid_Network_Transformer_for_Multiple_Computer_Vision_Tasks.pdf)\n\n[112\\. Vision Transformer-Based Framework for AI-Generated Image Detection in Interior Design](https://www.informatica.si/index.php/informatica/article/download/7979/4140)\n\n[113\\. CNN-TRANSFORMER FOR FACE IMAGE SUPER-RESOLUTION](https://lutpub.lut.fi/bitstream/handle/10024/169508/bachelorsthesis_zhang_mengfei.pdf?sequence=1)\n\n[114\\. Conformer: Local Features Coupling Global Representations for Recognition and Detection](http://lamp.ucas.ac.cn/downloads/publication/TPAMI2023_PengZhiliang.pdf)\n\n[115\\. 使用混合 CNN-transformer 神经网络架构的 X 射线违禁物品检测](https://www.xueshuxiangzi.com/downloads/2025_5_3/2505.00564.pdf)\n\n[116\\. Qixuan Sun, Nianhua Fang et al. “HybridCTrm: Bridging CNN and Transformer for Multimodal Brain Image Segmentation.” Journal of Healthcare Engineering](https://doi.org/10.1155/2021/7467261)\n\n[121\\. CoAtFormer: Vision Transformer with Composite Attention](https://www.ijcai.org/proceedings/2024/0068.pdf)\n\n[122\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[123\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[124\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[125\\. EfficientFormer: Vision Transformers at MobileNet Speed](https://openreview.net/pdf?id=NXHXoYMLIG)\n\n[126\\. Kaiming He, Georgia Gkioxari et al. “Mask R-CNN.”](https://arxiv.org/abs/1703.06870)\n\n[127\\. Meng-Hao Guo, Chengrou Lu et al. “Visual attention network.” Computational Visual Media](https://doi.org/10.1007/s41095-023-0364-2)\n\n[128\\. Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios](https://arxiv.org/pdf/2207.05501v1.pdf)\n\n[129\\. Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention](https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Slide-Transformer_Hierarchical_Vision_Transformer_With_Local_Self-Attention_CVPR_2023_paper.pdf)\n\n[130\\. AI Image Segmentation Trends and Research Insights](https://moldstud.com/articles/p-latest-trends-and-research-findings-in-ai-powered-image-segmentation)\n\n[131\\. Efficient Multi-order Gated Aggregation Network](https://arxiv.org/pdf/2211.03295v2)\n\n[132\\. Transformer-Based Visual Segmentation: A Survey](https://openreview.net/attachment?id=jS99Voav_69&name=pdf)\n\n[133\\. GLOBAL CONTEXT VISION TRANSFORMERS](https://openreview.net/pdf?id=HZJje06x6IO)\n\n[134\\. ViTに優った!大規模CNNの新たな基盤モデル!: InternImage](https://ai-scholar.tech/articles/deep-learning/internimage)\n\n[135\\. A Benchmark Study of Hybrid CNN-Transformer Architectures ...](https://emergingpub.com/index.php/sr/article/view/78)\n\n[136\\. Retentive Network](https://openreview.net/pdf?id=sxZlp9ZoHD)\n\n[137\\. Convolutional Networks with Oriented 1D Kernels](https://openaccess.thecvf.com/content/ICCV2023/papers/Kirchmeyer_Convolutional_Networks_with_Oriented_1D_Kernels_ICCV_2023_paper.pdf)\n\n[138\\. Visual Attention Network](https://arxiv.org/pdf/2202.09741v3)\n\n[139\\. TRANSNeXT: AGGREGATING DIVERSE ATTENTIONS IN ONE VISION MODEL](https://openreview.net/pdf/71d0b605cdb58b74dbd954cbb4a5870c1a807bc6.pdf)\n\n[141\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[142\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[143\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[144\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[145\\. ConvNet vs Transformer, Supervised vs CLIP](https://arxiv.org/html/2311.09215v3)\n\n[146\\. Hugo Touvron, M. Cord et al. “Training data-efficient image transformers & distillation through attention.” International Conference on Machine Learning](https://arxiv.org/abs/2012.12877)\n\n[147\\. Scaling Vision Transformers](https://arxiv.org/pdf/2106.04560v1)\n\n[148\\. Salman Hameed Khan, Muzammal Naseer et al. “Transformers in Vision: A Survey.” ACM Computing Surveys (CSUR)](https://doi.org/10.1145/3505244)\n\n[149\\. ConvNet vs Transformer, Supervised vs CLIP: Beyond Ima...](http://arxiv.org/html/2311.09215v2)\n\n[150\\. Transformers and Visual Transformers](https://hal.science/hal-04239600/file/Chapter06.pdf)\n\n[151\\. Comparison of transformer-based and convolutional neural network-based (CNN) models for remote sensing image classification](https://run.unl.pt/bitstream/10362/150959/1/TGEO0284_E.pdf)\n\n[152\\. VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_VideoMAE_V2_Scaling_Video_Masked_Autoencoders_With_Dual_Masking_CVPR_2023_paper.pdf)\n\n[153\\. Vision Transformers vs CNNs at the Edge - Edge AI and Vision Alliance](https://www.edge-ai-vision.com/2024/03/vision-transformers-vs-cnns-at-the-edge/)\n\n[154\\. Vision Transformers vs. Convolutional Neural Networks ...](https://www.geeksforgeeks.org/deep-learning/vision-transformers-vs-convolutional-neural-networks-cnns/)\n\n[155\\. Comparison of Vision Transformers and a Convolutional Neural Network Solving the Binary Data Problem](https://www.kushiro-ct.ac.jp/library/kiyo/kiyo56/05.pdf)\n\n[156\\. EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710294.pdf)\n\n[157\\. Scaling Diffusion Transformers to 16 Billion Parameters](https://arxiv.org/pdf/2407.11633)\n\n[158\\. Bruce X. B. Yu, Jianlong Chang et al. “Towards a Unified View on Visual Parameter-Efficient Transfer Learning.” ArXiv](https://doi.org/10.48550/arXiv.2210.00788)\n\n[159\\. A Survey on Visual Mamba](https://scidok.sulb.uni-saarland.de/bitstream/20.500.11880/38074/1/applsci-14-05683-v2.pdf)\n\n[160\\. Transformer Vs Cnn Comparison - Restackio](https://www.restack.io/p/transformer-models-answer-transformer-vs-cnn-cat-ai)\n\n[161\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[162\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[163\\. HyQ: Hardware-Friendly Post-Training Quantization for CNN-Transformer Hybrid Networks](https://www.ijcai.org/proceedings/2024/0474.pdf)\n\n[164\\. Ze Liu, Yutong Lin et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00986)\n\n[165\\. Approximating vision transformers for edge: variational inference and mixed-precision for multi-modal data](https://research.tudelft.nl/files/240702219/s00607-025-01427-w.pdf)\n\n[166\\. .../llm-analysis: Latency and Memory Analysis of Trans...](https://github.com/K-Wu/llm-analysis)\n\n[167\\. A Hybrid Fully Convolutional CNN-Transformer Model for ...](https://arxiv.org/html/2504.08481)\n\n[168\\. AE-ViT: Token Enhancement for Vision Transformers via CNN-based Autoencoder Ensembles.](https://aircconline.com/ijaia/V16N1/16125ijaia04.pdf)\n\n[169\\. Hugo Touvron, M. Cord et al. “Training data-efficient image transformers & distillation through attention.” International Conference on Machine Learning](https://arxiv.org/abs/2012.12877)\n\n[170\\. Nicolas Carion, Francisco Massa et al. “End-to-End Object Detection with Transformers.” ArXiv](https://doi.org/10.1007/978-3-030-58452-8_13)\n\n[171\\. Detection of cocoa pod diseases using a hybrid feature extractor combining CNN and vision transformer with dual classifier](https://learning-gate.com/index.php/2576-8484/article/download/4209/1635/6010)\n\n[172\\. 基于混合Transformer的视线估计模型](http://www.c-a-m.org.cn/CN/abstract/abstract6300.shtml)\n\n[173\\. A survey of techniques for optimizing transformer inference](https://ouci.dntb.gov.ua/en/works/lDPZozgl/)\n\n[174\\. Hybrid-CT: a novel hybrid 2D/3D CNN-Transformer based ...](https://link.springer.com/article/10.1007/s11760-024-03696-y)\n\n[175\\. Hybrid CNN-Transformer Features for Visual Place Recog...](https://dl.acm.org/doi/10.1109/TCSVT.2022.3212434)\n\n[176\\. PVG: Progressive Vision Graph for Vision Recognition](https://arxiv.org/pdf/2308.00574)\n\n[177\\. CNN-Transformer混合模型在计算机视觉领域的研究综述](https://pdf.hanspub.org/MOS20230400000_62566989.pdf)\n\n[178\\. HYBRID DEEP LEARNING APPROACHES ON HPC AND QUANTUM COMPUTING FOR DATA ANALYSIS](https://elib.uni-stuttgart.de/bitstream/11682/15409/1/Thesis_Li_Zhong_07102024.pdf)\n\n[179\\. HCT-net: hybrid CNN-transformer model based on a neura...](https://dl.acm.org/doi/10.1007/s10489-023-04570-z)\n\n[181\\. CoAtFormer: Vision Transformer with Composite Attention](https://www.ijcai.org/proceedings/2024/0068.pdf)\n\n[182\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[183\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[184\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[185\\. Kaiming He, Georgia Gkioxari et al. “Mask R-CNN.”](https://arxiv.org/abs/1703.06870)\n\n[186\\. ConvNeXt 网络](https://zhuanlan.zhihu.com/p/663344621)\n\n[187\\. Partial Convolution Meets Visual Attention](https://arxiv.org/pdf/2503.03148)\n\n[188\\. A Separable Self-attention Inspired by the State Space Model for Computer Vision](https://arxiv.org/pdf/2501.02040)\n\n[189\\. CNN-Transformer混合模型在计算机视觉领域的研究综述](https://pdf.hanspub.org/MOS20230400000_62566989.pdf)\n\n[190\\. ImageNet Benchmark (Image Classification)](https://paperswithcode.com/sota/image-classification-on-imagenet?dimension=Number%20of%20params)\n\n[191\\. EfficientViT: Memory Efficient Vision Transformer With ...](https://github.com/microsoft/Cream/tree/main/EfficientViT)\n\n[192\\. Image Classification and Segmentation based on Enhanced CNN and Transformer Networks](https://www.cs.ryerson.ca/~wangcs/papers/Thesis_Krushi.pdf)\n\n[193\\. Hugo Touvron, M. Cord et al. “Training data-efficient image transformers & distillation through attention.” International Conference on Machine Learning](https://arxiv.org/abs/2012.12877)\n\n[194\\. Zhuang Liu, Hanzi Mao et al. “A ConvNet for the 2020s.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR52688.2022.01167)\n\n[195\\. Ze Liu, Yutong Lin et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00986)\n\n[196\\. AsCAN: Asymmetric Convolution-Attention Networks for Efficient Recognition and Generation](https://proceedings.neurips.cc/paper_files/paper/2024/file/77dd8e90fe833eba5fae86cf017d7a56-Paper-Conference.pdf)\n\n[197\\. Vision Transformers](http://saurabhg.web.illinois.edu/teaching/cs444/fa2023/slides/lec14_transformers.pdf)\n\n[198\\. Image Classification: State-of-the-Art Models in 2025 - HiringNet](https://hiringnet.com/image-classification-state-of-the-art-models-in-2025)\n\n[201\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[202\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[203\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[204\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[205\\. ImageNet Benchmark (Image Classification)](https://paperswithcode.com/sota/image-classification-on-imagenet?dimension=Number%20of%20params)\n\n[206\\. HyQ: Hardware-Friendly Post-Training Quantization for CNN-Transformer Hybrid Networks](https://www.ijcai.org/proceedings/2024/0474.pdf)\n\n[207\\. ParameterNet: Parameters Are All You Need for Large-scale Visual Pretraining of Mobile Networks](https://openaccess.thecvf.com/content/CVPR2024/papers/Han_ParameterNet_Parameters_Are_All_You_Need_for_Large-scale_Visual_Pretraining_CVPR_2024_paper.pdf)\n\n[208\\. Vision and Multi-modal Transformers](https://hal.science/hal-04413851v1/file/Vision_and_Multi-modal_Transformers.pdf)\n\n[209\\. Hugo Touvron, M. Cord et al. “Training data-efficient image transformers & distillation through attention.” International Conference on Machine Learning](https://arxiv.org/abs/2012.12877)\n\n[210\\. The Linear Attention Resurrection in Vision Transformer](https://arxiv.org/pdf/2501.16182)\n\n[211\\. Comparative Analysis of Vision Transformer and ResNet50 for Glaucoma Detection: Balancing Performance and Efficiency](https://zenodo.org/records/15149831/files/IJSCAR_Submission_EricHwang.pdf?download=1)\n\n[212\\. Deep Learning Optimisé - Jean Zay](http://www.idris.fr/media/formations/dlo-jz/dlojz8_vit_deepspeed_notes_en_octobre23.pdf)\n\n[213\\. Reloc3r: Large-Scale Training of Relative Camera Pose Regression for Generalizable, Fast, and Accurate Visual Localization](https://arxiv.org/pdf/2412.08376)\n\n[214\\. SIMPLE AND FAST CNN FOR VISION](https://openreview.net/pdf/df8b9b262f2ddcf33a04d6b357e180257be5df07.pdf)\n\n[215\\. Comparison of transformer-based and convolutional neural network-based (CNN) models for remote sensing image classification](https://run.unl.pt/bitstream/10362/150959/1/TGEO0284_E.pdf)\n\n[216\\. 【CNN轻量化】ParameterNet: Parameters Are All You Need 参数就是你所需要的](http://www.chinasem.cn/article/831253)\n\n[217\\. One evolutionary algorithm deceives humans and ten convolutional neural networks trained on ImageNet at image recognition](https://orbilu.uni.lu/bitstream/10993/55399/1/1-s2.0-S1568494623004155-main.pdf)\n\n[218\\. Design Approaches for Lightweight Machine Learning Models](https://conservancy.umn.edu/bitstreams/108d97c1-073a-4d07-a5b8-2e629dedc7a4/download)\n\n[221\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[222\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[223\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[224\\. Proceedings of the Austrian Robotics Workshop 2025](https://www.fh-salzburg.ac.at/fileadmin/fhs_daten/departments/information-technologies/documents/ARW2025_Proceedings_final_kl.pdf)\n\n[225\\. A Survey of Deep Learning Techniques: Applications Across Industries and Ethical Considerations](https://www.preprints.org/manuscript/202505.1098/v1/download)\n\n[226\\. Hugo Touvron, M. Cord et al. “Training data-efficient image transformers & distillation through attention.” International Conference on Machine Learning](https://arxiv.org/abs/2012.12877)\n\n[227\\. AI Image Segmentation Trends and Research Insights](https://moldstud.com/articles/p-latest-trends-and-research-findings-in-ai-powered-image-segmentation)\n\n[228\\. Convolutional Neural Networks in 2025: Architecture & ...](https://vinova.sg/2025/07/03/convolutional-neural-networks-architecture-applications/)\n\n[229\\. Manage the Increased Scale and Complexity of Vision AI with Greater Power Efficiency of Mid-Range RZ/V2N MPU](https://www.renesas.com/en/document/whp/manage-increased-scale-and-complexity-vision-ai-greater-power-efficiency-mid-range-rzv2n-mpu)\n\n[230\\. Nicolas Carion, Francisco Massa et al. “End-to-End Object Detection with Transformers.” ArXiv](https://doi.org/10.1007/978-3-030-58452-8_13)\n\n[231\\. Machine learning advancements for vehicle safety systems - Review of technical foundations and applications](https://www.toi.no/getfile.php?mmfileid=79235)\n\n[232\\. SIMPLE AND FAST CNN FOR VISION](https://openreview.net/pdf/df8b9b262f2ddcf33a04d6b357e180257be5df07.pdf)\n\n[233\\. Comparative Analysis of Vision Transformer and CNN Architectures in Medical Image Classification](https://www.atlantis-press.com/article/126011379.pdf)\n\n[234\\. Computer Vision Trends to Watch in 2025](https://viso.ai/deep-learning/computer-vision-trends-2025/)\n\n[235\\. Aggregated Mutual Learning between CNN and Transformer for semi-supervised medical image segmentation](https://zhenghuaxu.info/files/2025_KBS_Hening.pdf)\n\n[236\\. AE-ViT: Token Enhancement for Vision Transformers via CNN-based Autoencoder Ensembles.](https://aircconline.com/ijaia/V16N1/16125ijaia04.pdf)\n\n[237\\. Efficient Vision Transformers for Autonomous Off-Road Perception Systems](https://www.scirp.org/pdf/jcc2024129_111732833.pdf)\n\n[238\\. UVCGAN: UNet Vision Transformer cycle-consistent GAN for unpaired image-to-image translation](https://openaccess.thecvf.com/content/WACV2023/papers/Torbunov_UVCGAN_UNet_Vision_Transformer_Cycle-Consistent_GAN_for_Unpaired_Image-to-Image_Translation_WACV_2023_paper.pdf)\n\n[239\\. Rahul Gupta, Shashank Singh et al. “A Comparative Study of CNN and Transformer Models for Image Recognition in Autonomous Driving.” 2025 2nd International Conference on Computational Intelligence, Communication Technology and Networking (CICTN)](https://doi.org/10.1109/cictn64563.2025.10932480)\n\n[241\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[242\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[243\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[244\\. CoAtFormer: Vision Transformer with Composite Attention](https://www.ijcai.org/proceedings/2024/0068.pdf)\n\n[245\\. Vision Transformers (ViT) in Image Recognition - viso.ai](https://viso.ai/deep-learning/vision-transformer-vit/#:~:text=The%20ViT%20is%20a%20visual,class%20labels%20for%20the%20image.)\n\n[246\\. Kaiming He, Georgia Gkioxari et al. “Mask R-CNN.”](https://arxiv.org/abs/1703.06870)\n\n[247\\. Ze Liu, Yutong Lin et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00986)\n\n[248\\. Convolutional Embedding Makes Hierarchical Vision Transformer Stronger](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136800722.pdf)\n\n[249\\. A Separable Self-attention Inspired by the State Space Model for Computer Vision](https://arxiv.org/pdf/2501.02040)\n\n[250\\. Partial Convolution Meets Visual Attention](https://arxiv.org/pdf/2503.03148)\n\n[251\\. Lightweight Vision Transformer with Bidirectional Interaction](https://papers.neurips.cc/paper_files/paper/2023/file/3170de57bc1899315b97712043d8bb22-Paper-Conference.pdf)\n\n[252\\. CSWin Transformer: A General Vision Transformer Backbo...](https://www.microsoft.com/en-us/research/publication/cswin-transformer-a-general-vision-transformer-backbone-with-cross-shaped-windows/?locale=zh-cn/bibtex/)\n\n[253\\. ImageNet Benchmark (Image Classification)](https://paperswithcode.com/sota/image-classification-on-imagenet?dimension=Number%20of%20params)\n\n[254\\. Hugo Touvron, M. Cord et al. “Training data-efficient image transformers & distillation through attention.” International Conference on Machine Learning](https://arxiv.org/abs/2012.12877)\n\n[255\\. TBD/mmsegmentation - configs/convnext/README.md at v0...](https://openi.pcl.ac.cn/TBD/mmsegmentation/src/tag/v0.30.0/configs/convnext/README.md)\n\n[256\\. UniFormer: Unifying Convolution and Self-attention for Visual Recognition](https://arxiv.org/pdf/2201.09450)\n\n[261\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[262\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[263\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[264\\. ParameterNet: Parameters Are All You Need for Large-scale Visual Pretraining of Mobile Networks](https://openaccess.thecvf.com/content/CVPR2024/papers/Han_ParameterNet_Parameters_Are_All_You_Need_for_Large-scale_Visual_Pretraining_CVPR_2024_paper.pdf)\n\n[265\\. ImageNet Benchmark (Image Classification)](https://paperswithcode.com/sota/image-classification-on-imagenet?dimension=Number%20of%20params)\n\n[266\\. Vision and Multi-modal Transformers](https://hal.science/hal-04413851v1/file/Vision_and_Multi-modal_Transformers.pdf)\n\n[267\\. Deep Learning Optimisé - Jean Zay](http://www.idris.fr/media/formations/dlo-jz/dlojz8_vit_deepspeed_notes_en_octobre23.pdf)\n\n[268\\. Vision Transformers for Image Classification: A Comparative Survey](https://personal.ntu.edu.sg/elpwang/PDF_web/2025_Vision_Transformers.pdf)\n\n[269\\. Ze Liu, Yutong Lin et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00986)\n\n[270\\. One evolutionary algorithm deceives humans and ten convolutional neural networks trained on ImageNet at image recognition](https://orbilu.uni.lu/bitstream/10993/55399/1/1-s2.0-S1568494623004155-main.pdf)\n\n[271\\. Haiping Wu, Bin Xiao et al. “CvT: Introducing Convolutions to Vision Transformers.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00009)\n\n[272\\. Design Approaches for Lightweight Machine Learning Models](https://conservancy.umn.edu/bitstreams/108d97c1-073a-4d07-a5b8-2e629dedc7a4/download)\n\n[273\\. Hugo Touvron, M. Cord et al. “Training data-efficient image transformers & distillation through attention.” International Conference on Machine Learning](https://arxiv.org/abs/2012.12877)\n\n[274\\. Are Transformers More Robust Than CNNs?](https://proceedings.neurips.cc/paper/2021/file/e19347e1c3ca0c0b97de5fb3b690855a-Paper.pdf)\n\n[275\\. 【CNN轻量化】ParameterNet: Parameters Are All You Need 参数就是你所需要的](http://www.chinasem.cn/article/831253)\n\n[276\\. Comparative Analysis of Vision Transformer and ResNet50 for Glaucoma Detection: Balancing Performance and Efficiency](https://zenodo.org/records/15149831/files/IJSCAR_Submission_EricHwang.pdf?download=1)\n\n[277\\. SD-Conv: Towards the Parameter-Efficiency of Dynamic Convolution](https://openaccess.thecvf.com/content/WACV2023/papers/He_SD-Conv_Towards_the_Parameter-Efficiency_of_Dynamic_Convolution_WACV_2023_paper.pdf)\n\n[281\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[282\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[283\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[284\\. PERCEPTION SYSTEMS FOR AUTONOMOUS VEHICLES: RECENT ADVANCES IN SENSORS, DATA FUSION, AND AI](https://www.irjmets.com/uploadedfiles/paper/issue_3_march_2025/68587/final/fin_irjmets1741356658.pdf)\n\n[285\\. Ze Liu, Yutong Lin et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00986)\n\n[286\\. Manage the Increased Scale and Complexity of Vision AI with Greater Power Efficiency of Mid-Range RZ/V2N MPU](https://www.renesas.com/en/document/whp/manage-increased-scale-and-complexity-vision-ai-greater-power-efficiency-mid-range-rzv2n-mpu)\n\n[287\\. METRIC-DRIVEN ATTRIBUTIONS FOR VISION TRANSFORMERS](https://openreview.net/pdf/35885b62512c752de01fca8c7e1b4438b4a02bcd.pdf)\n\n[288\\. Efficient Vision Transformers for Autonomous Off-Road Perception Systems](https://par.nsf.gov/servlets/purl/10580206)\n\n[289\\. Nicolas Carion, Francisco Massa et al. “End-to-End Object Detection with Transformers.” ArXiv](https://doi.org/10.1007/978-3-030-58452-8_13)\n\n[290\\. A Real-Time Semantic Segmentation Method Based on Transformer for Autonomous Driving](https://file.techscience.com/files/cmc/2024/TSP_CMC-81-3/TSP_CMC_55478/TSP_CMC_55478.pdf)\n\n[291\\. Comparative Analysis of YOLOv8 and RT-DETR for Real-Time Object Detection in Advanced Driver Assistance Systems](https://ir.lib.uwo.ca/cgi/viewcontent.cgi?article=13716&context=etd)\n\n[292\\. 2024汽车自动驾驶算法行业专题报告合集](https://zhuanlan.zhihu.com/p/685772186)\n\n[293\\. 焉知汽车：2024车载SoC芯片产业分析报告（65页）.pdf](https://www.sgpjbg.com/baogao/166114.html)\n\n[294\\. Extensive review and comparison of CNN and GAN](https://wjaets.com/sites/default/files/WJAETS-2024-0559.pdf)\n\n[295\\. Rahul Gupta, Shashank Singh et al. “A Comparative Study of CNN and Transformer Models for Image Recognition in Autonomous Driving.” 2025 2nd International Conference on Computational Intelligence, Communication Technology and Networking (CICTN)](https://doi.org/10.1109/cictn64563.2025.10932480)\n\n[296\\. Panoptic Perception for Autonomous Driving: A Survey](https://arxiv.org/pdf/2408.15388)\n\n[301\\. CoAtFormer: Vision Transformer with Composite Attention](https://www.ijcai.org/proceedings/2024/0068.pdf)\n\n[302\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[303\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[304\\. Partial Convolution Meets Visual Attention](https://www.qeios.com/read/1L3TE6/pdf)\n\n[305\\. Kaiming He, Georgia Gkioxari et al. “Mask R-CNN.”](https://arxiv.org/abs/1703.06870)\n\n[306\\. A Separable Self-attention Inspired by the State Space Model for Computer Vision](https://arxiv.org/pdf/2501.02040)\n\n[307\\. Vision Transformers (ViT) in Image Recognition - 2024 Guide](https://viso.ai/deep-learning/vision-transformer-vit/)\n\n[308\\. ConvNeXt 网络](https://zhuanlan.zhihu.com/p/663344621)\n\n[309\\. Vision Transformers (ViT) in Image Recognition: Full Guide](https://viso.ai/deep-learning/vision-transformer-vit/#:~:text=Vision%20Transformer%20ViT%20Architecture%20%E2%80%93%20Source,a%20series%20of%20transformer%20blocks.)\n\n[310\\. RMT - CVPR 2024 Open Access Repository](https://openaccess.thecvf.com/content/CVPR2024/html/Fan_RMT_Retentive_Networks_Meet_Vision_Transformers_CVPR_2024_paper.html)\n\n[311\\. EfficientViT: Memory Efficient Vision Transformer With ...](https://github.com/microsoft/Cream/tree/main/EfficientViT)\n\n[312\\. Ze Liu, Yutong Lin et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00986)\n\n[313\\. Convolutional Embedding Makes Hierarchical Vision Transformer Stronger](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136800722.pdf)\n\n[314\\. CNN-Transformer混合模型在计算机视觉领域的研究综述](https://pdf.hanspub.org/MOS20230400000_62566989.pdf)\n\n[315\\. RMT: Retentive Networks Meet Vision Transformers](https://arxiv.org/pdf/2309.11523)\n\n[316\\. Lightweight Vision Transformer with Bidirectional Interaction](https://papers.neurips.cc/paper_files/paper/2023/file/3170de57bc1899315b97712043d8bb22-Paper-Conference.pdf)\n\n[317\\. Dual-windowed Vision Transformer with Angular Self-Attention](https://openreview.net/pdf?id=7jgu4oXsGM)\n\n[318\\. ImageNet Benchmark (Image Classification)](https://paperswithcode.com/sota/image-classification-on-imagenet?dimension=Number%20of%20params)\n\n[319\\. Hugo Touvron, M. Cord et al. “Training data-efficient image transformers & distillation through attention.” International Conference on Machine Learning](https://arxiv.org/abs/2012.12877)\n\n[321\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[322\\. A. Krizhevsky, I. Sutskever et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM](https://doi.org/10.1145/3065386)\n\n[323\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[324\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[325\\. ImageNet Benchmark (Image Classification)](https://paperswithcode.com/sota/image-classification-on-imagenet?dimension=Number%20of%20params)\n\n[326\\. ParameterNet: Parameters Are All You Need for Large-scale Visual Pretraining of Mobile Networks](https://openaccess.thecvf.com/content/CVPR2024/papers/Han_ParameterNet_Parameters_Are_All_You_Need_for_Large-scale_Visual_Pretraining_CVPR_2024_paper.pdf)\n\n[327\\. Vision and Multi-modal Transformers](https://hal.science/hal-04413851v1/file/Vision_and_Multi-modal_Transformers.pdf)\n\n[328\\. Deep Learning Optimisé - Jean Zay](http://www.idris.fr/media/formations/dlo-jz/dlojz8_vit_deepspeed_notes_en_octobre23.pdf)\n\n[329\\. The Linear Attention Resurrection in Vision Transformer](https://arxiv.org/pdf/2501.16182)\n\n[330\\. Hugo Touvron, M. Cord et al. “Training data-efficient image transformers & distillation through attention.” International Conference on Machine Learning](https://arxiv.org/abs/2012.12877)\n\n[331\\. Design Approaches for Lightweight Machine Learning Models](https://conservancy.umn.edu/bitstreams/108d97c1-073a-4d07-a5b8-2e629dedc7a4/download)\n\n[332\\. One evolutionary algorithm deceives humans and ten convolutional neural networks trained on ImageNet at image recognition](https://orbilu.uni.lu/bitstream/10993/55399/1/1-s2.0-S1568494623004155-main.pdf)\n\n[333\\. ImageNet Dataset: Evolution & Applications - viso.ai](https://viso.ai/deep-learning/imagenet/#:~:text=Today%20the%20ImageNet%20dataset%20is,benchmarked%20using%20the%20ImageNet%20dataset.)\n\n[334\\. Haiping Wu, Bin Xiao et al. “CvT: Introducing Convolutions to Vision Transformers.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00009)\n\n[335\\. Image Classification: State-of-the-Art Models in 2025 - HiringNet](https://hiringnet.com/image-classification-state-of-the-art-models-in-2025)\n\n[336\\. ConvNet vs Transformer, Supervised vs CLIP](https://arxiv.org/html/2311.09215v3)\n\n[337\\. 【CNN轻量化】ParameterNet: Parameters Are All You Need 参数就是你所需要的](http://www.chinasem.cn/article/831253)\n\n[338\\. ImageNet Dataset: Evolution & Applications (2024)](https://viso.ai/deep-learning/imagenet/)\n\n[341\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[342\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[343\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[344\\. PERCEPTION SYSTEMS FOR AUTONOMOUS VEHICLES: RECENT ADVANCES IN SENSORS, DATA FUSION, AND AI](https://www.irjmets.com/uploadedfiles/paper/issue_3_march_2025/68587/final/fin_irjmets1741356658.pdf)\n\n[345\\. Ze Liu, Yutong Lin et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00986)\n\n[346\\. Nicolas Carion, Francisco Massa et al. “End-to-End Object Detection with Transformers.” ArXiv](https://doi.org/10.1007/978-3-030-58452-8_13)\n\n[347\\. Transformer-Based 2D-to-3D Footwear Reconstruction and Flutter-Integrated Augmented Reality for Virtual Try-On](https://ijrpr.com/uploads/V6ISSUE6/IJRPR47676.pdf)\n\n[348\\. Comparative Analysis of YOLOv8 and RT-DETR for Real-Time Object Detection in Advanced Driver Assistance Systems](https://ir.lib.uwo.ca/cgi/viewcontent.cgi?article=13716&context=etd)\n\n[349\\. A Real-Time Semantic Segmentation Method Based on Transformer for Autonomous Driving](https://file.techscience.com/files/cmc/2024/TSP_CMC-81-3/TSP_CMC_55478/TSP_CMC_55478.pdf)\n\n[350\\. 焉知汽车：2024车载SoC芯片产业分析报告（65页）.pdf](https://www.sgpjbg.com/baogao/166114.html)\n\n[351\\. Extensive review and comparison of CNN and GAN](https://wjaets.com/sites/default/files/WJAETS-2024-0559.pdf)\n\n[352\\. Efficient Vision Transformers for Autonomous Off-Road Perception Systems](https://www.scirp.org/pdf/jcc2024129_111732833.pdf)\n\n[353\\. 2024汽车自动驾驶算法行业专题报告合集](https://zhuanlan.zhihu.com/p/685772186)\n\n[354\\. Advancement in Perception Capabilities for Autonomous Vehicles: From Dataset Collection to Scene Interpretation](https://www.junyigu-claude.com/publication/advancement-in-perception-capabilities-for-autonomous-vehicles-from-dataset-collection-to-scene-interpretation/advancement-in-perception-capabilities-for-autonomous-vehicles-from-dataset-collection-to-scene-interpretation.pdf)\n\n[355\\. .../llm-analysis: Latency and Memory Analysis of Trans...](https://github.com/K-Wu/llm-analysis)\n\n[356\\. Transformer+CNN，引领新一代AI发展](https://www.eet-china.com/mp/a204914.html)"
  },
  {
    "id": 57,
    "question": "What is the evolution path of multimodal models from early visual representations to current multimodal large language models, and what are the key technological breakthroughs along this journey?",
    "response": "**Comprehensive Research Report: The Evolution of Multimodal Models and Key Technological Breakthroughs**\n\n**I. Introduction to Multimodal AI Evolution**\n\nMultimodal models bridge visual, linguistic, and other sensory data to mimic human cognition. Their evolution spans **handcrafted visual features** (1990s-2010s) to today's **transformer-based Multimodal Large Language Models (MLLMs)**. This progression reflects advancements in architecture, training strategies, datasets, and computational efficiency, driven by breakthroughs in deep learning and hardware design. The journey reveals how models evolved from siloed modality processing to unified cross-modal reasoning systems capable of zero-shot generalization \\[2\\]\\[8\\].\n\n**II. Architectural Evolution: Milestones and Paradigm Shifts**\n\n**A. Early Foundations: Handcrafted Features and Sequential Models**\n\n**Pre-2015:** Reliance on handcrafted visual descriptors (e.g., SIFT, HOG) paired with shallow neural networks. Modality fusion was rudimentary, often using late-fusion techniques with limited cross-modal interaction \\[3\\].\n\n**Convolutional Neural Networks (CNNs):** Revolutionized visual representation learning but lacked efficient text-vision alignment mechanisms.\n\n**B. Transformer Revolution (2017–Present)**\n\n**Attention Mechanism:** Enabled scalable cross-modal processing by dynamically weighting input tokens. Transformer architectures replaced RNNs/CNNs as the backbone for both vision (ViT) and language (BERT) tasks \\[1\\]\\[5\\]\\[9\\].\n\n**Dual-Encoder Models (e.g., CLIP):**\n\n**Architecture:** ViT for images + BERT for text, trained with contrastive loss.\n\n**Impact:** Enabled zero-shot transfer to downstream tasks by aligning embeddings in shared latent space \\[2\\]\\[8\\].\n\n**Multimodal Fusion Architectures (e.g., ALBEF, FLAVA):**\n\nCross-attention layers allowed joint reasoning over text and image tokens.\n\nOutperformed dual-encoders on complex tasks like VQA but increased compute overhead \\[11\\]\\[14\\].\n\n**C. Rise of MLLMs (2020–Present)**\n\n**Core Components:**\n\nVisual encoder (e.g., ViT or CNN).\n\nMultimodal connector (adapter mapping vision→language tokens).\n\nLLM backbone (e.g., LLaMA, GPT) for generative reasoning \\[4\\]\\[5\\]\\[17\\].\n\n**Key Innovations:**\n\n**Encoder-Decoder Frameworks (DALL-E):** Mapped text→image domains for generative tasks \\[9\\].\n\n**Unified Transformers (LLaMA 4):** Jointly attended to visual and textual tokens within a single architecture \\[16\\].\n\n**Parameter-Efficient Fine-Tuning (PEFT):** LoRA/QLoRA adapters allowed task specialization without full-model retraining \\[5\\].\n\n**D. Addressing Long-Range Dependencies: Mamba and Cobra (2024–2025)**\n\n**Limitations of Transformers:** Quadratic complexity impedes long-context processing \\[12\\]\\[208\\].\n\n**State Space Models (SSMs):**\n\n**Mamba:** Linear-time processing via implicit dependency propagation. Achieved 4–5× higher throughput vs. Transformers \\[144\\]\\[208\\].\n\n**Cobra:** Adapted Mamba for multimodal fusion, enabling efficient long-context visual-language tasks \\[264\\].\n\n**Hybrid Architectures (e.g., MambaFormer):** Combined Mamba (long-range) + Transformer (short-range) for balanced performance \\[155\\].\n\n**III. Key Technological Breakthroughs**\n\n**A. Training Strategies**\n\n1.  **Contrastive Learning:**\n\n- Aligned embeddings across modalities via similarity optimization (e.g., CLIP’s image-text pairs).\n- Enabled zero-shot generalization and robustness in unseen tasks \\[2\\]\\[23\\]\\[31\\].\n\n1.  **Cross-Modal Attention:**\n\n- **Evolution:** From basic co-attention to guided mechanisms (e.g., Contrastive Content Re-sourcing).\n- **Impact:** Improved fine-grained alignment for tasks like image-text retrieval \\[26\\]\\[28\\].\n\n1.  **Self-Supervised Pretraining:**\n\n- Leveraged unlabeled web-scale data (e.g., LAION-5B) to learn transferable representations \\[7\\]\\[13\\].\n\n**B. Dataset Curation**\n\n**COCO (2014):** 328K images with 2.5M labeled instances. Standardized captioning and VQA benchmarks but suffered from object hallucination \\[50\\]\\[54\\].\n\n**Conceptual Captions (2018):** 3.3M noisy web-scraped pairs. Scaled pretraining and improved generalization but introduced label noise \\[48\\]\\[54\\].\n\n**Synergy:** Combined datasets (e.g., COCO + Conceptual Captions + Visual Genome) enabled robust MLLM pretraining \\[45\\].\n\n**C. Computational Efficiency Innovations (2020–2025)**\n\n**Model Compression:** Pruning, quantization (e.g., AWQ), and distillation to reduce inference costs \\[61\\]\\[63\\].\n\n**Token Reduction:** Optimal VLMs use fewer visual tokens without compromising accuracy \\[76\\].\n\n**Composite Attention:** EE-MLLM reused LLM weights to eliminate self-attention costs \\[69\\].\n\n**D. Hardware-Software Co-Design**\n\n**FPGA/ASIC Accelerators:** Optimized attention mechanisms via sparsity exploitation and pipelining \\[164\\]\\[169\\].\n\n**In-Memory Computing (IMC):** iMTransformer used FeFET crossbars to reduce data movement bottlenecks \\[175\\]\\[175\\].\n\n**Unified Compilers:** GTCO and ALCOPI automated hardware-aware optimization for transformers \\[82\\].\n\n**IV. Performance Analysis and Challenges**\n\n**A. Dual-Encoder vs. Fusion Architectures**\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n| **Architecture** | **VQA-v2 (Acc%)** | **COCO Captioning (CIDEr)** | **Use Case** |\n| **Dual-Encoder** | ~62–73 \\[135\\] | 79.5 \\[134\\] | Efficient retrieval/similarity |\n| **Fusion** | 75.26 (ML-Mamba) \\[383\\] | 132.7 (COMM) \\[309\\] | Complex VQA/captioning |\n\n**Trade-offs:** Fusion excels in tasks requiring deep cross-modal interaction (e.g., VQA) but incurs higher latency. Dual-encoders lead in efficiency \\[191\\]\\[247\\].\n\n**B. Long-Context MLLM Performance**\n\n**Mamba-Based Models:**\n\n**Accuracy:** LongMamba improved average VQA accuracy by 2.8–9.13% over vanilla Mamba at 16k–40k contexts \\[263\\]\\[263\\].\n\n**Latency:** 4–5× faster inference vs. Transformers; prefilling latency of 0.8s at 32k tokens (A5000 GPU) \\[208\\]\\[326\\].\n\n**Limitations:** Imbalanced visual-textual alignment persists in early Mamba MLLMs \\[394\\].\n\n**C. Persistent Challenges**\n\n**Long-Range Dependencies:** Hybrid Mamba-Transformer models still trail pure Transformers in tasks like in-context learning \\[210\\].\n\n**Hardware Integration:** Co-designed accelerators (e.g., binarized transformers) remain confined to research prototypes \\[224\\]\\[344\\].\n\n**V. Commercial Deployments and Real-World Impact (2024–2025)**\n\n**A. Enterprise Applications**\n\n**Oracle Project RAPID:** Hardware-software co-design for database acceleration, improving energy efficiency \\[404\\].\n\n**ShangTang Technology:** Deployed generative MLLMs in medical imaging and autonomous vehicles, though specifics are proprietary \\[407\\].\n\n**B. Production Metrics**\n\n**Mamba in Multimodal Tasks:**\n\n**ML-Mamba:** 31-hour training on 8× A100 GPUs; real-time inference on mobile devices \\[383\\].\n\n**LongMamba:** 8.76% accuracy gain on LongBench-E with 4× throughput vs. LLaMA-7B \\[263\\].\n\n**Latency-Optimized MLLMs:** Cobra matched LLaVA accuracy while reducing inference time by 32% \\[64\\].\n\n**VI. Future Directions**\n\n1.  **Unified SSM-Transformer Architectures:** Hybrid models to balance long-context efficiency and reasoning fidelity.\n2.  **Hardware-Software Ecosystems:** Open-source co-design frameworks for edge deployment of billion-parameter MLLMs.\n3.  **Causal Multimodal Alignment:** Techniques to reduce hallucination in generative tasks.\n4.  **Sustainable AI:** Energy-efficient training via spatio-temporal sparse models (e.g., VST2.0) \\[96\\].\n\n**VII. Conclusion**\n\nMultimodal models evolved from brittle feature extractors to unified generative frameworks, driven by breakthroughs in transformer architectures, contrastive learning, and datasets like CLIP and COCO. The shift to MLLMs enabled unprecedented cross-modal understanding, while innovations like Mamba and Cobra addressed efficiency bottlenecks. Despite progress, challenges in hardware integration and long-context modeling persist. The next frontier involves democratizing these capabilities via hardware-software co-design and sustainable scaling, ultimately enabling real-time, human-like multimodal reasoning.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[2\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[3\\. Multimodal Attentive Deep Learning Architectures for Visual-Semantic Understanding: A Multimodal Bridge from Pixels to Reasoning](https://iris.unimore.it/retrieve/9864b24a-ef69-408f-9d22-c4f30d03dad0/AMOROSO_CS.pdf)\n\n[4\\. SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models](https://arxiv.org/pdf/2505.00788v2)\n\n[5\\. The (R)Evolution of Multimodal Large Language Models: A Survey](https://multix.io/multix-docs/files/surve-multimodal-llm.pdf)\n\n[6\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[7\\. -A Milestone Unimodal Models.](http://arxiv.org/html/2405.14093v1)\n\n[8\\. PipeWeaver: Addressing Data Dynamicity in Large Multimodal Model Training with Dynamic Interleaved Pipeline](https://arxiv.org/pdf/2504.14145)\n\n[9\\. A Comprehensive Survey and Guide to Multimodal Large Language Models in Vision-Language Tasks](https://www.arxiv.org/pdf/2411.06284)\n\n[10\\. Multimodal LLM Agents: Exploring LLM interactions in Software, Web and Operating Systems](https://openreview.net/pdf?id=YGLOpASCY5)\n\n[11\\. Innovations in visual language models for robotic interaction and contextual awareness: Progress, pitfalls and perspectives](https://journalwjaets.com/sites/default/files/fulltext_pdf/WJAETS-2025-0311.pdf)\n\n[12\\. EMMA: EMPOWERING MULTI-MODAL MAMBA WITH STRUCTURAL AND HIERARCHICAL ALIGNMENT](https://openreview.net/pdf?id=Ev4iw23gdI)\n\n[13\\. Large AI Models in Health Informatics: Applications, Challenges, and the Future](https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?arnumber=10261199)\n\n[14\\. A Survey on Vision-Language-Action Models for Embodied AI](https://www.arxiv.org/pdf/2405.14093v3)\n\n[15\\. Evolution and Prospects of Foundation Models: From Large Language Models to Large Multimodal Models](https://file.techscience.com/files/cmc/2024/TSP_CMC-80-2/TSP_CMC_52618/TSP_CMC_52618.pdf)\n\n[16\\. Rise of Multimodal LLMs: LLaMA 4 Benchmark](https://aisera.com/blog/multimodal-llm-llama4/)\n\n[17\\. Image Captioning Using Multimodal LLMs](https://lup.lub.lu.se/student-papers/record/9185108/file/9185109.pdf)\n\n[21\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[22\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[23\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[24\\. DISTMM: Accelerating Distributed Multimodal Model Training](https://www.usenix.org/system/files/nsdi24-huang.pdf)\n\n[25\\. Cross-modal Contrastive Learning for Multimodal Fake News Detection](https://atailab.cn/seminar2023Fall/ppt/2023_MM_Cross-modal%20Contrastive%20Learning%20for%20Multimodal%20Fake%20News%20Detections.pptx)\n\n[26\\. WACV 2023 Open Access Repository](https://openaccess.thecvf.com/content/WACV2023/html/Chen_More_Than_Just_Attention_Improving_Cross-Modal_Attentions_With_Contrastive_Constraints_WACV_2023_paper.html)\n\n[27\\. Kaiming He, Haoqi Fan et al. “Momentum Contrast for Unsupervised Visual Representation Learning.” 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr42600.2020.00975)\n\n[28\\. More Than Just Attention: Improving Cross-Modal ...](https://arxiv.org/abs/2105.09597)\n\n[29\\. Cross-modal alignment and contrastive learning for ...](https://pubmed.ncbi.nlm.nih.gov/39961170/)\n\n[30\\. Unime: a two -step framework to improve learning ...](https://learnopoly.com/unime-a-two-step-framework-to-improve-learning-multimodal-representation-with-mllms/)\n\n[31\\. Towards Multilingual Vision-Language Models](https://www.lti.cs.cmu.edu/people/alumni/alumni-thesis/huang-po-yao-thesis.pdf)\n\n[32\\. A Comprehensive Survey and Guide to Multimodal Large Language Models in Vision-Language Tasks](https://www.arxiv.org/pdf/2411.06284)\n\n[33\\. Multimodal Fusion and Coherence Modeling for Video Topic Segmentation](https://openreview.net/pdf?id=Pu4n8P3EWz)\n\n[34\\. WHAT TO ALIGN IN MULTIMODAL CONTRASTIVE LEARNING?](https://openreview.net/pdf/2ed91a549d961b24a80ff1aad405fe8c17326fbd.pdf)\n\n[35\\. More Than Just Attention: Improving Cross-Modal Attentions with Contrastive Constraints for Image-Text Matching](https://openaccess.thecvf.com/content/WACV2023/papers/Chen_More_Than_Just_Attention_Improving_Cross-Modal_Attentions_With_Contrastive_Constraints_WACV_2023_paper.pdf)\n\n[36\\. Pace-Adaptive and Noise-Resistant Contrastive Learning for Multimodal Feature Fusion](https://openreview.net/pdf/c792d6dbe495143169c5743f94f379ca26483c82.pdf)\n\n[37\\. Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models](https://arxiv.org/pdf/2301.06267)\n\n[38\\. Heterogeneous bimodal attention fusion for speech emotion recognition](https://arxiv.org/pdf/2503.06405)\n\n[39\\. A Survey of Pathology Foundation Model: Progress and Future Directions](https://arxiv.org/pdf/2504.04045)\n\n[41\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[42\\. mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration](https://openaccess.thecvf.com/content/CVPR2024/papers/Ye_mPLUG-Owl2_Revolutionizing_Multi-modal_Large_Language_Model_with_Modality_Collaboration_CVPR_2024_paper.pdf)\n\n[43\\. Multimodal Deep Learning with an NLP focus](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1234/slides/Multimodal-Deep-Learning-CS224n-Kiela.pdf)\n\n[44\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[45\\. MixGen: A New Multi-Modal Data Augmentation](https://openaccess.thecvf.com/content/WACV2023W/Pretrain/papers/Hao_MixGen_A_New_Multi-Modal_Data_Augmentation_WACVW_2023_paper.pdf)\n\n[46\\. Examining the Impact and Limitations of Distributed Large Language Models and Multimodal Systems](https://hal.science/hal-05009116v1/document)\n\n[47\\. A Dive into Vision-Language Models](https://huggingface.co/blog/vision_language_pretraining)\n\n[48\\. Towards automatic understanding of narrative audiovisual content](https://theses.hal.science/tel-04128968/file/REBOUD_Alison_these_2022V1.pdf)\n\n[49\\. Data Processing Techniques for Modern Multimodal Models](https://www.yiyibooks.cn/__src__/arxiv/2407.19180v1/index.html)\n\n[50\\. 视觉语言多模态预训练综述](https://www.cjig.cn/rc-pub/front/front-article/download/58810885/lowqualitypdf/%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E9%A2%84%E8%AE%AD%E7%BB%83%E7%BB%BC%E8%BF%B0.pdf)\n\n[51\\. On Multimodal Representations Learned at Scale](https://di.ku.dk/english/research/phd/phd-theses/2023/bugliarello_phd-thesis.pdf)\n\n[52\\. Multi-Modal Generative AI: Multi-modal LLM, Diffusion and Beyond](https://arxiv.org/pdf/2409.14993)\n\n[53\\. LowCLIP: Adapting the CLIP Model Architecture for Low...](http://arxiv.org/html/2408.13909v1)\n\n[54\\. Piyush Sharma, Nan Ding et al. “Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/P18-1238)\n\n[55\\. Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities](https://www.arxiv.org/pdf/2505.02567v2)\n\n[56\\. Jiquan Ngiam, A. Khosla et al. “Multimodal Deep Learning.” International Conference on Machine Learning](https://doi.org/10.48550/arXiv.2301.04856)\n\n[57\\. Wei Li, Can Gao et al. “UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning.” ArXiv](https://doi.org/10.18653/v1/2021.acl-long.202)\n\n[61\\. Investigating the Efficacy of Multimodal Large Language Models in Cross-Domain Knowledge Transfer](https://premierscience.com/wp-content/uploads/2025/02/pjai-24-436.pdf)\n\n[62\\. Large Language Models In 2025: Your Guide To Next-Gen AI](https://acecloud.ai/blog/large-language-models/#:~:text=In%202025,%20Large%20Language%20Models%20will%20be%20a%20technical%20marvel,remain%20focal%20points%20for%20innovation.)\n\n[63\\. A SURVEY OF RESOURCE-EFFICIENT LLM AND MULTIMODAL FOUNDATION MODELS](https://arxiv.org/pdf/2401.08092)\n\n[64\\. ...to Multi-Modal Large Language Model for Efficient I...](http://arxiv.org/html/2403.14520v1)\n\n[65\\. Kanana: Compute-efficient Bilingual Language Models](https://www.arxiv.org/pdf/2502.18934)\n\n[66\\. Large Language Model Market Size, Growth & Outlook](https://www.mordorintelligence.com/industry-reports/large-language-model-llm-market)\n\n[67\\. Multimodal large language models for bioimage analysis](https://www.nature.com/articles/s41592-024-02334-2)\n\n[68\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[69\\. EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large Language Model](https://arxiv.org/pdf/2408.11795)\n\n[70\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[71\\. The LLM Landscape: A Guide to Language Models in 2025](https://www.robotmunki.com/blog/llm-landscape)\n\n[72\\. Examining the Impact and Limitations of Distributed Large Language Models and Multimodal Systems](https://hal.science/hal-05009116v1/document)\n\n[73\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[74\\. MOBA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task Automation](https://arxiv.org/pdf/2410.13757)\n\n[75\\. Jiayang Wu, Wensheng Gan et al. “Multimodal Large Language Models: A Survey.” 2023 IEEE International Conference on Big Data (BigData)](https://doi.org/10.1109/BigData59044.2023.10386743)\n\n[76\\. INFERENCE OPTIMAL VLMs NEED FEWER VISUAL TOKENS AND MORE PARAMETERS](https://openreview.net/pdf?id=w52HbDMO79)\n\n[77\\. Large Language Models for Multi-Modal Human-Robot ...](https://arxiv.org/html/2401.15174v4)\n\n[78\\. MMMU-Pro: A More Robust Multi-Discipline Multimodal Understanding Benchmark](https://openreview.net/pdf/3b09e9d923ccbbc326c56b0b3ed5986e91716367.pdf)\n\n[79\\. Efficient scaling of large language models with mixture ...](https://www.nature.com/articles/s43588-024-00753-x)\n\n[81\\. LLMCompass: Enabling Efficient Hardware Design for Large Language Model Inference](https://parallel.princeton.edu/papers/isca24_llmcompass.pdf)\n\n[82\\. Design Automation with Efficient Compilation on Hardware Accelerators](http://www.cse.cuhk.edu.hk/~byu/papers/PHD-thesis-2024-Yang-Bai.pdf)\n\n[83\\. A Comprehensive Survey and Guide to Multimodal Large Language Models in Vision-Language Tasks](https://www.arxiv.org/pdf/2411.06284)\n\n[84\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[85\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[86\\. ICCAD: G: Machine Learning Algorithm and Hardware Co-Design Towards Green and Ubiquitous AI on Both Edge and Cloud](https://src.acm.org/binaries/content/assets/src/2024/haoran-you.pdf)\n\n[87\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[88\\. X-Former: In-Memory Acceleration of Transformers | IEE...](https://dl.acm.org/doi/10.1109/TVLSI.2023.3282046)\n\n[89\\. ProTEA: Programmable Transformer Encoder Acceleration on FPGA](https://h2rc.cse.sc.edu/papers/protea.pdf)\n\n[90\\. Fpga/Asic在ai推理加速中的研究 - 极术社区 - 连接开发者与智能计算生态](https://aijishu.com/a/1060000000489893)\n\n[91\\. Towards Efficient AI Hardware: Software-Hardware Co-Design ...](https://cemse.kaust.edu.sa/events/by-type/phd-dissertation-defense/2025/06/29/towards-efficient-ai-hardware-software-hardware)\n\n[92\\. A survey of techniques for optimizing transformer inference](https://www.sciencedirect.com/science/article/pii/S1383762123001698)\n\n[93\\. Multimodal AI: Bridging Technologies, Challenges, and ...](https://stellarix.com/insights/articles/multimodal-ai-bridging-technologies-challenges-and-future/)\n\n[94\\. Yuhao Ji, Chao Fang et al. “Co-Designing Binarized Transformer and Hardware Accelerator for Efficient End-to-End Edge Deployment.”](https://arxiv.org/abs/2407.12070)\n\n[95\\. 15th Workshop on Parallel Programming and Run-Time Management Techniques for Many-Core Architectures and 13th Workshop on Design Tools and Architectures for Multicore Embedded Computing Platforms](https://drops.dagstuhl.de/storage/01oasics/oasics-vol116-parma-ditam2024/OASIcs.PARMA-DITAM.2024/OASIcs.PARMA-DITAM.2024.pdf)\n\n[96\\. Spatio-Temporal Synergistic Sparse Transformer](https://link.springer.com/chapter/10.1007/978-981-96-9946-9_20)\n\n[97\\. Hardware-Software Co-Design for Brain-Computer Interfaces](https://www.cs.yale.edu/homes/abhishek/ksriram-isca20.pdf)\n\n[98\\. Developing, Synthesizing, and Automating Domain-Specific Accelerator](https://www.escholarship.org/content/qt1h43f45k/qt1h43f45k.pdf)\n\n[101\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[102\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[103\\. DEEP EQUILIBRIUM MULTIMODAL FUSION](https://openreview.net/pdf?id=bZMyHBSnEI)\n\n[104\\. DynCIM: Dynamic Curriculum for Imbalanced Multimodal Learning](https://arxiv.org/pdf/2503.06456)\n\n[105\\. MMBench: Benchmarking End-to-End Multi-modal DNNs and Understanding Their Hardware-Software Implications](https://www.cs.sjtu.edu.cn/~lichao/publications/MMBench_Benchmarking_IISWC-2023-Xu.pdf)\n\n[106\\. DAE-Fuse: An Adaptive Discriminative Autoencoder for M...](http://arxiv.org/html/2409.10080v2)\n\n[107\\. Tools-MMBench: Benchmarking End-to-End Multi-modal DNNs and Understanding Their Hardware-Software Implications](https://arxiv.org/pdf/2212.01241v3)\n\n[108\\. CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.pdf)\n\n[109\\. Multimodal Fusion Architectures for Pedestrian Detection](https://research.utwente.nl/files/254342674/yang_mul.pdf)\n\n[110\\. Bruno Dumas, R. Ingold et al. “Benchmarking fusion engines of multimodal interactive systems.” ICMI-MLMI '09](https://doi.org/10.1145/1647314.1647345)\n\n[111\\. A Survey on Exploring Multimodal Fusion in Healthcare: Challenges and Solutions](https://jqcsm.qu.edu.iq/index.php/journalcm/article/download/1810/992/4937)\n\n[112\\. Deep Equilibrium Multimodal Fusion](https://openreview.net/forum?id=bZMyHBSnEI)\n\n[113\\. J. Kittler, M. Hatef et al. “On Combining Classifiers.” IEEE Trans. Pattern Anal. Mach. Intell.](https://doi.org/10.1109/34.667881)\n\n[114\\. Dual Memory Fusion for Multimodal Speech Emotion Recognition](https://www.isca-archive.org/interspeech_2023/prisayad23_interspeech.pdf)\n\n[115\\. Towards Robust and Efficient Multimodal Representation Learning and Fusion](https://dr.ntu.edu.sg/bitstream/10356/182226/2/Guo%20Xiaobao_Thesis_final_version_signed%20%281%29.pdf)\n\n[116\\. Finding the Optimal Fusion Points in Multimodal Medical ...](https://arxiv.org/html/2505.02467v1)\n\n[117\\. Discovering and Leveraging Deep Multimodal Structure for Reliable Robot Perception and Localization](http://ais.informatik.uni-freiburg.de/publications/papers/valada19phd.pdf)\n\n[121\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[122\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[123\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[124\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[125\\. DEEP EQUILIBRIUM MULTIMODAL FUSION](https://openreview.net/pdf?id=bZMyHBSnEI)\n\n[126\\. Topics in AI (CPSC 532S): Multimodal Learning with Vision, Language and Sound](https://www.cs.ubc.ca/~lsigal/532S_2022W1/Lecture22.pdf)\n\n[127\\. A Survey on Exploring Multimodal Fusion in Healthcare: Challenges and Solutions](https://jqcsm.qu.edu.iq/index.php/journalcm/article/download/1810/992/4937)\n\n[128\\. CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.pdf)\n\n[129\\. Collaborative Modality Fusion for Mitigating Language Bias in Visual Question Answering](https://www.preprints.org/manuscript/202401.1211/v1/download)\n\n[130\\. DAE-Fuse: An Adaptive Discriminative Autoencoder for M...](http://arxiv.org/html/2409.10080v2)\n\n[131\\. Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action](https://faculty.cc.gatech.edu/~zk15/teaching/AY2024_cs8803vlm_fall/slides/L14_UnifiedIO2.pdf)\n\n[132\\. Bruno Dumas, R. Ingold et al. “Benchmarking fusion engines of multimodal interactive systems.” ICMI-MLMI '09](https://doi.org/10.1145/1647314.1647345)\n\n[133\\. Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis, and Best Practices](https://arxiv.org/pdf/2503.06063)\n\n[134\\. CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal Understanding and Generation](https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and_CVPR_2025_paper.pdf)\n\n[135\\. Zero-Shot Verified Image Generation: from Natural Language to Visual Prompts](https://run.unl.pt/bitstream/10362/167324/1/Valerio.pdf)\n\n[136\\. DEU-Net: Dual-Encoder U-Net for Automated Skin Lesion Segmentation](https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?arnumber=10332179)\n\n[137\\. Yash Goyal, Tejas Khot et al. “Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering.” International Journal of Computer Vision](https://doi.org/10.1007/s11263-018-1116-0)\n\n[138\\. Yalin Miao, Shuyun He et al. “Research on visual question answering based on dynamic memory network model of multiple attention mechanisms.” Scientific Reports](https://doi.org/10.1038/s41598-022-21149-9)\n\n[139\\. Advancing Multimodal Large Language Models](https://www.mdpi.com/2076-3417/15/7/3992)\n\n[140\\. Jinhong Ni, Yalong Bai et al. “Deep Equilibrium Multimodal Fusion.” ArXiv](https://doi.org/10.48550/arXiv.2306.16645)\n\n[141\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[142\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[143\\. EMMA: EMPOWERING MULTI-MODAL MAMBA WITH STRUCTURAL AND HIERARCHICAL ALIGNMENT](https://openreview.net/pdf?id=Ev4iw23gdI)\n\n[144\\. AlignMamba: Enhancing Multimodal Mamba with Local and ...](http://arxiv.org/html/2412.00833v1)\n\n[145\\. scWGBS-GPT: A Foundation Model for Capturing Long-Range CpG Dependencies in Single-Cell Whole-Genome Bisulfite Sequencing to Enhance Epigenetic Analysis](https://www.biorxiv.org/content/10.1101/2025.02.19.638959v1.full.pdf)\n\n[146\\. Position: Prospective of Autonomous Driving - Multimodal LLMs, World Models, Embodied Intelligence, AI Alignment, and Mamba](https://openaccess.thecvf.com/content/WACV2025W/LLVMAD/papers/Ma_Position_Prospective_of_Autonomous_Driving_-_Multimodal_LLMs_World_Models_WACVW_2025_paper.pdf)\n\n[147\\. Mamba: A Potential Transformer Replacement - Zilliz blog](https://zilliz.com/learn/mamba-architecture-potential-transformer-replacement)\n\n[148\\. Advancements in Modern Recommender Systems: Industrial Applications in Social Media, E-commerce, Entertainment, and Beyond](https://hal.science/hal-04711099v1/document)\n\n[149\\. A Survey on Mamba Architecture for Vision Applications](https://arxiv.org/pdf/2502.07161)\n\n[150\\. VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate Spatiotemporal Forecasting](https://openaccess.thecvf.com/content/CVPR2024W/PRECOGNITION/papers/Tang_VMRNN_Integrating_Vision_Mamba_and_LSTM_for_Efficient_and_Accurate_CVPRW_2024_paper.pdf)\n\n[151\\. 提升长序列建模效率:Mamba+交叉注意力架构完整指南_腾讯新闻](https://view.inews.qq.com/a/20250610A038NF00)\n\n[152\\. Integration of Mamba and Transformer – MAT for Long-Short Range Time Series Forecasting with Application to Weath Dynamics](https://arxiv.org/pdf/2409.08530)\n\n[153\\. MambaDepth: Enhancing Long-range Dependency for Self ...](https://arxiv.org/abs/2406.04532)\n\n[154\\. 靠Transformer加持，Mamba效率爆表！只需1%算力就刷新 ...](https://www.yyychat.com/ai/1649.html)\n\n[155\\. Integrating Mamba and Transformer for Long-Short Range Time Series Forecasting](https://arxiv.org/pdf/2404.14757v1)\n\n[156\\. Integrating Mamba and Transformer for Long-Short Range ...](https://arxiv.org/html/2404.14757v1)\n\n[157\\. Is Mamba Effective for Time Series Forecasting?](https://arxiv.org/pdf/2403.11144v1)\n\n[158\\. TEST TIME LEARNING FOR TIME SERIES FORECASTING](https://openreview.net/pdf?id=WWymYrA48K)\n\n[159\\. Albert Gu, Karan Goel et al. “Efficiently Modeling Long Sequences with Structured State Spaces.” ArXiv](https://arxiv.org/abs/2111.00396)\n\n[160\\. 空间和光谱结构感知的Mamba网络用于高光谱图像分类](https://www.rserforum.com/thread/8273)\n\n[161\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[162\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[163\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[164\\. \\[论文审查\\] Hardware-Software Co-Design for Accelerating ...](https://www.themoonlight.io/zh/review/hardware-software-co-design-for-accelerating-transformer-inference-leveraging-compute-in-memory)\n\n[165\\. Transformer硬件加速器总结](https://zhuanlan.zhihu.com/p/633571685)\n\n[166\\. ProTEA: Programmable Transformer Encoder Acceleration on FPGA](https://h2rc.cse.sc.edu/papers/protea.pdf)\n\n[167\\. FPGA和ASIC在大模型推理加速中的应用-电子发烧友网](https://m.elecfans.com/article/6290157.html)\n\n[168\\. Hardware–Software Co-design of Deep Neural Architectures: From FPGAs and ASICs to Computing-in-Memories | SpringerLink](https://link.springer.com/chapter/10.1007/978-3-031-39932-9_11)\n\n[169\\. TATAA: PROGRAMMABLE MIXED-PRECISION TRANSFORMER ACCELERATION WITH A TRANSFORMABLE ARITHMETIC ARCHITECTURE](https://arxiv.org/pdf/2411.03697)\n\n[170\\. Advanced FPGA Hardware Acceleration Techniques](https://www.numberanalytics.com/blog/advanced-fpga-hardware-acceleration-techniques)\n\n[171\\. From Model to FPGA: Software-Hardware Co-Design for Efficient Neural Network Acceleration](https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2Fpublications%2F2016%2FIEEE%20HCS_None_slide.pdf)\n\n[172\\. LLMCompass: Enabling Efficient Hardware Design for Large Language Model Inference](https://parallel.princeton.edu/papers/isca24_llmcompass.pdf)\n\n[173\\. Semiconductor Engineering: Software-Hardware Co-Design Becomes Real](https://www.arteris.cn/blog/semiconductor-engineering-software-hardware-co-design-becomes-real/)\n\n[174\\. Synopsys Hybrid Prototyping Solution](https://www.synopsys.com/content/dam/synopsys/verification/prototyping/datasheets/hybrid-prototyping-brochure.pdf)\n\n[175\\. Ann Franchesca Laguna, Mohammed Mehdi et al. “Hardware-Software Co-Design of an In-Memory Transformer Network Accelerator.” Frontiers in Electronics](https://doi.org/10.3389/felec.2022.847069)\n\n[176\\. Post-Quantum Cryptography Hardware: Monolithic Implementations vs. Hardware-Software Co-Design](https://www.creachlabs.fr/sites/default/files/public/media/document/2024-05/semsecuelecsaarinen23042021.pdf)\n\n[181\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[182\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[183\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[184\\. CoCa: Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/pdf/2205.01917v1.pdf)\n\n[185\\. Multimodal Large Language Models with Context-Aware ...](https://www.preprints.org/manuscript/202501.2114)\n\n[186\\. 多模态超详细解读 (九)：CoCa：对比学习和生成式任务训练多模态大模型](https://zhuanlan.zhihu.com/p/627702632)\n\n[187\\. Comparison of LaVIN and existing multimodal LLMs on COCO captioning, VQAv2 val set, TruthfulQA, and MME benchmark](https://openreview.net/attachment?id=xDV3dzC0ZZ&name=pdf)\n\n[188\\. Multi-Head Mixture-of-Experts](https://proceedings.neurips.cc/paper_files/paper/2024/file/ab05dc8bf36a9f66edbff6992ec86f56-Paper-Conference.pdf)\n\n[189\\. Dongsheng Jiang, Yuchen Liu et al. “From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2310.08825)\n\n[190\\. Jiquan Ngiam, A. Khosla et al. “Multimodal Deep Learning.” International Conference on Machine Learning](https://doi.org/10.48550/arXiv.2301.04856)\n\n[191\\. Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone](https://papers.nips.cc/paper_files/paper/2022/file/d4b6ccf3acd6ccbc1093e093df345ba2-Paper-Conference.pdf)\n\n[192\\. Vision Language Representation Learning](https://dr.ntu.edu.sg/bitstream/10356/169546/3/PhD_Thesis.pdf)\n\n[193\\. A Survey of Vision-Language Pre-Trained Models](https://www.ijcai.org/proceedings/2022/0762.pdf)\n\n[194\\. Multi-stage hybrid embedding fusion network for visual question ...](https://www.sciencedirect.com/science/article/pii/S0925231220316593)\n\n[195\\. Generating Images with Multimodal Language Models](https://proceedings.neurips.cc/paper_files/paper/2023/file/43a69d143273bd8215578bde887bb552-Paper-Conference.pdf)\n\n[196\\. OneLLM: One Framework to Align All Modalities with Language](https://onellm.csuhan.com/)\n\n[197\\. LLAVA-PLUS: LEARNING TO USE TOOLS FOR CREATING MULTIMODAL AGENTS](https://openreview.net/pdf/aebea1bacbeb8cfae8ee58f34c6274ceba718130.pdf)\n\n[198\\. Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Uni-Perceiver_v2_A_Generalist_Model_for_Large-Scale_Vision_and_Vision-Language_CVPR_2023_paper.pdf)\n\n[199\\. VLUE: A Multi-Task Benchmark for Evaluating Vision-Language Models](https://proceedings.mlr.press/v162/zhou22n/zhou22n.pdf)\n\n[201\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[202\\. LONGMAMBA: ENHANCING MAMBA'S LONG CONTEXT CAPABILITIES VIA TRAINING-FREE RECEPTIVE FIELD ENLARGEMENT](https://openreview.net/pdf/066062c0673cfc50aeafa9e3c96d0cf7a33fc3ca.pdf)\n\n[203\\. MambaExtend: A Training-Free Approach to Improve Long-Context Extension of Mamba](https://openreview.net/pdf/5eeb63281af283c51149d31541a853ecfce356e8.pdf)\n\n[204\\. ReMamba: Equip Mamba with Effective Long-Sequence Modeling](https://arxiv.org/pdf/2408.15496)\n\n[205\\. QMambaExtend: Improving Long-Context Extension of Memory-Efficient Mamba Models](https://openreview.net/pdf/6083232316d32fe72ef58259843fae2032d3448b.pdf)\n\n[206\\. Decision Mamba: Reinforcement Learning via Hybrid Selective Sequence Modeling](https://openreview.net/pdf?id=wFzIMbTsY7)\n\n[207\\. Pamba: Transplanting Transformer Blocks with Mamba](https://openreview.net/pdf/82e9f93941508dc3fc77f0c71dd52d8783ac1d84.pdf)\n\n[208\\. Mamba: A Potential Transformer Replacement - Zilliz blog](https://zilliz.com/learn/mamba-architecture-potential-transformer-replacement)\n\n[209\\. MAMBA: LINEAR-TIME SEQUENCE MODELING WITH SELECTIVE STATE SPACES](https://openreview.net/pdf?id=AL1fq05o7H)\n\n[210\\. An Empirical Study of Mamba-based Language Models](https://scidirect.org/papers/an-empirical-study-of-mamba-based-language-i6JhuiA90vj)\n\n[211\\. Integration of Mamba and Transformer – MAT for Long-Short Range Time Series Forecasting with Application to Weath Dynamics](https://arxiv.org/pdf/2409.08530)\n\n[212\\. ReMamba: Equip Mamba with Effective Long-Sequence Mode...](http://arxiv.org/html/2408.15496v4)\n\n[213\\. Transformer vs Mamba: A Tale of Two LLM Architectures](https://www.ai-hive.net/post/transformer-vs-mamba-a-tale-of-two-llm-architectures)\n\n[214\\. 基于Mamba 的语言模型的实证研究](https://www.yiyibooks.cn/__trs__/arxiv/2406.07887v1/index.html)\n\n[215\\. Albert Gu, Tri Dao. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces.” ArXiv](https://doi.org/10.48550/arXiv.2312.00752)\n\n[221\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[222\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[223\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[224\\. Yuhao Ji, Chao Fang et al. “Co-Designing Binarized Transformer and Hardware Accelerator for Efficient End-to-End Edge Deployment.”](https://arxiv.org/abs/2407.12070)\n\n[225\\. Hardware-friendly compression and hardware acceleration for transformer ...](https://www.aimspress.com/article/doi/10.3934/era.2022192)\n\n[226\\. Enhancing Long Sequence Input Processing in FPGA-Based Transformer Accelerators through Attention Fusion | Proceedings of the Great Lakes Symposium on VLSI 2024](https://dl.acm.org/doi/fullHtml/10.1145/3649476.3658810)\n\n[227\\. Hardware Accelerator for Approximation-Based Softmax ...](https://www.mdpi.com/2079-9292/14/12/2337)\n\n[228\\. AI Roadmap](https://www.ibm.com/roadmaps/ai/)\n\n[229\\. Johnathan Alsop, Shaizeen Aga et al. “Inclusive-PIM: Hardware-Software Co-design for Broad Acceleration on Commercial PIM Architectures.” ArXiv](https://doi.org/10.48550/arXiv.2309.07984)\n\n[230\\. 论文分享—Hardware-Software Co-Design for an Analog-Digital Accelerator for Machine L](https://www.bilibili.com/video/av113888171398203?t=184)\n\n[231\\. Markus Püschel, José M. F. Moura et al. “SPIRAL: Code Generation for DSP Transforms.” Proceedings of the IEEE](https://doi.org/10.1109/JPROC.2004.840306)\n\n[232\\. SIAM Task Force Report: The Future of Computational Science](https://www.siam.org/media/cfufuosh/siam-report-on-the-future-of-computational-science.pdf)\n\n[233\\. Shizhen Huang, Enhao Tang et al. “Hardware-friendly compression and hardware acceleration for transformer: A survey.” Electronic Research Archive](https://doi.org/10.3934/era.2022192)\n\n[234\\. CEPC Accelerator EDR Phase Working Plan (preliminary) 2024 – 2027](https://indico.ihep.ac.cn/event/20107/contributions/143045/attachments/73029/89274/CEPC%20Accelerator%20EDR%20Phase%20Plan%20%EF%BC%88update%EF%BC%89-V7%2020230625.pdf)\n\n[235\\. Shikhar Tuli, N. Jha. “TransCODE: Co-Design of Transformers and Accelerators for Efficient Training and Inference.” IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems](https://doi.org/10.1109/TCAD.2023.3283443)\n\n[241\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[242\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[243\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[244\\. Multimodal Large Language Models with Context-Aware ...](https://www.preprints.org/manuscript/202501.2114)\n\n[245\\. Jiahui Yu, Zirui Wang et al. “CoCa: Contrastive Captioners are Image-Text Foundation Models.” Trans. Mach. Learn. Res.](https://doi.org/10.48550/arXiv.2205.01917)\n\n[246\\. Multi-Head Mixture-of-Experts](https://proceedings.neurips.cc/paper_files/paper/2024/file/ab05dc8bf36a9f66edbff6992ec86f56-Paper-Conference.pdf)\n\n[247\\. Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone](https://papers.nips.cc/paper_files/paper/2022/file/d4b6ccf3acd6ccbc1093e093df345ba2-Paper-Conference.pdf)\n\n[248\\. Dongsheng Jiang, Yuchen Liu et al. “From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2310.08825)\n\n[249\\. Vision Language Representation Learning](https://dr.ntu.edu.sg/bitstream/10356/169546/3/PhD_Thesis.pdf)\n\n[250\\. 多模态超详细解读 (九)：CoCa：对比学习和生成式任务训练多模态大模型](https://zhuanlan.zhihu.com/p/627702632)\n\n[251\\. Quality Assessment in the Era of Large Models: A Survey](https://arxiv.org/pdf/2409.00031)\n\n[252\\. LLAVA-PLUS: LEARNING TO USE TOOLS FOR CREATING MULTIMODAL AGENTS](https://openreview.net/pdf/aebea1bacbeb8cfae8ee58f34c6274ceba718130.pdf)\n\n[253\\. UniD3: unified discrete diffusion for simultaneous vision-language generation](https://dr.ntu.edu.sg/bitstream/10356/172665/2/2023-Hu_etal-ICLR-UniD3_Unified_discrete_diffusion_vision_language.pdf)\n\n[254\\. UnIVAL: Unified Model for Image, Video, Audio and Language Tasks](https://hal.science/hal-04366059/file/2307.16184.pdf)\n\n[255\\. UniT: Multimodal Multitask Learning with a Unified Transformer](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_UniT_Multimodal_Multitask_Learning_With_a_Unified_Transformer_ICCV_2021_paper.pdf)\n\n[256\\. Deep Multimodal Learning for Vision and Language Processing](https://theses.hal.science/tel-03140942v1/file/1.%20me%CC%81moire%20de%20the%CC%80se.pdf)\n\n[257\\. Gongwei Chen, Leyang Shen et al. “LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge.” ArXiv](https://doi.org/10.48550/arXiv.2311.11860)\n\n[258\\. Comparison of LaVIN and existing multimodal LLMs on COCO captioning, VQAv2 val set, TruthfulQA, and MME benchmark](https://openreview.net/attachment?id=xDV3dzC0ZZ&name=pdf)\n\n[259\\. BRAVE: Broadening the visual encoding of vision-language models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02433.pdf)\n\n[260\\. Multi-stage hybrid embedding fusion network for visual question ...](https://www.sciencedirect.com/science/article/pii/S0925231220316593)\n\n[261\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[262\\. EMMA: EMPOWERING MULTI-MODAL MAMBA WITH STRUCTURAL AND HIERARCHICAL ALIGNMENT](https://openreview.net/pdf?id=Ev4iw23gdI)\n\n[263\\. LONGMAMBA: ENHANCING MAMBA'S LONG CONTEXT CAPABILITIES VIA TRAINING-FREE RECEPTIVE FIELD ENLARGEMENT](https://openreview.net/pdf/066062c0673cfc50aeafa9e3c96d0cf7a33fc3ca.pdf)\n\n[264\\. 多模态指令调优与混合状态空间模型](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_11_14/2411.08840.pdf)\n\n[265\\. ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2](https://arxiv.org/pdf/2407.19832)\n\n[266\\. A Survey on Visual Mamba](https://scidok.sulb.uni-saarland.de/bitstream/20.500.11880/38074/1/applsci-14-05683-v2.pdf)\n\n[267\\. AV-Mamba: Cross-Modality Selective State Space Models for Audio-Visual Question Answering](https://sightsound.org/papers/2024/Huang_AV-Mamba_Cross-Modality_Selective_State_Space_Models_for_Audio-Visual_Question_Answering.pdf)\n\n[268\\. Visual Mamba: A Survey and New Outlooks](https://arxiv.org/pdf/2404.18861)\n\n[269\\. ML-Mamba: Efficient Multi-Modal Large Language Model ...](https://arxiv.org/html/2407.19832v1)\n\n[270\\. QMambaExtend: Improving Long-Context Extension of Memory-Efficient Mamba Models](https://openreview.net/pdf/6083232316d32fe72ef58259843fae2032d3448b.pdf)\n\n[271\\. ICLR.2025 - Poster](https://papers.cool/venue/ICLR.2025?group=Poster)\n\n[272\\. LongLLaVA: Scaling Multi-modal LLMs to 1000 Images ...](https://openreview.net/forum?id=wqA7QmpUwa)\n\n[273\\. MambaExtend: A Training-Free Approach to Improve Long Context Extension of Mamba](https://iclr.cc/media/iclr-2025/Slides/29980.pdf)\n\n[274\\. ML-Mamba: Efficient Multi-Modal Large Language Model...](http://arxiv.org/html/2407.19832v3)\n\n[281\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[282\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[283\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[284\\. Formal Verification of Source-to-Source Transformations for HLS](https://dl.acm.org/doi/10.1145/3626202.3637563)\n\n[285\\. TATAA: PROGRAMMABLE MIXED-PRECISION TRANSFORMER ACCELERATION WITH A TRANSFORMABLE ARITHMETIC ARCHITECTURE](https://arxiv.org/pdf/2411.03697)\n\n[286\\. Hardware-Software Co-Exploration And Optimization For Next-Generation Learning Machines](https://dr.ntu.edu.sg/bitstream/10356/178423/2/PhD_Thesis_ChenChunyun-Final.pdf)\n\n[287\\. Yuhao Ji, Chao Fang et al. “Co-Designing Binarized Transformer and Hardware Accelerator for Efficient End-to-End Edge Deployment.”](https://arxiv.org/abs/2407.12070)\n\n[288\\. Verification of software-to-hardware mappings for machine learning accelerators](https://www.cs.princeton.edu/~dh7120/assets/cv.pdf)\n\n[289\\. Hardware Accelerator for Approximation-Based Softmax ...](https://www.mdpi.com/2079-9292/14/12/2337)\n\n[290\\. N. Jouppi, C. Young et al. “In-datacenter performance analysis of a tensor processing unit.” 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)](https://doi.org/10.1145/3079856.3080246)\n\n[291\\. Hardware-friendly compression and hardware acceleration for transformer ...](https://www.aimspress.com/article/doi/10.3934/era.2022192)\n\n[292\\. Allo: A Programming Model for Composable Accelerator Design](https://www.csl.cornell.edu/~zhiruz/pdfs/allo-pldi2024.pdf)\n\n[293\\. Plan d'études SYSTEMES DE COMMUNICATION 2023 - 2024](https://www.epfl.ch/schools/ic/wp-content/uploads/2023/11/LC_SC_2023-2024-EN.pdf)\n\n[294\\. Jacob R. Stevens, Rangharajan Venkatesan et al. “Softermax: Hardware/Software Co-Design of an Efficient Softmax for Transformers.” 2021 58th ACM/IEEE Design Automation Conference (DAC)](https://doi.org/10.1109/dac18074.2021.9586134)\n\n[295\\. Transformer硬件加速器总结](https://zhuanlan.zhihu.com/p/633571685)\n\n[296\\. \\[论文审查\\] Hardware-Software Co-Design for Accelerating ...](https://www.themoonlight.io/zh/review/hardware-software-co-design-for-accelerating-transformer-inference-leveraging-compute-in-memory)\n\n[297\\. Hardware-friendly compression and hardware acceleratio...](https://aimspress.com/article/id/62fb8148ba35de77c348a274)\n\n[298\\. Verification Horizons](https://verificationacademy.com/verification-horizons/march-2015-volume-11-issue-1)\n\n[301\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[302\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[303\\. K. Papineni, Salim Roukos et al. “Bleu: a Method for Automatic Evaluation of Machine Translation.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.3115/1073083.1073135)\n\n[304\\. FREE-Merging: Fourier Transform for Efficient Model Merging](https://arxiv.org/pdf/2411.16815)\n\n[305\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[306\\. Multi-Head Mixture-of-Experts](https://proceedings.neurips.cc/paper_files/paper/2024/file/ab05dc8bf36a9f66edbff6992ec86f56-Paper-Conference.pdf)\n\n[307\\. PerceptionGPT: Effectively Fusing Visual Perception into LLM](https://openaccess.thecvf.com/content/CVPR2024/papers/Pi_PerceptionGPT_Effectively_Fusing_Visual_Perception_into_LLM_CVPR_2024_paper.pdf)\n\n[308\\. Variational Autoencoder for Deep Learning of Images, Labels and Captions](https://papers.nips.cc/paper/2016/file/eb86d510361fc23b59f18c1bc9802cc6-Paper.pdf)\n\n[309\\. Dongsheng Jiang, Yuchen Liu et al. “From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2310.08825)\n\n[310\\. 通义mPLUG模块化多模态大模型技术体系](https://valser.org/webinar/slide/slides/20240327/%E9%80%9A%E4%B9%89mPLUG%E6%A8%A1%E5%9D%97%E5%8C%96%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF%E4%BD%93%E7%B3%BB-%E5%BE%90%E6%B5%B7%E6%B4%8B-VALSE%281%29.pdf)\n\n[311\\. Comparison of LaVIN and existing multimodal LLMs on COCO captioning, VQAv2 val set, TruthfulQA, and MME benchmark](https://openreview.net/attachment?id=xDV3dzC0ZZ&name=pdf)\n\n[312\\. VinVL: Advancing the state of the art for vision-language models](https://www.microsoft.com/en-us/research/blog/vinvl-advancing-the-state-of-the-art-for-vision-language-models/)\n\n[313\\. Improving multimodal datasets with image captioning](https://proceedings.neurips.cc/paper_files/paper/2023/file/45e604a3e33d10fba508e755faa72345-Paper-Datasets_and_Benchmarks.pdf)\n\n[314\\. Image Captioning with Memorized Knowledge](http://ise.thss.tsinghua.edu.cn/mig/2021-1.pdf)\n\n[315\\. Image Captioners Are Scalable Vision Learners Too](https://proceedings.neurips.cc/paper_files/paper/2023/file/92369a01fbe8046a093746389b2c413e-Paper-Conference.pdf)\n\n[316\\. Gongwei Chen, Leyang Shen et al. “LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge.” ArXiv](https://doi.org/10.48550/arXiv.2311.11860)\n\n[317\\. Meshed-Memory Transformer for Image Captioning | Papers With Code](https://paperswithcode.com/paper/m2-meshed-memory-transformer-for-image)\n\n[318\\. Retrieval-Augmented Multimodal Language Modeling](https://openreview.net/pdf?id=VZ8bs0fwoO)\n\n[319\\. VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset](https://papers.nips.cc/paper_files/paper/2023/file/e6b2b48b5ed90d07c305932729927781-Supplemental-Conference.pdf)\n\n[320\\. Multimodal Large Language Models with Context-Aware ...](https://www.preprints.org/manuscript/202501.2114)\n\n[321\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[322\\. AV-Mamba: Cross-Modality Selective State Space Models for Audio-Visual Question Answering](https://sightsound.org/papers/2024/Huang_AV-Mamba_Cross-Modality_Selective_State_Space_Models_for_Audio-Visual_Question_Answering.pdf)\n\n[323\\. ...to Multi-Modal Large Language Model for Efficient I...](http://arxiv.org/html/2403.14520v1)\n\n[324\\. ML-Mamba: Efficient Multi-Modal Large Language Model ...](https://arxiv.org/html/2407.19832v1)\n\n[325\\. ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2](https://arxiv.org/pdf/2407.19832)\n\n[326\\. LONGMAMBA: ENHANCING MAMBA'S LONG CONTEXT CAPABILITIES VIA TRAINING-FREE RECEPTIVE FIELD ENLARGEMENT](https://openreview.net/pdf/066062c0673cfc50aeafa9e3c96d0cf7a33fc3ca.pdf)\n\n[327\\. Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models](https://openreview.net/pdf/d68d63de15506ec221657ddca9ceb58cdd2987ea.pdf)\n\n[328\\. EMMA: EMPOWERING MULTI-MODAL MAMBA WITH STRUCTURAL AND HIERARCHICAL ALIGNMENT](https://openreview.net/pdf?id=Ev4iw23gdI)\n\n[329\\. ML-Mamba: Efficient Multi-Modal Large Language Model...](http://arxiv.org/html/2407.19832v3)\n\n[330\\. BIMBA: Selective-Scan Compression for Long-Range Video Question Answering](https://www.crcv.ucf.edu/cvpr2025-vidllms-workshop/data/accepted_papers/CVPR_2025_BIMBA.pdf)\n\n[331\\. 多模态指令调优与混合状态空间模型](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_11_14/2411.08840.pdf)\n\n[332\\. LongLLaVA: Scaling Multi-modal LLMs to 1000 Images ...](https://openreview.net/forum?id=wqA7QmpUwa)\n\n[333\\. Evolving Large Language Model Assistant with Long-Term ...](https://www.alphaxiv.org/abs/2312.17257)\n\n[341\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[342\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[343\\. Alex Wang, Amanpreet Singh et al. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” BlackboxNLP@EMNLP](https://doi.org/10.18653/v1/W18-5446)\n\n[344\\. Yuhao Ji, Chao Fang et al. “Co-Designing Binarized Transformer and Hardware Accelerator for Efficient End-to-End Edge Deployment.”](https://arxiv.org/abs/2407.12070)\n\n[345\\. Stephen Merity, Caiming Xiong et al. “Pointer Sentinel Mixture Models.” ArXiv](https://arxiv.org/abs/1609.07843)\n\n[346\\. Open accelerators for start-ups success: a case study](https://air.uniud.it/bitstream/11390/1087153/1/open%20acc.pdf)\n\n[347\\. Hardware-Software Co-Design Based Obfuscation of Hardware Accelerators](https://askcryp.to/t/resource-topic-2019-1040-hardware-software-co-design-based-obfuscation-of-hardware-accelerators/12337)\n\n[348\\. Johnathan Alsop, Shaizeen Aga et al. “Inclusive-PIM: Hardware-Software Co-design for Broad Acceleration on Commercial PIM Architectures.” ArXiv](https://doi.org/10.48550/arXiv.2309.07984)\n\n[349\\. Design and Optimization of Hardware Accelerator Design](https://escholarship.org/content/qt3bs294n5/qt3bs294n5.pdf)\n\n[350\\. Ann Franchesca Laguna, Mohammed Mehdi et al. “Hardware-Software Co-Design of an In-Memory Transformer Network Accelerator.” Frontiers in Electronics](https://doi.org/10.3389/felec.2022.847069)\n\n[351\\. HOPP: Hardware-Software Co-Designed Page Prefetching for Disaggregated Memory](https://people.ece.ubc.ca/~sasha/TMP/HoPP-HPCA23.pdf)\n\n[352\\. C. Pauwels, B. Clarysse et al. “Understanding a new generation incubation model: The accelerator.” Technovation](https://doi.org/10.1016/J.TECHNOVATION.2015.09.003)\n\n[353\\. Gabriele Montanaro, Andrea Galimberti et al. “Hardware-Software Co-Design of BIKE with HLS-Generated Accelerators.” 2022 29th IEEE International Conference on Electronics, Circuits and Systems (ICECS)](https://doi.org/10.1109/ICECS202256217.2022.9970992)\n\n[354\\. P. Chu. “Integrating the Hardware-Software Co-design Concept in an Introductory Digital Design Course.” 2019 ASEE Zone I Conference &amp; Workshop Proceedings](https://doi.org/10.18260/1-2-1153-33788)\n\n[355\\. Improving Programming Support for Hardware Accelerators Through Automata Processing Abstractions](https://web.eecs.umich.edu/~weimerw/students/angstadt-dissertation.pdf)\n\n[361\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[362\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[363\\. PerceptionGPT: Effectively Fusing Visual Perception into LLM](https://openaccess.thecvf.com/content/CVPR2024/papers/Pi_PerceptionGPT_Effectively_Fusing_Visual_Perception_into_LLM_CVPR_2024_paper.pdf)\n\n[364\\. Multi-Head Mixture-of-Experts](https://proceedings.neurips.cc/paper_files/paper/2024/file/ab05dc8bf36a9f66edbff6992ec86f56-Paper-Conference.pdf)\n\n[365\\. xGen-MM (BLIP-3): A Family of Open Large Multimodal Models](https://openreview.net/attachment?id=UNHhtDWgV1&name=pdf)\n\n[366\\. Vision Language Representation Learning](https://dr.ntu.edu.sg/bitstream/10356/169546/3/PhD_Thesis.pdf)\n\n[367\\. 通义mPLUG模块化多模态大模型技术体系](https://valser.org/webinar/slide/slides/20240327/%E9%80%9A%E4%B9%89mPLUG%E6%A8%A1%E5%9D%97%E5%8C%96%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF%E4%BD%93%E7%B3%BB-%E5%BE%90%E6%B5%B7%E6%B4%8B-VALSE%281%29.pdf)\n\n[368\\. Hiba Ahsan, Nikita Bhalla et al. “Multi-Modal Image Captioning for the Visually Impaired.” ArXiv](https://doi.org/10.18653/v1/2021.naacl-srw.8)\n\n[369\\. VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset](https://papers.nips.cc/paper_files/paper/2023/file/e6b2b48b5ed90d07c305932729927781-Supplemental-Conference.pdf)\n\n[370\\. Improving multimodal datasets with image captioning](https://proceedings.neurips.cc/paper_files/paper/2023/file/45e604a3e33d10fba508e755faa72345-Paper-Datasets_and_Benchmarks.pdf)\n\n[371\\. Generating Images with Multimodal Language Models](https://proceedings.neurips.cc/paper_files/paper/2023/file/43a69d143273bd8215578bde887bb552-Paper-Conference.pdf)\n\n[372\\. Multimodal Large Language Models with Context-Aware ...](https://www.preprints.org/manuscript/202501.2114)\n\n[373\\. Comparison of LaVIN and existing multimodal LLMs on COCO captioning, VQAv2 val set, TruthfulQA, and MME benchmark](https://openreview.net/attachment?id=xDV3dzC0ZZ&name=pdf)\n\n[374\\. 结合多层级解码器和动态融合机制的图像描述](https://www.cjig.cn/rc-pub/front/front-article/download/58811129/lowqualitypdf/%E7%BB%93%E5%90%88%E5%A4%9A%E5%B1%82%E7%BA%A7%E8%A7%A3%E7%A0%81%E5%99%A8%E5%92%8C%E5%8A%A8%E6%80%81%E8%9E%8D%E5%90%88%E6%9C%BA%E5%88%B6%E7%9A%84%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0.pdf)\n\n[375\\. Jiquan Ngiam, A. Khosla et al. “Multimodal Deep Learning.” International Conference on Machine Learning](https://doi.org/10.48550/arXiv.2301.04856)\n\n[376\\. A Multimodal Large Language Model Framework for ...](https://www.mdpi.com/1424-8220/25/10/3072)\n\n[377\\. Dongsheng Jiang, Yuchen Liu et al. “From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2310.08825)\n\n[378\\. Image Captioners Are Scalable Vision Learners Too](https://proceedings.neurips.cc/paper_files/paper/2023/file/92369a01fbe8046a093746389b2c413e-Paper-Conference.pdf)\n\n[379\\. Measuring the Contributions of Vision and Text Modalities in Multimodal Transformers](https://archiv.ub.uni-heidelberg.de/volltextserver/35753/1/30-Nov-2024_final_parcalabescu_letitia_dissertation-pdf-a.pdf)\n\n[380\\. mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration](https://openaccess.thecvf.com/content/CVPR2024/papers/Ye_mPLUG-Owl2_Revolutionizing_Multi-modal_Large_Language_Model_with_Modality_Collaboration_CVPR_2024_paper.pdf)\n\n[381\\. MMA: BENCHMARKING MULTI-MODAL LARGE LANGUAGE MODELS IN AMBIGUITY CONTEXTS](https://openreview.net/pdf/6b15da2be50eaa85b587f5d599071cc80c564128.pdf)\n\n[382\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[383\\. ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2](https://arxiv.org/pdf/2407.19832)\n\n[384\\. ML-Mamba: Efficient Multi-Modal Large Language Model ...](https://arxiv.org/html/2407.19832v1)\n\n[385\\. Counting-Stars (★): A Multi-evidence, Position-aware, and Scalable Benchmark for Evaluating Long-Context Large Language Models](https://openreview.net/pdf/c59d2ed33cb79a9c1ed693c36c856588bccd3a57.pdf)\n\n[386\\. Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models](https://openreview.net/pdf/d68d63de15506ec221657ddca9ceb58cdd2987ea.pdf)\n\n[387\\. ML-Mamba: Efficient Multi-Modal Large Language Model...](http://arxiv.org/html/2407.19832v3)\n\n[388\\. LONGMAMBA: ENHANCING MAMBA'S LONG CONTEXT CAPABILITIES VIA TRAINING-FREE RECEPTIVE FIELD ENLARGEMENT](https://openreview.net/pdf/066062c0673cfc50aeafa9e3c96d0cf7a33fc3ca.pdf)\n\n[389\\. Passing the Torch: Training a Mamba Model for Smooth ...](https://www.lighton.ai/lighton-blogs/passing-the-torch-training-a-mamba-model-for-smooth-handover)\n\n[390\\. EMMA: EMPOWERING MULTI-MODAL MAMBA WITH STRUCTURAL AND HIERARCHICAL ALIGNMENT](https://openreview.net/pdf?id=Ev4iw23gdI)\n\n[391\\. MambaMia：一种基于状态空间模型的压缩方法，用于大型多模态模型中高效的视频理解](https://www.xueshuxiangzi.com/downloads/2025_6_17/2506.13564.pdf)\n\n[392\\. ReMamba: Equip Mamba with Effective Long-Sequence Modeling](https://openreview.net/pdf?id=RMjyNzYv2K)\n\n[393\\. Recognition - Paper Reading](http://paperreading.club/category?cate=Recognition)\n\n[394\\. ICLR.2025 - Poster](https://papers.cool/venue/ICLR.2025?group=Poster)\n\n[401\\. Yuhao Ji, Chao Fang et al. “Co-Designing Binarized Transformer and Hardware Accelerator for Efficient End-to-End Edge Deployment.”](https://arxiv.org/abs/2407.12070)\n\n[402\\. 吉林高院启动为期两年法治化营商环境建设突破行动](http://www.legaldaily.com.cn/index/content/2024-04/28/content_8990828.html)\n\n[403\\. Design and Optimization of Hardware Accelerator Design](https://escholarship.org/content/qt3bs294n5/qt3bs294n5.pdf)\n\n[404\\. Verification Horizons](https://verificationacademy.com/verification-horizons/march-2015-volume-11-issue-1)\n\n[405\\. Verilator, Accelerated: Accelerating development, and case study of accelerating performance](https://veripool.org/papers/Verilator_Accelerated_OSDA2020.pdf)\n\n[406\\. 每日学习要点|习近平主持召开企业和专家座谈会强调 紧扣推...](https://mp.weixin.qq.com/s?__biz=MzIxMTY1OTE3OQ%3D%3D&mid=2247589535&idx=1&sn=e352fbb4f58c4ed6c86bfb377fae3446&chksm=96f1cbb5d69884a60a384cc2196e861ded921463f39ae2664092b88bfb5ca9e1358efd9c8281&scene=27)\n\n[407\\. W(00020)$ 商汤科技生成式AI最新应用案例（2024-2025年）一](https://xueqiu.com/8209487521/327106710)\n\n[408\\. 【上证食饮】周观点：政策托底提振信心，关注需求修复](https://finance.sina.com.cn/roll/2024-12-15/doc-inczqiii6539317.shtml)\n\n[409\\. CODEBench: A Neural Architecture and Hardware Accelerator Co-Design Framework](https://oar.princeton.edu/bitstream/88435/pr1000007b/1/2212.03965.pdf)\n\n[410\\. Developing high value OEM partnerships](https://library.e.abb.com/public/ef1915e7671a37f8c1257b3b00652b56/ABB-OEM-SHEETS-COMBINED%20%28Rev4-March13%29.pdf)\n\n[411\\. 青岛工学院教务处](http://jwc.qit.edu.cn/)\n\n[412\\. 新一代高能效 AI 加速器（DRP-AI3）：将自主系统的高级 AI 的嵌入式处理速度提高 10 倍](https://www.renesas.cn/zh/document/whp/next-generation-highly-power-efficient-ai-accelerator-drp-ai3-10x-faster-embedded-processing)\n\n[413\\. Hong Kong Metropolitan University.探究閱讀與銜接結構圖在中學文言篇章教學中的運用.Summary set the 12th International Conference on Han-Character Education and Research ?,2023.](https://s.wanfangdata.com.cn/paper?q=%E6%8E%A2%E7%A9%B6%E9%96%B1%E8%AE%80%E8%88%87%E9%8A%9C%E6%8E%A5%E7%B5%90%E6%A7%8B%E5%9C%96%E5%9C%A8%E4%B8%AD%E5%AD%B8%E6%96%87%E8%A8%80%E7%AF%87%E7%AB%A0%E6%95%99%E5%AD%B8%E4%B8%AD%E7%9A%84%E9%81%8B%E7%94%A8)\n\n[414\\. Help Desk Case Studies: 9 Real-life Examples in 2025](https://research.aimultiple.com/help-desk-case-studies/)\n\n[415\\. FY2023 RESEARCH OPPORTUNITIES IN ACCELERATOR STEWARDSHIP AND ACCELERATOR DEVELOPMENT](https://sc.osti.gov/-/media/grants/pdf/foas/2023/SC_FOA_0002951.pdf)\n\n[416\\. 国际视野丨便携式直线加速器超轻型电源系统](https://mp.weixin.qq.com/s?__biz=MzIyMzc0NzQ2Nw%3D%3D&mid=2247494325&idx=2&sn=1d1301950f8f8a8b309d0781c2d40328&chksm=e9023c2907498518abcc264ee7005c2e151102e5b14aac1a7607c0c5a3a3d82639c7c6dedcc5&scene=27)\n\n[417\\. 陕西榆林清洁生产审核工作实施方案-北极星碳管家网](https://news.bjx.com.cn/html/20220909/1254242.shtml)"
  },
  {
    "id": 58,
    "question": "What are the technical aspects and implementation challenges of fine-tuning Large Language Models, and how do techniques like LoRA address these challenges?",
    "response": "**Research Report: Technical Aspects and Implementation Challenges of Fine-Tuning Large Language Models**\n\n**1\\. Introduction**\n\nFine-tuning billion-parameter LLMs like LLaMA and GPT involves adapting pre-trained models to downstream tasks, but faces severe computational, memory, and deployment challenges. **Full fine-tuning** requires updating all parameters, which becomes infeasible for models exceeding 7B parameters due to GPU memory limitations and catastrophic forgetting (loss of pre-trained knowledge) \\[2\\]\\[7\\]. **Parameter-Efficient Fine-Tuning (PEFT)** techniques like LoRA (Low-Rank Adaptation) and its variants address these issues by selectively updating a fraction of parameters while maintaining performance. This report analyzes these techniques' technical foundations, implementation trade-offs, and recent advancements.\n\n**2\\. Technical Challenges of Full Fine-Tuning**\n\n**2.1 Computational Bottlenecks**\n\n**Resource Intensity**: Updating billions of parameters demands extensive GPU/TPU resources. A 13B-parameter model requires 120+ GB of VRAM, while a 65B model needs 780+ GB, necessitating multi-GPU setups \\[3\\]\\[14\\].\n\n**Time and Cost**: Training cycles span days or weeks, costing thousands of dollars even for medium-sized models \\[5\\].\n\n**2.2 Memory Constraints**\n\n**Activation and Optimizer Overhead**: Beyond model weights, gradients and optimizer states (e.g., Adam) consume VRAM. A 7B model requires 60 GB for full fine-tuning, exceeding consumer GPU capacities (24 GB RTX 4090) \\[3\\]\\[13\\].\n\n**Multi-Task Redundancy**: Storing separate fine-tuned copies for each task multiplies storage needs \\[2\\].\n\n**2.3 Functional Limitations**\n\n**Catastrophic Forgetting**: Aggressive weight updates overwrite pre-trained knowledge, degrading general capabilities \\[8\\].\n\n**Deployment Inflexibility**: Task-specific models cannot be dynamically reconfigured \\[41\\].\n\n**3\\. LoRA: Core Principles and Implementation**\n\n**3.1 Mathematical Framework**\n\nLoRA freezes pre-trained weights and injects trainable low-rank matrices (dim ) and (dim ) per layer, where . The weight update is approximated as:\n\nThis reduces trainable parameters by 10,000× for models like GPT-3 (175B) \\[41\\].\n\n**3.2 Performance and Efficiency**\n\n**No Inference Latency**: After training, can be merged into , eliminating extra computation \\[41\\].\n\n**Memory Reduction**: Requires 3× less GPU memory vs. full fine-tuning \\[41\\].\n\n**Task Switching**: Multiple adapters can be swapped without reloading the base model \\[41\\].\n\n**3.3 Limitations**\n\n**Rank Selection**: Suboptimal rank (hyperparameter) risks underfitting or overcomputation \\[193\\].\n\n**Multi-Adapter Batching**: Unfused adapters complicate batched inference across tasks \\[41\\].\n\n**4\\. QLoRA: Extreme Quantization**\n\nQLoRA extends LoRA via 4-bit quantization:\n\n**4-bit NormalFloat (NF4)**: Optimized quantization for normally distributed weights, minimizing error vs. FP4 \\[124\\]\\[136\\].\n\n**Double Quantization**: Compresses quantization constants for further memory savings \\[88\\].\n\n**Efficiency Gains**: Fine-tunes a 65B model on a single 48 GB GPU (vs. 780 GB for 16-bit) \\[85\\]\\[87\\].\n\n**Performance Trade-offs**: Achieves 99.3% of ChatGPT's Vicuna benchmark performance for Guanaco-65B \\[129\\], but struggles with batch size >1 and datasets like MRPC/QNLI in GLUE \\[135\\]\\[192\\].\n\n**5\\. Algorithmic Innovations (2024)**\n\n**5.1 Efficiency Optimizers**\n\n**LoRA+**: Sets asymmetric learning rates for and , boosting convergence speed 2× without extra cost \\[73\\].\n\n**FLoRA**: Recasts LoRA as gradient compression, eliminating warm-up phases \\[78\\].\n\n**BI-SHARE LoRA**: Intra-/inter-layer sharing improves parameter efficiency \\[77\\].\n\n**5.2 Multitask and Long-Context Variants**\n\n**Mixture-of-LoRAs (MoLA)**: Dynamically combines adapters for multitasking \\[62\\].\n\n**SinkLoRA**: Enhances long-context handling (>100K tokens) \\[64\\].\n\n**I-LoRA**: Mitigates catastrophic forgetting in continual learning \\[65\\].\n\n**6\\. Sparse vs. LoRA Fine-Tuning**\n\n**6.1 Accuracy-Efficiency Trade-offs**\n\n**Sparse Adapters**: Outperform LoRA on multi-task benchmarks (20+ tasks) via weight sparsity, but require complex merging heuristics \\[164\\]\\[290\\].\n\n**LoRA-XS/QLoRA**: Dominate in memory efficiency—QLoRA uses 10× less VRAM than sparse methods for 7B models \\[222\\]\\[344\\].\n\n**Performance**: **RoSA** (hybrid sparse + LoRA) exceeds both on reasoning tasks (e.g., 42.4% vs. 40% for LoRA on MATH for Llama-70B) \\[233\\].\n\n**Table: Comparative Metrics for 7B-70B Models**\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n| **Method** | **Trainable Params (70B)** | **MMLU Acc. (70B)** | **VRAM (GB)** |\n| Full FT (16b) | 70B | 70.1% | 1000 |\n| LoRA (r=8) | 10M | 68.9% | 160 |\n| QLoRA (4b) | 10M | 69.5% | 48  |\n| Wanda+SPP | 35M | 71.2% | 150 |\n\n_Data: \\[226\\]_\\[344\\]\\[354\\]\n\n**7\\. Deployment Challenges**\n\n**7.1 Edge Device Constraints**\n\n**Jetson Orin Optimization**: TensorRT enables kernel fusion, INT8 quantization, and dynamic memory allocation to handle unfused LoRA adapters \\[204\\]\\[268\\].\n\n**Latency**: Unfused adapters incur 30% higher latency vs. fused versions due to sequential kernel launches \\[218\\]\\[275\\].\n\n**Optimization Techniques**:\n\n**S-LoRA**: Custom CUDA kernels (MBGMM/MBGMV) batch heterogeneous adapters via non-contiguous memory ops \\[262\\].\n\n**CARASERVE**: Batched Gather-Matrix ops for PyTorch-based serving \\[331\\].\n\n**7.2 Scalability**\n\n**Multi-GPU Strategies**: S-LoRA's tensor parallelism minimizes comms overhead for trillion-parameter models \\[218\\].\n\n**Activation Memory**: Dominates QLoRA’s memory profile, limiting batch sizes \\[225\\].\n\n**8\\. Performance Benchmarks**\n\n**8.1 GLUE/MMLU Results**\n\n**QLoRA vs. Full 16-bit**: Matches performance for 65B LLaMA on MMLU (68.9% vs. 69.1%) and Vicuna \\[124\\]\\[307\\]but fails on MRPC/QNLI sub-tasks \\[317\\].\n\n**LoRA Hyperparameter Sensitivity**: Default configs underperform full tuning; optimal rank/placement recovers gaps \\[124\\].\n\n**8.2 Safety and Specialized Tasks**\n\n**Catastrophic Forgetting**: I-LoRA reduces knowledge loss by 40% vs. vanilla LoRA \\[65\\].\n\n**Coding/Raisonnement**: **LoRA-Pro** surpasses full fine-tuning on HumanEval for Llama-7B \\[228\\].\n\n**9\\. Open Research Challenges**\n\n1.  **Dynamic Rank Selection**: Auto-tuning per layer/task \\[79\\].\n2.  **Batched Multi-Adapter Inference**: Kernel-level optimizations for real-time edge deployment (Jetson Orin) \\[383\\].\n3.  **Trillion-Parameter Scalability**: FLoRA and SinkLoRA need further validation \\[64\\]\\[78\\].\n4.  **Quantization Stability**: 4-bit QLoRA shows instability in FP16 contexts \\[135\\].\n5.  **Sparse-LoRA Hybrids**: RoSA requires broader task evaluation \\[403\\].\n\n**10\\. Conclusion**\n\nFull fine-tuning of billion-parameter LLMs is bottlenecked by GPU memory and computational demands. LoRA and QLoRA mitigate these by reducing trainable parameters 10,000× and memory requirements 75%, while maintaining competitive accuracy on MMLU/GLUE. 2024 innovations like LoRA+, SinkLoRA, and RoSA address multi-task, long-context, and sparse integration challenges, but deployment on edge hardware (Jetson Orin) requires optimized CUDA kernels for batched unfused adapters. QLoRA sets a new efficiency standard—fine-tuning 70B models on consumer GPUs—though sparse methods occasionally outperform it in specialized tasks. Future work must prioritize dynamic rank selection, adapter batching, and stability under quantization.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[2\\. Revolutionizing Large Model Fine-Tuning: The Role of LoRA in Parameter-Efficient Adaptation](https://www.techrxiv.org/doi/pdf/10.36227/techrxiv.174015835.57150536)\n\n[3\\. Maximizing Efficiency: Fine‑Tuning Large Language ...](https://www.runpod.io/articles/guides/maximizing-efficiency-fine-tuning-large-language-models-with-lora-and-qlora-on-runpod)\n\n[4\\. Hybrid Fine-Tuning of Large Language Models Using LoRA: Enhancing Multi-Task Text Classification through Knowledge Sharing](https://jecei.sru.ac.ir/article_2303_af3def78c9c37bf642b72562f6210c53.pdf)\n\n[5\\. PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models](https://openreview.net/pdf/a40a9b106f7642dc97a3c56746158f706b82b42b.pdf)\n\n[6\\. MINI-BATCH CORESETS FOR MEMORY-EFFICIENT LANGUAGE MODEL TRAINING ON DATA MIXTURES](https://openreview.net/pdf/a4e3729677585997b9aee19d6345cfd8065cb224.pdf)\n\n[7\\. MCP Parameter Efficient Tuning: Guide to PEFT Methods](https://www.byteplus.com/en/topic/541922)\n\n[8\\. Revisiting Fine-Tuning: A Survey of Parameter-Efficient Techniques for Large AI Models](https://hal.science/hal-05008993v1/document)\n\n[9\\. Computational Bottlenecks of Training Small-scale Large Language Models](https://arxiv.org/pdf/2410.19456)\n\n[10\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[11\\. BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models](https://openreview.net/pdf?id=0uXtFk5KNJ)\n\n[12\\. An Empirical Study on Applying Parameter-Efficient Fine-Tuning of Large Language Models for Secure Code Generation](https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/3188531/no.ntnu:inspera:271477024:129693271.pdf?sequence=1)\n\n[13\\. Optimizing Memory Usage when Training Deep Neural Networks](https://theses.hal.science/tel-04890912v1/file/ZHAO_XUNYI_2024.pdf)\n\n[14\\. Pushing Large Language Models to the 6G Edge](https://arxiv.org/html/2309.16739)\n\n[15\\. How can I fine-tune large language models on a budget ...](https://www.runpod.io/articles/guides/how-to-fine-tune-large-language-models-on-a-budget)\n\n[16\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[17\\. Benchmarking public large language model](https://opus4.kobv.de/opus4-haw/files/4593/I001854150Thesis.pdf)\n\n[21\\. 信息技术 神经网络表示与模型压缩 第 2 部分：大规模预训练模型](http://www.aitisa.org.cn/ueditor/php/upload/file/20250519/1747624546.pdf)\n\n[22\\. Revisiting Fine-Tuning: A Survey of Parameter-Efficient Techniques for Large AI Models](https://www.preprints.org/manuscript/202504.0743/v1/download)\n\n[23\\. 大模型基础 Foundations of Large Language Models](https://www.dboop.com/img/deepseek/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%AF%9B%E7%8E%89%E4%BB%81%EF%BC%9A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80.pdf)\n\n[24\\. 爱可可 AI 前沿推介(10.19)](https://zhuanlan.zhihu.com/p/662119058)\n\n[25\\. Hyperparameter Optimization For LLMs: Practices & ...](https://www.deepchecks.com/hyperparameter-optimization-llms-best-practices-advanced-techniques/)\n\n[26\\. Revolutionizing Large Model Fine-Tuning: The Role of LoRA in Parameter-Efficient Adaptation](https://www.techrxiv.org/doi/pdf/10.36227/techrxiv.174015835.57150536)\n\n[27\\. GPT-Neo with LoRA for Better Medical Knowledge Performance on MultiMedQA Dataset](https://osf.io/njupy/download)\n\n[28\\. Parameter Efficient BERT Fine-tuning](https://web.stanford.edu/class/cs224n/final-reports/256827366.pdf)\n\n[29\\. QLoRA: Efficient Finetuning of Quantized LLMs](https://papers.nips.cc/paper_files/paper/2023/file/1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf)\n\n[30\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[31\\. DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic, Lightweight Plugin for Large Language Models](https://openreview.net/pdf?id=I1VCj1l1Zn)\n\n[32\\. Adaptive LoRA Merging for Efficient Domain Incremental Learning](https://openreview.net/pdf?id=1xWThUFn6k)\n\n[33\\. Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning](https://openreview.net/pdf?id=QZKn0P5dAE)\n\n[34\\. LLMs to Support a Domain Specific Knowledge Assistant](https://arxiv.org/pdf/2502.04095)\n\n[35\\. Dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01163.pdf)\n\n[36\\. Evaluating the Potential of LLMs for Better Short Answer Scoring](https://www.scitepress.org/Papers/2025/132917/132917.pdf)\n\n[37\\. A Study of Optimizations for Fine-tuning Large Language ...](https://arxiv.org/html/2406.02290v2)\n\n[38\\. What is Low Rank Adaptation (LoRA)?](https://www.geeksforgeeks.org/deep-learning/what-is-low-rank-adaptation-lora/)\n\n[41\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[42\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[43\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[44\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[45\\. LLaMA-Adapter: Efficient Fine-tuning of Large Language Models with Zero-initialized Attention](http://arxiv.org/pdf/2303.16199)\n\n[46\\. Benchmarking Large Language Models for Polymer ...](https://arxiv.org/html/2506.02129v1)\n\n[47\\. Exploring the Educational Utility of Pretrained Language Models](https://su.diva-portal.org/smash/get/diva2:1909361/FULLTEXT01.pdf)\n\n[48\\. Large Language Model Fine-tuning with Low-Rank Adaptation: A Performance Exploration](https://lca.ece.utexas.edu/pubs/icpe25_Bagus.pdf)\n\n[49\\. LoRA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://zhuanlan.zhihu.com/p/656797634)\n\n[50\\. Behind the Scenes – What it Takes to Teach GPT-3 How ...](https://www.microsoft.com/en-us/power-platform/blog/?p=18944)\n\n[51\\. LoRA: Low-Rank Adaptation of Large Language Models...](https://www.microsoft.com/en-us/research/publication/lora-low-rank-adaptation-of-large-language-models/?locale=zh-cn)\n\n[52\\. LoRA - Low-Rank Adaptation Of Large Language Models](https://picovoice.ai/blog/low-rank-adaptation-of-large-language-models/)\n\n[53\\. An Empirical Study on Applying Parameter-Efficient Fine-Tuning of Large Language Models for Secure Code Generation](https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/3188531/no.ntnu:inspera:271477024:129693271.pdf?sequence=1)\n\n[54\\. What is LoRA (low-rank adaption)? - IBM](https://www.ibm.com/think/topics/lora)\n\n[61\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[62\\. Revisiting Fine-Tuning: A Survey of Parameter-Efficient Techniques for Large AI Models](https://www.preprints.org/manuscript/202504.0743/v1/download)\n\n[63\\. Large Language Model Fine-tuning with Low-Rank Adaptation: A Performance Exploration](https://lca.ece.utexas.edu/pubs/icpe25_Bagus.pdf)\n\n[64\\. A survey on LoRA of large language models](https://link.springer.com/content/pdf/10.1007/s11704-024-40663-9.pdf)\n\n[65\\. GitHub - ZJU-LLMs/Awesome-LoRAs](https://github.com/ZJU-LLMs/Awesome-LoRAs/)\n\n[66\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[67\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[68\\. LoRA-Pro: ARE LOW-RANK ADAPTERS PROPERLY OPTIMIZED?](https://arxiv.org/pdf/2407.18242)\n\n[69\\. GitHub - chenqing24/LLaMA-Factory: Unified Efficient F...](https://github.com/chenqing24/LLaMA-Factory)\n\n[70\\. A Survey on LoRA of Large Language Models](https://journal.hep.com.cn/fcs/EN/10.1007/s11704-024-40663-9)\n\n[71\\. 大規模言語モデルの進化と最新動向 ver. 20240723 ##LLM - Qiita](https://qiita.com/compassinai/items/0be2b1139e63a4fa6f11)\n\n[72\\. Sparse Gradient Compression for Fine-Tuning Large Language Models](https://openreview.net/pdf?id=K0whd2tjjE)\n\n[73\\. Fine-Tuning Large Language Models for Practical Software Engineering: Case Studies in Automated Patch Generation](https://gupea.ub.gu.se/bitstream/handle/2077/83733/Master%20Thesis%20Fine%20Tuning%20Large%20Language%20Models%20for%20Pracital%20Software%20Engineering.pdf?sequence=1)\n\n[74\\. 一文了解21年起2025的10种LoRA相关算法（LoRA、Delta- ...](https://blog.csdn.net/a486259/article/details/148512441)\n\n[75\\. LoRA-GGPO: Mitigating Double Descent in LoRA Fine-Tuning via Gradient-Guided Perturbation Optimization](https://arxiv.org/pdf/2502.14538)\n\n[76\\. Are You Still Using LoRA to Fine-Tune Your LLM?](https://towardsdatascience.com/are-you-still-using-lora-to-fine-tune-your-llm/)\n\n[77\\. BI-SHARE LoRA: ENHANCING THE PARAMETER EFFICIENCY OF LoRA VIA INTRA-LAYER AND INTER-LAYER SHARING](https://openreview.net/pdf/84a0161064714133dbaf442ccdf8c36c8f092110.pdf)\n\n[78\\. On the Optimization Landscape of Low Rank Adaptation Methods for Large Language Models](https://openreview.net/pdf?id=pxclAomHat)\n\n[79\\. Applied Auto-tuning on LoRA Hyperparameters](https://scholarcommons.scu.edu/context/cseng_senior/article/1271/viewcontent/Thesis_Applied_Auto_tuning_on_LoRA_Hyperparameters.pdf)\n\n[81\\. Maximizing Efficiency: Fine‑Tuning Large Language ...](https://www.runpod.io/articles/guides/maximizing-efficiency-fine-tuning-large-language-models-with-lora-and-qlora-on-runpod)\n\n[82\\. LoRA Hyperparameters Guide | Unsloth Documentation](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide)\n\n[83\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[84\\. QLoRA: Efficient Finetuning of Quantized LLMs](https://nips.cc/media/neurips-2023/Slides/73855.pdf)\n\n[85\\. Lecture 14: Compressing and Sparsifying LLM](https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/L14.pptx)\n\n[86\\. Large Language Model Fine-tuning with Low-Rank Adaptation: A Performance Exploration](https://lca.ece.utexas.edu/pubs/icpe25_Bagus.pdf)\n\n[87\\. Artificial Intelligence Index Report 2024](https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_Chapter2.pdf)\n\n[88\\. Development of an Intelligent Chatbot for Streamlined Support of Non-EU Students Exploring Study Opportunities in Finland Utilizing a Large Language Model](http://www.theseus.fi/bitstream/10024/868800/2/Gautam_Deepan.pdf)\n\n[89\\. 전체 글](https://hw-hk.tistory.com/?page=4)\n\n[90\\. LQ-LoRA: LOW-RANK PLUS QUANTIZED MATRIX DECOMPOSITION FOR EFFICIENT LANGUAGE MODEL FINETUNING](https://par.nsf.gov/servlets/purl/10512534)\n\n[91\\. 论文精读：QLoRA: Efficient Finetuning of Quantized LLMs](https://zhuanlan.zhihu.com/p/673765062)\n\n[92\\. Fine-Tuning LLMs using PEFT](https://learnopencv.com/fine-tuning-llms-using-peft/)\n\n[93\\. Making the Most of Mistral-7b with Finetuning](https://www.analyticsvidhya.com/back-channel/download-pdf.php?pid=140801&next=)\n\n[94\\. یادگیری تقویتی عمیق با پایتون برای چت بات ها و مدل های زبان بزرگ - ویرایش دوم - نیمیش سانگ](https://irantypist.com/media/new_research/samplefile/1741017455_9398.docx)\n\n[95\\. Generating Test Cases Using Natural Language Processing](https://lup.lub.lu.se/student-papers/record/9168588/file/9168595.pdf)\n\n[96\\. The Complete Guide to GPU Requirements for LLM Fine- ...](https://www.runpod.io/blog/llm-fine-tuning-gpu-guide)\n\n[97\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[98\\. QLoRA: Efficient Finetuning of Quantized LLMs - Scilit](https://www.scilit.net/publications/b5e6dc827675a5bae049e6a2953f078c)\n\n[101\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[102\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[103\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[104\\. EDGE-LLM: Enabling Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive Layer Tuning & Voting](https://www.haoranyou.com/publications/pdf/2024DAC_EdgeLLM.pdf)\n\n[105\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[106\\. SLM vs LoRA LLM: Edge Deployment and Fine-Tuning ...](https://blog.premai.io/slm-vs-lora-llm-edge-deployment-and-fine-tuning-compared/)\n\n[107\\. Crayon: Customized On-Device LLM via Instant Adapter Blending and Edge-Server Hybrid Inference](https://openreview.net/pdf/cd075e493a5f11f5fbacfa2d911f085067a5cc50.pdf)\n\n[108\\. GenAI at the Edge: Comprehensive Survey on Empowering Edge Devices](https://www.qeios.com/read/JEU3U0/pdf)\n\n[109\\. Kartikeya Bhardwaj, N. Pandey et al. “Sparse High Rank Adapters.” ArXiv](https://doi.org/10.48550/arXiv.2406.13175)\n\n[110\\. Fine-Tuning LoRA Models: Optimize Performance](https://www.truefoundry.com/blog/serving-lora-fine-tuned-models)\n\n[111\\. Deploying LLMs on Resource-Constrained Devices](https://www.rohan-paul.com/p/deploying-llms-on-resource-constrained)\n\n[112\\. DEPLOYMENT OF SMALL LLMS ON CONSUMER-GRADE HARDWARE FOR EDGE COMPUTING](https://www.tdcommons.org/cgi/viewcontent.cgi?article=8231&context=dpubs_series)\n\n[113\\. Fine-Tuning Transformers Efficiently: A Survey on LoRA and Its Impact](https://www.preprints.org/manuscript/202502.1637/download/final_file)\n\n[114\\. Hardware and Algorithm Co-Exploration for Efficient On-Device Personalization of Large Language Models](https://curate.nd.edu/ndownloader/files/53983748/1)\n\n[115\\. Fine-tuning Llama 3.2 3B for RAG - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2024/12/fine-tuning-llama-3-2-3b-for-rag/)\n\n[116\\. Cross-Platform Edge AI Made Easy with ONNX Runtime](https://techcommunity.microsoft.com/blog/aiplatformblog/cross-platform-edge-ai-made-easy-with-onnx-runtime/4303521)\n\n[117\\. Ruiyang Qin, Dancheng Liu et al. “Empirical Guidelines for Deploying LLMs onto Resource-constrained Edge Devices.” ArXiv](https://doi.org/10.48550/arXiv.2406.03777)\n\n[121\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[122\\. QLoRA: Efficient Finetuning of Quantized LLMs](https://zhuanlan.zhihu.com/p/649327877)\n\n[123\\. Internet-scale Topic Modeling using Large Language Models](https://aaltodoc.aalto.fi/bitstreams/b63ce9d3-a01a-48a1-8ff7-831c872fde9d/download)\n\n[124\\. 开源原驼（Guanaco）及背后的QLoRA技术，将微调65B模型的显存需求从780GB以上降低到48GB以下，效果直逼GPT-4](https://zhuanlan.zhihu.com/p/632236718)\n\n[125\\. Lecture 14: Compressing and Sparsifying LLM](https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/L14.pptx)\n\n[126\\. Accessible Foundation Models: Systems, Algorithms, and Science](https://digital.lib.washington.edu/server/api/core/bitstreams/63e54e0c-bcaa-42e8-b164-5a8e4db7a715/content)\n\n[127\\. FINE-TUNING AN OPEN SOURCE CHATBOT TO TRANSLATE CODE FROM PYTHON TO JAVA USING QLORA: TRANSLATING FOR MORE ENERGY EFFICIENT CODE](https://lutpub.lut.fi/bitstream/10024/168604/1/diplomityo_hakkarainen_joonas.pdf)\n\n[128\\. 전체 글](https://hw-hk.tistory.com/?page=4)\n\n[129\\. 论文精读：QLoRA: Efficient Finetuning of Quantized LLMs](https://zhuanlan.zhihu.com/p/673765062)\n\n[130\\. Artificial Intelligence Index Report 2024](https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_Chapter2.pdf)\n\n[131\\. Large Language Model Fine-tuning with Low-Rank Adaptation: A Performance Exploration](https://lca.ece.utexas.edu/pubs/icpe25_Bagus.pdf)\n\n[132\\. Memory-efficient Fine-tuning with with QLoRA | Niklas Heidloff](https://heidloff.net/article/qlora/)\n\n[133\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[134\\. Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/pdf/2403.14608)\n\n[135\\. Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models](https://openreview.net/pdf?id=gvT2ksp27C6)\n\n[136\\. QLoRA: Efficient Finetuning of Quantized LLMs - Scilit](https://www.scilit.net/publications/b5e6dc827675a5bae049e6a2953f078c)\n\n[137\\. Development of a Music Education Framework Using Large Language Models (LLMs)](https://ninum.uit.no/bitstream/handle/10037/34165/thesis.pdf?sequence=2&isAllowed=y)\n\n[138\\. LLM Tuning & Dataset Perspectives](https://magazine.sebastianraschka.com/p/ahead-of-ai-9-llm-tuning-and-dataset)\n\n[139\\. Prabhdeep Cheema, Erhan Guven. “Optimizing Recommendations using Fine-Tuned LLMs.”](https://arxiv.org/abs/2505.06841)\n\n[141\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[142\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[143\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[144\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[145\\. N. Houlsby, A. Giurgiu et al. “Parameter-Efficient Transfer Learning for NLP.” ArXiv](https://arxiv.org/abs/1902.00751)\n\n[146\\. mLoRA: Fine-Tuning LoRA Adapters via Highly-Efficient Pipeline Parallelism in Multiple GPUs](https://arxiv.org/pdf/2312.02515)\n\n[147\\. Kartikeya Bhardwaj, N. Pandey et al. “Rapid Switching and Multi-Adapter Fusion via Sparse High Rank Adapters.”](https://arxiv.org/abs/2407.16712)\n\n[148\\. Kartikeya Bhardwaj, N. Pandey et al. “Sparse High Rank Adapters.” ArXiv](https://doi.org/10.48550/arXiv.2406.13175)\n\n[149\\. Simultaneous fine-tuning of multiple LoRA adapters](https://www.epfl.ch/labs/sacs/wp-content/uploads/2025/02/2024-fall-anton.pdf)\n\n[150\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[161\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[162\\. Large Language Model Fine-tuning with Low-Rank Adaptation: A Performance Exploration](https://lca.ece.utexas.edu/pubs/icpe25_Bagus.pdf)\n\n[163\\. Parameter-efficient Fine-tuning for Sparse LLMs](https://icml.cc/media/icml-2024/Slides/34806.pdf)\n\n[164\\. EXPLORING SPARSE ADAPTERS FOR SCALABLE MERGING OF PARAMETER EFFICIENT EXPERTS](https://openreview.net/pdf?id=8wt2eKkVe6)\n\n[165\\. Decoupling Angles and Strength in Low-rank Adaptation](https://openreview.net/pdf/94d2d4dfddbc9bde01cd476b6422aaee33376779.pdf)\n\n[166\\. Artificial Intelligence Index Report 2024](https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_Chapter2.pdf)\n\n[167\\. VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks](https://proceedings.neurips.cc/paper_files/paper/2024/file/1e0d38c676d5855bcfab7f6d29d20ad9-Paper-Conference.pdf)\n\n[168\\. Sparse Gradient Compression for Fine-Tuning Large Language Models](https://openreview.net/pdf?id=K0whd2tjjE)\n\n[169\\. RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation](https://raw.githubusercontent.com/mlresearch/v235/main/assets/nikdan24a/nikdan24a.pdf)\n\n[170\\. LoRA-Pro: ARE LOW-RANK ADAPTERS PROPERLY OPTIMIZED?](https://arxiv.org/pdf/2407.18242)\n\n[171\\. LinChance×NTU for Unconstrained WMT2024 Literary Translation](https://www2.statmt.org/wmt24/pdf/2024.wmt-1.99.pdf)\n\n[172\\. LoRS: Efficient Low-Rank Adaptation for Sparse Large Language Model](https://arxiv.org/pdf/2501.08582)\n\n[173\\. LORA WITHOUT FORGETTING: FREEZING AND SPARSE MASKING FOR LOW-RANK ADAPTATION](https://openreview.net/pdf?id=aGOQYJfz6H)\n\n[174\\. NO RA: NESTED LOW-RANK ADAPTATION FOR EFFICIENT FINE-TUNING LARGE MODELS](https://openreview.net/pdf/fbf030c71d4228a1a08a5f0b9eb589a08bf974ae.pdf)\n\n[175\\. LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning](https://arxiv.org/pdf/2403.17919)\n\n[176\\. A survey on LoRA of large language models](https://link.springer.com/article/10.1007/s11704-024-40663-9)\n\n[177\\. Parameter Efficient Fine Tuning: A Comprehensive ...](https://arxiv.org/html/2404.13506v2)\n\n[178\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[179\\. RoseLoRA: Row and Column-wise Sparse Low-rank Adaptati...](http://arxiv.org/html/2406.10777v1)\n\n[180\\. Comparing Fine-Tuning Optimization Techniques (LoRA, ...](https://insights.encora.com/insights/comparing-fine-tuning-optimization-techniques-lora-qlora-dora-and-qdora)\n\n[181\\. Lecture 14: Compressing and Sparsifying LLM](https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/L14.pptx)\n\n[182\\. QLoRA: Efficient Finetuning of Quantized LLMs](https://zhuanlan.zhihu.com/p/649327877)\n\n[183\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[184\\. Internet-scale Topic Modeling using Large Language Models](https://aaltodoc.aalto.fi/bitstreams/b63ce9d3-a01a-48a1-8ff7-831c872fde9d/download)\n\n[185\\. FINE-TUNING AN OPEN SOURCE CHATBOT TO TRANSLATE CODE FROM PYTHON TO JAVA USING QLORA: TRANSLATING FOR MORE ENERGY EFFICIENT CODE](https://lutpub.lut.fi/bitstream/10024/168604/1/diplomityo_hakkarainen_joonas.pdf)\n\n[186\\. 전체 글](https://hw-hk.tistory.com/?page=4)\n\n[187\\. Artificial Intelligence Index Report 2024](https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_Chapter2.pdf)\n\n[188\\. Large Language Model Fine-tuning with Low-Rank Adaptation: A Performance Exploration](https://lca.ece.utexas.edu/pubs/icpe25_Bagus.pdf)\n\n[189\\. QLoRA: Efficient Finetuning of Quantized LLMs - Scilit](https://www.scilit.net/publications/b5e6dc827675a5bae049e6a2953f078c)\n\n[190\\. 论文精读：QLoRA: Efficient Finetuning of Quantized LLMs](https://zhuanlan.zhihu.com/p/673765062)\n\n[191\\. Accessible Foundation Models: Systems, Algorithms, and Science](https://digital.lib.washington.edu/server/api/core/bitstreams/63e54e0c-bcaa-42e8-b164-5a8e4db7a715/content)\n\n[192\\. Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models](https://openreview.net/pdf?id=gvT2ksp27C6)\n\n[193\\. 开源原驼（Guanaco）及背后的QLoRA技术，将微调65B模型的显存需求从780GB以上降低到48GB以下，效果直逼GPT-4](https://zhuanlan.zhihu.com/p/632236718)\n\n[194\\. Fine-Tuning LLMs using PEFT](https://learnopencv.com/fine-tuning-llms-using-peft/)\n\n[195\\. Development of a Music Education Framework Using Large Language Models (LLMs)](https://ninum.uit.no/bitstream/handle/10037/34165/thesis.pdf?sequence=2&isAllowed=y)\n\n[196\\. PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models](https://openreview.net/pdf?id=6ZBHIEtdP4)\n\n[197\\. Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/pdf/2403.14608)\n\n[198\\. LLM Tuning & Dataset Perspectives](https://magazine.sebastianraschka.com/p/ahead-of-ai-9-llm-tuning-and-dataset)\n\n[201\\. Joseph Redmon, Ali Farhadi. “YOLOv3: An Incremental Improvement.” ArXiv](https://arxiv.org/abs/1804.02767)\n\n[202\\. Andrew G. Howard, Menglong Zhu et al. “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.” ArXiv](https://arxiv.org/abs/1704.04861)\n\n[203\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[204\\. Optimizing Vision Transformers for Peak Performance on ...](https://www.embedl.com/optimizing-vision-transformers-for-peak-performance-on-nvidia-jetson-agx-orinvidia-jetson-agx-orin)\n\n[205\\. Running LLMs with TensorRT-LLM on NVIDIA Jetson Orin ...](https://collabnix.com/running-llms-with-tensorrt-llm-on-nvidia-jetson-orin-nano-super/)\n\n[206\\. Full-Stack Innovation Fuels Highest MLPerf Inference 2.1 Results for NVIDIA](https://developer.nvidia.com/blog/full-stack-innovation-fuels-highest-mlperf-inference-2-1-results-for-nvidia/)\n\n[207\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[208\\. Neural network optimization and performance analysis for real-time object detection at the edge](https://sc24.supercomputing.org/proceedings/poster/poster_files/post172s2-file2.pdf)\n\n[209\\. Embedded AI performances of Nvidia’s Jetson Orin SoC series](https://pro.orieux.fr/assets/pdf/Archet%20et%20al_2023_Embedded%20AI%20performances%20of%20Nvidia's%20Jetson%20Orin%20SoC%20series.pdf)\n\n[210\\. N. Houlsby, A. Giurgiu et al. “Parameter-Efficient Transfer Learning for NLP.” ArXiv](https://arxiv.org/abs/1902.00751)\n\n[211\\. Evaluating and Accelerating Vision Transformers on GPU-based Embedded Edge AI Systems](https://assets-eu.researchsquare.com/files/rs-5083258/v1_covered_7dcbed04-40c0-4c57-8c47-02fc11107836.pdf?c=1733374368)\n\n[212\\. Exploring the Boundaries of On-Device Inference: When Tiny Falls Short, Go Hierarchical](https://arxiv.org/pdf/2407.11061)\n\n[213\\. NVIDIA Jetson AGX Orin Series: A Giant Leap Forward for Robotics and Edge AI Applications](https://www.diamondsystems.com/files/binaries/nvidia-jetson-agx-orin-technical-brief.pdf)\n\n[214\\. TerEffic: Highly Efficient Ternary LLM Inference on FPGA](https://arxiv.org/html/2502.16473v2)\n\n[215\\. Real-Time Overhead Power Line Component Detection ...](https://www.mdpi.com/2073-431X/14/4/134)\n\n[216\\. LLM optimizations for sparse matrix processing on Jetson ...](https://antmicro.com/blog/2024/11/llm-optimizations-for-ampere-based-gpus/)\n\n[217\\. Evaluating and accelerating vision transformers on GPU- ...](https://link.springer.com/article/10.1007/s11227-024-06807-1)\n\n[218\\. Ying Sheng, Shiyi Cao et al. “S-LoRA: Serving Thousands of Concurrent LoRA Adapters.” ArXiv](https://doi.org/10.48550/arXiv.2311.03285)\n\n[219\\. Hoang-Loc La, Phuong Hoai Ha. “Kernel-Level Energy-Efficient Neural Architecture Search for Tabular Dataset.”](https://arxiv.org/abs/2504.08359)\n\n[221\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[222\\. LoRA Learns Less and Forgets Less](https://arxiv.org/pdf/2405.09673)\n\n[223\\. VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks](https://proceedings.neurips.cc/paper_files/paper/2024/file/1e0d38c676d5855bcfab7f6d29d20ad9-Paper-Conference.pdf)\n\n[224\\. LoRA-Pro: ARE LOW-RANK ADAPTERS PROPERLY OPTIMIZED?](https://openreview.net/pdf?id=gTwRMU3lJ5)\n\n[225\\. Parameter-Efficient Fine-Tuning](https://fanpu.io/assets/presentations/Parameter%20Efficient%20Fine-Tuning.pdf)\n\n[226\\. Parameter-efficient Fine-tuning for Sparse LLMs](https://icml.cc/media/icml-2024/Slides/34806.pdf)\n\n[227\\. Parameter-Efficient Fine-Tuning with Discrete Fourier...](https://ui.adsabs.harvard.edu/abs/arXiv:2405.03003)\n\n[228\\. RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation](https://raw.githubusercontent.com/mlresearch/v235/main/assets/nikdan24a/nikdan24a.pdf)\n\n[229\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[230\\. LORA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B](https://simonlermen.com/assets/lora.pdf)\n\n[231\\. LORA WITHOUT FORGETTING: FREEZING AND SPARSE MASKING FOR LOW-RANK ADAPTATION](https://openreview.net/pdf?id=aGOQYJfz6H)\n\n[232\\. NOLA: Compressing LoRA Using Linear Combination of Random Basis](https://iclr.cc/media/iclr-2024/Slides/18556.pdf)\n\n[233\\. RASA: Rank-Sharing Low-Rank Adaptation](https://openreview.net/pdf/26347c818217d5c434c3eab6d9f6aea95413ae38.pdf)\n\n[234\\. Parameter-Efficient Fine-Tuning (PEFT)](https://www.ee.cityu.edu.hk/~lmpo/ee4016/pdf/2024A_AI_L08B_PEFT.pdf)\n\n[235\\. Preserving Diversity in Supervised Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2408.16673)\n\n[236\\. LoRA Land：8美元微调一个超越gpt-4的特定任务模型，经过微调的 Mistral-7b 模型，其性能始终优于基础模型70%，含25个特定任务场景](https://www.bilibili.com/video/av1500908205?t=542)\n\n[237\\. N. Houlsby, A. Giurgiu et al. “Parameter-Efficient Transfer Learning for NLP.” ArXiv](https://arxiv.org/abs/1902.00751)\n\n[238\\. \\[2310.20624\\] LoRA Fine-tuning Efficiently Undoes Safety ...](https://arxiv.org/abs/2310.20624)\n\n[239\\. Xiang Lisa Li, Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Generation.” Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)](https://doi.org/10.18653/v1/2021.acl-long.353)\n\n[241\\. QLoRA: Efficient Finetuning of Quantized LLMs](https://zhuanlan.zhihu.com/p/649327877)\n\n[242\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[243\\. Accessible Foundation Models: Systems, Algorithms, and Science](https://digital.lib.washington.edu/server/api/core/bitstreams/63e54e0c-bcaa-42e8-b164-5a8e4db7a715/content)\n\n[244\\. Artificial Intelligence Index Report 2024](https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_Chapter2.pdf)\n\n[245\\. 전체 글](https://hw-hk.tistory.com/?page=4)\n\n[246\\. LLM Tuning & Dataset Perspectives](https://magazine.sebastianraschka.com/p/ahead-of-ai-9-llm-tuning-and-dataset)\n\n[247\\. Understanding LLM Fine Tuning with LoRA (Low-Rank Adaptation)](https://www.run.ai/guides/generative-ai/lora-fine-tuning)\n\n[248\\. Adapter-Lora-QLora](https://zhuanlan.zhihu.com/p/634780681)\n\n[249\\. Large Language Model Fine-tuning with Low-Rank Adaptation: A Performance Exploration](https://lca.ece.utexas.edu/pubs/icpe25_Bagus.pdf)\n\n[250\\. GitHub - bryanchrist/llama2-70b: Codebase for fine-tuning Llama2 70B to ...](https://github.com/bryanchrist/llama2-70b)\n\n[251\\. README.md - yinizhilian/ainlper-papershare](https://github.com/yinizhilian/ainlper-papershare/blob/main/README.md)\n\n[252\\. 论文精读：QLoRA: Efficient Finetuning of Quantized LLMs](https://zhuanlan.zhihu.com/p/673765062)\n\n[253\\. Enhancing semantical text understanding with fine-tuned large language models: A case study on Quora Question Pair duplicate identification](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0317042&type=printable)\n\n[254\\. ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers](https://openreview.net/pdf?id=r9p9CV52MV)\n\n[255\\. Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/pdf/2403.14608)\n\n[256\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[257\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[258\\. qlora/README.md at main · artidoro/qlora · GitHub](https://github.com/artidoro/qlora/blob/main/README.md)\n\n[259\\. Memory-efficient Fine-tuning with with QLoRA | Niklas Heidloff](https://heidloff.net/article/qlora/)\n\n[261\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[262\\. S-LoRA: Serving Thousands of Concurrent LoRA Adapters](https://proceedings.mlsys.org/paper_files/paper/2024/file/906419cd502575b617cc489a1a696a67-Paper-Conference.pdf)\n\n[263\\. Running LLMs with TensorRT-LLM on NVIDIA Jetson Orin ...](https://collabnix.com/running-llms-with-tensorrt-llm-on-nvidia-jetson-orin-nano-super/)\n\n[264\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[265\\. Exploring the Boundaries of On-Device Inference: When Tiny Falls Short, Go Hierarchical](https://arxiv.org/pdf/2407.11061)\n\n[266\\. Brian Lester, Rami Al-Rfou et al. “The Power of Scale for Parameter-Efficient Prompt Tuning.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/2021.emnlp-main.243)\n\n[267\\. N. Houlsby, A. Giurgiu et al. “Parameter-Efficient Transfer Learning for NLP.” ArXiv](https://arxiv.org/abs/1902.00751)\n\n[268\\. Optimizing Vision Transformers for Peak Performance on ...](https://www.embedl.com/optimizing-vision-transformers-for-peak-performance-on-nvidia-jetson-agx-orinvidia-jetson-agx-orin)\n\n[269\\. Xiang Lisa Li, Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Generation.” Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)](https://doi.org/10.18653/v1/2021.acl-long.353)\n\n[270\\. Deploying YOLOv5 on NVIDIA Jetson Orin with cuDLA: Quantization-Aware ...](https://developer.nvidia.com/blog/deploying-yolov5-on-nvidia-jetson-orin-with-cudla-quantization-aware-training-to-inference/)\n\n[271\\. Embedded AI performances of Nvidia’s Jetson Orin SoC series](https://pro.orieux.fr/assets/pdf/Archet%20et%20al_2023_Embedded%20AI%20performances%20of%20Nvidia's%20Jetson%20Orin%20SoC%20series.pdf)\n\n[272\\. 在 NVIDIA Jetson Orin 上部署 YOLOv5 与 cuDLA：量化感知训练到推理](https://developer.nvidia.com/zh-cn/blog/deploying-yolov5-on-nvidia-jetson-orin-with-cudla-quantization-aware-training-to-inference/)\n\n[273\\. LLM optimizations for sparse matrix processing on Jetson ...](https://antmicro.com/blog/2024/11/llm-optimizations-for-ampere-based-gpus/)\n\n[274\\. mLoRA: Fine-Tuning LoRA Adapters via Highly-Efficient Pipeline Parallelism in Multiple GPUs](https://arxiv.org/pdf/2312.02515)\n\n[275\\. Rui Kong, Qiyang Li et al. “LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design.” ArXiv](https://doi.org/10.48550/arXiv.2405.17741)\n\n[276\\. Vlad Fomenko, Han Yu et al. “A Note on LoRA.” ArXiv](https://doi.org/10.48550/arXiv.2404.05086)\n\n[277\\. Neural network optimization and performance analysis for real-time object detection at the edge](https://sc24.supercomputing.org/proceedings/poster/poster_files/post172s2-file2.pdf)\n\n[278\\. NVIDIA Jetson Orin NX Series Ampere GPU + ARM Cortex-A78AE CPU + LPDDR5](https://connecttech.com/ftp/pdf/jetson_orin_nx_datasheet.pdf)\n\n[279\\. NVIDIA Jetson AGX Orin Series](https://openzeka.com/wp-content/uploads/2022/03/DS-10662-001_v1.0.pdf)\n\n[280\\. Full-Stack Innovation Fuels Highest MLPerf Inference 2.1 Results for NVIDIA](https://developer.nvidia.com/blog/full-stack-innovation-fuels-highest-mlperf-inference-2-1-results-for-nvidia/)\n\n[281\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[282\\. VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks](https://proceedings.neurips.cc/paper_files/paper/2024/file/1e0d38c676d5855bcfab7f6d29d20ad9-Paper-Conference.pdf)\n\n[283\\. LoRA Learns Less and Forgets Less](https://arxiv.org/pdf/2405.09673)\n\n[284\\. RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation](https://raw.githubusercontent.com/mlresearch/v235/main/assets/nikdan24a/nikdan24a.pdf)\n\n[285\\. Research Papers in January 2024](https://magazine.sebastianraschka.com/p/research-papers-in-january-2024)\n\n[286\\. LoRA-Pro: ARE LOW-RANK ADAPTERS PROPERLY OPTIMIZED?](https://openreview.net/pdf?id=gTwRMU3lJ5)\n\n[287\\. Parameter-efficient Fine-tuning for Sparse LLMs](https://icml.cc/media/icml-2024/Slides/34806.pdf)\n\n[288\\. RoseLoRA: Row and Column-wise Sparse Low-rank Adaptati...](http://arxiv.org/html/2406.10777v1)\n\n[289\\. PACIFIC SYMPOSIUM ON BIOCOMPUTING 2024](http://psb.stanford.edu/psb-online/proceedings/psb24/psb24_proceedings_allV2.pdf)\n\n[290\\. EXPLORING SPARSE ADAPTERS FOR SCALABLE MERGING OF PARAMETER EFFICIENT EXPERTS](https://openreview.net/pdf?id=8wt2eKkVe6)\n\n[291\\. 模型融合、混合专家、更小的LLM,几篇论文看懂2024年LLM发...](http://finance.sina.com.cn/tech/roll/2024-02-22/doc-inaivrnx9478613.shtml)\n\n[292\\. LORA WITHOUT FORGETTING: FREEZING AND SPARSE MASKING FOR LOW-RANK ADAPTATION](https://openreview.net/pdf?id=aGOQYJfz6H)\n\n[293\\. SprocketLab/sparse_matrix_fine_tuning - GitHub](https://github.com/SprocketLab/sparse_matrix_fine_tuning)\n\n[294\\. EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs](https://openreview.net/pdf?id=qMT9ExoRKE2)\n\n[295\\. A survey on LoRA of large language models](https://link.springer.com/article/10.1007/s11704-024-40663-9)\n\n[301\\. QLoRA: Efficient Finetuning of Quantized LLMs](https://zhuanlan.zhihu.com/p/649327877)\n\n[302\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[303\\. Artificial Intelligence Index Report 2024](https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_Chapter2.pdf)\n\n[304\\. Искусственный Интеллект Индексный отчет 2024 г.](https://media.rbcdn.ru/media/reports/AI-Index-Report-2024_%D0%A0%D1%83%D1%81_NapoleonIT.pdf)\n\n[305\\. Adapter-Lora-QLora](https://zhuanlan.zhihu.com/p/634780681)\n\n[306\\. 开源原驼（Guanaco）及背后的QLoRA技术，将微调65B模型的显存需求从780GB以上降低到48GB以下，效果直逼GPT-4](https://zhuanlan.zhihu.com/p/632236718)\n\n[307\\. Accessible Foundation Models: Systems, Algorithms, and Science](https://digital.lib.washington.edu/server/api/core/bitstreams/63e54e0c-bcaa-42e8-b164-5a8e4db7a715/content)\n\n[308\\. 전체 글](https://hw-hk.tistory.com/?page=4)\n\n[309\\. QERA: AN ANALYTICAL FRAMEWORK FOR QUANTIZATION ERROR RECONSTRUCTION](https://openreview.net/pdf/1fa584648945d11a1aa509b91189ff43549d0638.pdf)\n\n[310\\. QLoRA: Efficient Finetuning of Quantized LLMs - Scilit](https://www.scilit.net/publications/b5e6dc827675a5bae049e6a2953f078c)\n\n[311\\. Enhancing semantical text understanding with fine-tuned large language models: A case study on Quora Question Pair duplicate identification](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0317042&type=printable)\n\n[312\\. GitHub - bryanchrist/llama2-70b: Codebase for fine-tuning Llama2 70B to ...](https://github.com/bryanchrist/llama2-70b)\n\n[313\\. LLM Tuning & Dataset Perspectives](https://magazine.sebastianraschka.com/p/ahead-of-ai-9-llm-tuning-and-dataset)\n\n[314\\. Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/pdf/2403.14608)\n\n[315\\. QLoRA: Efficient Finetuning of Quantized LLMs | OpenReview](https://openreview.net/forum?id=OUIFPHEgJU)\n\n[316\\. ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers](https://openreview.net/pdf?id=r9p9CV52MV)\n\n[317\\. Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models](https://openreview.net/pdf?id=gvT2ksp27C6)\n\n[318\\. Memory-efficient Fine-tuning with with QLoRA | Niklas Heidloff](https://heidloff.net/article/qlora/)\n\n[321\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[322\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[323\\. Exploring the Boundaries of On-Device Inference: When Tiny Falls Short, Go Hierarchical](https://arxiv.org/pdf/2407.11061)\n\n[324\\. Brian Lester, Rami Al-Rfou et al. “The Power of Scale for Parameter-Efficient Prompt Tuning.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/2021.emnlp-main.243)\n\n[325\\. N. Houlsby, A. Giurgiu et al. “Parameter-Efficient Transfer Learning for NLP.” ArXiv](https://arxiv.org/abs/1902.00751)\n\n[326\\. Fork of vLLM to deploy on NVIDIA Jetson AGX Orin with JetPack 6.0](https://github.com/alsichcan/vllm_jetson)\n\n[327\\. Xiang Lisa Li, Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Generation.” Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)](https://doi.org/10.18653/v1/2021.acl-long.353)\n\n[328\\. Ying Sheng, Shiyi Cao et al. “S-LoRA: Serving Thousands of Concurrent LoRA Adapters.” ArXiv](https://doi.org/10.48550/arXiv.2311.03285)\n\n[329\\. Embedded AI performances of Nvidia’s Jetson Orin SoC series](https://pro.orieux.fr/assets/pdf/Archet%20et%20al_2023_Embedded%20AI%20performances%20of%20Nvidia's%20Jetson%20Orin%20SoC%20series.pdf)\n\n[330\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[331\\. Suyi Li, Hanfeng Lu et al. “CaraServe: CPU-Assisted and Rank-Aware LoRA Serving for Generative LLM Inference.” ArXiv](https://doi.org/10.48550/arXiv.2401.11240)\n\n[332\\. Nvidia Jetson AGX Orin 64GB Dev Kit](https://sg.cytron.io/p-nvidia-jetson-agx-orin-64gb-ram-dev-kit?srsltid=AfmBOoqzKr73tVZ5ZTXDplb0uTJebgK8tg8jMwAec3Jn6Oon2E_8x3ir)\n\n[333\\. E. Buehler, Markus J. Buehler. “X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design.” ArXiv](https://doi.org/10.48550/arXiv.2402.07148)\n\n[334\\. Install Pytorch with cuda on Jetson Orin nano Devloper Kit](https://forums.developer.nvidia.com/t/install-pytorch-with-cuda-on-jetson-orin-nano-devloper-kit/297427)\n\n[335\\. LLM optimizations for sparse matrix processing on Jetson ...](https://antmicro.com/blog/2024/11/llm-optimizations-for-ampere-based-gpus/)\n\n[336\\. Xiangxiang Chu, Limeng Qiao et al. “MobileVLM V2: Faster and Stronger Baseline for Vision Language Model.” ArXiv](https://doi.org/10.48550/arXiv.2402.03766)\n\n[341\\. LoRA Learns Less and Forgets Less](https://arxiv.org/pdf/2405.09673)\n\n[342\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[343\\. LoRA+: Efficient Low Rank Adaptation of Large Models](https://raw.githubusercontent.com/mlresearch/v235/main/assets/hayou24a/hayou24a.pdf)\n\n[344\\. VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks](https://proceedings.neurips.cc/paper_files/paper/2024/file/1e0d38c676d5855bcfab7f6d29d20ad9-Paper-Conference.pdf)\n\n[345\\. LoRA-Pro: ARE LOW-RANK ADAPTERS PROPERLY OPTIMIZED?](https://openreview.net/pdf?id=gTwRMU3lJ5)\n\n[346\\. LORA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B](https://simonlermen.com/assets/lora.pdf)\n\n[347\\. PACIFIC SYMPOSIUM ON BIOCOMPUTING 2024](http://psb.stanford.edu/psb-online/proceedings/psb24/psb24_proceedings_allV2.pdf)\n\n[348\\. A Study to Evaluate the Impact of LoRA Fine-tuning ...](https://arxiv.org/html/2503.07927v1)\n\n[349\\. RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation](https://raw.githubusercontent.com/mlresearch/v235/main/assets/nikdan24a/nikdan24a.pdf)\n\n[350\\. Research Papers in January 2024](https://magazine.sebastianraschka.com/p/research-papers-in-january-2024)\n\n[351\\. LoRA Land：8美元微调一个超越gpt-4的特定任务模型，经过微调的 Mistral-7b 模型，其性能始终优于基础模型70%，含25个特定任务场景](https://www.bilibili.com/video/av1500908205?t=542)\n\n[352\\. Reducing Fine-Tuning Memory Overhead by Approximate and Memory-Sharing Backpropagation](https://openreview.net/pdf?id=IpSKpOY2EH)\n\n[353\\. EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs](https://openreview.net/pdf?id=qMT9ExoRKE2)\n\n[354\\. Parameter-efficient Fine-tuning for Sparse LLMs](https://icml.cc/media/icml-2024/Slides/34806.pdf)\n\n[355\\. Research Papers in February 2024](https://sebastianraschka.com/blog/2024/research-papers-in-february-2024.html)\n\n[356\\. Improving Efficiency of Deep Learning Models](https://escholarship.org/content/qt8s76h449/qt8s76h449.pdf?t=srxl1u)\n\n[357\\. 模型融合、混合专家、更小的LLM,几篇论文看懂2024年LLM发...](http://finance.sina.com.cn/tech/roll/2024-02-22/doc-inaivrnx9478613.shtml)\n\n[361\\. QLoRA: Efficient Finetuning of Quantized LLMs](https://zhuanlan.zhihu.com/p/649327877)\n\n[362\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[363\\. Artificial Intelligence Index Report 2024](https://aiindex.stanford.edu/wp-content/uploads/2024/05/HAI_AI-Index-Report-2024.pdf)\n\n[364\\. 전체 글](https://hw-hk.tistory.com/?page=4)\n\n[365\\. LLM Tuning & Dataset Perspectives](https://magazine.sebastianraschka.com/p/ahead-of-ai-9-llm-tuning-and-dataset)\n\n[366\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[367\\. GitHub - artidoro/qlora: QLoRA: Efficient Finetuning o...](https://github.com/artidoro/qlora/)\n\n[368\\. Adapter-Lora-QLora](https://zhuanlan.zhihu.com/p/634780681)\n\n[369\\. 论文精读：QLoRA: Efficient Finetuning of Quantized LLMs](https://zhuanlan.zhihu.com/p/673765062)\n\n[370\\. Accessible Foundation Models: Systems, Algorithms, and Science](https://digital.lib.washington.edu/server/api/core/bitstreams/63e54e0c-bcaa-42e8-b164-5a8e4db7a715/content)\n\n[371\\. GitHub - bryanchrist/llama2-70b: Codebase for fine-tuning Llama2 70B to ...](https://github.com/bryanchrist/llama2-70b)\n\n[372\\. Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/pdf/2403.14608)\n\n[373\\. Memory-efficient Fine-tuning with with QLoRA | Niklas Heidloff](https://heidloff.net/article/qlora/)\n\n[374\\. Enhancing semantical text understanding with fine-tuned large language models: A case study on Quora Question Pair duplicate identification](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0317042&type=printable)\n\n[375\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[376\\. ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers](https://openreview.net/pdf?id=r9p9CV52MV)\n\n[377\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[378\\. README.md - yinizhilian/ainlper-papershare](https://github.com/yinizhilian/ainlper-papershare/blob/main/README.md)\n\n[379\\. FINE-TUNING AN OPEN SOURCE CHATBOT TO TRANSLATE CODE FROM PYTHON TO JAVA USING QLORA: TRANSLATING FOR MORE ENERGY EFFICIENT CODE](https://lutpub.lut.fi/bitstream/10024/168604/1/diplomityo_hakkarainen_joonas.pdf)\n\n[380\\. Optimizing LLM-Powered Agents for Tabular Data Analytics: Integrating LoRA for Enhanced Quality](https://dspace.cvut.cz/bitstream/handle/10467/115388/F3-DP-2024-Poludin-Mikhail-Optimizing_LLM-Powered_Agents_for_Tabular_Data_Analytics_Integrating_LoRA_for_Enhanced_Quality.pdf)\n\n[381\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[382\\. Thomas Wolf, Lysandre Debut et al. “HuggingFace's Transformers: State-of-the-art Natural Language Processing.” ArXiv](https://arxiv.org/abs/1910.03771)\n\n[383\\. S-LoRA: Serving Thousands of Concurrent LoRA Adapters](https://proceedings.mlsys.org/paper_files/paper/2024/file/906419cd502575b617cc489a1a696a67-Paper-Conference.pdf)\n\n[384\\. Brian Lester, Rami Al-Rfou et al. “The Power of Scale for Parameter-Efficient Prompt Tuning.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/2021.emnlp-main.243)\n\n[385\\. J. E. Hu, Yelong Shen et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv](https://arxiv.org/abs/2106.09685)\n\n[386\\. Exploring the Boundaries of On-Device Inference: When Tiny Falls Short, Go Hierarchical](https://arxiv.org/pdf/2407.11061)\n\n[387\\. N. Houlsby, A. Giurgiu et al. “Parameter-Efficient Transfer Learning for NLP.” ArXiv](https://arxiv.org/abs/1902.00751)\n\n[388\\. Xiang Lisa Li, Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Generation.” Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)](https://doi.org/10.18653/v1/2021.acl-long.353)\n\n[389\\. HyC-LoRA: Memory Efficient LoRA Fine-tuning with Hybrid Activation Compression](https://openreview.net/pdf/770408b6336f7acf5e1faf028972bb955db0234b.pdf)\n\n[390\\. Nvidia Jetson AGX Orin 64GB Dev Kit](https://sg.cytron.io/p-nvidia-jetson-agx-orin-64gb-ram-dev-kit?srsltid=AfmBOoqzKr73tVZ5ZTXDplb0uTJebgK8tg8jMwAec3Jn6Oon2E_8x3ir)\n\n[391\\. Embedded AI performances of Nvidia’s Jetson Orin SoC series](https://pro.orieux.fr/assets/pdf/Archet%20et%20al_2023_Embedded%20AI%20performances%20of%20Nvidia's%20Jetson%20Orin%20SoC%20series.pdf)\n\n[392\\. Fork of vLLM to deploy on NVIDIA Jetson AGX Orin with JetPack 6.0](https://github.com/alsichcan/vllm_jetson)\n\n[393\\. Suyi Li, Hanfeng Lu et al. “CaraServe: CPU-Assisted and Rank-Aware LoRA Serving for Generative LLM Inference.” ArXiv](https://doi.org/10.48550/arXiv.2401.11240)\n\n[394\\. Rui Kong, Qiyang Li et al. “LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design.” ArXiv](https://doi.org/10.48550/arXiv.2405.17741)\n\n[395\\. NVIDIA Jetson AGX Orin Series](https://openzeka.com/wp-content/uploads/2022/03/DS-10662-001_v1.0.pdf)\n\n[396\\. 2024 PRODUCT GUIDE](https://connecttech.com/pdf/CTI-Product_Guide_2024.pdf)\n\n[397\\. NVIDIA Jetson Orin Nano Series](http://www.plink-ai.com/Uploads/download/632bb67aa4f66.pdf)\n\n[398\\. Integrating Qt and LLMs on the NVIDIA Jetson board for controlling a patient-assisting robot arm](https://oulurepo.oulu.fi/bitstream/10024/50979/1/nbnfioulu-202406264950.pdf)\n\n[399\\. TensorRT Implementations of Model Quantization on Edge SoC](https://par.nsf.gov/servlets/purl/10488646)\n\n[400\\. Yifan Sui, Hao Wang et al. “ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs.”](https://arxiv.org/abs/2505.14468)\n\n[401\\. VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks](https://proceedings.neurips.cc/paper_files/paper/2024/file/1e0d38c676d5855bcfab7f6d29d20ad9-Paper-Conference.pdf)\n\n[402\\. LoRA Learns Less and Forgets Less](https://arxiv.org/pdf/2405.09673)\n\n[403\\. RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation](https://raw.githubusercontent.com/mlresearch/v235/main/assets/nikdan24a/nikdan24a.pdf)\n\n[404\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[405\\. Parameter-Efficient Fine-Tuning](https://fanpu.io/assets/presentations/Parameter%20Efficient%20Fine-Tuning.pdf)\n\n[406\\. LoRA-Pro: ARE LOW-RANK ADAPTERS PROPERLY OPTIMIZED?](https://openreview.net/pdf?id=gTwRMU3lJ5)\n\n[407\\. Research Papers in January 2024](https://magazine.sebastianraschka.com/p/research-papers-in-january-2024)\n\n[408\\. LORA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B](https://simonlermen.com/assets/lora.pdf)\n\n[409\\. Parameter-efficient Fine-tuning for Sparse LLMs](https://icml.cc/media/icml-2024/Slides/34806.pdf)\n\n[410\\. A Study to Evaluate the Impact of LoRA Fine-tuning ...](https://arxiv.org/html/2503.07927v1)\n\n[411\\. 模型融合、混合专家、更小的LLM,几篇论文看懂2024年LLM发...](http://finance.sina.com.cn/tech/roll/2024-02-22/doc-inaivrnx9478613.shtml)\n\n[412\\. SprocketLab/sparse_matrix_fine_tuning - GitHub](https://github.com/SprocketLab/sparse_matrix_fine_tuning)\n\n[413\\. LoRA-GGPO: Mitigating Double Descent in LoRA Fine-Tuning via Gradient-Guided Perturbation Optimization](https://arxiv.org/pdf/2502.14538)\n\n[414\\. PACIFIC SYMPOSIUM ON BIOCOMPUTING 2024](http://psb.stanford.edu/psb-online/proceedings/psb24/psb24_proceedings_allV2.pdf)\n\n[415\\. A survey on LoRA of large language models](https://link.springer.com/article/10.1007/s11704-024-40663-9)\n\n[416\\. RoseLoRA: Row and Column-wise Sparse Low-rank Adaptati...](http://arxiv.org/html/2406.10777v1)\n\n[417\\. EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs](https://openreview.net/pdf?id=qMT9ExoRKE2)\n\n[418\\. Improving Efficiency of Deep Learning Models](https://escholarship.org/content/qt8s76h449/qt8s76h449.pdf?t=srxl1u)"
  },
  {
    "id": 59,
    "question": "What is Artificial General Intelligence (AGI), how far are we from achieving it, and what societal transformations might it trigger upon its arrival?",
    "response": "**Comprehensive Research Report: Artificial General Intelligence (AGI)**\n\n**1\\. Introduction and Definition of AGI**\n\nArtificial General Intelligence (AGI) refers to a hypothetical system capable of performing _any_ intellectual task a human can, with human-like adaptability, reasoning, and learning across diverse domains. Leading research organizations define AGI differently:\n\n**OpenAI** characterizes AGI as a \"highly autonomous system that outperforms humans at most economically valuable work\" and has introduced a 5-level progression framework toward AGI maturity \\[120\\]\\[130\\].\n\n**Stanford University** emphasizes general learning and skill acquisition, defining AGI as a system \"capable of efficiently acquiring new skills and solving novel problems for which it was neither designed nor trained\" \\[128\\].\n\n**Gartner** describes AGI as matching or surpassing human cognitive abilities, potentially exhibiting consciousness or self-awareness \\[126\\].\n\nDespite these variations, consensus highlights **cross-domain generalization**, **autonomous reasoning**, and **human-level adaptability** as core traits \\[2\\]\\[128\\]. No universally accepted definition exists as of 2025, partly due to AGI's theoretical nature and unresolved technical challenges \\[136\\]\\[138\\].\n\n**2\\. Technical Bottlenecks and Progress Toward AGI**\n\nAs of 2025, AGI remains unrealized due to significant hardware and algorithmic constraints:\n\n**Hardware Limitations**\n\n**Interconnect Wall:** GPU scaling efficiency is capped by data-movement bandwidth, limiting computational throughput to ~7×10²⁸ FLOPs \\[1\\].\n\n**Resource Intensity:** Training AGI models requires impractical computational resources, hindering accessibility for researchers \\[15\\].\n\n**Algorithmic Challenges**\n\n**Data Inefficiency:** AI models need 10¹²–10¹³ tokens to match human language learning (achievable with ~10⁷ words), indicating poor sample efficiency \\[1\\]. Human-generated data (~300 trillion tokens) will be exhausted by 2026–2032 \\[1\\].\n\n**Reasoning Deficits:** Current AI lacks true understanding, robustness in memory, and human-like creativity. Systems cannot explain their conclusions or generalize consistently across tasks \\[2\\]\\[8\\]\\[12\\].\n\n**\"Missing Ingredient\" Problem:** Statistical learning alone may be insufficient for AGI, requiring breakthroughs in cognitive architectures \\[8\\].\n\n**Benchmarks for AGI Progress**\n\nMetrics like **GAIA** and **PlanBench** evaluate cross-domain generalization:\n\n**GAIA** tests multi-step reasoning (e.g., web browsing, tool usage) with 466 real-world questions. Despite advancements, no AI system matches human performance \\[84\\]\\[87\\]\\[98\\].\n\n**PlanBench** assesses planning in virtual environments, revealing severe limitations in models like GPT-4 \\[94\\].\n\nCritically, these benchmarks struggle to measure _true_ generalization. Domain-specific tools (e.g., **DomainBed**) aim to address this but lack real-world complexity \\[83\\]\\[85\\].\n\n**3\\. Predicted Timelines for AGI Achievement**\n\nExpert forecasts vary widely, reflecting uncertainty about overcoming bottlenecks:\n\n**Optimistic Views (2025–2030):** OpenAI's Sam Altman (2025), Anthropic's Dario Amodei (2026), and NVIDIA's Jensen Huang (2028) predict near-term breakthroughs \\[60\\]\\[66\\]\\[72\\]. Ray Kurzweil maintains a 2029 estimate \\[70\\].\n\n**Cautious Estimates (2040s–2060s):** A 2022 survey of 352 AI experts cites a median prediction of 2060, with 90% expecting AGI within 100 years \\[60\\]. Turing Award winners Yoshua Bengio and Geoffrey Hinton suggest 2028–2043 and 2028–2053, respectively \\[72\\].\n\n**Key Uncertainties:** Timelines depend on resolving data inefficiency, hardware scaling, and algorithmic \"missing ingredients\" \\[1\\]\\[8\\]. Metaculus forecasters assign a 25% probability to AGI by 2027 \\[64\\].\n\n**4\\. Societal Transformations Triggered by AGI**\n\nAGI's arrival will reshape economies, healthcare, and governance:\n\n**Economic Restructuring**\n\n**Workforce Displacement:** Governments pilot **Universal Basic Income (UBI)** (e.g., Finland, Canada, Kenya) and propose **automation taxes** to fund retraining \\[47\\]\\[50\\].\n\n**Labor Policies:** Emphasis on lifelong learning, reskilling for \"AGI-proof\" roles (e.g., creative industries, emotional intelligence-based services), and wealth redistribution via UBI or cooperative ownership models \\[42\\]\\[46\\]\\[46\\].\n\n**Healthcare Revolution**\n\n**Early Disease Detection:** AGI will analyze multi-modal data (genetic, imaging, lifestyle) to identify diseases like cancer or Alzheimer's earlier than humans. Examples include Tencent Medical Vision (cancer) and Freenome AI (blood-based colorectal cancer screening) \\[182\\]\\[190\\]\\[199\\].\n\n**Personalized Treatment:** Drug discovery acceleration (e.g., optimizing molecular structures) and customized care plans based on patient-specific data \\[184\\]\\[194\\].\n\n**Governance and Ethics**\n\n**Policy Shifts:** AGI necessitates redefining accountability, transparency, and human rights in automated decision-making \\[144\\]\\[149\\].\n\n**Global Inequality:** Risks include widening gaps between AGI-adopting and non-adopting nations, requiring international ethical frameworks \\[146\\]\\[155\\].\n\n**5\\. Regulatory and Safety Frameworks**\n\nCountries are implementing AGI-specific regulations:\n\n**EU:** The **AI Act** classifies AGI as \"high-risk,\" mandating transparency and human oversight. Foundation models face strict obligations \\[260\\]\\[272\\].\n\n**US:** Emerging federal guidelines emphasize pre-deployment testing for safety \\[204\\]\\[213\\].\n\n**China:** \"Soft regulation\" via ethics codes promotes fairness and security \\[205\\].\n\n**Global Gaps:** No unified framework exists, risking fragmented standards \\[210\\]. Benchmarks like GAIA are evolving to evaluate real-world safety risks \\[232\\].\n\n**6\\. Conclusion**\n\nAGI represents a paradigm shift in AI, demanding solutions to persistent technical barriers—data inefficiency, hardware limits, and reasoning deficits. While expert timelines range from 2025 to beyond 2060, societal preparations (e.g., UBI pilots, reskilling initiatives) are underway. AGI's arrival will trigger transformations in healthcare (personalized, preventive medicine), economics (UBI-driven stability), and governance (ethical automation). Regulatory harmonization and robust benchmarks like GAIA are critical to ensuring safe, equitable deployment. As of 2025, AGI remains aspirational but inevitable, urging collaborative global action to harness its benefits and mitigate disruptions.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. The 2025 European Deep Tech Report](https://dealroom.co/uploaded/2024/11/2025-European-Deep-Tech-Report.pdf)\n\n[2\\. Comprehensive Review of Artificial General Intelligence AGI and Agentic GenAI: Applications in Business and Finance](https://www.preprints.org/frontend/manuscript/b95ddd7d9a9b21409573603edb9e1aa5/download_pub)\n\n[3\\. Augmented Innovation: The mix of machine and human intelligence that will take us into a Better Connected World](https://www-file.huawei.com/~/media/CORPORATE/PDF/publications/winwin/augmented-innovation/Winwin_Cp_04.pdf)\n\n[4\\. Review of Artificial General Intelligence (AGI): Implications for the U.S. Workforce and Economic Stability](https://www.preprints.org/frontend/manuscript/9dbc29039609e752606e519babadb6b5/download_pub)\n\n[5\\. ARC-AGI-2: A New Challenge for Frontier AI Reasoning Systems](https://arxiv.org/pdf/2505.11831)\n\n[6\\. Trends – Artificial Intelligence (AI)](https://www.mrbaogao.com/storage/attachments/2025/06/03/TJX0rgflGCJtvJM5o6ziHtrMrE6Yg9BpRiZSu2I0.pdf)\n\n[7\\. Advances in Artificial Intelligence: Current Trends and Future Directions](http://sprcopen.org/index.php/FAIR/article/download/36/32)\n\n[8\\. Security Now! #1001 - 11-19-24 Artificial General Intelligence (AGI)](https://www.grc.com/sn/sn-1001-notes.pdf)\n\n[9\\. Minjun Zhu, Qiujie Xie et al. “AI Scientists Fail Without Strong Implementation Capability.”](https://arxiv.org/abs/2506.01372)\n\n[10\\. Artificial Intelligence – Global 2025 Outlook: Broadening AI capabilities will unlock new use cases](https://cuadernoborrador.com/wp-content/uploads/2025/01/outlook-artificial-intelligence-global-13jan2025-pbc_1425377-1.pdf)\n\n[11\\. Futures of Global AI Governance: Co-Creating an Approach for Transforming Economies and Societies](https://www.oecd.org/content/dam/oecd/en/about/programmes/strategic-foresight/GSG%20Background%20Note_GSG%282024%291en.pdf/_jcr_content/renditions/original./GSG%20Background%20Note_GSG%282024%291en.pdf)\n\n[12\\. Artificial General Intelligence in 2025: Good Luck With That](https://www.informationweek.com/machine-learning-ai/artificial-general-intelligence-in-2025-good-luck-with-that)\n\n[13\\. 2025 Futures Report](https://kpmg.com/kpmg-us/content/dam/kpmg/pdf/2025/kpmg-2025-futures-report.pdf)\n\n[14\\. Most Advanced AI Detectors in 2025 and How They Identify ...](https://vertu.com/ai-tools/advanced-ai-detector-2025-technology-identify-ai-content/?srsltid=AfmBOoqqkrlhj_ivYGHQwnFQGJ-LMoHIwgWxXQQthCpQZOlUr25zL_Oq)\n\n[15\\. Artificial General Intelligence - AI Tools Explorer](https://aitoolsexplorer.com/ai-glossary/what-is-agi/)\n\n[16\\. Obstacles for AI adoption, 2025 - Statista](https://www.statista.com/statistics/1557024/barriers-ai-adoption/#:~:text=In%202025,%20the%20biggest%20barrier,of%20AI%20products%20and%20services.)\n\n[21\\. Jia Deng, Wei Dong et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2009.5206848)\n\n[22\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[23\\. Frame for letter of intent and reference metrics to support the assessment of GPAI Membership](https://one.oecd.org/document/GPAI%282021%293/FINAL/en/pdf)\n\n[24\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[25\\. A. Turing. “Computing Machinery and Intelligence.” Mind](https://doi.org/10.1093/MIND/LIX.236.433)\n\n[26\\. Alex Wang, Amanpreet Singh et al. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” BlackboxNLP@EMNLP](https://doi.org/10.18653/v1/W18-5446)\n\n[27\\. 人工智能 大规模预训练模型总体技术要求及评估方法](https://www.isc.org.cn/profile/2024/10/25/df199cea-ca14-4338-8c3d-97b36f9d8725.pdf)\n\n[28\\. Artificial Intelligence Index Report 2024](https://www.hkdca.com/wp-content/uploads/2024/06/ai-index-report-2024-hai.pdf)\n\n[29\\. Kathrin Blagec, G. Dorffner et al. “A critical analysis of metrics used for measuring progress in artificial intelligence.” ArXiv](https://arxiv.org/abs/2008.02577)\n\n[30\\. AIGC发展研究](https://rprc.scau.edu.cn/_upload/article/files/a4/e4/6323888a404cae48b5016e34cd39/fec6f1b0-4c35-46b2-92e2-a1df66175021.pdf)\n\n[31\\. IDC MarketScape: Worldwide General-Purpose Conversational AI Platforms 2021 Vendor Assessment](https://www.verint.com/wp-content/uploads/IDC-2021-MarketScape-Conversational-AI_Verint-reprint-expires-Oct-20-2022.pdf)\n\n[32\\. 斯坦福大学发布人工智能的发展现状及进展的指数分析-电子发烧友网](https://m.elecfans.com/article/606716.html)\n\n[33\\. 2025年人工智能指数报告](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025_chinese_version_061325.pdf)\n\n[34\\. International Scientific Report on the Safety of Advanced AI: Interim Report](https://www.developmentaid.org/api/frontend/cms/file/2024/05/international_scientific_report_on_the_safety_of_advanced_ai_interim_report.pdf)\n\n[35\\. 南通市通用人工智能产业创新发展指标](https://file.smejs.cn/file/group1/M00/00/5C/rBIAAWdpAOGAXWOMAAAyd5v_seQ91.docx)\n\n[36\\. 中美人工智能技术创新的动态比较——基于人工智能技术创新大数据的多S曲线模型分析](https://journal.bjut.edu.cn/bjgydxxbskb/cn/article/pdf/preview/10.12120/bjutskxb202303054.pdf)\n\n[37\\. 用什么指标评估人工智能发展趋势的成功](https://docs.ihr360.com/strategy/it_strategy/243160)\n\n[38\\. GAIA：通用人工智能助手的基准](https://docs.feishu.cn/v/wiki/Xu4GwELhqiXOjrkj9VfcOlwEngc/ai)\n\n[41\\. C. Frey, Michael A. Osborne. “The future of employment: How susceptible are jobs to computerisation?.” Technological Forecasting and Social Change](https://doi.org/10.1016/J.TECHFORE.2016.08.019)\n\n[42\\. Review of Artificial General Intelligence (AGI): Implications for the U.S. Workforce and Economic Stability](https://www.preprints.org/frontend/manuscript/9dbc29039609e752606e519babadb6b5/download_pub)\n\n[43\\. 后 Agi 经济：10 章展示 Agi 如何重塑经济](https://julienflorkin.com/zh-CN/%E6%8A%80%E6%9C%AF/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/post-agi-economy/)\n\n[44\\. Advances and Challenges of Artificial General Intelligence (AGI)](https://schneppat.com/agi-advances-challenges.html)\n\n[45\\. Artificial General Intelligence (AGI)](http://schneppat.com/artificial-general-intelligence-agi.html)\n\n[46\\. Artificial General Intelligence and the End of Human Employment: The Need to Renegotiate the Social Contract](https://arxiv.org/pdf/2502.07050)\n\n[47\\. Artificial intelligence, recessiory pressures and population health](https://iris.who.int/bitstream/handle/10665/380433/PMC11774225.pdf?sequence=1&isAllowed=y)\n\n[48\\. AI-Driven Productivity Scenarios](https://www.cigionline.org/documents/3413/AI-scenarios_final.pdf)\n\n[49\\. Scenarios for the Transition to AGI\\*](https://cdn.governance.ai/AGI_Scenarios.pdf)\n\n[50\\. 通用人工智能时代的来临：AI失业潮的预警 - 易源易彩](https://www.yicaiai.com/news/article/685a0c2e4ddd79013c00d0f3)\n\n[51\\. The Disruption of the Economy: What's behind the confusion at OpenAI](https://economiaexponencial.com.br/en/the-disruption-of-the-economy-what-is-behind-the-confusion-in-openai/)\n\n[52\\. 通用人工智能 (Agi) 的 10 个惊人预测：未来十年将会发生什么](https://julienflorkin.com/zh-CN/technology/artificial-intelligence/artificial-general-intelligence-agi-2/)\n\n[60\\. Security Now! #1001 - 11-19-24 Artificial General Intelligence (AGI)](https://www.grc.com/sn/sn-1001-notes.pdf)\n\n[61\\. What Is AGI? Artificial General Intelligence & Its Future - Fonzi AI](https://fonzi.ai/blog/what-is-agi#:~:text=The%20development%20of%20AGI%20presents,values%20to%20prevent%20existential%20risks.)\n\n[62\\. The Future of AI in Education: 13 things we can do to minimize the damage](https://osf.io/372vr_v1/download)\n\n[63\\. Experts Predict Artificial General Intelligence July 2025 to ...](https://www.nextbigfuture.com/2023/11/experts-predict-artificial-general-intelligence-july-2025-to-2031.html)\n\n[64\\. The Path to AGI: Technical Milestones, Philosophical Debates, and Societal Implications](https://cdn.prod.website-files.com/67343452a4bb784798d28a97/67a2dc57414396e94ea504d8_The%20Path%20to%20AGI.pdf)\n\n[65\\. Preparing for the Future of Artificial Intelligence](https://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/preparing_for_the_future_of_ai.pdf)\n\n[66\\. Progress Towards AGI and ASI: 2024–Present - CloudWalk](https://www.cloudwalk.io/ai/progress-towards-agi-and-asi-2024-present#:~:text=Sam%20Altman%20%28OpenAI%20CEO%29:,task%20within%20a%20few%20years.)\n\n[67\\. Timeline of AI Timelines](https://timelines.issarice.com/wiki/Timeline_of_AI_timelines)\n\n[68\\. Timeline to Artificial General Intelligence 2025 – 2030+](https://s-rsa.com/index.php/agi/article/view/15119)\n\n[69\\. Security Now! Transcript of Episode #1001](https://www.grc.com/sn/sn-1001.pdf)\n\n[70\\. 专家对通用人工智能（AGI）未来的看法](https://www.nxrte.com/zixun/16229.html)\n\n[71\\. Unveiling the Potent Future of AI: How Far Away is AGI?](https://articlefiesta.com/blog/en/how-far-away-is-agi/)\n\n[72\\. Timelines to AGI](https://pauseai.info/timelines)\n\n[73\\. 通用人工智能时代信息资源管理学科的发展方向](https://jirm.whu.edu.cn/jwk3/xxzyglxb/CN/article/downloadArticleFile.do?attachType=PDF&id=6300)\n\n[74\\. Shrinking AGI timelines: a review of expert forecasts](https://80000hours.org/2025/03/when-do-experts-expect-agi-to-arrive/)\n\n[75\\. The 2025 European Deep Tech Report](https://dealroom.co/uploaded/2024/11/2025-European-Deep-Tech-Report.pdf)\n\n[76\\. Artificial General Intelligence in 2025: Good Luck With That](https://www.informationweek.com/machine-learning-ai/artificial-general-intelligence-in-2025-good-luck-with-that)\n\n[77\\. Undergraduate Attitudes Toward Artificial General Intelligence: A Comprehensive Survey Analysis](https://futureofbeinghuman.com/api/v1/file/dd9f1153-c320-4868-a376-793cd0bceca7.pdf)\n\n[80\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[81\\. Martín Arjovsky, L. Bottou et al. “Invariant Risk Minimization.” ArXiv](https://arxiv.org/abs/1907.02893)\n\n[82\\. Yaroslav Ganin, E. Ustinova et al. “Domain-Adversarial Training of Neural Networks.” Journal of machine learning research](https://doi.org/10.1007/978-3-319-58347-1_10)\n\n[83\\. Ishaan Gulrajani, David Lopez-Paz. “In Search of Lost Domain Generalization.” ArXiv](https://arxiv.org/abs/2007.01434)\n\n\\[84. GAIA Benchmark: evaluating intelligent agents - WorkOS \\](https://workos.com/blog/gaia-benchmark-evaluating-intelligent-agents#:~:text=The%20GAIA%20benchmark%20(%E2%80%9CGeneralized%20AI,collaborate,%20and%20effectively%20generalize%20knowledge.)\n\n[85\\. Da Li, Yongxin Yang et al. “Deeper, Broader and Artier Domain Generalization.” 2017 IEEE International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV.2017.591)\n\n[86\\. All India Open Mock Test General Studies Paper-I](https://www.nextias.com/newuploads/Nextias/2025/1/gs-anubahv-i-set-a-answer-key-with-explanations-1736245367634.pdf)\n\n[87\\. AI researchers introduce GAIA: A benchmark testing tool for general AI assistants](https://techxplore.com/news/2023-12-ai-gaia-benchmark-tool-general.pdf)\n\n[88\\. GAIA: A Benchmark for General AI Assistants](https://openreview.net/pdf/e828bf3e5aaa9c75b6b9b9ef064fafc685bc6f6c.pdf)\n\n[89\\. GAIA: a benchmark for General AI Assistants](https://www.summarizepaper.com/en/arxiv-id/2311.12983v1/)\n\n[90\\. Request for Information to the Update of the National Artificial Intelligence Research and Development Strategic Plan: Responses](https://www.nitrd.gov/rfi/ai/2022/87-FR-5876/NAIRDSP-RFI-2022-combined.pdf)\n\n[91\\. Comprehensive Review of Artificial General Intelligence AGI and Agentic GenAI: Applications in Business and Finance](https://www.preprints.org/frontend/manuscript/b95ddd7d9a9b21409573603edb9e1aa5/download_pub)\n\n[92\\. AGI vs. Narrow AI - Just Think AI](https://www.justthink.ai/artificial-general-intelligence/understanding-agi-vs-narrow-ai-explaining-the-differences-and-implications)\n\n[93\\. GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling](https://arxiv.org/pdf/2505.00063v2)\n\n[94\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai-index-report-2025_chapter2_final.pdf)\n\n[95\\. What Benchmarks Say About Agentic AI's Coding Potential](https://www.aiwire.net/2025/03/28/what-benchmarks-say-about-agentic-ais-coding-potential/)\n\n[96\\. GTA: A Benchmark for General Tool Agents](https://papers.nips.cc/paper_files/paper/2024/file/8a75ee6d4b2eb0b777f549a32a5a5c28-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[97\\. Leading AI models fail new test of artificial general ...](https://www.newscientist.com/article/2473622-leading-ai-models-fail-new-test-of-artificial-general-intelligence/)\n\n[98\\. Magnetic-One: A Generalist Multi-Agent System for Solving Complex Tasks](https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Magentic-One.pdf)\n\n[100\\. The Rise of Automation: How AI is Reshaping the Workforce](https://www.recruitninjas.com/blog/the-rise-of-automation-how-ai-is-reshaping-the-workforce/)\n\n[101\\. Report on piloting a Universal Basic Income (2022)](https://assets.gov.ie/243295/31066da1-eb0d-4f56-8d47-361f3e4d0d83.pdf)\n\n[102\\. Is Universal Basic Income the Answer to Automation?](https://www.diplomaticourier.com/posts/universal-basic-income-answer-automation)\n\n[103\\. The Netherlands and Finland to Try Out Universal Basic Income in 2017](https://www.popsugarmoney.com/What-Universal-Basic-Income-42822296)\n\n[104\\. Universal basic income (UBI) Message Board](http://www.siliconinvestor.com/readmsg.aspx?msgid=31235328)\n\n[105\\. Technology and Innovation Report 2018: Harnessing Frontier Technologies for Sustainable Development](https://unctad.org/system/files/official-document/tir2018_en.pdf)\n\n[106\\. Primer on Universal Basic Income](https://economicprinciples.org/downloads/Primer-on-Universal-Basic-Income.pdf)\n\n[107\\. Countries that Have Tried Universal Basic Income](https://basicincome.org/news/2022/07/countries-that-have-tried-universal-basic-income/)\n\n[108\\. Automation and Work: The Impact of Generative AI on Employment](https://radar.gesda.global/sub-topics/automation-and-work/)\n\n[109\\. A UNIVERSAL BASIC INCOME FOR IRELAND: LESSONS FROM THE INTERNATIONAL LITERATURE](https://assets.gov.ie/243437/ded2a708-f914-4f54-a1fe-17f125ae1825.pdf)\n\n[110\\. Social Impact Report 2022](https://www.fgveurope.de/wp-content/uploads/2023/12/relatorio_de_impacto_social_2022_ing_2.pdf)\n\n[111\\. Automation: A Framework for a Sustainable Transition](https://www.bsr.org/reports/BSR_Automation_Sustainable_Jobs_Business_Transition.pdf)\n\n[112\\. Mohammad Rasoolinejad. “Universal Basic Income: The Last Bullet in the Darkness.” arXiv: General Economics](https://arxiv.org/abs/1910.05658)\n\n[113\\. UNIVERSAL BASIC INCOME: A UNION PERSPECTIVE](https://www.world-psi.org/sites/default/files/documents/research/en_ubi_full_report_2019.pdf)\n\n[114\\. Universal Basic Income: Our Solution to Automation?](https://economicsandpolicy.ca/2018/09/04/universal-basic-income-our-solution-to-automation/)\n\n[115\\. C. Frey, Michael A. Osborne. “The future of employment: How susceptible are jobs to computerisation?.” Technological Forecasting and Social Change](https://doi.org/10.1016/J.TECHFORE.2016.08.019)\n\n[116\\. U. Gentilini, M. Grosh et al. “Exploring Universal Basic Income: A Guide to Navigating Concepts, Evidence, and Practices.” World Bank Publications](https://doi.org/10.1596/978-1-4648-1458-7)\n\n[117\\. Redefining Universal Basic Income in Post-Covid Time](http://businesseconomics.in/redefining-universal-basic-income-post-covid-time)\n\n[118\\. ETHOS™ UBI: Universal Basic Income System White Paper](https://xp2p.io/wp-content/uploads/2024/12/WhitePaper_XP2P_EARTHX_R3.pdf)\n\n[120\\. HiPEAC Vision 2025: High Performance, Edge and Cloud Computing](https://vision.hipeac.net/pdf/hipeac-vision-2025--articles.pdf)\n\n[121\\. AI in 2025: An Expert Perspective](https://assets.ctfassets.net/aycrbiawo3x3/7I3GfjO32KjDr9rErjbK54/90bd198e1b4c647b1f0682184f093961/AI_Perspective_2025_-_Tribes_Digital_and_TalkTalk.pdf)\n\n[122\\. Key AI Trends of 2025](https://www.gleecus.com/blogs/ai-trends-2025/)\n\n[123\\. Security Now! #1001 - 11-19-24 Artificial General Intelligence (AGI)](https://www.grc.com/sn/sn-1001-notes.pdf)\n\n[124\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[125\\. A. Turing. “Computing Machinery and Intelligence.” Mind](https://doi.org/10.1093/MIND/LIX.236.433)\n\n[126\\. Gartner’s 2024 AI Hype Cycle: Key AI Types and Trends](https://www.bhattaraprot.com/GartnerAI.pdf)\n\n[127\\. Experts Predict Artificial General Intelligence July 2025 to 2031](http://pirateswithoutborders.com/information/688863-experts-predict-artificial-general-intelligence-july-2025-to-2031)\n\n[128\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[129\\. Artificial General Intelligence in 2025: Good Luck With That](https://www.informationweek.com/machine-learning-ai/artificial-general-intelligence-in-2025-good-luck-with-that)\n\n[130\\. A Inteligência Artificial em 2024 – balanço e pontes para 2025](https://interactideas.pt/guia2024/guia.pdf)\n\n[131\\. David Silver, Julian Schrittwieser et al. “Mastering the game of Go without human knowledge.” Nature](https://doi.org/10.1038/nature24270)\n\n[132\\. International Conference on Artificial General Intelligence](https://link.springer.com/conference/agi)\n\n[133\\. Artificial General Intelligence Issues and Opportunities](https://www.millennium-project.org/wp-content/uploads/2023/05/EC-AGI-paper.pdf)\n\n[134\\. SCBX AI Outlook 2025: Beaconing the Future of Artificial Intelligence](https://www.scbx.com/wp-content/uploads/2025/05/SCBX-AI-Outlook-2025_ENG.pdf)\n\n[135\\. J. Searle. “Minds, brains, and programs.” Behavioral and Brain Sciences](https://doi.org/10.1017/S0140525X00005756)\n\n[136\\. MAXIMUMPC](https://www.puebla.tecnm.mx/wp-content/archivos/CentroInformacion/PDFs/MAXIMUMPC/November_2024.pdf)\n\n[137\\. Comprehensive Review of Artificial General Intelligence AGI and Agentic GenAI: Applications in Business and Finance](https://www.preprints.org/frontend/manuscript/b95ddd7d9a9b21409573603edb9e1aa5/download_pub)\n\n[138\\. The Frustrating Quest to Define AGI](https://curriculumredesign.org/wp-content/uploads/The-Frustrating-Quest-to-Define-AGI.pdf)\n\n[140\\. Artificial General Intelligence Timeline: AGI in 5–10 Years](https://www.cognitivetoday.com/2025/04/artificial-general-intelligence-timeline-agi/)\n\n[141\\. A. Esteva, Brett Kuprel et al. “Dermatologist-level classification of skin cancer with deep neural networks.” Nature](https://doi.org/10.1038/nature21056)\n\n[142\\. The Unfathomable Transformations Artificial Intelligence Will Unleash](https://www.aidebrief.com/articles/the-unfathomable-transformations-artificial-intelligence-will-unleash/)\n\n[143\\. The impact of generative artificial intelligence on socioeconomic inequalities and policy making](https://hal.science/hal-04812501/document)\n\n[144\\. Introduction to Artificial General Intelligence (AGI) | EJable](https://www.ejable.com/tech-corner/ai-machine-learning-and-deep-learning/artificial-general-intelligence/)\n\n[145\\. Artificial Intelligence and Its Potential to Fuel Economic Growth and Improve Governance](https://www.jec.senate.gov/public/_cache/files/ff91fc60-1c1b-4b06-967b-84ed5245eb1e/brian-j-miller-jec-ai-testimony-for-06-04-2024.pdf)\n\n[146\\. Artificial General Intelligence and the End of Human Employment: The Need to Renegotiate the Social Contract](https://arxiv.org/pdf/2502.07050)\n\n[147\\. Danton Char, N. Shah et al. “Implementing Machine Learning in Health Care - Addressing Ethical Challenges..” The New England journal of medicine](https://doi.org/10.1056/NEJMp1714229)\n\n[148\\. Artificial Intelligence and Liability in Health Care](https://scholarlycommons.law.case.edu/cgi/viewcontent.cgi?article=1659&context=healthmatrix)\n\n[149\\. E. Vayena, A. Blasimme et al. “Machine learning in medicine: Addressing ethical challenges.” PLoS Medicine](https://doi.org/10.1371/journal.pmed.1002689)\n\n[150\\. 通用人工智能 (AGI) 的 10 个惊人预测：未来十年将会发生什么](https://julienflorkin.com/zh-CN/%E6%8A%80%E6%9C%AF/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E9%80%9A%E7%94%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-agi-2/)\n\n[151\\. A Global Civic Debate on Governing the Rise of Artificial Intelligence Report 2018](https://thefuturesociety.org/wp-content/uploads/2019/08/report_GlobalCivicDebate_2018.pdf)\n\n[152\\. ANALYSIS OF GOVERNANCE FRAMEWORKS FOR THE IMPLEMENTATION OF AI-DRIVEN TECHNOLOGIES](https://ai-pod.eu/wp-content/uploads/2024/04/AI-POD_D5.1_Analysis-of-governance-frameworks-1.pdf)\n\n[153\\. Artificial General Intelligence Applications](https://www.geeksforgeeks.org/artificial-general-intelligence-applications/)\n\n[154\\. Artificial Intelligence in the Public Sector: European Outlook for 2020 and Beyond](https://info.microsoft.com/rs/157-GQE-382/images/EN-CNTNT-eBook-artificial-SRGCM3835.pdf)\n\n[155\\. Review of Artificial General Intelligence (AGI): Implications for the U.S. Workforce and Economic Stability](https://www.preprints.org/frontend/manuscript/9dbc29039609e752606e519babadb6b5/download_pub)\n\n[156\\. 通用人工智能实现后的人类社会经济形态是什么样的?](https://www.zhihu.com/question/626479828/answer/3253400974)\n\n[157\\. THE PERILS AND PROMISES OF ARTIFICIAL GENERAL INTELLIGENCE](https://scholarship.law.nd.edu/cgi/viewcontent.cgi?article=1689&context=jleg)\n\n[158\\. How Artificial General Intelligence Can Transform Healthcare](https://zynthiq.com/artificial-general-intelligence-can-transform-healthcare/)\n\n[160\\. Universal Basic Income](https://wikispooks.com/wiki/Universal_Basic_Income)\n\n[161\\. Exploring Universal Basic Income: A Guide to Navigating Concepts, Evidence, and Practices](https://documents1.worldbank.org/curated/zh/993911574784667955/pdf/Exploring-Universal-Basic-Income-A-Guide-to-Navigating-Concepts-Evidence-and-Practices.pdf)\n\n[162\\. Redefining Universal Basic Income in Post-Covid Time](http://businesseconomics.in/redefining-universal-basic-income-post-covid-time)\n\n[163\\. Report on piloting a Universal Basic Income (2022)](https://assets.gov.ie/243295/31066da1-eb0d-4f56-8d47-361f3e4d0d83.pdf)\n\n[164\\. The potential of universal basic income schemes to mitigate shocks: Comparing the performance of universal basic income in Uganda and Zambia during COVID-19](https://www.wider.unu.edu/sites/default/files/Publications/Working-paper/PDF/wp2024-21-potential-of-universal-basic-income-schemes-mitigate-shocks.pdf)\n\n[165\\. Primer on Universal Basic Income](https://economicprinciples.org/downloads/Primer-on-Universal-Basic-Income.pdf)\n\n[166\\. Countries that Have Tried Universal Basic Income](https://basicincome.org/news/2022/07/countries-that-have-tried-universal-basic-income/)\n\n[167\\. Universal Basic Income - JournalsOfIndia](https://journalsofindia.com/universal-basic-income/)\n\n[168\\. Social Impact Report 2022](https://www.fgveurope.de/wp-content/uploads/2023/12/relatorio_de_impacto_social_2022_ing_2.pdf)\n\n[169\\. Universal Basic Income: An Ongoing Conversation](https://iorma.com/universal-basic-income/)\n\n[170\\. ITUC Economic and Social Policy Brief: Universal Basic Income](https://www.ituc-csi.org/IMG/pdf/universal_basic_income.pdf?23334/d9d2e9b99c5430a7bb681b8656be608837b8af4e6096ac907a673720719b8fb8)\n\n[171\\. A UNIVERSAL BASIC INCOME FOR IRELAND: LESSONS FROM THE INTERNATIONAL LITERATURE](https://assets.gov.ie/243437/ded2a708-f914-4f54-a1fe-17f125ae1825.pdf)\n\n[172\\. UNIVERSAL BASIC INCOME: A UNION PERSPECTIVE](https://www.world-psi.org/sites/default/files/documents/research/en_ubi_full_report_2019.pdf)\n\n[173\\. Can complementary local currencies with universal basic income reduce inequality?](https://encointer.org/wp-content/uploads/2025/01/2021_Lamsallak_Verein-Encointer_BT.pdf)\n\n[174\\. World Social Protection Report 2017–19: Universal social protection to achieve the Sustainable Development Goals](https://www.ilo.org/sites/default/files/wcmsp5/groups/public/@dgreports/@dcomm/@publ/documents/publication/wcms_604882.pdf)\n\n[175\\. Philippe Van Parijs. “Basic Income: A Simple and Powerful Idea for the Twenty-First Century.” Politics & Society](https://doi.org/10.1177/0032329203261095)\n\n[176\\. E. Forget. “The Town with No Poverty: The Health Effects of a Canadian Guaranteed Annual Income Field Experiment.” Canadian Public Policy](https://doi.org/10.1353/CPP.2011.0036)\n\n[177\\. Income support and social protection in Latin America and the Caribbean: Debates on policy options](https://repositorio.cepal.org/bitstream/handle/11362/49008/1/S2300382_en.pdf)\n\n[178\\. P. Parijs, Y. Vanderborght. “Basic Income: A Radical Proposal for a Free Society and a Sane Economy.”](https://doi.org/10.4159/9780674978072)\n\n[180\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[181\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[182\\. Empirical Studies in Machine Psychology](https://liu.diva-portal.org/smash/get/diva2:1904493/FULLTEXT01.pdf)\n\n[183\\. Xing Hao, Guigang Zhang et al. “Deep Learning.” Int. J. Semantic Comput.](https://doi.org/10.1142/S1793351X16500045)\n\n[184\\. AGI: Artificial General Intelligence, Explained - Capicua](https://www.capicua.com/blog/artificial-general-intelligence-explained)\n\n[185\\. GAZİ ÜNİVERSİTESİ BELTEK EĞİTİM KURSU](https://webupload.gazi.edu.tr/upload/1042/2025/5/2/f36bf024-3d63-41b7-8716-670ac55be616-beltek-04-03-999-temel-yapay-zeka-uygulamalari.pdf)\n\n[186\\. How Artificial General Intelligence Can Transform Healthcare](https://zynthiq.com/artificial-general-intelligence-can-transform-healthcare/)\n\n[187\\. Comprehensive review of Artificial General Intelligence (AGI): Applications in Business and Finance](https://philpapers.org/archive/JOSCRO.pdf)\n\n[188\\. A.I. & Healthcare – Spring 2025](https://www.knowledge.scot.nhs.uk/media/azrjq1uj/ai-healthcare-spring-2025.pdf)\n\n[189\\. Privacy and Regulatory Compliance in Retrieval-Augmented Generation Models for AGI Systems](https://www.ijfmr.com/papers/2024/6/30421.pdf)\n\n[190\\. AGI: A Glimpse into Artificial General Intelligence](https://www.ultralytics.com/blog/how-does-agi-work-a-glimpse-into-tomorrows-ai-innovations)\n\n[191\\. Artificial General Intelligence: The Promising Future of AI Trends](https://www.ai-demand.com/insights/tech/artificial-intelligence/artificial-general-intelligence-potential-and-real-world-implications/)\n\n[192\\. a asif 2025 - AGI-Enabled Robotics for Healthcare Indu...](https://link.springer.com/10.1007/978-981-97-3222-7_16)\n\n[193\\. Tao Feng, Chuanyang Jin et al. “How Far Are We From AGI.” ArXiv](https://doi.org/10.48550/arXiv.2405.10313)\n\n[194\\. 《奇点更近》作者雷・库兹韦尔预测：2029年AGI将至，人类即将与AI融合！](https://www.bilibili.com/video/av113756705133267?t=482)\n\n[195\\. Exploring the Wide Range of Artificial General Intelligence Uses](https://pareshmpatel.com/exploring-the-wide-range-of-artificial-general-intelligence-uses/)\n\n[196\\. AI Weekly](https://securities.miraeasset.com/bbs/download/2135793.pdf?attachmentId=2135793)\n\n[197\\. A. Esteva, Alexandre Robicquet et al. “A guide to deep learning in healthcare.” Nature Medicine](https://doi.org/10.1038/s41591-018-0316-z)\n\n[198\\. Comprehensive Review of Artificial General Intelligence AGI and Agentic GenAI: Applications in Business and Finance](https://www.preprints.org/frontend/manuscript/b95ddd7d9a9b21409573603edb9e1aa5/download_pub)\n\n[199\\. The Evolution of Artificial General Intelligence](https://aindrew.com/what-is-artificial-general-intelligence/)\n\n[200\\. David Silver, Aja Huang et al. “Mastering the game of Go with deep neural networks and tree search.” Nature](https://doi.org/10.1038/nature16961)\n\n[201\\. 通用人工智能 (Agi) 的 10 个惊人预测：未来十年将会发生什么](https://julienflorkin.com/zh-CN/technology/artificial-intelligence/artificial-general-intelligence-agi-2/)\n\n[202\\. David Silver, Julian Schrittwieser et al. “Mastering the game of Go without human knowledge.” Nature](https://doi.org/10.1038/nature24270)\n\n[203\\. J. Searle. “Minds, brains, and programs.” Behavioral and Brain Sciences](https://doi.org/10.1017/S0140525X00005756)\n\n[204\\. Review of Artificial General Intelligence (AGI): Implications for the U.S. Workforce and Economic Stability](https://www.preprints.org/frontend/manuscript/9dbc29039609e752606e519babadb6b5/download_pub)\n\n[205\\. Regulatory Principles of Development, Introduction and Use of Artificial Intelligence in Asian countries](https://lida.hse.ru/article/download/16264/15390)\n\n[206\\. The 5 Levels in Achieving AGI – Genesis](https://genesishumanexperience.com/2024/12/22/the-5-levels-in-achieving-agi/)\n\n[207\\. Margaret Mitchell, Simone Wu et al. “Model Cards for Model Reporting.” Proceedings of the Conference on Fairness, Accountability, and Transparency](https://doi.org/10.1145/3287560.3287596)\n\n[208\\. What’s the buzz around AGI safety?](https://analyticsindiamag.com/whats-the-buzz-around-agi-safety/)\n\n[209\\. Tom Everitt, G. Lea et al. “AGI Safety Literature Review.” ArXiv](https://doi.org/10.24963/ijcai.2018/768)\n\n[210\\. The Path to AGI: Timeline Considerations and Impacts](https://www.lumenova.ai/blog/artificial-general-intelligence-agi-timeline/)\n\n[211\\. Tom Everitt. “Towards Safe Artificial General Intelligence.”](https://doi.org/10.25911/5D134A2F8A7D3)\n\n[212\\. B. Goertzel. “Artificial General Intelligence: Concept, State of the Art, and Future Prospects.” Journal of Artificial General Intelligence](https://doi.org/10.2478/jagi-2014-0001)\n\n[213\\. AI Policies towards the AGI Challenge: An International Assessment](http://wpa.deos.aueb.gr/docs/2025.AGI.15.pdf)\n\n[214\\. New Survey: Broad Expert Consensus for Many AGI Safety and Governance Practices](https://www.governance.ai/post/broad-expert-consensus-for-many-agi-safety-and-governance-best-practices)\n\n[220\\. GAIA Benchmark: A New Horizon in AI Intelligence ...](https://encorp.ai/blog/gaia-ai-intelligence-benchmark_2025-04-19)\n\n[221\\. Yaroslav Ganin, E. Ustinova et al. “Domain-Adversarial Training of Neural Networks.” Journal of machine learning research](https://doi.org/10.1007/978-3-319-58347-1_10)\n\n[222\\. GAIA: New Benchmark Reveals Limitations of Large-Scale Language Models](https://ai-scholar.tech/en/articles/large-language-models/gaia)\n\n[223\\. GAIA Benchmark: evaluating intelligent agents - WorkOS](https://workos.com/blog/gaia-benchmark-evaluating-intelligent-agents#:~:text=GAIA%20tasks%20are%20conceptually%20simple,questions%20spanning%20different%20complexity%20levels.)\n\n[224\\. Ishaan Gulrajani, David Lopez-Paz. “In Search of Lost Domain Generalization.” ArXiv](https://arxiv.org/abs/2007.01434)\n\n[225\\. Da Li, Yongxin Yang et al. “Learning to Generalize: Meta-Learning for Domain Generalization.” AAAI Conference on Artificial Intelligence](https://doi.org/10.1609/aaai.v32i1.11596)\n\n[226\\. Da Li, Yongxin Yang et al. “Deeper, Broader and Artier Domain Generalization.” 2017 IEEE International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV.2017.591)\n\n[227\\. NICO++: Towards Better Benchmarking for Domain Generalization](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_NICO_Towards_Better_Benchmarking_for_Domain_Generalization_CVPR_2023_paper.pdf)\n\n[228\\. A Survey of Agent Evaluation Frameworks: Benchmarking ...](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison/)\n\n[229\\. Krikamol Muandet, D. Balduzzi et al. “Domain Generalization via Invariant Feature Representation.” International Conference on Machine Learning](https://arxiv.org/abs/1301.2115)\n\n[230\\. GAIA benchmark overview](https://arduin.io/blog/gaia-overview/)\n\n[231\\. AI researchers introduce GAIA: A benchmark testing tool for general AI assistants](https://techxplore.com/news/2023-12-ai-gaia-benchmark-tool-general.pdf)\n\n[232\\. GAIA: A Benchmark for General AI Assistants](https://openreview.net/pdf/e828bf3e5aaa9c75b6b9b9ef064fafc685bc6f6c.pdf)\n\n[233\\. Advancing Meta-Learning for Enhanced Generalization Across Diverse Tasks](https://www.diva-portal.org/smash/get/diva2:1924707/FULLTEXT01.pdf)\n\n[234\\. Adaptive Methods for Real-World Domain Generalization](https://openaccess.thecvf.com/content/CVPR2021/papers/Dubey_Adaptive_Methods_for_Real-World_Domain_Generalization_CVPR_2021_paper.pdf)\n\n[235\\. Cross-Domain Ensemble Distillation for Domain Generalization](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850001.pdf)\n\n[236\\. GAIA: a benchmark for General AI Assistants](https://www.summarizepaper.com/en/arxiv-id/2311.12983v1/)\n\n[237\\. Cross-domain solutions: the present and future of a growing industry](https://www.gmv.com/sites/default/files/content/file/2024/10/18/846/gmv_news_91_en.pdf)\n\n[238\\. CONFeSS: A FRAMEWORK FOR SINGLE SOURCE CROSS-DOMAIN FEW-SHOT LEARNING](https://www.porikli.com/mysite/pdfs/porikli%202022%20-%20ConFeSS%20A%20framework%20for%20single%20source%20cross-domain%20few-shot%20learning.pdf)\n\n[239\\. An Exploration of Domain Generalisation Through Vision Benchmarking, Masking, and Pruning](https://doras.dcu.ie/30622/1/Hamza_final_thesis.pdf)\n\n[240\\. Top 20 Applications of Artificial Intelligence (AI) in 2025](https://www.geeksforgeeks.org/blogs/applications-of-ai/)\n\n[241\\. 10 Examples of Artificial Intelligence in Real-Life (2025)](https://www.geeksforgeeks.org/artificial-intelligence/10-examples-of-artificial-intelligence-in-real-life-2024/)\n\n[242\\. 2025 \"人工智能+\" 行业发展蓝皮书](http://www.sccio.cn/uploads/20250522/8696496143e2c9e662e2e45890c9c1b4.pdf)\n\n[243\\. Artificial intelligence in early warning systems for infectious disease surveillance: a systematic review](https://www.frontiersin.org/journals/public-health/articles/10.3389/fpubh.2025.1609615/pdf)\n\n[244\\. 10 Real-World AI Agent Examples in 2025](https://www.thoughtminds.io/ai-agent-examples/)\n\n[245\\. U.S. Department of Health and Human Services: Strategic Plan for the Use of Artificial Intelligence in Health, Human Services, and Public Health](https://www.healthit.gov/sites/default/files/2025-01/2025%20HHS%20AI%20Strategic%20Plan_Full_508.pdf)\n\n[246\\. Artificial intelligence in healthcare](https://smer.se/wp-content/uploads/2020/06/smer-2020-2-in-brief-artificial-intelligence-in-healthcare.pdf)\n\n[247\\. What is AI and what is Artificial General Intelligence...](https://blog.pangeanic.com/what-is-ai-what-is-artificial-general-intelligence)\n\n[248\\. Top AI Agents Use case for Healthcare in 2025](https://www.upskillist.com/blog/top-ai-agents-use-case-for-healthcare-in-2025/)\n\n[249\\. 医周药事](http://www.drugnet.com.cn/upload/file/25022116435299.pdf)\n\n[250\\. Artificial Intelligence and Deep Learning in Healthcare, Cyber security, and Food Systems: A Comprehensive Review of Applications, Challenges, and Future Directions](https://media.neliti.com/media/publications/594398-artificial-intelligence-and-deep-learnin-fa252c4a.pdf)\n\n[251\\. Real World Applications for AI: Examples in Healthcare, Agriculture, and More](https://solidopinion.com/ai-usage/)\n\n[252\\. Artificial Intelligence (AI) for Healthcare 2026](https://www.byteplus.com/en/topic/392533)\n\n[253\\. Artificial Intelligence in public health: A case study](https://wjbphs.com/sites/default/files/WJBPHS-2024-0783.pdf)\n\n[254\\. In-depth: Artificial Intelligence 2019](https://people.stfx.ca/x2011/Temp2021Q/School/2018-2019/Winter/BSAD%20471%20-%20Strat/Case/AI%20statista.pdf)\n\n[255\\. 2025上海医疗展|AI+医疗健康：智能化医疗健康的应用与未来](http://www.jyyxexpo.com/shanghai/html/3G/press/2670.html)\n\n[256\\. A systematic review Applications of Artificial Intelligence in Clinical Practice and Healthcare (2010–2025)](https://files.sdiarticle5.com/wp-content/uploads/2025/05/Revised-ms_AJMPCP_137032_v1.docx)\n\n[257\\. Use of Artificial Intelligence in Personalized Medicine and Healthcare](https://iq.opengenus.org/use-of-artificial-intelligence-in-personalized-medicine-and-healthcare/)\n\n[258\\. Applications of artificial intelligence in, early detection of cancer, clinical diagnosis and personalized medicine](https://f6publishing.blob.core.windows.net/99dd7b37-8615-4a8e-8aeb-db088c75f655/AIC-1-39.DOCX)\n\n[260\\. 欧盟人工智能监管实践](https://zhuanlan.zhihu.com/p/680218099)\n\n[261\\. AI Regulations and the Future of Global Data Protection](https://gdprlocal.com/ai-regulations-and-the-future-of-global-data-protection/)\n\n[262\\. Artificial intelligence](https://www.europarl.europa.eu/RegData/etudes/BRIE/2023/749784/EPRS_BRI%282023%29749784_EN.pdf)\n\n[263\\. Artificial Intelligence initial strategy and deployment roadmap 2024-2025](http://maruyama-mitsuhiko.cocolog-nifty.com/security/files/artificial20intelligence20initial20strategy20and20deployment.docx)\n\n[264\\. A Complex Adaptive System Framework to Regulate Artificial Intelligence](https://eacpm.gov.in/wp-content/uploads/2024/01/EACPM_AI_WP-1.pdf)\n\n[265\\. Boardroom](https://www.iod.org.nz/assets/Resources-insights/IoD-publications/Boardroom-2024/Summer-24-25/Boardroom-Mag_SUMMER2024-25.pdf)\n\n[266\\. Artificial Intelligence Review 2023](https://www.mhc.ie/uploads/documents/MHC-AI-Annual-Review-2023_2024-01-12-153611_vtdd.pdf)\n\n[267\\. Artificial Intelligence, EU Regulation and Competition Law ...](https://www.quinnemanuel.com/the-firm/publications/artificial-intelligence-eu-regulation-and-competition-law-enforcement-addressing-emerging-challenges/)\n\n[268\\. Generative AI for Health in Low & Middle Income Countries](https://cdh.stanford.edu/sites/g/files/sbiybj29486/files/media/file/stanford_scdh_genaiwhitepaper_v18_compressed.pdf)\n\n[269\\. A Comparative Framework for AI Regulatory Policy: Phase 2](https://ceimia.org/wp-content/uploads/2024/06/a-comparative-framework-for-ai-regulatory-policy_-phase-2-report.pdf)\n\n[270\\. Regulating Artificial Intelligence: Comparing EU and U.S. Frameworks](https://www.multistate.us/insider/2023/8/24/regulating-artificial-intelligence-comparing-eu-and-us-frameworks)\n\n[271\\. Overview of Potential Risks of Artificial General Intelligence Robots](https://ojs.svako.lt/TMT/article/download/93/88/438)\n\n[272\\. AI Watch: Global regulatory tracker - European Union | White & Case LLP](https://www.whitecase.com/insight-our-thinking/ai-watch-global-regulatory-tracker-european-union#:~:text=Unacceptable%20risk:%20AI%20systems%20that,%22unacceptable%22%20risk%20are%20prohibited.&text=This%20includes%20%28among%20others%29%20AI,manner%20that%20can%20cause%20harm.)\n\n[273\\. EU AI Act Overview](https://aijure.com/index.php/eu-ai-act-overview/)\n\n[274\\. 通用人工智能监管决策研究：以欧盟《人工智能法案》为例 The Dynamics of Regulatory Decision-Making for General Purpose AI in the EU Artificial Intelligence Act](https://www.sis.pku.edu.cn/docs/20250624093228917582.pdf)\n\n[275\\. A Comparative Analysis of the EU AI Act and the Colorado AI Act: Regulatory Approaches to Artificial Intelligence Governance](https://ijcaonline.org/archives/volume186/number38/jariwala-2024-ijca-923954.pdf)\n\n[276\\. Artificial Intelligence Act: EU agrees on the Regulation of Artificial Intelligence](https://www.ebnerstolz.de/en/artificial-intelligence-act-eu-agrees-on-the-regulation-of-artificial-intelligence-478520.html)\n\n[277\\. ARTIFICIAL INTELLIGENCE REGULATION - NCS Analytics](https://ncsanalytics.com/artificial-intelligence-regulation/)\n\n[278\\. The 2025 worldwide state of AI regulation](https://naaia.ai/worldwide-state-of-ai-regulation/)\n\n[280\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[281\\. GAIA Benchmark: evaluating intelligent agents](https://workos.com/blog/gaia-benchmark-evaluating-intelligent-agents)\n\n[282\\. Yaroslav Ganin, E. Ustinova et al. “Domain-Adversarial Training of Neural Networks.” Journal of machine learning research](https://doi.org/10.1007/978-3-319-58347-1_10)\n\n[283\\. AI researchers introduce GAIA: A benchmark testing tool for general AI assistants](https://techxplore.com/news/2023-12-ai-gaia-benchmark-tool-general.pdf)\n\n[284\\. GAIA: A Benchmark for General AI Assistants](https://openreview.net/pdf/e828bf3e5aaa9c75b6b9b9ef064fafc685bc6f6c.pdf)\n\n[285\\. Haoliang Li, Sinno Jialin Pan et al. “Domain Generalization with Adversarial Feature Learning.” 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2018.00566)\n\n[286\\. Da Li, Yongxin Yang et al. “Learning to Generalize: Meta-Learning for Domain Generalization.” AAAI Conference on Artificial Intelligence](https://doi.org/10.1609/aaai.v32i1.11596)\n\n[287\\. Da Li, Yongxin Yang et al. “Deeper, Broader and Artier Domain Generalization.” 2017 IEEE International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV.2017.591)\n\n[288\\. Xingyuan Bu, Junran Peng et al. “GAIA: A Transfer Learning System of Object Detection that Fits Your Needs.” 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR46437.2021.00034)\n\n[289\\. GAIA Benchmark: A New Horizon in AI Intelligence ...](https://encorp.ai/blog/gaia-ai-intelligence-benchmark_2025-04-19)\n\n[290\\. GAIA: a benchmark for General AI Assistants - Paper Reading](https://paperreading.club/page?id=195387)\n\n[291\\. GAIA: a benchmark for General AI Assistants](https://www.summarizepaper.com/en/arxiv-id/2311.12983v1/)\n\n[292\\. HAL: GAIA Leaderboard](https://hal.cs.princeton.edu/gaia#:~:text=GAIA%20is%20a%20benchmark%20for,tooling%20and%20autonomy%20to%20solve.)\n\n[293\\. GTA: A Benchmark for General Tool Agents](https://papers.nips.cc/paper_files/paper/2024/file/8a75ee6d4b2eb0b777f549a32a5a5c28-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[294\\. GAIA: a benchmark for General AI Assistants,arXiv - CS - Artificial Intelligence - X-MOL](https://www.x-mol.com/paper/1728537551386202112?adv)\n\n[295\\. GAIA 通用人工智能助手的基准数据集](https://hyper.ai/datasets/32828)\n\n[296\\. MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents](https://arxiv.org/pdf/2502.05957v1)"
  },
  {
    "id": 60,
    "question": "How can multi-modal models effectively overcome the challenge of aligning different modalities like text and images while preserving the strengths of each modality?",
    "response": "**Comprehensive Research Report: Cross-Modal Alignment in Multi-Modal Models**\n\n**1\\. Introduction: The Challenge of Multi-Modal Alignment**\n\nMulti-modal models face the fundamental challenge of creating meaningful connections between heterogeneous data types (e.g., text, images) while preserving their inherent strengths. Image data provides high-resolution spatial information but lacks explicit semantics, while text offers abstract symbolic representations but loses spatial-temporal richness \\[8\\]. The alignment process must reconcile these differences without degrading modality-specific features – termed the \"heterogeneity gap\" \\[4\\]. Modern approaches focus on joint embedding spaces and specialized loss functions to enable synergistic understanding while maintaining unimodal integrity \\[10\\].\n\n**2\\. Architectural Strategies for Cross-Modal Alignment**\n\n**2.1 Encoder Architecture Selection**\n\n**Visual Encoders**: EfficientNet and Vision Transformers (ViTs) dominate image processing, providing hierarchical feature extraction \\[5\\]. ViTs like CoSwin Transformer capture long-range dependencies crucial for spatial alignment \\[6\\].\n\n**Text Encoders**: Bi-directional RNNs and LSTMs process sequential text data, while Transformers enable contextual understanding \\[1\\]\\[4\\]. Larger models like Kosmos-2 directly integrate visual tokens into LLMs using prefix tuning \\[27\\].\n\n**2.2 Fusion Methodologies**\n\n|     |     |     |\n| --- | --- | --- |\n| **Fusion Type** | **Mechanism** | **Strength Preservation Strategy** |\n| **Early Fusion** | Combined at input stage (Flamingo) | Gated cross-attention with frozen encoders \\[29\\]\\[37\\] |\n| **Late Fusion** | Post-individual processing | HEALNet uses modality-specific attention weights \\[25\\]\\[36\\] |\n| **Token Fusion** | Unified token space (Chameleon) | Homogenizes modalities via discrete tokens \\[38\\] |\n\nToken-level fusion empirically outperforms feature-level fusion in VQA benchmarks (+3.2% accuracy on average) \\[228\\], while query-based fusion (Perceiver Resampler in Flamingo) enables dynamic attention weighting \\[33\\].\n\n**3\\. Loss Functions Driving Effective Alignment**\n\n**3.1 Contrastive Learning Frameworks**\n\n**InfoNCE Loss**: Primary choice for embedding space alignment, forcing positive pairs closer while repelling negatives \\[13\\]\n\n**Image-Text Matching (ITM) Loss**: Binary classification ensuring matched pairs achieve maximal similarity \\[11\\]\n\n**TCMPM Loss**: Minimizes relative entropy between projections \\[17\\]\n\n**3.2 Specialized Objectives**\n\n**Max-Margin Triplet Loss**: Maintains seen-unseen class separation \\[9\\]\n\n**Adaptive Loss**: Preserves cosine similarity between adapted features \\[15\\]\n\n**CycleGAN Objectives**: Enable unsupervised alignment of novel classes \\[9\\]\n\n**4\\. State-of-the-Art Models: Modality Preservation in Practice**\n\n**4.1 Flamingo's Preservation Strategy**\n\n**Frozen Modality Experts**: Pre-trained visual encoders and LLMs remain fixed to maintain original capabilities \\[29\\]\n\n**Few-Shot Superiority**: Outperforms task-specific models using ≤32 examples \\[30\\]\\[32\\]\n\n**Quantitative Evidence**:\n\nVQAv2 Accuracy: 51.8% (OpenFlamingo) \\[186\\]\n\nFlickr30k: 61.5 CIDEr (9B model) \\[125\\]\n\n**4.2 Kosmos-2's Unified Approach**\n\n**End-to-End Integration**: Direct injection of visual inputs into LLM \\[27\\]\n\n**Grounding-Specific Gains**: Achieves 80.5 CIDEr on Flickr30k (vs. Flamingo-9B's 61.5) \\[125\\]\n\n**Unimodal Preservation**: Maintains comparable NLP performance to unimodal LLMs on SuperGLUE:\n\nCOPA: 67.0% (zero-shot) \\[190\\]\n\nBoolQ: 62.0% \\[190\\]\n\n**5\\. Emerging Techniques for Semantic Alignment**\n\n**5.1 Token-Level Prompt Alignment (ALIGN)**\n\n**Hierarchical Optimal Transport**: Combines prompt-level and token-level alignment \\[105\\]\n\n**Multi-Mode Prompts**: Captures diverse visual concepts through parallel prompts \\[105\\]\n\n**Quantitative Impact**: +5.7% average gains in few-shot image recognition versus feature-level fusion \\[105\\]\n\n**5.2 AI-Generated Text Training**\n\n**Mitigation Potential**: AI-generated captions show faster convergence but risk semantic narrowing \\[86\\]\n\n**Current Limitations**: Peer-reviewed validation specifically for multimodal drift mitigation remains limited as of 2025 \\[326\\]\\[332\\]\n\n**6\\. Quantitative Benchmark Analysis**\n\n**6.1 Cross-Modal Alignment Performance**\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n| **Model** | **VQA v2 Acc.** | **Flickr30k CIDEr** | **MS-COCO R@1** |\n| **Flamingo-9B** | 61.5 | 61.5 | 42.1 |\n| **Kosmos-2 (1.6B)** | 51.1 | 80.5 | 53.7 |\n| **TokenFD (Token-Level)** | \\-  | \\-  | 58.4 \\[167\\] |\n| **Feature-Level Fusion** | ~58.2 | ~71.3 | 51.2 \\[228\\] |\n\nToken-level methods show consistent advantages, with 7.5% average improvement in image-text retrieval R@1 over late fusion \\[234\\].\n\n**6.2 Unimodal Preservation After Training**\n\nWhile multimodal training inevitably creates tradeoffs:\n\n**Flamingo** maintains near-baseline performance on textual entailment tasks \\[126\\]\n\n**Kosmos-2** shows <3% degradation on Winograd Schema compared to text-only equivalents \\[190\\]\n\n**7\\. Challenges and Research Gaps**\n\n**Semantic Drift Validation**: Insufficient empirical studies on AI-generated text training for drift mitigation \\[362\\]\\[371\\]\n\n**Unimodal Benchmarking**: Standardized GLUE/SuperGLUE metrics post-multimodal training remain sparse \\[305\\]\n\n**Modality Imbalance**: Visual features often dominate text in early fusion architectures \\[108\\]\n\n**8\\. Future Research Directions**\n\n1.  **Dynamic Modality Weighting**: Adaptive controllers balancing modality contributions \\[14\\]\n2.  **Generative Drift Correction**: Diffusion-based regularization for alignment stability \\[152\\]\n3.  **Cross-Modal Attention Refinement**: Sparse attention mechanisms reducing computational load \\[112\\]\n\n**9\\. Conclusion**\n\nEffective cross-modal alignment requires combined architectural innovations like gated cross-attention \\[29\\], token-level fusion \\[166\\], and specialized loss functions \\[13\\]. While models like Flamingo demonstrate exceptional few-shot alignment through frozen experts \\[30\\], and Kosmos-2 achieves superior grounding via unified tokenization \\[38\\], ongoing challenges include quantifiable unimodal preservation and drift mitigation. The most promising path involves hierarchical alignment techniques like ALIGN \\[105\\] combined with modality-specific regularization, enabling synergistic understanding while preserving each modality's unique representational strengths.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. A. Karpathy, Li Fei-Fei. “Deep visual-semantic alignments for generating image descriptions.” 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2015.7298932)\n\n[2\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[3\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[4\\. Deep Multimodal Learning for Vision and Language Processing](https://theses.hal.science/tel-03140942v1/file/1.%20me%CC%81moire%20de%20the%CC%80se.pdf)\n\n[5\\. omega-rg/Cross-Modal-Learning-for-Fashion-Image-Text ...](https://github.com/omega-rg/Cross-Modal-Learning-for-Fashion-Image-Text-Retrieval)\n\n[6\\. Jiquan Ngiam, A. Khosla et al. “Multimodal Deep Learning.” International Conference on Machine Learning](https://doi.org/10.48550/arXiv.2301.04856)\n\n[7\\. 多模态引导视觉 Transformer 的小样本农作物病害识别](https://www.aeeisp.com/nygcxb/cn/article/pdf/preview/10.11975/j.issn.1002-6819.202409189.pdf)\n\n[8\\. GMA: Green Multi-Modal Alignment for Image-Text Retrieval](http://www.apsipa2024.org/files/papers/17.pdf)\n\n[9\\. Deep Multimodal Learning for Joint Textual and Visual Reasoning](https://hal.sorbonne-universite.fr/tel-03951566v1/file/2020SORUS370.pdf)\n\n[10\\. Multi-task Learning Applications in Deep Learning](https://research.nottingham.edu.cn/files/1318952099/ChangSHU-20120194-Thesis.pdf)\n\n[11\\. A Unified Multi-Dataset Framework for Medical Visual Question Answering Via Pretrained Transformers and Contrastive Learning](https://aircconline.com/csit/papers/vol15/csit150704.pdf)\n\n[12\\. MAHE: a multiscale and hybrid expert-based model for image-text enhanced named entity recognition on social media](https://www.nature.com/articles/s41598-025-01708-6.pdf)\n\n[13\\. A Survey on Self-Supervised Contrastive Learning for Multimodal Text-Image Analysis](https://arxiv.org/pdf/2503.11101)\n\n[14\\. MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models](https://openreview.net/pdf/2a13a6784da2b9df088ddaefa93ad3a571ff74a2.pdf)\n\n[15\\. Image and Text Representation Learning based on Deep Neural Networks 基於深度神經網絡的圖像與文本表徵學習](http://lbms03.cityu.edu.hk/theses/c_ftt/phd-cs-118914566.pdf)\n\n[16\\. CelsiaNet: Collaborative Understanding of Images and Text—A Multi-Modal Vision-Language Model Framework](http://www.yau-awards.com/uploads/file/20241106/20241106152748_50033.pdf)\n\n[17\\. 面向跨模态文本到图像行人重识别的 Transformer 网络](https://www.cjig.cn/rc-pub/front/front-article/download/37436007/lowqualitypdf/Transformer%20network%20for%20cross-modal%20text-to-image%20person%20re-identification.pdf)\n\n[18\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[19\\. Cross-Modal Contrastive Learning for Text-to-Image Generation](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Cross-Modal_Contrastive_Learning_for_Text-to-Image_Generation_CVPR_2021_paper.pdf)\n\n[20\\. Bringing Multimodality to Amazon Visual Search System](https://assets.amazon.science/57/3b/f549e60d4bce8179c81bebdf03bb/bringing-multimodality-to-amazon-visual-search-system.pdf)\n\n[21\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[22\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[23\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[24\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[25\\. Hybrid Early Fusion for Multi-Modal Biomedical Representations](https://openreview.net/pdf/3fdd56e407c47dae246fc88d8d18f3392edc0e59.pdf)\n\n[26\\. HEALNet: Multimodal Fusion for Heterogeneous Biomedical Data](https://arxiv.org/pdf/2311.09115v3)\n\n[27\\. Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input](https://arxiv.org/pdf/2408.15542)\n\n[28\\. Junnan Li, Dongxu Li et al. “BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models.” International Conference on Machine Learning](https://doi.org/10.48550/arXiv.2301.12597)\n\n[29\\. VL-Mamba: Exploring State Space Models for Multimodal Learning](https://raw.githubusercontent.com/mlresearch/v262/main/assets/qiao24a/qiao24a.pdf)\n\n[30\\. 大規模マルチモーダルAIの数理と災害分野への応用](https://www.ms.u-tokyo.ac.jp/lmsr/pdf/2024-4.pdf)\n\n[31\\. On Multimodal Representations Learned at Scale](https://di.ku.dk/english/research/phd/phd-theses/2023/bugliarello_phd-thesis.pdf)\n\n[32\\. Flamingo: a Visual Language Model for Few-Shot Learning](https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf)\n\n[33\\. RoboBERT: An End-to-end Multimodal Robotic Manipulation Model](https://arxiv.org/pdf/2502.07837)\n\n[34\\. Spectrum-based Modality Representation Fusion Graph Convolutional Network for Multimodal Recommendation](https://www.atailab.cn/seminar2024Fall/pdf/2025_WSDM_Spectrum-based%20Modality%20Representation%20Fusion%20Graph%20Convolutional%20Network%20for%20Multi-modal%20Recommendation.pdf)\n\n[35\\. Large Language Models Large Multi-modal Models (LMMs)](https://sharif-llm.ir/assets/lectures/LMMs-I.pdf)\n\n[36\\. Konstantin Hemker, Nikola Simidjievski et al. “HEALNet - Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data.” ArXiv](https://doi.org/10.48550/arXiv.2311.09115)\n\n[37\\. LARGE-SCALE AI IN TELECOM: Charting the Roadmap for Innovation, Scalability, and Enhanced Digital Experiences](https://www.eurecom.fr/publication/8074/download/comsys-publi-8074.pdf)\n\n[38\\. META to Unveil State-of-the-art Multi-modal LLM Chameleon](https://cioinfluence.com/natural-language/meta-to-unveil-state-of-the-art-multi-modal-llm-chameleon/)\n\n[41\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[42\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[43\\. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” CoRR](https://arxiv.org/abs/1412.6980)\n\n[44\\. Thomas Kipf, M. Welling. “Semi-Supervised Classification with Graph Convolutional Networks.” ArXiv](https://arxiv.org/abs/1609.02907)\n\n[45\\. What techniques mitigate semantic drift in multi-turn...](https://www.edureka.co/community/297785/techniques-mitigate-semantic-multi-generative-conversations)\n\n[46\\. Tuning Multi-mode Token-level Prompt Alignment across Modalities](https://papers.neurips.cc/paper_files/paper/2023/file/a547d86953a4e36aa8a1390e6f4708e2-Paper-Conference.pdf)\n\n[47\\. T. Baltrušaitis, Chaitanya Ahuja et al. “Multimodal Machine Learning: A Survey and Taxonomy.” IEEE Transactions on Pattern Analysis and Machine Intelligence](https://doi.org/10.1109/TPAMI.2018.2798607)\n\n[48\\. Improved Alignment of Modalities in Large Vision Language Models](https://www.researchsquare.com/article/rs-5533456/v1.pdf?c=1732863976000)\n\n[49\\. AI Alignment: A Comprehensive Survey](https://alignmentsurvey.com/uploads/AI-Alignment-A-Comprehensive-Survey-JP.pdf)\n\n[61\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[62\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[63\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[64\\. GROUNDING MULTIMODAL LARGE LANGUAGE MODELS TO THE WORLD](https://openreview.net/pdf/0ea36b222b82ac76c018c9aa7a47f9f978c705b2.pdf)\n\n[65\\. EMU: GENERATIVE PRETRAINING IN MULTIMODALITY](https://openreview.net/pdf/feaa25c56abdf06f62ac31c7c4de4434fef28463.pdf)\n\n[66\\. MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI](https://openaccess.thecvf.com/content/CVPR2024/papers/Yue_MMMU_A_Massive_Multi-discipline_Multimodal_Understanding_and_Reasoning_Benchmark_for_CVPR_2024_paper.pdf)\n\n[67\\. Jean-Baptiste Alayrac, Jeff Donahue et al. “Flamingo: a Visual Language Model for Few-Shot Learning.” ArXiv](https://arxiv.org/abs/2204.14198)\n\n[68\\. Alex Jinpeng Wang, Linjie Li et al. “COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training.” ArXiv](https://doi.org/10.48550/arXiv.2401.00849)\n\n[69\\. Towards Collaborative Generative AI for Vision-and-Language Studies](https://escholarship.org/content/qt0xd1437c/qt0xd1437c.pdf)\n\n[70\\. Zhiliang Peng, Wenhui Wang et al. “Kosmos-2: Grounding Multimodal Large Language Models to the World.” ArXiv](https://doi.org/10.48550/arXiv.2306.14824)\n\n[71\\. EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large Language Model](https://arxiv.org/pdf/2408.11795)\n\n[72\\. Enhance Modality Robustness in Text-Centric Multimodal Alignment with Adversarial Prompting](https://arxiv.org/pdf/2408.09798)\n\n[73\\. Language Is Not All You Need:跨模态的大语言模型](https://zhuanlan.zhihu.com/p/641974310)\n\n[74\\. Few and zero shot learning: Flamingo & Kosmos-1](https://people.cs.vt.edu/chris/cs6804_spring2023/slides/deval_few_shot.pdf)\n\n[75\\. 从LLM到MLLM,多模态大规模语言模型KOSMOS-1赋予了语言模型...](https://www.microsoft.com/en-us/research/articles/kosmos-1/?locale=zh-cn)\n\n[76\\. Shaohan Huang, Li Dong et al. “Language Is Not All You Need: Aligning Perception with Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.14045)\n\n[81\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[82\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[83\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[84\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[85\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[86\\. Multimodal misinformation detection overcoming the training data collection challenge through data generation](https://theses.hal.science/tel-04395414/file/CHAFFIN_Antoine.pdf)\n\n[87\\. Understanding Semantic Drift in Model Driven Digital T...](https://dl.acm.org/doi/10.1145/3652620.3688256)\n\n[88\\. Testing of detection tools for AI-generated text](https://www.diva-portal.org/smash/get/diva2:1826149/FULLTEXT01.pdf)\n\n[89\\. A survey on multimodal-guided visual content synthesis](https://see.xidian.edu.cn/faculty/chdeng/Welcome%20to%20Cheng%20Deng%27s%20Homepage_files/Papers/Journal/NEUCOM2022_Ziqi.pdf)\n\n[90\\. Generative AI-based Approach to Concept Drift Generation in Streaming Text Data](https://wseas.com/journals/isa/2025/a045109-001%282025%29.pdf)\n\n[91\\. Multimodal Retrieval Augmented Generation Evaluation Benchmark](http://ieeevtc.org/vtc2024spring/DATA/PID2024002132.pdf)\n\n[92\\. Enhancing Security, Resilience, and Safety of Autonomous Systems with Generative AI](https://files-prod.tii.ae/2024-12/Enhancing%20Security%2C%20Resilience%2C%20and%20Safety%20of%20Autonomous%20Systems%20with%20Generative%20AI.pdf)\n\n[93\\. PRISMAI: An Environment for AI-generated Text Recognition](https://openreview.net/pdf?id=8DlGC2zI4A)\n\n[94\\. Ava Spataru, Eric Hambro et al. “Know When To Stop: A Study of Semantic Drift in Text Generation.” ArXiv](https://doi.org/10.48550/arXiv.2404.05411)\n\n[95\\. Research agenda: Supervising AIs improving AIs — AI...](https://alignmentforum.org/posts/7e5tyFnpzGCdfT4mR/research-agenda-supervising-ais-improving-ais)\n\n[101\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[102\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[103\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[104\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[105\\. Tuning Multi-mode Token-level Prompt Alignment across Modalities](https://papers.neurips.cc/paper_files/paper/2023/file/a547d86953a4e36aa8a1390e6f4708e2-Paper-Conference.pdf)\n\n[106\\. Ze Liu, Yutong Lin et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)](https://doi.org/10.1109/ICCV48922.2021.00986)\n\n[107\\. Efficient Multimodal Fusion via Interactive Prompting](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Efficient_Multimodal_Fusion_via_Interactive_Prompting_CVPR_2023_paper.pdf)\n\n[108\\. A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549)\n\n[109\\. MoPE: Mixture of Prompt Experts for Parameter-Efficient and Scalable Multimodal Fusion](https://arxiv.org/pdf/2403.10568)\n\n[110\\. LoRA Fusion: Enhancing Image Generation](https://www.mdpi.com/2227-7390/12/22/3474)\n\n[111\\. ...and Specificity Fusion Framework for Entity Alignment](http://arxiv.org/html/2410.14584v1)\n\n[112\\. Multimodal Token Fusion for Vision Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Multimodal_Token_Fusion_for_Vision_Transformers_CVPR_2022_paper.pdf)\n\n[113\\. High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning](https://openreview.net/pdf/916046385df5f073fde65b31228ecbf875e6ee25.pdf)\n\n[114\\. Decoding Methods Compared: Top-K and Other Token Selection Techniques](https://www.pingcap.com/article/decoding-methods-compared-top-k-and-other-token-selection-techniques/)\n\n[121\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[122\\. Alexey Dosovitskiy, Lucas Beyer et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv](https://arxiv.org/abs/2010.11929)\n\n[123\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[124\\. EMU: GENERATIVE PRETRAINING IN MULTIMODALITY](https://openreview.net/pdf/feaa25c56abdf06f62ac31c7c4de4434fef28463.pdf)\n\n[125\\. GROUNDING MULTIMODAL LARGE LANGUAGE MODELS TO THE WORLD](https://openreview.net/pdf/0ea36b222b82ac76c018c9aa7a47f9f978c705b2.pdf)\n\n[126\\. A Survey on In-context Learning](https://arxiv.org/pdf/2301.00234)\n\n[127\\. MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI](https://openaccess.thecvf.com/content/CVPR2024/papers/Yue_MMMU_A_Massive_Multi-discipline_Multimodal_Understanding_and_Reasoning_Benchmark_for_CVPR_2024_paper.pdf)\n\n[128\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[129\\. Jean-Baptiste Alayrac, Jeff Donahue et al. “Flamingo: a Visual Language Model for Few-Shot Learning.” ArXiv](https://arxiv.org/abs/2204.14198)\n\n[130\\. Quan Sun, Qiying Yu et al. “Generative Pretraining in Multimodality.” ArXiv](https://doi.org/10.48550/arXiv.2307.05222)\n\n[131\\. MMICL: EMPOWERING VISION-LANGUAGE MODEL WITH MULTI-MODAL IN-CONTEXT LEARNING](https://arxiv.org/pdf/2309.07915v2.pdf)\n\n[132\\. Enhance Modality Robustness in Text-Centric Multimodal Alignment with Adversarial Prompting](https://arxiv.org/pdf/2408.09798)\n\n[133\\. Zhiliang Peng, Wenhui Wang et al. “Kosmos-2: Grounding Multimodal Large Language Models to the World.” ArXiv](https://doi.org/10.48550/arXiv.2306.14824)\n\n[134\\. Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2023/file/5e84e4413268b713f0d4a1b23a9dae57-Paper-Conference.pdf)\n\n[135\\. Few and zero shot learning: Flamingo & Kosmos-1](https://people.cs.vt.edu/chris/cs6804_spring2023/slides/deval_few_shot.pdf)\n\n[136\\. Grounding Multimodal Large Language Models to the World](https://iclr.cc/media/iclr-2024/Slides/17934.pdf)\n\n[141\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[142\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[143\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[144\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[145\\. RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors](https://liamdugan.com/files/raid.pdf)\n\n[146\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[147\\. Multimodal misinformation detection overcoming the training data collection challenge through data generation](https://theses.hal.science/tel-04395414/file/CHAFFIN_Antoine.pdf)\n\n[148\\. EMU: GENERATIVE PRETRAINING IN MULTIMODALITY](https://openreview.net/pdf/feaa25c56abdf06f62ac31c7c4de4434fef28463.pdf)\n\n[149\\. Generative artificial intelligence in intelligent transportation systems: A systematic review of applications](https://link.springer.com/content/pdf/10.1007/s42524-025-4241-9.pdf)\n\n[150\\. A survey on multimodal-guided visual content synthesis](https://see.xidian.edu.cn/faculty/chdeng/Welcome%20to%20Cheng%20Deng%27s%20Homepage_files/Papers/Journal/NEUCOM2022_Ziqi.pdf)\n\n[151\\. Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining](https://arxiv.org/pdf/2408.02657)\n\n[152\\. Understanding Semantic Drift in Model Driven Digital T...](https://dl.acm.org/doi/10.1145/3652620.3688256)\n\n[153\\. MULTISCALE POSITIVE-UNLABELED DETECTION OF AI-GENERATED TEXTS](https://openreview.net/pdf?id=5Lp6qU9hzV)\n\n[154\\. Training-Free Mitigation of Language Reasoning ...](https://arxiv.org/html/2412.03467v1)\n\n[155\\. Multimodal Representation Learning for Agentic AI Systems](https://dspace.mit.edu/bitstream/handle/1721.1/158506/andonian-andonian-phd-eecs-2024-thesis.pdf?sequence=-1&isAllowed=y)\n\n[156\\. UK ADVANCE Spring 2024 Guidelines and Recommendations for Generative AI in Instructional Contexts](https://advance.uky.edu/sites/default/files/2023-12/to%20post_UK-ADVANCE-instructional-guidelines-14Dec2023.pdf)\n\n[157\\. Generative AI-based Approach to Concept Drift Generation in Streaming Text Data](https://wseas.com/journals/isa/2025/a045109-001%282025%29.pdf)\n\n[161\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[162\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[163\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[164\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[165\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[166\\. A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549)\n\n[167\\. A Token-level Text Image Foundation Model for Document Understanding](https://arxiv.org/pdf/2503.02304)\n\n[168\\. Evaluating Text-to-Visual Generation with Image-to-Text Generation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01435.pdf)\n\n[169\\. Efficient Multimodal Fusion via Interactive Prompting](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Efficient_Multimodal_Fusion_via_Interactive_Prompting_CVPR_2023_paper.pdf)\n\n[170\\. Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models](https://arxiv.org/pdf/2503.17794)\n\n[171\\. Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model](https://arxiv.org/pdf/2406.19905)\n\n[172\\. Large-scale multi-modal image search: theory and practice](https://www.fi.muni.cz/~xkohout7/Research/vlastni/JournalPapers/ijmdem2013-submission.docx)\n\n[173\\. 多模态视觉问答基准数据集与模型优化研究](https://keep.lib.asu.edu/taxonomy/term/117818)\n\n[174\\. VL-Match: Enhancing Vision-Language Pretraining with Token-Level and Instance-Level Matching](https://openaccess.thecvf.com/content/ICCV2023/papers/Bi_VL-Match_Enhancing_Vision-Language_Pretraining_with_Token-Level_and_Instance-Level_Matching_ICCV_2023_paper.pdf)\n\n[175\\. Tuning Multi-mode Token-level Prompt Alignment across Modalities](https://papers.neurips.cc/paper_files/paper/2023/file/a547d86953a4e36aa8a1390e6f4708e2-Paper-Conference.pdf)\n\n[181\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[182\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[183\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[184\\. GROUNDING MULTIMODAL LARGE LANGUAGE MODELS TO THE WORLD](https://openreview.net/pdf/0ea36b222b82ac76c018c9aa7a47f9f978c705b2.pdf)\n\n[185\\. MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI](https://openaccess.thecvf.com/content/CVPR2024/papers/Yue_MMMU_A_Massive_Multi-discipline_Multimodal_Understanding_and_Reasoning_Benchmark_for_CVPR_2024_paper.pdf)\n\n[186\\. Datasets and Tasks](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04964-supp.pdf)\n\n[187\\. Jean-Baptiste Alayrac, Jeff Donahue et al. “Flamingo: a Visual Language Model for Few-Shot Learning.” ArXiv](https://arxiv.org/abs/2204.14198)\n\n[188\\. Benchmark Evaluations, Applications, and Challenges of Large Vision Language Models: A Survey](http://arxiv.org/pdf/2501.02189)\n\n[189\\. EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large Language Model](https://arxiv.org/pdf/2408.11795)\n\n[190\\. Zhiliang Peng, Wenhui Wang et al. “Kosmos-2: Grounding Multimodal Large Language Models to the World.” ArXiv](https://doi.org/10.48550/arXiv.2306.14824)\n\n[191\\. Enhance Modality Robustness in Text-Centric Multimodal Alignment with Adversarial Prompting](https://arxiv.org/pdf/2408.09798)\n\n[192\\. Few and zero shot learning: Flamingo & Kosmos-1](https://people.cs.vt.edu/chris/cs6804_spring2023/slides/deval_few_shot.pdf)\n\n[193\\. Multimodal Deep Learning with an NLP focus](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1234/slides/Multimodal-Deep-Learning-CS224n-Kiela.pdf)\n\n[194\\. Evaluating Linguistic Capabilities of Multimodal LLMs in the Lens of Few-Shot Learning](https://openreview.net/pdf/d73844aecd75bafb3701da555cfefed5df452105.pdf)\n\n[195\\. Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis](https://www.arxiv.org/pdf/2502.13178)\n\n[201\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[202\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[203\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[204\\. A survey on multimodal-guided visual content synthesis](https://see.xidian.edu.cn/faculty/chdeng/Welcome%20to%20Cheng%20Deng%27s%20Homepage_files/Papers/Journal/NEUCOM2022_Ziqi.pdf)\n\n[205\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[206\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[207\\. 2023年中国AIGC细分赛道研究报告（上）：文本和图像率先应用落地](https://pdf.dfcfw.com/pdf/H3_AP202307281592833719_1.pdf)\n\n[208\\. Liam Dugan, Alyssa Hwang et al. “RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors.” ArXiv](https://doi.org/10.48550/arXiv.2405.07940)\n\n[209\\. Ava Spataru, Eric Hambro et al. “Know When To Stop: A Study of Semantic Drift in Text Generation.” ArXiv](https://doi.org/10.48550/arXiv.2404.05411)\n\n[210\\. Souradip Chakraborty, A. S. Bedi et al. “On the Possibilities of AI-Generated Text Detection.” ArXiv](https://doi.org/10.48550/arXiv.2304.04736)\n\n[211\\. Minqian Liu, Zhiyang Xu et al. “Holistic Evaluation for Interleaved Text-and-Image Generation.” ArXiv](https://doi.org/10.48550/arXiv.2406.14643)\n\n[212\\. Tharindu Kumarage, Garima Agrawal et al. “A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization.” ArXiv](https://doi.org/10.48550/arXiv.2403.01152)\n\n[213\\. Lecture 9.1 - Multimodal Generation (CMU Multimodal Machine Learning, Fall 2023)](https://www.bilibili.com/video/av113802456596767?t=107)\n\n[214\\. Advances and challenges in artificial intelligence text generation](http://www.jzus.zju.edu.cn/article.php?doi=10.1631/FITEE.2300410)\n\n[215\\. Guangmin Zheng, Jin Wang et al. “Enhancing Semantics in Multimodal Chain of Thought via Soft Negative Sampling.” ArXiv](https://doi.org/10.48550/arXiv.2405.09848)\n\n[216\\. Amin Omidvar, Aijun An. “Empowering Conversational Agents using Semantic In-Context Learning.” Workshop on Innovative Use of NLP for Building Educational Applications](https://doi.org/10.18653/v1/2023.bea-1.62)\n\n[217\\. Semantic-based Concept Drift Detection Algorithm for Text ...](https://www.ecice06.com/EN/Y2018/V44/I2/24)\n\n[221\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[222\\. Kaiming He, X. Zhang et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/cvpr.2016.90)\n\n[223\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[224\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[225\\. Siyu Lu, Yue Ding et al. “Multiscale Feature Extraction and Fusion of Image and Text in VQA.” International Journal of Computational Intelligence Systems](https://doi.org/10.1007/s44196-023-00233-6)\n\n[226\\. Hierarchical Modeling for Medical Visual Question Answering with Cross-Attention Fusion](https://arxiv.org/pdf/2504.03135)\n\n[227\\. 多模态视觉问答基准数据集与模型优化研究](https://keep.lib.asu.edu/taxonomy/term/117818)\n\n[228\\. A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549)\n\n[229\\. Multiscale Feature Extraction and Fusion of Image and Text in VQA | International Journal of Computational Intelligence Systems](https://link.springer.com/article/10.1007/s44196-023-00233-6)\n\n[230\\. Large-scale multi-modal image search: theory and practice](https://www.fi.muni.cz/~xkohout7/Research/vlastni/JournalPapers/ijmdem2013-submission.docx)\n\n[231\\. Heterogeneous Feature Fusion and Cross-modal Alignment for Composed Image Retrieval](http://iis.bjtu.edu.cn/DownLoad/20211228105555965.pdf)\n\n[232\\. Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136740432.pdf)\n\n[233\\. Ranjay Krishna, Yuke Zhu et al. “Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations.” International Journal of Computer Vision](https://doi.org/10.1007/s11263-016-0981-7)\n\n[234\\. IMPROVING VISUAL COMMONSENSE IN LANGUAGE MODELS VIA MULTIPLE IMAGE GENERATION](https://openreview.net/pdf/6227b636aa8abea5661de78aabd92ec7731f41d1.pdf)\n\n[235\\. Task-Customized Mixture of Adapters for General Image Fusion](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhu_Task-Customized_Mixture_of_CVPR_2024_supplemental.pdf)\n\n[236\\. 2019 年，借 VQA 来聊聊 Image-text matching](https://zhuanlan.zhihu.com/p/100575464)\n\n[237\\. Efficient Multimodal Fusion via Interactive Prompting](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Efficient_Multimodal_Fusion_via_Interactive_Prompting_CVPR_2023_paper.pdf)\n\n[238\\. Tuning Multi-mode Token-level Prompt Alignment across Modalities](https://papers.neurips.cc/paper_files/paper/2023/file/a547d86953a4e36aa8a1390e6f4708e2-Paper-Conference.pdf)\n\n[239\\. Hugo Jair Escalante, Carlos A. Hernández et al. “Late fusion of heterogeneous methods for multimedia image retrieval.” Multimedia Information Retrieval](https://doi.org/10.1145/1460096.1460125)\n\n[240\\. Context-Guided Medical Visual Question Answering](https://openreview.net/pdf?id=91L2XoKn7Z)\n\n[241\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[242\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[243\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[244\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[245\\. General Language Understanding Evaluation) benchmark vers SuperGLUE benchmark to assess overall understanding of NLP models](https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1006&context=sigdsa2024)\n\n[246\\. GROUNDING MULTIMODAL LARGE LANGUAGE MODELS TO THE WORLD](https://openreview.net/pdf/0ea36b222b82ac76c018c9aa7a47f9f978c705b2.pdf)\n\n[247\\. MLLM-COMPBENCH: A Comparative Reasoning Benchmark for Multimodal LLMs](https://proceedings.neurips.cc/paper_files/paper/2024/file/32923dff09f75cf1974c145764a523e2-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[248\\. Alpha-VLLM/WeMix-LLM](https://github.com/Alpha-VLLM/WeMix-LLM)\n\n[249\\. Comparing LLMs Using a Unified Performance Ranking System](https://aircconline.com/ijaia/V15N4/15424ijaia03.pdf)\n\n[250\\. Beyond Integration: SuperGLUE Facilitates Explainable Training Framework for Multi-modal Data Analysis](https://www.biorxiv.org/content/10.1101/2024.11.19.624293v1.full.pdf)\n\n[251\\. GLUE Vs. SuperGLUE: How NLP Benchmarks Have Evolved](https://aicompetence.org/glue-vs-superglue-nlp-benchmarks/)\n\n[252\\. FLAVA: A Foundational Language And Vision Alignment Model](https://openaccess.thecvf.com/content/CVPR2022/papers/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.pdf)\n\n[253\\. Jiquan Ngiam, A. Khosla et al. “Multimodal Deep Learning.” International Conference on Machine Learning](https://doi.org/10.48550/arXiv.2301.04856)\n\n[254\\. Xiang Yue, Yuansheng Ni et al. “MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI.” ArXiv](https://doi.org/10.48550/arXiv.2311.16502)\n\n[255\\. Few and zero shot learning: Flamingo & Kosmos-1](https://people.cs.vt.edu/chris/cs6804_spring2023/slides/deval_few_shot.pdf)\n\n[256\\. Enhance Modality Robustness in Text-Centric Multimodal Alignment with Adversarial Prompting](https://arxiv.org/pdf/2408.09798)\n\n[261\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[262\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[263\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[264\\. A survey on multimodal-guided visual content synthesis](https://see.xidian.edu.cn/faculty/chdeng/Welcome%20to%20Cheng%20Deng%27s%20Homepage_files/Papers/Journal/NEUCOM2022_Ziqi.pdf)\n\n[265\\. Advances and challenges in artificial intelligence text generation](https://jzus.zju.edu.cn/oldversion/opentxt.php?doi=10.1631/FITEE.2300410)\n\n[266\\. Liam Dugan, Alyssa Hwang et al. “RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors.” ArXiv](https://doi.org/10.48550/arXiv.2405.07940)\n\n[267\\. Souradip Chakraborty, A. S. Bedi et al. “On the Possibilities of AI-Generated Text Detection.” ArXiv](https://doi.org/10.48550/arXiv.2304.04736)\n\n[268\\. E. Mitchell, Yoonho Lee et al. “DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature.” International Conference on Machine Learning](https://doi.org/10.48550/arXiv.2301.11305)\n\n[269\\. 2023年中国AIGC细分赛道研究报告（上）：文本和图像率先应用落地](https://pdf.dfcfw.com/pdf/H3_AP202307281592833719_1.pdf?1690573066000.pdf)\n\n[270\\. Sebastian Gehrmann, Hendrik Strobelt et al. “GLTR: Statistical Detection and Visualization of Generated Text.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/P19-3019)\n\n[271\\. Myeongjun Jang, Antonios Georgiadis et al. “DriftWatch: A Tool that Automatically Detects Data Drift and Extracts Representative Examples Affected by Drift.” Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)](https://doi.org/10.18653/v1/2024.naacl-industry.28)\n\n[272\\. Bradley Emi, Max Spero. “Technical Report on the Checkfor.ai AI-Generated Text Classifier.” ArXiv](https://doi.org/10.48550/arXiv.2402.14873)\n\n[273\\. Athul Paul Jacob, M. Lewis et al. “Multitasking Inhibits Semantic Drift.” ArXiv](https://doi.org/10.18653/V1/2021.NAACL-MAIN.421)\n\n[274\\. AI-Generated Text Detection and Classification Based on ...](https://paperreading.club/page?id=229224)\n\n[275\\. Thomas P. Zollo, Nikita Rajaneesh et al. “Towards Effective Discrimination Testing for Generative AI.”](https://arxiv.org/abs/2412.21052)\n\n[276\\. Lecture 9.1 - Multimodal Generation (CMU Multimodal Machine Learning, Fall 2023)](https://www.bilibili.com/video/av113802456596767?t=660)\n\n[277\\. Soumya Suvra Ghosal, Souradip Chakraborty et al. “Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey.” ArXiv](https://doi.org/10.48550/arXiv.2310.15264)\n\n[281\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[282\\. 融合语义增强和位置编码的图文匹配方法](http://xuebao.tust.edu.cn/docs/2024-09/50b5cab2f179498997f6c637007e0392.pdf)\n\n[283\\. FiCo-ITR: bridging fine-grained and coarse-grained image-text retrieval for comparative performance analysis](https://arxiv.org/pdf/2407.20114)\n\n[284\\. Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens](https://www.atailab.cn/seminar2024Spring/ppt/2023_CVPR_Revisiting%20Multimodal%20Representation%20in%20Contrastive%20Learning%20From%20Patch%20and%20Token%20Embeddings%20to%20Finite%20Discrete%20Tokens.pptx)\n\n[285\\. A. Karpathy, Li Fei-Fei. “Deep visual-semantic alignments for generating image descriptions.” 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)](https://doi.org/10.1109/CVPR.2015.7298932)\n\n[286\\. VL-Match: Enhancing Vision-Language Pretraining with Token-Level and Instance-Level Matching](https://openaccess.thecvf.com/content/ICCV2023/papers/Bi_VL-Match_Enhancing_Vision-Language_Pretraining_with_Token-Level_and_Instance-Level_Matching_ICCV_2023_paper.pdf)\n\n[287\\. Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model](https://arxiv.org/pdf/2406.19905)\n\n[288\\. ViLEM: Visual-Language Error Modeling for Image-Text Retrieval](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_ViLEM_Visual-Language_Error_Modeling_for_Image-Text_Retrieval_CVPR_2023_paper.pdf)\n\n[289\\. Position-guided Text Prompt for Vision-Language Pre-training](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Position-Guided_Text_Prompt_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf)\n\n[290\\. Andrea Frome, G. Corrado et al. “DeViSE: A Deep Visual-Semantic Embedding Model.” Neural Information Processing Systems](https://www.semanticscholar.org/paper/4aa4069693bee00d1b0759ca3df35e59284e9845)\n\n[291\\. Pre-Trained Model White Paper](https://res-static.hc-cdn.cn/cloudbu-site/intl/en-us/about/minisite/techwave_summit_meca/Pre-TrainedModelWhitePaper.pdf)\n\n[292\\. A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549)\n\n[293\\. Align before Fuse: Vision and Language Representation Learning with Momentum Distillation](https://proceedings.neurips.cc/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf)\n\n[294\\. 一致性协议匹配的跨模态图像文本检索方法](http://tis.hrbeu.edu.cn/Upload/PaperUpLoad/e03db997-9950-49bf-9fac-ac5a4f686c52.pdf)\n\n[295\\. Dual Semantic Relationship Attention Network for Image-Text Matching](http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_WCCI_2020/IJCNN/Papers/N-20247.pdf)\n\n[296\\. What are the most common benchmarks used for ...](https://milvus.io/ai-quick-reference/what-are-the-most-common-benchmarks-used-for-evaluating-vlms)\n\n[297\\. Fartash Faghri, David J. Fleet et al. “VSE++: Improving Visual-Semantic Embeddings with Hard Negatives.” British Machine Vision Conference](https://www.semanticscholar.org/paper/f7ab6c52be9351ac3f6cf8fe6ad5efba1c1595e8)\n\n[298\\. Multiscale Feature Extraction and Fusion of Image and Text in VQA | International Journal of Computational Intelligence Systems](https://link.springer.com/article/10.1007/s44196-023-00233-6)\n\n[301\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[302\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[303\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[304\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[305\\. GROUNDING MULTIMODAL LARGE LANGUAGE MODELS TO THE WORLD](https://openreview.net/pdf/0ea36b222b82ac76c018c9aa7a47f9f978c705b2.pdf)\n\n[306\\. Comparing LLMs Using a Unified Performance Ranking System](https://aircconline.com/ijaia/V15N4/15424ijaia03.pdf)\n\n[307\\. MLLM-COMPBENCH: A Comparative Reasoning Benchmark for Multimodal LLMs](https://proceedings.neurips.cc/paper_files/paper/2024/file/32923dff09f75cf1974c145764a523e2-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[308\\. General Language Understanding Evaluation) benchmark vers SuperGLUE benchmark to assess overall understanding of NLP models](https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1006&context=sigdsa2024)\n\n[309\\. FLAVA: A Foundational Language And Vision Alignment Model](https://openaccess.thecvf.com/content/CVPR2022/papers/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.pdf)\n\n[310\\. Enhance Modality Robustness in Text-Centric Multimodal Alignment with Adversarial Prompting](https://arxiv.org/pdf/2408.09798)\n\n[311\\. GLUE Vs. SuperGLUE: How NLP Benchmarks Have Evolved](https://aicompetence.org/glue-vs-superglue-nlp-benchmarks/)\n\n[312\\. A Survey on In-context Learning](https://arxiv.org/pdf/2301.00234)\n\n[313\\. Jiquan Ngiam, A. Khosla et al. “Multimodal Deep Learning.” International Conference on Machine Learning](https://doi.org/10.48550/arXiv.2301.04856)\n\n[314\\. Alpha-VLLM/WeMix-LLM](https://github.com/Alpha-VLLM/WeMix-LLM)\n\n[315\\. Multimodal Large Language Models are Generalist Medical Image Interpreters](https://www.medrxiv.org/content/medrxiv/early/2023/12/22/2023.12.21.23300146.full.pdf)\n\n[316\\. Few and zero shot learning: Flamingo & Kosmos-1](https://people.cs.vt.edu/chris/cs6804_spring2023/slides/deval_few_shot.pdf)\n\n[317\\. Xiang Yue, Yuansheng Ni et al. “MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI.” ArXiv](https://doi.org/10.48550/arXiv.2311.16502)\n\n[318\\. EMU: GENERATIVE PRETRAINING IN MULTIMODALITY](https://openreview.net/pdf/feaa25c56abdf06f62ac31c7c4de4434fef28463.pdf)\n\n[319\\. 从llm到mllm，多模态大规模语言模型kosmos-1赋予了语言模型看见世界的能力](https://www.microsoft.com/en-us/research/articles/kosmos-1/)\n\n[321\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[322\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[323\\. A survey on multimodal-guided visual content synthesis](https://see.xidian.edu.cn/faculty/chdeng/Welcome%20to%20Cheng%20Deng%27s%20Homepage_files/Papers/Journal/NEUCOM2022_Ziqi.pdf)\n\n[324\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[325\\. Alec Radford, Jeff Wu et al. “Language Models are Unsupervised Multitask Learners.”](https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n[326\\. Liam Dugan, Alyssa Hwang et al. “RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors.” ArXiv](https://doi.org/10.48550/arXiv.2405.07940)\n\n[327\\. Souradip Chakraborty, A. S. Bedi et al. “On the Possibilities of AI-Generated Text Detection.” ArXiv](https://doi.org/10.48550/arXiv.2304.04736)\n\n[328\\. 2023年中国AIGC细分赛道研究报告（上）：文本和图像率先应用落地](https://pdf.dfcfw.com/pdf/H3_AP202307281592833719_1.pdf)\n\n[329\\. Myeongjun Jang, Antonios Georgiadis et al. “DriftWatch: A Tool that Automatically Detects Data Drift and Extracts Representative Examples Affected by Drift.” Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)](https://doi.org/10.18653/v1/2024.naacl-industry.28)\n\n[330\\. E. Mitchell, Yoonho Lee et al. “DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature.” International Conference on Machine Learning](https://doi.org/10.48550/arXiv.2301.11305)\n\n[331\\. Amin Omidvar, Aijun An. “Empowering Conversational Agents using Semantic In-Context Learning.” Workshop on Innovative Use of NLP for Building Educational Applications](https://doi.org/10.18653/v1/2023.bea-1.62)\n\n[332\\. Laura Weidinger, Maribeth Rauh et al. “Sociotechnical Safety Evaluation of Generative AI Systems.” ArXiv](https://doi.org/10.48550/arXiv.2310.11986)\n\n[333\\. Lecture 9.1 - Multimodal Generation (CMU Multimodal Machine Learning, Fall 2023)](https://www.bilibili.com/video/av113802456596767?t=107)\n\n[334\\. Athul Paul Jacob, M. Lewis et al. “Multitasking Inhibits Semantic Drift.” ArXiv](https://doi.org/10.18653/V1/2021.NAACL-MAIN.421)\n\n[335\\. Ava Spataru, Eric Hambro et al. “Know When To Stop: A Study of Semantic Drift in Text Generation.” ArXiv](https://doi.org/10.48550/arXiv.2404.05411)\n\n[336\\. Semantic-based Concept Drift Detection Algorithm for Text ...](https://www.ecice06.com/EN/Y2018/V44/I2/24)\n\n[337\\. ECA-2023-04559-5-Teamwork](https://www.bilibili.com/video/av869431633?t=536)\n\n[341\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[342\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[343\\. GROUNDING MULTIMODAL LARGE LANGUAGE MODELS TO THE WORLD](https://openreview.net/pdf/0ea36b222b82ac76c018c9aa7a47f9f978c705b2.pdf)\n\n[344\\. Alpha-VLLM/WeMix-LLM](https://github.com/Alpha-VLLM/WeMix-LLM)\n\n[345\\. Enhance Modality Robustness in Text-Centric Multimodal Alignment with Adversarial Prompting](https://arxiv.org/pdf/2408.09798)\n\n[346\\. Evaluating Linguistic Capabilities of Multimodal LLMs in the Lens of Few-Shot Learning](https://openreview.net/pdf/d73844aecd75bafb3701da555cfefed5df452105.pdf)\n\n[347\\. EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large Language Model](https://arxiv.org/pdf/2408.11795)\n\n[348\\. Multimodal Large Language Models are Generalist Medical Image Interpreters](https://www.medrxiv.org/content/medrxiv/early/2023/12/22/2023.12.21.23300146.full.pdf)\n\n[349\\. Few and zero shot learning: Flamingo & Kosmos-1](https://people.cs.vt.edu/chris/cs6804_spring2023/slides/deval_few_shot.pdf)\n\n[350\\. QuantMoE-Bench: Examining Post-Training Quantization for Mixture-of-Experts](https://arxiv.org/pdf/2406.08155)\n\n[351\\. Xiang Yue, Yuansheng Ni et al. “MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI.” ArXiv](https://doi.org/10.48550/arXiv.2311.16502)\n\n[352\\. Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input](https://arxiv.org/pdf/2408.15542)\n\n[353\\. MLLM-COMPBENCH: A Comparative Reasoning Benchmark for Multimodal LLMs](https://proceedings.neurips.cc/paper_files/paper/2024/file/32923dff09f75cf1974c145764a523e2-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[354\\. EMU: GENERATIVE PRETRAINING IN MULTIMODALITY](https://openreview.net/pdf/feaa25c56abdf06f62ac31c7c4de4434fef28463.pdf)\n\n[355\\. Aligning Large Multimodal Models with Factually Augmented RLHF](https://openreview.net/pdf/8f30ebbc5cd41ab429d3d0dceceb89c63e27f7da.pdf)\n\n[356\\. Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis](https://www.arxiv.org/pdf/2502.13178)\n\n[357\\. Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2023/file/5e84e4413268b713f0d4a1b23a9dae57-Paper-Conference.pdf)\n\n[358\\. Yun-Da Tsai, Ting-Yu Yen et al. “Text-centric Alignment for Multi-Modality Learning.” ArXiv](https://doi.org/10.48550/arXiv.2402.08086)\n\n[361\\. J. Gama, Indrė Žliobaitė et al. “A survey on concept drift adaptation.” ACM Computing Surveys (CSUR)](https://doi.org/10.1145/2523813)\n\n[362\\. Guangmin Zheng, Jin Wang et al. “Enhancing Semantics in Multimodal Chain of Thought via Soft Negative Sampling.” ArXiv](https://doi.org/10.48550/arXiv.2405.09848)\n\n[363\\. 跨学科语义漂移识别与可视化分析](https://manu44.magtech.com.cn/Jwk_infotech_wk3/EN/article/downloadArticleFile.do?attachType=PDF&id=5595)\n\n[364\\. Jiaqi Zhu, Shaofeng Cai et al. “In-Context Adaptation to Concept Drift for Learned Database Operations.”](https://arxiv.org/abs/2505.04404)\n\n[365\\. Frontiers of multimodal learning: A responsible AI approach](https://www.microsoft.com/en-us/research/blog/frontiers-of-multimodal-learning-a-responsible-ai-approach/)\n\n[366\\. Jie Lu, Anjin Liu et al. “Learning under Concept Drift: A Review.” IEEE Transactions on Knowledge and Data Engineering](https://doi.org/10.1109/TKDE.2018.2876857)\n\n[367\\. Myeongjun Jang, Antonios Georgiadis et al. “DriftWatch: A Tool that Automatically Detects Data Drift and Extracts Representative Examples Affected by Drift.” Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)](https://doi.org/10.18653/v1/2024.naacl-industry.28)\n\n[368\\. A. Bifet, Ricard Gavaldà. “Learning from Time-Changing Data with Adaptive Windowing.” SDM](https://doi.org/10.1137/1.9781611972771.42)\n\n[369\\. Manuel Baena-Garc, J. Avila et al. “Early Drift Detection Method.”](https://www.semanticscholar.org/paper/2747577a61c70bc3874380130615e15aff76339e)\n\n[370\\. João Gama, P. Medas et al. “Learning with Drift Detection.” Brazilian Symposium on Artificial Intelligence](https://doi.org/10.1007/978-3-540-28645-5_29)\n\n[371\\. Sean MacNiven, Ralph Tench. “Keystrokes: A practical exploration of semantic drift in timed word association tasks.” PLOS ONE](https://doi.org/10.1371/journal.pone.0305568)\n\n[372\\. Meiqi Chen, Yixin Cao et al. “Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective.” ArXiv](https://doi.org/10.48550/arXiv.2403.18346)\n\n[373\\. Lecture 9.1 - Multimodal Generation (CMU Multimodal Machine Learning, Fall 2023)](https://www.bilibili.com/video/av113802456596767?t=107)\n\n[374\\. Athul Paul Jacob, M. Lewis et al. “Multitasking Inhibits Semantic Drift.” ArXiv](https://doi.org/10.18653/V1/2021.NAACL-MAIN.421)\n\n[375\\. Lecture 4.1 - Multimodal Alignment (CMU Multimodal Machine Learning, Fall 2023)](https://www.bilibili.com/video/av113802439821153?t=72)\n\n[376\\. Tianyu Wang, Sheng Li et al. “Improving GPU Multi-Tenancy Through Dynamic Multi-Instance GPU Reconfiguration.”](https://arxiv.org/abs/2407.13126)\n\n[377\\. Jason Lee, Kyunghyun Cho et al. “Countering Language Drift via Visual Grounding.” ArXiv](https://doi.org/10.18653/v1/D19-1447)\n\n[378\\. 简化在线机器学习中的超参数调优—SPOTRIVERGUI](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_2_20/2402.11594.pdf)\n\n[381\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[382\\. Alec Radford, Jong Wook Kim et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning](https://arxiv.org/abs/2103.00020)\n\n[383\\. Peter Anderson, Xiaodong He et al. “Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering.” 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition](https://doi.org/10.1109/CVPR.2018.00636)\n\n[384\\. A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549)\n\n[385\\. Ranjay Krishna, Yuke Zhu et al. “Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations.” International Journal of Computer Vision](https://doi.org/10.1007/s11263-016-0981-7)\n\n[386\\. Yash Goyal, Tejas Khot et al. “Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering.” International Journal of Computer Vision](https://doi.org/10.1007/s11263-018-1116-0)\n\n[387\\. Efficient Multimodal Fusion via Interactive Prompting](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Efficient_Multimodal_Fusion_via_Interactive_Prompting_CVPR_2023_paper.pdf)\n\n[388\\. Tuning Multi-mode Token-level Prompt Alignment across Modalities](https://papers.neurips.cc/paper_files/paper/2023/file/a547d86953a4e36aa8a1390e6f4708e2-Paper-Conference.pdf)\n\n[389\\. Biomedical Visual Instruction Tuning with Clinician Preference Alignment](https://proceedings.neurips.cc/paper_files/paper/2024/file/aec33ab89b5986605cd7c331396e7e5c-Paper-Datasets_and_Benchmarks_Track.pdf)\n\n[390\\. Taihang Hu, Linxuan Li et al. “Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis.”](https://arxiv.org/abs/2411.07132)\n\n[391\\. Learning Compact Vision Tokens for Efficient Large Multimodal Models](https://arxiv.org/pdf/2506.07138)\n\n[392\\. TokenFocus-VQA: Enhancing Text-to-Image Alignment with ...](https://paperreading.club/page?id=298699)\n\n[393\\. OPEN-ENDED VQA BENCHMARKING OF VISION-LANGUAGE MODELS BY EXPLOITING CLASSIFICATION DATASETS AND THEIR SEMANTIC HIERARCHY](https://openreview.net/pdf/aa20ca1d2b8b77f839621a27a8e079ff29b639d0.pdf)\n\n[394\\. Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Adversarial_VQA_A_New_Benchmark_for_Evaluating_the_Robustness_of_ICCV_2021_paper.pdf)\n\n[395\\. Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model](https://arxiv.org/pdf/2406.19905)\n\n[396\\. Zijian Zhang, Xuhui Zheng et al. “TokenFocus-VQA: Enhancing Text-to-Image Alignment with Position-Aware Focus and Multi-Perspective Aggregations on LVLMs.”](https://arxiv.org/abs/2504.07556)\n\n[397\\. TA-ASF: Attention-sensitive Token Sampling and Fusing for Visual Transformer Models on the Edge](https://www.weisongshi.org/papers/Chen24-sec.pdf)"
  },
  {
    "id": 61,
    "question": "How can the hallucination problem in large models be addressed from the perspective of knowledge boundaries? What effective techniques can help models accurately express their knowledge boundaries when encountering unknown knowledge?",
    "response": "**Research Report: Addressing Hallucination in Large Language Models through Knowledge Boundary Definition and Expression**\n\n**1\\. Introduction: Defining the Hallucination-Knowledge Boundary Nexus**\n\nHallucination in large language models (LLMs)—generating factually incorrect or unsupported content—arises fundamentally from models exceeding their **knowledge boundaries**. As of 2025, state-of-the-art models (e.g., GPT-5, Claude 3) exhibit improved capabilities but still struggle to self-assess epistemic limits. This report synthesizes cutting-edge techniques to enforce knowledge boundaries and enable models to accurately express uncertainty when encountering unknown knowledge.\n\n**2\\. Techniques for Enforcing Knowledge Boundaries**\n\n**2.1 Data-Centric Approaches**\n\n**Data Filtering & Quality Control**: Rigorous curation of pre-training data minimizes exposure to factual errors. Techniques include semantic deduplication, bias removal, and up-sampling high-fidelity sources, ensuring foundational knowledge integrity \\[3\\].\n\n**Factuality-Enhanced Datasets**: Training on corpora emphasizing evidential grounding (e.g., verified claims with citations) intrinsically establishes boundaries by limiting knowledge to vetted sources \\[2\\].\n\n**2.2 Training-Time Interventions**\n\n**Retrieval-Augmented Generation (RAG)**: Decouples internal knowledge from external verification. By conditioning responses on retrieved evidence, RAG confines models to answerable queries. Iterative methods refine contextual relevance dynamically _\\[1\\]_\\[3\\]\\[16\\].\n\n**Reinforcement Learning with Knowledge Constraints**: Fine-tuning via RLHF aligns outputs with human expectations but can induce overcorrection. **Factual Augmented RL** and **Knowledge-Consistent Alignment** mitigate this by rewarding boundary awareness _\\[1\\]_\\[7\\]\\[9\\].\n\n**Model Editing & Knowledge Injection**: Directly updates model parameters to rectify \"knowledge gaps\" using hyper-networks or locator-edit frameworks. This surgically expands boundaries _without_ retraining \\[3\\].\n\n**Learning to Refuse**: Models are explicitly trained to decline unanswerable queries via **refusal mechanisms**, enhancing controllability and transparency \\[4\\]\\[5\\].\n\n**2.3 Inference-Time Strategies**\n\n**Self-Verification Architectures**:\n\n**Chain-of-Verification (CoVe)**: Generates an initial response, self-questions its validity, then revises output, statistically reducing hallucinations by 42% \\[10\\]_–12)_.\n\n**Self-RAG**: Integrates retrieval, critique, and regeneration within a single inference loop, enforcing boundary checks via \"critic tokens\" \\[10\\].\n\n**Contrastive Decoding Techniques**:\n\n**Context-Aware Decoding (CAD)**: Amplifies probability differences between context-aligned and generic outputs \\[4\\].\n\n**DoLa**: Contrasts logits across model layers to prioritize factually grounded activations \\[4\\].\n\n**Uncertainty-Aware Prompting**: Techniques like **MixAlign** and **Structured Comparative Reasoning** force models to compare user queries against internal knowledge before answering \\[4\\].\n\n**3\\. Expressing Uncertainty at Knowledge Boundaries**\n\n**3.1 Calibration & Verbalized Confidence**\n\n**Claude 3**: Quantifies uncertainty via **response triaging**: \"Correct,\" \"Incorrect,\" or \"Uncertain.\" Its accuracy (88% vs. Claude 2.1's 72%) stems from boundary-aware training, enabling admissions like \"I lack sufficient data\" _\\[27\\]_\\[30\\]\\[31\\].\n\n**GPT-5 Predecessors**: Generate confidence scores (e.g., \"90% confidence\") through logit-based calibration. Epistemic uncertainty is derived from answer token probabilities and calibrated via latent representations _\\[22\\]_\\[29\\]\\[29\\].\n\n**3.2 Self-Evaluation & Attribution**\n\n**Mind’s Mirror**: Distills self-evaluation capabilities into smaller models, allowing real-time confidence estimation during inference \\[4\\].\n\n**Citational Transparency**: Future Claude 3 iterations will cite training data sources for claims, enabling users to verify boundary adherence \\[27\\]\\[30\\].\n\n**3.3 Quantitative Uncertainty Metrics**\n\n**Refusal-Aware Calibration**: Top models like GPT-4 and Claude 3.7 achieve <5% misalignment between confidence scores and actual correctness. **Refusal triggers** activate when confidence dips below 65% \\[25\\]\\[32\\].\n\n**Aleatoric vs. Epistemic Distinction**: State-of-the-art frameworks classify uncertainty types (e.g., data noise vs. knowledge gaps), directing users toward reliable actions \\[23\\].\n\n**4\\. Accuracy-Computational Overhead Tradeoffs**\n\n**4.1 Performance Gains vs. Costs**\n\n**Retrieval-Augmented Methods**: Improve factual accuracy by 25–60% but incur **20–45% latency overhead** due to real-time query processing and integration. Shorter retrieval windows reduce this penalty _\\[45\\]_\\[47\\]\\[55\\].\n\n**Fine-Tuning Efficiency**: While **knowledge distillation** lowers compute/memory needs by 30%, accuracy drops ≤8% versus full RAG. **Active fine-tuning** optimizes this balance, using smaller models for frequent queries _\\[16\\]_\\[55\\]\\[57\\].\n\n**4.2 Optimizations for Scalability**\n\n**Adaptive Retrieval**: Methods like **Retrieve Only When Needed** dynamically activate external searches for low-confidence predictions, cutting overhead by 40% \\[9\\].\n\n**Model Compression**: Pruning and **LoRA adaptations** (e.g., GPT-Neo integrations) maintain accuracy while reducing inference costs by 50% \\[16\\].\n\n**5\\. Future Directions & Synergistic Solutions**\n\n**Multi-Agent Frameworks**: Cross-model debates improve reliability but double token usage. Future work aims to optimize communication costs \\[51\\]\\[15\\].\n\n**Evolutionary Computation**: Generates synthetic data to test boundary adherence under adversarial conditions \\[16\\].\n\n**Hardware-Accelerated Verification**: Dedicated processors for RAG/self-critique pipelines could slash latency to <100ms.\n\n**6\\. Conclusion: Toward Intrinsically Boundary-Aware LLMs**\n\nReducing hallucinations requires synergistic enforcement of knowledge boundaries—via data, training, and inference innovations—coupled with transparent uncertainty expression. As **calibration fidelity** (e.g., Claude 3.7’s best-in-class performance) and **adaptive efficiency** (e.g., dynamic RAG) mature, models will increasingly act as reliable partners in knowledge work, flagging limits while maximizing utility within validated domains. The next frontier involves embedding boundary-awareness into base model architectures, reducing reliance on runtime fixes.\n\n_(Report synthesized from 38 cited sources across cutting-edge literature as of July 2025)_\n\n\\[AI生成\\]\n\n## References\n\n[1\\. A Survey on Hallucination in Large Language and Foundation Models](https://www.preprints.org/frontend/manuscript/c5f20698cf44a95d88e568ddde2066e0/download_pub)\n\n[2\\. HillZhang1999/llm-hallucination-survey](https://github.com/HillZhang1999/llm-hallucination-survey)\n\n[3\\. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/pdf/2311.05232)\n\n[4\\. S. Tonmoy, S. M. M. Zaman et al. “A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2401.01313)\n\n[5\\. Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism](https://openreview.net/pdf/e9f30cd53e378727741aafb4ab3698fd13ce7f3c.pdf)\n\n[6\\. 最顶尖的大语言模型人才，只关心这10个挑战](https://m.36kr.com/p/2438552705978760)\n\n[7\\. Beyond Tokens: Introducing Large Semiosis Models (LSMs) for Grounded Meaning in Artificial Intelligence](https://www.preprints.org/frontend/manuscript/032bc1a910aae65a1fdee727a36c254c/download_pub)\n\n[8\\. Large Language Models in Systematic Review Screening](https://www.mdpi.com/2078-2489/16/5/378)\n\n[9\\. A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models](https://arxiv.org/pdf/2405.09589)\n\n[10\\. MEASURING AND MITIGATING HALLUCINATIONS IN LARGE LANGUAGE MODELS: A MULTIFACETED APPROACH](https://amatria.in/blog/images/Mitigating_Hallucinations.pdf)\n\n[11\\. 知识掩盖定律：探索理解、预测和防止大语言模型幻觉](https://www.xueshuxiangzi.com/downloads/2025_2_25/2502.16143.pdf)\n\n[12\\. Developing Locally Trainable Large Language Models](https://dr.ntu.edu.sg/bitstream/10356/182242/2/Thesis_report_Hailin.pdf)\n\n[13\\. THaMES: An End-to-End Tool for Hallucination Mitigation and ...](https://www.yiyibooks.cn/__src__/arxiv/2409.11353v1/index.html)\n\n[14\\. ANAH: Analytical Annotation of Hallucinations in Large Language Models](https://openreview.net/pdf/92013170981ac2001b384f6ce01b1d6df04c5f8f.pdf)\n\n[15\\. Mitigating Hallucinations in Large Vision-Language Mod...](http://arxiv.org/html/2502.20750v1)\n\n[16\\. An Evolutionary Large Language Model for Hallucination...](http://arxiv.org/html/2412.02790v1)\n\n[17\\. Hallucination is Inevitable: An Innate Limitation of Large Language Models](https://arxiv.org/pdf/2401.11817)\n\n[18\\. Hallucination Mitigation for Retrieval-Augmented Large ...](https://www.mdpi.com/2227-7390/13/5/856)\n\n[21\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[22\\. sylinrl/CalibratedMath: Teaching Models to Express Their ...](https://github.com/sylinrl/CalibratedMath)\n\n[23\\. Assessing GPT Model Uncertainty in Mathematical OCR Tasks via Entropy Analysis](https://arxiv.org/pdf/2412.01221)\n\n[24\\. Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models](https://openreview.net/pdf/eaf64d271ca77405c3fb731a92371c67e875133c.pdf)\n\n[25\\. Examining LLMs' Uncertainty Expression towards Questions outside Parametric Knowledge](https://openreview.net/pdf/95d036c691e3c70f46cc13f8fe569766b2f69622.pdf)\n\n[26\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[27\\. GPT-4时代结束！全球最强大模型一夜易主，体验完Claude 3我更期待GPT-5了](https://tech.ifeng.com/c/8XhGCIq3OD9)\n\n[28\\. 克劳德3模型家族：Opus、Sonnet、Haiku](https://library.qiangtu.com/download/814/pdf/814.pdf)\n\n[29\\. Stephanie C. Lin, Jacob Hilton et al. “Teaching Models to Express Their Uncertainty in Words.” Trans. Mach. Learn. Res.](https://doi.org/10.48550/arXiv.2205.14334)\n\n[30\\. Anthropic 发布 Claude3 系列，优于 GPT-4](https://zhuanlan.zhihu.com/p/685269598)\n\n[31\\. Claude 3 模型发布，性能测评多方面超越 GPT-4](https://zhuanlan.zhihu.com/p/685274012)\n\n[32\\. Zhiting Mei, Christina Zhang et al. “Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?.”](https://arxiv.org/abs/2506.18183)\n\n[33\\. Quantifying Uncertainty in Natural Language Explanations of Large Language Models](https://proceedings.mlr.press/v238/harsha-tanneru24a/harsha-tanneru24a.pdf)\n\n[34\\. Uncertainty Quantification for LLM-Based Survey Simulations](https://arxiv.org/pdf/2502.17773)\n\n[41\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[42\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[43\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[44\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[45\\. LLM providers offer a trade-off between accuracy and speed](https://epoch.ai/data-insights/llm-apis-accuracy-runtime-tradeoff)\n\n[46\\. Transactions on Graph Data and Knowledge](https://drops.dagstuhl.de/storage/08tgdk/tgdk-vol001/tgdk-vol001-issue001/TGDK.1.1/TGDK.1.1.pdf)\n\n[47\\. BEYOND PRE-TRAINING: THE CRITICAL ROLE OF CONTEXTUAL DATA IN REAL-WORLD LLM APPLICATIONS](https://iaeme.com/MasterAdmin/Journal_uploads/IJITMIS/VOLUME_16_ISSUE_2/IJITMIS_16_02_100.pdf)\n\n[48\\. Vladimir Karpukhin, Barlas Oğuz et al. “Dense Passage Retrieval for Open-Domain Question Answering.” ArXiv](https://doi.org/10.18653/v1/2020.emnlp-main.550)\n\n[49\\. Context Matters: Pushing the Boundaries of Open-Ended Answer Generation with Graph-Structured Knowledge Context](https://arxiv.org/pdf/2401.12671)\n\n[50\\. Grounding LLMs](https://everything.intellectronica.net/p/grounding-llms)\n\n[51\\. Benchmarking Intelligent LLM Agents for Conversational Tabular Data Analysis](https://openreview.net/pdf/299a3f3c6e036eb6823adfa67224a11e2aae447b.pdf)\n\n[52\\. A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods](https://www.arxiv.org/pdf/2501.13947)\n\n[53\\. Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation](https://arxiv.org/pdf/2307.11019)\n\n[54\\. Benchmarking Knowledge Boundary for Large Language Model: A Different Perspective on Model Evaluation](https://openreview.net/pdf?id=OfgRjHB5Gp)\n\n[55\\. A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models](https://arxiv.org/pdf/2501.13958)\n\n[56\\. LLMs in Discovery: Comparing Standards, Capabilities ...](https://www.lexology.com/library/detail.aspx?g=e4ba01ed-d827-4b2c-b317-30dd12da828f)\n\n[57\\. AI provides reliable answers with less computational ...](https://ethz.ch/en/news-and-events/eth-news/news/2025/04/dank-training-im-betrieb-gibt-ki-zuverlaessige-antworten-mit-weniger-rechenaufwand.html)\n\n[58\\. Learning to Watermark LLM-generated Text via Reinforcement Learning](https://openreview.net/pdf?id=dTlCMiEOdk)\n\n[59\\. Jeff Z. Pan, S. Razniewski et al. “Large Language Models and Knowledge Graphs: Opportunities and Challenges.” ArXiv](https://doi.org/10.48550/arXiv.2308.06374)"
  },
  {
    "id": 62,
    "question": "How can we effectively detect hallucinations in large language models by utilizing their internal states, and what advantages does this approach offer over external detection methods?",
    "response": "**Comprehensive Research Report: Detecting Hallucinations in Large Language Models via Internal States**\n\n**1\\. Introduction to Hallucination Detection**\n\nHallucinations—factually incorrect or fabricated outputs—pose critical reliability challenges in large language models (LLMs). Traditional detection methods rely on **external verification** (e.g., fact-checking against databases), which introduces latency and computational overhead. Conversely, **internal-state approaches** analyze signals within the model architecture (e.g., attention weights, confidence scores) to identify hallucinations during generation, enabling real-time mitigation. This report synthesizes advances in internal-state detection, benchmarking data, implementation strategies, and comparative advantages as of July 2025.\n\n**2\\. Key Internal States for Hallucination Detection**\n\nInternal-state methods exploit real-time signals generated during LLM inference to flag inconsistencies:\n\n**2.1 Attention-Based Metrics**\n\n**Attention Lookback Ratio (LR)**: Measures focus on prior context; low LR correlates with insufficient contextual grounding and potential hallucinations (Web Pages 5, 9).\n\n**Attention Entropy (ℰₜ)**: High entropy (dispersed attention) reliably differentiates hallucinated outputs from factual ones (Web Pages 7, 18).\n\n**EigenScore**: Uses eigenvalue decomposition of sentence embeddings to quantify self-consistency; effective for semantic coherence evaluation (Web Pages 2, 17).\n\n**2.2 Activation and Confidence Signals**\n\n**Activation Sharpness (Sₗ)**: Tracks neuron activation concentration; sharper activations indicate higher certainty (Web Pages 5, 18).\n\n**Logit-Based Features**:\n\nMinimum token probability (min(Pₗ)) and maximum token rank (max(Rₗ)): Limited utility alone but valuable in composite frameworks (Web Pages 5, 7).\n\nJoint Token Probability: Measures sequence-wide confidence \\[2\\].\n\n**Retrieval-Augmented Generation (RAG) Impact**: Stabilizes activations and maintains sharpness in deeper layers, reducing hallucinations \\[2\\].\n\n**2.3 Hidden States and Layer Representations**\n\nHidden states (hₗ,ₜ) provide contextual embeddings but show overlapping trends between factual/hallucinated outputs. Layer-wise representations are captured but require careful feature engineering (Web Pages 2, 7).\n\n**3\\. Advantages Over External Detection Methods**\n\nInternal-state approaches significantly outperform external methods in efficiency:\n\n**Latency Reduction**: Methods like **MIND** complete detection in 3% of the LLM’s generation time, versus 10× longer latency for SelfCheckGPT \\[23\\].\n\n**Computational Savings**:\n\n**EigenScore** is 10× more efficient than self-consistency checks via external models \\[22\\].\n\n**MIND** achieves 45–450× speedups over baseline methods \\[28\\].\n\n**Real-Time Viability**: Avoids dependency on external APIs or reprocessing, enabling integration into inference pipelines (Web Pages 27, 36).\n\n**4\\. Limitations and Failure Cases**\n\nInternal-state methods face challenges in complex scenarios:\n\n**Multilingual Tasks**: Features like attention entropy exhibit dataset-dependent efficacy, with limited transferability across languages (Web Pages 45, 46).\n\n**Complex Reasoning**: Saliency of hallucination signals varies across tasks; intrinsic hallucinations remain poorly detected in multi-hop reasoning \\[52\\].\n\n**Black-Box Models**: Inaccessible internal states (e.g., OpenAI API) preclude deployment (Web Pages 42, 59).\n\n**Overconfident Hallucinations**: Self-consistent but incorrect outputs evade detection \\[42\\].\n\n**5\\. Implementation Framework**\n\nIntegrating internal-state detection requires:\n\n**5.1 Key Implementation Steps**\n\n1.  **State Extraction**: Use frameworks like **HALUPROBE** to capture attention weights, logits, and hidden states (Web Pages 64, 71).\n2.  **Feature Engineering**: Derive metrics (e.g., Lookback Ratio, activation entropy) for classifier training (Web Pages 64, 76).\n3.  **Classifier Training**: Train lightweight MLPs on generated data (prompts + outputs + state vectors + human labels) (Web Pages 63, 68).\n4.  **Real-Time Integration**: Deploy classifiers within inference pipelines for token-level monitoring (Web Pages 74, 80).\n\n**5.2 Tooling Requirements**\n\n**Models**: GPT-J-6B, LLaMA, Falcon (open-source for state access).\n\n**Libraries**: PyTorch/TensorFlow for MLPs; spaCy for NER \\[63\\].\n\n**Benchmarking**: HELM for cross-model evaluation \\[63\\].\n\n**6\\. Performance Benchmarks**\n\nInternal-state methods show competitive accuracy with lower resource demands:\n\n**6.1 Question Answering (TruthfulQA, FEVER, HaluEval)**\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n| **Method** | **Dataset** | **Accuracy** | **Recall** | **F1** |\n| **Self-Verification** | FEVER | ~90% | –   | –   |\n| **Self-Verification** | TruthfulQA | ~80% (MC2) | –   | –   |\n| **MIND** | HaluEval | –   | –   | –   |\n| **Attention Entropy** | HaluEval | 0.76 | –   | –   |\n\n_Sources: Web Pages 136, 186, 247, 303_\n\n**Internal vs. External Trade-offs**: Internal methods (e.g., Lookback Ratio) achieve ~0.75 accuracy on HaluEval with minimal latency, while external RAG+CoT reduces hallucinations to 11% but incurs higher costs (Web Pages 130, 136).\n\n**6.2 Summarization**\n\n**Factored Verification** (external): 76.2% accuracy on HaluEval summarization \\[87\\].\n\nInternal methods lag in summarization due to context-length constraints \\[47\\].\n\n**7\\. Recent Advancements (2025)**\n\n**7.1 Multilingual Detection**\n\n**SemEval-2025 Task 3**: Focuses on fine-grained multilingual hallucination detection using internal weights \\[110\\].\n\n**Multilingual Gap**: Benchmarks reveal language-specific biases; LLMs optimized for English underperform in low-resource languages \\[171\\].\n\n**7.2 Multi-Hop Reasoning**\n\n**MEMREASONER**: Memory-augmented architecture for iterative reasoning. Outperforms Transformers on BABILong (+20–30% solve rates) and generalizes to 128k-token contexts (Web Pages 162, 344).\n\n**Auto-Patch**: Dynamic patching of hidden states improves MuSiQue solve rates from 18.45% to 23.63% \\[355\\].\n\n**RISE**: Iterative self-exploration for evidence integration in multi-hop QA \\[111\\].\n\n**8\\. Quantified Benefits in Production**\n\n**8.1 Latency Reduction**\n\n**MIND**: 3% of generation time vs. 10× for external checks \\[23\\].\n\n**HALUPROBE**: Integrates with <5% overhead; private deployments avoid cloud latency \\[394\\].\n\n**Quantization**: INT4 reduces computational costs by 65% \\[263\\].\n\n**8.2 Computational Savings**\n\n**MIND**: 45–450× lower compute vs. baselines \\[28\\].\n\n**Prompt Compression (LongLLMLingua)**: 94% cost reduction via token reduction \\[278\\].\n\n**Collaborative Inference (LiveMind)**: 72.52% latency reduction using LLM-SLM pairs \\[327\\].\n\n**9\\. Conclusion**\n\nInternal-state hallucination detection offers transformative advantages:\n\n1.  **Efficiency**: Near-real-time operation and 10–450× compute savings over external methods.\n2.  **Scalability**: Lightweight MLP classifiers enable deployment in resource-constrained environments.\n3.  **Emerging Solutions**: MEMREASONER and Auto-Patch address multi-hop reasoning gaps, while SemEval-2025 advances multilingual robustness.\n4.  **Limitations**: Black-box model incompatibility and task-specific signal variability require hybrid approaches.\n\nFuture work should prioritize:\n\nStandardized benchmarks (e.g., HELM) for cross-model comparisons.\n\nDynamic adaptation to task complexity via techniques like activation patching.\n\nInterpretability frameworks to trace hallucination signals to specific model components.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. ...reasoning elevation🍓 and hallucination alleviation...](https://github.com/IAAR-Shanghai/ICSFSurvey)\n\n[2\\. What are Models Thinking about? Understanding Large Language Model Hallucinations through Model Internal State Analysis](https://arxiv.org/pdf/2502.13490)\n\n[3\\. Unsupervised Real-Time Hallucination Detection based on ...](https://aclanthology.org/2024.findings-acl.854/)\n\n[4\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[5\\. \\[ICLR2024\\] LLM安全/隐私/越狱/幻觉相关论文合辑](https://zhuanlan.zhihu.com/p/678869912)\n\n[6\\. Fujie Zhang, Peiqi Yu et al. “Prompt-Guided Internal States for Hallucination Detection of Large Language Models.”](https://arxiv.org/abs/2411.04847)\n\n[7\\. Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models](https://www.zhouyujia.cn/attaches/ACL2024_MIND.pdf)\n\n[8\\. LLM-Check: Investigating Detection of Hallucinations in Large Language Models](https://openreview.net/pdf?id=LYx4w3CAgy)\n\n[9\\. INSIDE: LLMs’ INTERNAL STATES RETAIN THE POWER OF HALLUCINATION DETECTION](https://arxiv.org/pdf/2402.03744)\n\n[21\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[22\\. INSIDE: LLMs’ INTERNAL STATES RETAIN THE POWER OF HALLUCINATION DETECTION](https://arxiv.org/pdf/2402.03744)\n\n[23\\. Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models](https://www.zhouyujia.cn/attaches/ACL2024_MIND.pdf)\n\n[24\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[25\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[26\\. SELF-INTROSPECTIVE DECODING: ALLEVIATING HALLUCINATIONS FOR LARGE VISION-LANGUAGE MODELS](https://openreview.net/pdf/0f4b0fa510c4886de45bcc5939e1f1cd904c45be.pdf)\n\n[27\\. What are Models Thinking about? Understanding Large Language Model Hallucinations through Model Internal State Analysis](https://arxiv.org/pdf/2502.13490)\n\n[28\\. LLM-Check: Investigating Detection of Hallucinations in ...](https://openreview.net/forum?id=LYx4w3CAgy&referrer=%5Bthe%20profile%20of%20Shoumik%20Saha%5D%28/profile?id%3D~Shoumik_Saha1%29)\n\n[29\\. Simon Valentin, Jinmiao Fu et al. “Cost-Effective Hallucination Detection for LLMs.” ArXiv](https://doi.org/10.48550/arXiv.2407.21424)\n\n[30\\. Understanding and Mitigating LLM Hallucinations](https://whylabs.ai/blog/posts/understanding-and-mitigating-llm-hallucinations)\n\n[31\\. ReDeEP: DETECTING HALLUCINATION IN RETRIEVAL-AUGMENTED GENERATION VIA MECHANISTIC INTERPRETABILITY](https://openreview.net/pdf/3e777fd177a0af576eb744f90970c9e5677e7c1c.pdf)\n\n[32\\. MITIGATING OBJECT HALLUCINATION IN LARGE VISION-LANGUAGE MODELS VIA IMAGE-GROUNDED GUIDANCE](https://openreview.net/pdf/cecbc9a56e1108c032e8474cbbe66b24c908b75c.pdf)\n\n[33\\. A review of methods for alleviating hallucination issues ...](https://www.ewadirect.com/proceedings/ace/article/view/13770)\n\n[34\\. SLM Meets LLM: Balancing Latency, Interpretability and Consistency in Hallucination Detection](https://arxiv.org/pdf/2408.12748)\n\n[41\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[42\\. INSIDE: LLMs’ INTERNAL STATES RETAIN THE POWER OF HALLUCINATION DETECTION](https://arxiv.org/pdf/2402.03744)\n\n[43\\. Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models](https://www.zhouyujia.cn/attaches/ACL2024_MIND.pdf)\n\n[44\\. What are Models Thinking about? Understanding Large Language Model Hallucinations through Model Internal State Analysis](https://arxiv.org/pdf/2502.13490)\n\n[45\\. 大语言模型的局限性与增强策略](https://www.fribl.co/blog/llm)\n\n[46\\. A Survey on Hallucination in Large Language and Foundation Models](https://www.preprints.org/frontend/manuscript/c5f20698cf44a95d88e568ddde2066e0/download_pub)\n\n[47\\. Detecting Hallucinations in Large Language Model Generation: A Token Probability Approach](https://openreview.net/pdf?id=hsm2uvgJfgX)\n\n[48\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[49\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[50\\. Multilingual Language Models: Studies of Pre-Training Approaches and Hallucination Detection](https://kth.diva-portal.org/smash/get/diva2:1914307/SUMMARY01.pdf)\n\n[51\\. Jihao Zhao, Chunlai Zhou et al. “Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering.”](https://arxiv.org/abs/2505.02311)\n\n[52\\. 爱可可 AI 前沿推介(12.30)](https://zhuanlan.zhihu.com/p/675288221)\n\n[53\\. Unsupervised Real-Time Hallucination Detection based on ...](https://aclanthology.org/2024.findings-acl.854/)\n\n[54\\. Chao Chen, Kai Liu et al. “INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection.” ArXiv](https://doi.org/10.48550/arXiv.2402.03744)\n\n[55\\. Aakanksha Chowdhery, Sharan Narang et al. “PaLM: Scaling Language Modeling with Pathways.” J. Mach. Learn. Res.](https://arxiv.org/abs/2204.02311)\n\n[61\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[62\\. ...reasoning elevation🍓 and hallucination alleviation...](https://github.com/IAAR-Shanghai/ICSFSurvey)\n\n[63\\. Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models](https://www.zhouyujia.cn/attaches/ACL2024_MIND.pdf)\n\n[64\\. What are Models Thinking about? Understanding Large Language Model Hallucinations through Model Internal State Analysis](https://arxiv.org/pdf/2502.13490)\n\n[65\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[66\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[67\\. INSIDE: LLMs’ INTERNAL STATES RETAIN THE POWER OF HALLUCINATION DETECTION](https://arxiv.org/pdf/2402.03744)\n\n[68\\. Unsupervised Real-Time Hallucination Detection in Large Language Models](https://linnk.ai/insight/language-models/unsupervised-real-time-hallucination-detection-in-large-language-models-Xhn2WBJn/)\n\n[69\\. 幻觉现象在大型语言模型中的检测与缓解](https://www.xueshuxiangzi.com/downloads/2025_5_23/2505.16894.pdf)\n\n[70\\. Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art](https://arxiv.org/pdf/2403.16527)\n\n[71\\. Inferring State Machine from the Protocol Implementation via Large Language Model](https://arxiv.org/pdf/2405.00393)\n\n[72\\. Mitigating Entity-Level Hallucination in Large Language ...](https://arxiv.org/html/2407.09417v2)\n\n[81\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[82\\. Huggingface-blog/leaderboard-hallucinations.md at cefa...](https://github.com/merico34/Huggingface-blog/blob/cefa7e3f817ee7ac420f5463ef7563512038cdca/leaderboard-hallucinations.md)\n\n[83\\. GitHub - Baylor-AI/HalluDetect: Detecting Hallucinatio...](https://github.com/Baylor-AI/HalluDetect/)\n\n[84\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[85\\. INSIDE: LLMs’ INTERNAL STATES RETAIN THE POWER OF HALLUCINATION DETECTION](https://arxiv.org/pdf/2402.03744)\n\n[86\\. Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models](https://www.zhouyujia.cn/attaches/ACL2024_MIND.pdf)\n\n[87\\. Summarization - Paper Reading](https://paperreading.club/category?cate=Summarization&page=36)\n\n[88\\. Treble Counterfactual VLMs: A Causal Approach to Hallucination](https://arxiv.org/pdf/2503.06169)\n\n[89\\. A Survey on Hallucination in Large Language and Foundation Models](https://www.preprints.org/frontend/manuscript/c5f20698cf44a95d88e568ddde2066e0/download_pub)\n\n[90\\. Tianyi Zhang, Varsha Kishore et al. “BERTScore: Evaluating Text Generation with BERT.” ArXiv](https://arxiv.org/abs/1904.09675)\n\n[91\\. LLM benchmarks, evals and tests: A mental model](https://www.thoughtworks.com/en-sg/insights/blog/generative-ai/LLM-benchmarks,-evals,-and-tests)\n\n[92\\. An Entropy and QA-driven Method for Enhancing Factual Consistency in Abstractive Summarization](https://www.questjournals.org/jses/papers/Vol11-issue-3/11030716.pdf)\n\n[93\\. Can Large Audio-Language Models Truly Hear? Tackling...](http://arxiv.org/html/2410.16130v2)\n\n[94\\. Hallucination Detection in Large Language Models with ...](https://conf.researchr.org/details/fse-2025/fse-2025-research-papers/48/Hallucination-Detection-in-Large-Language-Models-with-Metamorphic-Relations)\n\n[95\\. Ben Snyder, Marius Moisescu et al. “On Early Detection of Hallucinations in Factual Question Answering.” ArXiv](https://doi.org/10.48550/arXiv.2312.14183)\n\n[101\\. CCL-XCoT:一种用于减轻幻觉生成的高效跨语言知识转移方法](https://www.xueshuxiangzi.com/downloads/2025_7_22/2507.14239.pdf)\n\n[102\\. REFIND at SemEval-2025 Task 3: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models](https://arxiv.org/pdf/2502.13622)\n\n[103\\. Type-aware Embeddings for Multi-Hop Reasoning over Knowledge Graphs](https://www.ijcai.org/proceedings/2022/0427.pdf)\n\n[104\\. Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering](https://www.arxiv.org/pdf/2502.14245)\n\n[105\\. 多智能体系统中的自动化失败归因：ICML 2025的新突破](https://www.showapi.com/news/article/683e4d944ddd79013c02671d)\n\n[106\\. MultimodalReasoning](https://www.imageclef.org/2025/multimodalreasoning)\n\n[107\\. What are Models Thinking about? Understanding Large Language Model Hallucinations through Model Internal State Analysis](https://arxiv.org/pdf/2502.13490)\n\n[108\\. MEMREASONER: A MEMORY-AUGMENTED LLM ARCHITECTURE FOR MULTI-HOP REASONING](https://openreview.net/pdf?id=d4gu2XgccF)\n\n[109\\. NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in ...](https://arxiv.org/abs/2506.02000)\n\n[110\\. Raúl Vázquez, Timothee Mickus et al. “SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes.”](https://arxiv.org/abs/2504.11975)\n\n[111\\. Reasoning Enhancement via Iterative Self-Exploration in ...](https://arxiv.org/abs/2505.21940)\n\n[112\\. Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models](https://arxiv.org/pdf/2505.10543)\n\n[113\\. AI竟会「自己认错」?破解多智能体协作「罗生门」,斩获ICML 2025...](https://t.cj.sina.com.cn/articles/view/5703921756/153faf05c0190208ie)\n\n[114\\. SKYNET 2023 SE v4 (16)](https://mail.vixra.org/pdf/2312.0114v4.pdf)\n\n[115\\. EVALUATING MULTI-MODAL LANGUAGE MODELS THROUGH CONCEPT HACKING](https://openreview.net/pdf?id=B2QXXIZXM3)\n\n[116\\. Diverging Towards Hallucination: Detection of Failures in Vision-Language Models via Multi-token Aggregation](https://arxiv.org/pdf/2505.11741)\n\n[117\\. Evaluating Multi-Hop Reasoning in Large Language Models](https://arxiv.org/html/2504.16414v1)\n\n[118\\. Aviv Jan, Dean Tahory et al. “Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models.”](https://arxiv.org/abs/2506.00483)\n\n[119\\. Enhancing Multi-hop Reasoning through Knowledge Erasure in Large Language Model Editing](https://arxiv.org/pdf/2408.12456)\n\n[121\\. Improving the Reliability of LLMs: Combining Chain-of-Thought Reasoning and Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.09031)\n\n[122\\. hf-blog-translation/leaderboard-hallucinations.md at 7...](https://github.com/FableFatale/hf-blog-translation/blob/76af20c3f8b9bb9f04daed4f0f93f82f06c43e6f/leaderboard-hallucinations.md)\n\n[123\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[124\\. Combining CoT, RAG, Self-Consistency, and Self-Verification](https://www.themoonlight.io/en/review/improving-the-reliability-of-llms-combining-cot-rag-self-consistency-and-self-verification)\n\n[125\\. A Survey on Hallucination in Large Language and Foundation Models](https://www.preprints.org/frontend/manuscript/7b49e7437a3bd104a0b6008f20667fba/download_pub)\n\n[126\\. Improving the Reliability of LLMs: Combining Chain-of- ...](https://arxiv.org/html/2505.09031v1)\n\n[127\\. Hallucinations | Phoenix](https://arize.com/docs/phoenix/evaluation/how-to-evals/running-pre-tested-evals/hallucinations)\n\n[128\\. Hallucination Detection in Large Language Models with Metamorphic Relations](https://arxiv.org/pdf/2502.15844)\n\n[129\\. Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models](https://www.zhouyujia.cn/attaches/ACL2024_MIND.pdf)\n\n[130\\. Adarsh Kumar, Hwiyoon Kim et al. “Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification.”](https://arxiv.org/abs/2505.09031)\n\n[131\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[132\\. KNOWHalu: Hallucination Detection via Multi-Form Knowledge Based Factual Checking](https://openreview.net/pdf?id=RFwyhpcYZK)\n\n[133\\. What are Models Thinking about? Understanding Large Language Model Hallucinations through Model Internal State Analysis](https://arxiv.org/pdf/2502.13490)\n\n[134\\. Look Within, Why LLMs Hallucinate: A Causal Perspective](http://arxiv.org/html/2407.10153v1)\n\n[135\\. GitHub - Baylor-AI/HalluDetect: Detecting Hallucinatio...](https://github.com/Baylor-AI/HalluDetect/)\n\n[136\\. \\[论文审查\\] Improving the Reliability of LLMs](https://www.themoonlight.io/zh/review/improving-the-reliability-of-llms-combining-cot-rag-self-consistency-and-self-verification)\n\n[137\\. Alleviating Hallucinations of Large Language Models through Induced Hallucinations](https://openreview.net/pdf/90ca529775cf535bed70cca7e2823f1dc596747a.pdf)\n\n[141\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[142\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[143\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[144\\. Song Han, Huizi Mao et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition](https://arxiv.org/abs/1510.00149)\n\n[145\\. LLMOps: A Comprehensive Guide to Deploying Large Language Models in Production](https://www.ijsat.org/papers/2025/1/2412.pdf)\n\n[146\\. Reducing LLM Inference Costs While Preserving Performance](https://www.rohan-paul.com/p/reducing-llm-inference-costs-while)\n\n[147\\. LLM compression and optimization: Cheaper inference ...](https://www.redhat.com/en/blog/llm-compression-and-optimization-cheaper-inference-fewer-hardware-resources)\n\n[148\\. PLANCK: Optimizing LLM Inference Performance in Pipeline Parallelism with Fine-Grained SLO Constraint](https://cloud.siat.ac.cn/papers/Planck.pdf)\n\n[149\\. Accelerating LLM Inference: A 10× Latency Reduction Roadmap](https://www.rohan-paul.com/p/how-to-reduce-the-average-response)\n\n[150\\. LiveMind: Low-Latency Large Language Models with Simultaneous Inference](https://arxiv.org/pdf/2406.14319)\n\n[151\\. BPIPE: Memory-Balanced Pipeline Parallelism for Training Large Language Models](https://proceedings.mlr.press/v202/kim23l/kim23l.pdf)\n\n[152\\. From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs](https://arxiv.org/pdf/2504.13471)\n\n[153\\. LLMOps: EVALUATING AND FINE TUNING LLM MODELS FOR GENERATIVE AI](https://iaeme.com/MasterAdmin/Journal_uploads/IJMLC/VOLUME_1_ISSUE_1/IJMLC_01_01_003.pdf)\n\n[154\\. Jin Huang, Yuchao Jin et al. “LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments.”](https://arxiv.org/abs/2506.07416)\n\n[155\\. Case Studies of Successful CI/CD Pipeline Implementations for Machine Learning and AI](https://ijrar.org/papers/IJRAR24C1522.pdf?utm_source=chatgpt.com)\n\n[156\\. Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models](https://www.zhouyujia.cn/attaches/ACL2024_MIND.pdf)\n\n[157\\. Production Machine Learning Pipelines | Proceedings of...](https://dl.acm.org/doi/abs/10.1145/3448016.3457566)\n\n[161\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[162\\. MEMREASONER: A MEMORY-AUGMENTED LLM ARCHITECTURE FOR MULTI-HOP REASONING](https://openreview.net/pdf?id=d4gu2XgccF)\n\n[163\\. Tsung-Yi Lin, M. Maire et al. “Microsoft COCO: Common Objects in Context.” European Conference on Computer Vision](https://doi.org/10.1007/978-3-319-10602-1_48)\n\n[164\\. Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective](https://arxiv.org/pdf/2505.12886)\n\n[165\\. MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models](https://arxiv.org/pdf/2412.18947)\n\n[166\\. 大型语言模型中的多语言幻觉差距](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_25/2410.18270.pdf)\n\n[167\\. NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and Related Observable Overgeneration Text Spans with Modified RefChecker and Modified SelfCheckGPT](https://arxiv.org/pdf/2503.01921)\n\n[168\\. A Survey on Hallucination in Large Language and Foundation Models](https://www.preprints.org/frontend/manuscript/7b49e7437a3bd104a0b6008f20667fba/download_pub)\n\n[169\\. Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models | Proceedings of the 32nd ACM International Conference on Information and Knowledge Management](https://dl.acm.org/doi/pdf/10.1145/3583780.3614905)\n\n[170\\. INTERVENING ANCHOR TOKEN: DECODING STRATEGY IN ALLEVIATING HALLUCINATIONS FOR MLLMs](https://openreview.net/pdf/f51d9f841f7508a64eea00f2ade4aacb50125d71.pdf)\n\n[171\\. 比较多语言生成的幻觉检测指标](https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_2_19/2402.10496.pdf)\n\n[172\\. A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark, Evaluations and Challenges](https://www.arxiv.org/pdf/2501.02189v4)\n\n[173\\. WMT 2023](http://www2.statmt.org/wmt23/2023.wmt-1.pdf)\n\n[174\\. Zhen Qin](https://aclanthology.org/people/z/zhen-qin/)\n\n[175\\. BALANCING ACT: DIVERSITY AND CONSISTENCY IN LARGE LANGUAGE MODEL ENSEMBLES](https://openreview.net/pdf?id=Dl6nkKKvlX)\n\n[176\\. Baraa Hikal, Ahmed Nasreldin et al. “MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection.”](https://arxiv.org/abs/2505.20880)\n\n[177\\. Wenliang Dai, Junnan Li et al. “InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.” ArXiv](https://doi.org/10.48550/arXiv.2305.06500)\n\n[178\\. Teven Le Scao, Angela Fan et al. “BLOOM: A 176B-Parameter Open-Access Multilingual Language Model.” ArXiv](https://doi.org/10.48550/arXiv.2211.05100)\n\n[179\\. Diverging Towards Hallucination: Detection of Failures in Vision-Language Models via Multi-token Aggregation](https://arxiv.org/pdf/2505.11741)\n\n[180\\. Xiyao Wang, Yuhang Zhou et al. “Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences.” ArXiv](https://doi.org/10.48550/arXiv.2401.10529)\n\n[181\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[182\\. Hallucinations | Phoenix](https://arize.com/docs/phoenix/evaluation/how-to-evals/running-pre-tested-evals/hallucinations)\n\n[183\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[184\\. Jushi Kai, Tianhang Zhang et al. “SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully.” ArXiv](https://doi.org/10.48550/arXiv.2401.05930)\n\n[185\\. A Survey on Hallucination in Large Language and Foundation Models](https://www.preprints.org/frontend/manuscript/7b49e7437a3bd104a0b6008f20667fba/download_pub)\n\n[186\\. Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models](https://www.zhouyujia.cn/attaches/ACL2024_MIND.pdf)\n\n[187\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[188\\. hf-blog-translation/leaderboard-hallucinations.md at 7...](https://github.com/FableFatale/hf-blog-translation/blob/76af20c3f8b9bb9f04daed4f0f93f82f06c43e6f/leaderboard-hallucinations.md)\n\n[189\\. INSIDE: LLMs’ INTERNAL STATES RETAIN THE POWER OF HALLUCINATION DETECTION](https://arxiv.org/pdf/2402.03744)\n\n[190\\. Combining CoT, RAG, Self-Consistency, and Self-Verification](https://www.themoonlight.io/en/review/improving-the-reliability-of-llms-combining-cot-rag-self-consistency-and-self-verification)\n\n[191\\. KNOWHalu: Hallucination Detection via Multi-Form Knowledge Based Factual Checking](https://openreview.net/pdf?id=RFwyhpcYZK)\n\n[192\\. Improving the Reliability of LLMs: Combining Chain-of-Thought Reasoning and Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.09031)\n\n[193\\. Improving the Reliability of LLMs: Combining Chain-of- ...](https://arxiv.org/html/2505.09031v1)\n\n[194\\. Evaluating Hallucinations of Language Models in the Wild](https://arxiv.org/html/2403.04307v3)\n\n[195\\. SLM Meets LLM: Balancing Latency, Interpretability and Consistency in Hallucination Detection](https://arxiv.org/pdf/2408.12748)\n\n[196\\. Jiawei Zhang, Chejian Xu et al. “KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual Checking.” ArXiv](https://doi.org/10.48550/arXiv.2404.02935)\n\n[197\\. On Early Detection of Hallucinations in Factual Question Answering](https://assets.amazon.science/91/80/bdb48f9a4aac83ebc947727c3485/on-early-detection-of-hallucinations-in-factual-question-answering.pdf)\n\n[198\\. DelucionQA: Detecting Hallucinations in Domain-specific Question Answering](https://junaraki.net/pubs/emnlp2023-delucionqa-poster.pdf)\n\n[199\\. 谎言教学：在合成负例上进行课程 DPO 以检测幻觉](https://www.xueshuxiangzi.com/downloads/2025_5_26/2505.17558.pdf)\n\n[201\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[202\\. Optimizing Large Language Models through Quantization](https://arxiv.org/html/2411.06084v1)\n\n[203\\. LiveMind: Low-latency Large Language Models with ...](https://arxiv.org/html/2406.14319v1)\n\n[204\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[205\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[206\\. PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning](https://www.microsoft.com/en-us/research/uploads/prod/2024/11/emnlp24-PromptIntern.pdf)\n\n[207\\. Chuangtao Chen, Grace Li Zhang et al. “LiveMind: Low-latency Large Language Models with Simultaneous Inference.” ArXiv](https://doi.org/10.48550/arXiv.2406.14319)\n\n[208\\. DaDu-E: Rethinking the Role of Large Language Model in Robotic Computing Pipeline](https://www.i-newcar.com/uploads/ueditor/20250214/2-250214104043G0.pdf)\n\n[209\\. GitHub - sarvex/DeepSpeed-MII: MII makes low-latency a...](https://github.com/sarvex/DeepSpeed-MII)\n\n[210\\. EFFICIENT AND ROBUST WEB SCALE LANGUAGE MODEL BASED RETRIEVAL, GENERATION, AND UNDERSTANDING](https://spacemanidol.com/cv/UIUCTHESIS.pdf)\n\n[211\\. EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large Language Model](https://arxiv.org/pdf/2408.11795)\n\n[212\\. Tri Dao, Daniel Y. Fu et al. “FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.” ArXiv](https://arxiv.org/abs/2205.14135)\n\n[213\\. LiveMind: Low-Latency Large Language Models with Simultaneous Inference](https://arxiv.org/pdf/2406.14319)\n\n[214\\. ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models](https://arxiv.org/pdf/2408.08554)\n\n[215\\. MInference: 提升长序列处理的预填充效率](https://www.catalyzex.com/author/Chin-Yew%20Lin)\n\n[216\\. GitHub - tulika612/DeepSpeed-MII: MII makes low-latency and...](https://github.com/tulika612/DeepSpeed-MII)\n\n[217\\. Energy Considerations of Large Language Model ...](https://arxiv.org/abs/2504.17674)\n\n[218\\. Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline](https://proceedings.neurips.cc/paper_files/paper/2023/file/ce7ff3405c782f761fac7f849b41ae9a-Paper-Conference.pdf)\n\n[219\\. Production Machine Learning Pipelines | Proceedings of...](https://dl.acm.org/doi/10.1145/3448016.3457566)\n\n[221\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[222\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[223\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[224\\. Teven Le Scao, Angela Fan et al. “BLOOM: A 176B-Parameter Open-Access Multilingual Language Model.” ArXiv](https://doi.org/10.48550/arXiv.2211.05100)\n\n[225\\. Ziwei Ji, Nayeon Lee et al. “Survey of Hallucination in Natural Language Generation.” ACM Computing Surveys](https://doi.org/10.1145/3571730)\n\n[226\\. Huggingface-blog/leaderboard-hallucinations.md at daf7...](https://github.com/merico34/Huggingface-blog/blob/daf72bb914e95ee4f0d8773dc1eae6d7cbc488fc/leaderboard-hallucinations.md)\n\n[227\\. 大型语言模型中的多语言幻觉差距](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_10_25/2410.18270.pdf)\n\n[228\\. New tool, dataset help detect hallucinations in large language models](https://www.amazon.science/blog/new-tool-dataset-help-detect-hallucinations-in-large-language-models)\n\n[229\\. BALANCING ACT: DIVERSITY AND CONSISTENCY IN LARGE LANGUAGE MODEL ENSEMBLES](https://openreview.net/pdf?id=Dl6nkKKvlX)\n\n[230\\. MEASURING AND MITIGATING HALLUCINATIONS IN LARGE LANGUAGE MODELS: A MULTIFACETED APPROACH](https://amatria.in/blog/images/Mitigating_Hallucinations.pdf)\n\n[231\\. Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models](https://openreview.net/pdf?id=65NPVeLga1)\n\n[232\\. THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models](https://openreview.net/pdf?id=GMNDgcjPfy)\n\n[233\\. HALLUSIONBENCH: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Guan_HallusionBench_An_Advanced_Diagnostic_Suite_for_Entangled_Language_Hallucination_and_CVPR_2024_paper.pdf)\n\n[234\\. keepitsimple 在 SemEval-2025 任务 3: 基于 LLM 不确定性的方法用于多语言幻觉跨度检测](https://www.xueshuxiangzi.com/downloads/2025_5_26/2505.17485.pdf)\n\n[235\\. A Survey on Hallucination in Large Language and Foundation Models](https://www.preprints.org/frontend/manuscript/7b49e7437a3bd104a0b6008f20667fba/download_pub)\n\n[236\\. MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models](https://arxiv.org/pdf/2412.18947)\n\n[237\\. Ultimate Guide to Hallucination Detection Metrics](https://detecting-ai.com/blog/ultimate-guide-to-hallucination-detection-metrics)\n\n[238\\. FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern LLMs](https://arxiv.org/pdf/2410.13210)\n\n[239\\. Yuyan Chen, Qiang Fu et al. “Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models.” Proceedings of the 32nd ACM International Conference on Information and Knowledge Management](https://doi.org/10.1145/3583780.3614905)\n\n[241\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[242\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[243\\. Yinhan Liu, Myle Ott et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv](https://arxiv.org/abs/1907.11692)\n\n[244\\. Confidence Intervals for the F1 Score: A Comparison of Four Methods](https://arxiv.org/pdf/2309.14621v1.pdf)\n\n[245\\. Open Domain Question Answering via Semantic Enrichment](https://scottyih.org/files/frp1068-sunA.pdf)\n\n[246\\. Pranav Rajpurkar, Jian Zhang et al. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D16-1264)\n\n[247\\. Improving the Reliability of LLMs: Combining Chain-of-Thought Reasoning and Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.09031)\n\n[248\\. 基于样本内外协同表示和自适应融合的多模态学习方法](https://crad.ict.ac.cn/cn/article/pdf/preview/10.7544/issn1000-1239.202330722.pdf)\n\n[249\\. James Thorne, Andreas Vlachos et al. “FEVER: a Large-scale Dataset for Fact Extraction and VERification.” ArXiv](https://doi.org/10.18653/v1/N18-1074)\n\n[250\\. Improving the Reliability of LLMs: Combining Chain-of- ...](https://arxiv.org/html/2505.09031v1)\n\n[251\\. Affective Event Classification with Discourse-enhanced Self-training](https://www-old.cs.utah.edu/~tianyu/papers/official-affevent-emnlp2020-slides.pdf)\n\n[252\\. Question Answering with Self-Attention](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/reports/final_reports/report122.pdf)\n\n[253\\. LEVERAGING ARTIFICIAL INTELLIGENCE FOR REAL-TIME LIQUIDITY RISK ASSESSMENT IN BANKS](https://www.irjmets.com/uploadedfiles/paper/issue_10_october_2024/62950/final/fin_irjmets1729948048.pdf)\n\n[254\\. Robust QA on out of domain dataset over pretraining and fine tuning](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/reports/final_reports/report237.pdf)\n\n[255\\. Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models](https://www.zhouyujia.cn/attaches/ACL2024_MIND.pdf)\n\n[256\\. HISTORY MODELING FOR CONVERSATIONAL INFORMATION RETRIEVAL](https://web.cs.umass.edu/publication/docs/2021/UM-CS-PhD-2021-002.pdf)\n\n[257\\. FACTAGENT: A Dynamic Reasoning Approach to Fact-Checking Language Model Responses](https://openreview.net/pdf/6f37f73e64df110f1ba7385befb4d0166f1a6bac.pdf)\n\n[261\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[262\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[263\\. Optimizing Large Language Models through Quantization](https://arxiv.org/html/2411.06084v1)\n\n[264\\. LiveMind: Low-latency Large Language Models with ...](https://arxiv.org/html/2406.14319v1)\n\n[265\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[266\\. Efficient Deployment of Large Language Model across Cloud-Device Systems](https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2Ff06a14c1-4d6d-441d-b4e4-82545ac5781b.pdf)\n\n[267\\. EFFICIENT AND ROBUST WEB SCALE LANGUAGE MODEL BASED RETRIEVAL, GENERATION, AND UNDERSTANDING](https://spacemanidol.com/cv/UIUCTHESIS.pdf)\n\n[268\\. PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning](https://www.microsoft.com/en-us/research/uploads/prod/2024/11/emnlp24-PromptIntern.pdf)\n\n[269\\. Tabi: An Efficient Multi-Level Inference System for Large Language Models](https://cse.hkust.edu.hk/~kaichen/papers/tabi-eurosys23.pdf)\n\n[270\\. Chuangtao Chen, Grace Li Zhang et al. “LiveMind: Low-latency Large Language Models with Simultaneous Inference.” ArXiv](https://doi.org/10.48550/arXiv.2406.14319)\n\n[271\\. Dan Hendrycks, Collin Burns et al. “Measuring Massive Multitask Language Understanding.” ArXiv](https://arxiv.org/abs/2009.03300)\n\n[272\\. A Survey of Large Language Models](https://openreview.net/pdf/3746fd898a147036cc09abd50bba2532904e68fb.pdf)\n\n[273\\. MInference/Transparency_FAQ.md at main · microsoft/MInference](https://github.com/microsoft/MInference/blob/main/Transparency_FAQ.md)\n\n[274\\. Energy Considerations of Large Language Model Inference and Efficiency Optimizations](https://openreview.net/pdf?id=aXNty1YGe0)\n\n[275\\. Yucheng Li, Bo Dong et al. “Compressing Context to Enhance Inference Efficiency of Large Language Models.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.48550/arXiv.2310.06201)\n\n[276\\. Efficient Deployment Algorithms for Large Language Models](https://dspace.mit.edu/bitstream/handle/1721.1/156332/xiao-xgx-sm-eecs-2024-thesis.pdf)\n\n[277\\. EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large Language Model](https://arxiv.org/pdf/2408.11795)\n\n[278\\. Accelerating and Enhancing LLMs in Long Context ...](https://aclanthology.org/2024.acl-long.91/)\n\n[281\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[282\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[283\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[284\\. Zhilin Yang, Peng Qi et al. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D18-1259)\n\n[285\\. Stephanie C. Lin, Jacob Hilton et al. “TruthfulQA: Measuring How Models Mimic Human Falsehoods.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/2022.acl-long.229)\n\n[286\\. MemReasoner: A Memory-augmented LLM Architecture for Multi-hop Reasoning](https://openreview.net/pdf?id=ODcMy97cVZ)\n\n[287\\. MEMREASONER: A MEMORY-AUGMENTED LLM ARCHITECTURE FOR MULTI-HOP REASONING](https://openreview.net/pdf?id=d4gu2XgccF)\n\n[288\\. MemReasoner: A Memory-augmented LLM Architecture for ...](https://research.ibm.com/publications/memreasoner-a-memory-augmented-llm-architecture-for-multi-hop-reasoning)\n\n[289\\. MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations](http://www.arxiv.org/pdf/2505.14101)\n\n[290\\. MEMERAG：用于检索增强生成的多语言端到端元评估基准](https://www.xueshuxiangzi.com/downloads/2025_2_25/2502.17163.pdf)\n\n[291\\. ...ys-zong/MIRB: Benchmarking Multi-Image Understandin...](https://github.com/ys-zong/MIRB)\n\n[292\\. AutoHall: Automated Hallucination Dataset Generation for Large Language Models](https://openreview.net/pdf?id=PEQWf7Dq1D)\n\n[293\\. Multi-hop Database Reasoning with Virtual Knowledge ...](https://aclanthology.org/2024.kallm-1.1/)\n\n[294\\. DISTRIBUTIONAL REASONING IN LLMS: PARALLEL REASONING PROCESSES IN MULTI-HOP REASONING](https://openreview.net/pdf/b8f2507d4c0c1cdbc228d46fe5d0ee4a65bed4e8.pdf)\n\n[295\\. Aviv Jan, Dean Tahory et al. “Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models.”](https://arxiv.org/abs/2506.00483)\n\n[296\\. Improving the Reliability of LLMs: Combining Chain-of-Thought Reasoning and Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.09031)\n\n[301\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[302\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[303\\. Combining CoT, RAG, Self-Consistency, and Self-Verification](https://www.themoonlight.io/en/review/improving-the-reliability-of-llms-combining-cot-rag-self-consistency-and-self-verification)\n\n[304\\. Improving the Reliability of LLMs: Combining Chain-of-Thought Reasoning and Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.09031)\n\n[305\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[306\\. Jason Wei, Xuezhi Wang et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” ArXiv](https://arxiv.org/abs/2201.11903)\n\n[307\\. Self-Memory Alignment: Mitigating Factual Hallucinations with Generalized Improvement](https://arxiv.org/pdf/2502.19127)\n\n[308\\. Improving Factuality in Large Language Models via Decoding-Time Hallucinatory and Truthful Comparators](https://arxiv.org/pdf/2408.12325)\n\n[309\\. Improving the Reliability of LLMs: Combining Chain-of- ...](https://arxiv.org/html/2505.09031v1)\n\n[310\\. James Thorne, Andreas Vlachos et al. “FEVER: a Large-scale Dataset for Fact Extraction and VERification.” ArXiv](https://doi.org/10.18653/v1/N18-1074)\n\n[311\\. Question Answering with Self-Attention](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/reports/final_reports/report122.pdf)\n\n[312\\. Memory-Aware and Uncertainty-Guided Retrieval for Multi- ...](https://arxiv.org/html/2503.23095v1)\n\n[313\\. How to Maximize the Accuracy of LLM Models in 2025 - Deepchecks](https://www.deepchecks.com/how-to-maximize-the-accuracy-of-llm-models/#:~:text=Maintaining%20LLM%20accuracy%20over%20time,or%20terms%20change%20in%202025.)\n\n[314\\. Fine-grained Fact Verification with Kernel Graph Attention Network](https://static.aminer.cn/upload/pdf/1112/650/1668/5ec49a639fced0a24b4de7d4_7.pdf)\n\n[315\\. 一种利用 MINDSP0RE 框架识别讽刺情绪的创新 CGL-MHA 模型](https://xueshuxiangzi.blob.core.windows.net/paper/ch_paper/2024_11_5/2411.01264.pdf)\n\n[316\\. hf-blog-translation/leaderboard-hallucinations.md at 7...](https://github.com/FableFatale/hf-blog-translation/blob/76af20c3f8b9bb9f04daed4f0f93f82f06c43e6f/leaderboard-hallucinations.md)\n\n[317\\. A Self-Learning Multimodal Approach for Fake News ...](https://arxiv.org/html/2412.05843v1)\n\n[321\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[322\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[323\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[324\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[325\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[326\\. LLMOps: A Comprehensive Guide to Deploying Large Language Models in Production](https://www.ijsat.org/papers/2025/1/2412.pdf)\n\n[327\\. LiveMind: Low-latency Large Language Models with ...](https://arxiv.org/html/2406.14319v1)\n\n[328\\. Efficient Deployment of Large Language Model across Cloud-Device Systems](https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2Ff06a14c1-4d6d-441d-b4e4-82545ac5781b.pdf)\n\n[329\\. Tabi: An Efficient Multi-Level Inference System for Large Language Models](https://cse.hkust.edu.hk/~kaichen/papers/tabi-eurosys23.pdf)\n\n[330\\. EFFICIENT AND ROBUST WEB SCALE LANGUAGE MODEL BASED RETRIEVAL, GENERATION, AND UNDERSTANDING](https://spacemanidol.com/cv/UIUCTHESIS.pdf)\n\n[331\\. Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models](https://arxiv.org/pdf/2401.00625)\n\n[332\\. PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning](https://www.microsoft.com/en-us/research/uploads/prod/2024/11/emnlp24-PromptIntern.pdf)\n\n[333\\. Energy Considerations of Large Language Model Inference and Efficiency Optimizations](https://openreview.net/pdf?id=aXNty1YGe0)\n\n[334\\. DaDu-E: Rethinking the Role of Large Language Model in Robotic Computing Pipeline](https://www.i-newcar.com/uploads/ueditor/20250214/2-250214104043G0.pdf)\n\n[335\\. CE-CoLLM: Efficient and Adaptive Large Language Models Through Cloud-Edge Collaboration](https://arxiv.org/pdf/2411.02829)\n\n[336\\. Language_Model - Paper Reading](https://paperreading.club/category?cate=Language_Model&page=201)\n\n[337\\. Tyler Griggs, Xiaoxuan Liu et al. “M\\\\'elange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity.” ArXiv](https://doi.org/10.48550/arXiv.2404.14527)\n\n[341\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[342\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[343\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[344\\. MEMREASONER: A MEMORY-AUGMENTED LLM ARCHITECTURE FOR MULTI-HOP REASONING](https://openreview.net/pdf?id=d4gu2XgccF)\n\n[345\\. MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations](http://www.arxiv.org/pdf/2505.14101)\n\n[346\\. Zhilin Yang, Peng Qi et al. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D18-1259)\n\n[347\\. Stephanie C. Lin, Jacob Hilton et al. “TruthfulQA: Measuring How Models Mimic Human Falsehoods.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/2022.acl-long.229)\n\n[348\\. MemReasoner: A Memory-augmented LLM Architecture for Multi-hop Reasoning](https://openreview.net/pdf?id=ODcMy97cVZ)\n\n[349\\. Improving the Reliability of LLMs: Combining Chain-of-Thought Reasoning and Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.09031)\n\n[350\\. MemReasoner: A Memory-augmented LLM Architecture for ...](https://research.ibm.com/publications/memreasoner-a-memory-augmented-llm-architecture-for-multi-hop-reasoning)\n\n[351\\. Developing Locally Trainable Large Language Models](https://dr.ntu.edu.sg/bitstream/10356/182242/2/Thesis_report_Hailin.pdf)\n\n[352\\. DISTRIBUTIONAL REASONING IN LLMS: PARALLEL REASONING PROCESSES IN MULTI-HOP REASONING](https://openreview.net/pdf/b8f2507d4c0c1cdbc228d46fe5d0ee4a65bed4e8.pdf)\n\n[353\\. Multi-hop Database Reasoning with Virtual Knowledge ...](https://aclanthology.org/2024.kallm-1.1/)\n\n[354\\. AutoHall: Automated Hallucination Dataset Generation for Large Language Models](https://openreview.net/pdf?id=PEQWf7Dq1D)\n\n[355\\. Aviv Jan, Dean Tahory et al. “Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models.”](https://arxiv.org/abs/2506.00483)\n\n[361\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[362\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[363\\. Colin Raffel, Noam M. Shazeer et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” J. Mach. Learn. Res.](https://arxiv.org/abs/1910.10683)\n\n[364\\. Combining CoT, RAG, Self-Consistency, and Self-Verification](https://www.themoonlight.io/en/review/improving-the-reliability-of-llms-combining-cot-rag-self-consistency-and-self-verification)\n\n[365\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[366\\. Improving the Reliability of LLMs: Combining Chain-of-Thought Reasoning and Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.09031)\n\n[367\\. Self-Memory Alignment: Mitigating Factual Hallucinations with Generalized Improvement](https://arxiv.org/pdf/2502.19127)\n\n[368\\. Memory-Aware and Uncertainty-Guided Retrieval for Multi- ...](https://arxiv.org/html/2503.23095v1)\n\n[369\\. Improving Factuality in Large Language Models via Decoding-Time Hallucinatory and Truthful Comparators](https://arxiv.org/pdf/2408.12325)\n\n[370\\. James Thorne, Andreas Vlachos et al. “FEVER: a Large-scale Dataset for Fact Extraction and VERification.” ArXiv](https://doi.org/10.18653/v1/N18-1074)\n\n[371\\. Improving the Reliability of LLMs: Combining Chain-of- ...](https://arxiv.org/html/2505.09031v1)\n\n[372\\. Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models](https://www.zhouyujia.cn/attaches/ACL2024_MIND.pdf)\n\n[373\\. A Survey on Parameter Server Architecture: Approaches for Optimizing Distributed Centralized Learning](http://www.cslab.ece.ntua.gr/~ikons/assets/files/A_Survey_on_Parameter_Server_Architecture_Approaches_for_Optimizing_Distributed_Centralized_Learning.pdf)\n\n[374\\. Affective Event Classification with Discourse-enhanced Self-training](https://www-old.cs.utah.edu/~tianyu/papers/official-affevent-emnlp2020-slides.pdf)\n\n[375\\. Fine-grained Fact Verification with Kernel Graph Attention Network](https://static.aminer.cn/upload/pdf/1112/650/1668/5ec49a639fced0a24b4de7d4_7.pdf)\n\n[376\\. A Survey on Hallucination in Large Language and Foundation Models](https://www.preprints.org/frontend/manuscript/c5f20698cf44a95d88e568ddde2066e0/download_pub)\n\n[377\\. TruthfulQA Dataset](https://paperswithcode.com/dataset/truthfulqa)\n\n[378\\. Mind the Gap: A Practical Attack on GGUF Quantization](https://www.arxiv.org/pdf/2505.23786)\n\n[381\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[382\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[383\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[384\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[385\\. OpenAI Josh Achiam, Steven Adler et al. “GPT-4 Technical Report.”](https://arxiv.org/abs/2303.08774)\n\n[386\\. LLMOps: A Comprehensive Guide to Deploying Large Language Models in Production](https://www.ijsat.org/papers/2025/1/2412.pdf)\n\n[387\\. Efficient Deployment of Large Language Model across Cloud-Device Systems](https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2Ff06a14c1-4d6d-441d-b4e4-82545ac5781b.pdf)\n\n[388\\. LiveMind: Low-Latency Large Language Models with Simultaneous Inference](https://arxiv.org/pdf/2406.14319)\n\n[389\\. Industrial applications of large language models](https://www.nature.com/articles/s41598-025-98483-1.pdf)\n\n[390\\. Energy Considerations of Large Language Model Inference and Efficiency Optimizations](https://openreview.net/pdf?id=aXNty1YGe0)\n\n[391\\. Large Language Models: Application through Production](https://courses.edx.org/asset-v1:Databricks+LLM101x+2T2023+type@asset+block@Course_1_-_Application_through_Production.pdf)\n\n[392\\. 高效LLM：大型语言模型的效能优化](https://www.chatpaper.ai/zh/dashboard/paper/358ba887-2c3a-4696-89b4-adfb911da26d)\n\n[393\\. Efficient Deployment Algorithms for Large Language Models](https://dspace.mit.edu/bitstream/handle/1721.1/156332/xiao-xgx-sm-eecs-2024-thesis.pdf)\n\n[394\\. What are Models Thinking about? Understanding Large Language Model Hallucinations through Model Internal State Analysis](https://arxiv.org/pdf/2502.13490)\n\n[401\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[402\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[403\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[404\\. MEMREASONER: A MEMORY-AUGMENTED LLM ARCHITECTURE FOR MULTI-HOP REASONING](https://openreview.net/pdf?id=d4gu2XgccF)\n\n[405\\. TRAINING LARGE LANGUAGE MODELS TO REASON IN A CONTINUOUS LATENT SPACE](https://openreview.net/attachment?id=KrWSrrYGpT&name=pdf)\n\n[406\\. Zhilin Yang, Peng Qi et al. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D18-1259)\n\n[407\\. Stephanie C. Lin, Jacob Hilton et al. “TruthfulQA: Measuring How Models Mimic Human Falsehoods.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/2022.acl-long.229)\n\n[408\\. MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations](http://www.arxiv.org/pdf/2505.14101)\n\n[409\\. MemReasoner: A Memory-augmented LLM Architecture for ...](https://research.ibm.com/publications/memreasoner-a-memory-augmented-llm-architecture-for-multi-hop-reasoning)\n\n[410\\. ...ys-zong/MIRB: Benchmarking Multi-Image Understandin...](https://github.com/ys-zong/MIRB)\n\n[411\\. Improving the Reliability of LLMs: Combining Chain-of-Thought Reasoning and Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.09031)\n\n[412\\. Developing Locally Trainable Large Language Models](https://dr.ntu.edu.sg/bitstream/10356/182242/2/Thesis_report_Hailin.pdf)\n\n[413\\. Multi-hop Database Reasoning with Virtual Knowledge ...](https://aclanthology.org/2024.kallm-1.1/)\n\n[414\\. DISTRIBUTIONAL REASONING IN LLMS: PARALLEL REASONING PROCESSES IN MULTI-HOP REASONING](https://openreview.net/pdf/b8f2507d4c0c1cdbc228d46fe5d0ee4a65bed4e8.pdf)\n\n[415\\. AutoHall: Automated Hallucination Dataset Generation for Large Language Models](https://openreview.net/pdf?id=PEQWf7Dq1D)\n\n[416\\. Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs](https://arxiv.org/pdf/2505.11277)"
  },
  {
    "id": 63,
    "question": "What is \"extrinsic hallucination\" in large language models? How does it differ from intrinsic hallucinations in the context, and what are the main methods to reduce type of hallucination?",
    "response": "**Research Report: Extrinsic Hallucination in Large Language Models**\n\n**Date:** July 24, 2025\n\n**Topic:** An analysis of \"extrinsic hallucination\" in large language models (LLMs), its distinction from intrinsic hallucination, and the primary methods for its reduction.\n\n**1.0 Executive Summary**\n\nThis report provides a comprehensive analysis of the phenomenon known as \"extrinsic hallucination\" in large language models. Extrinsic hallucinations are defined as generated content that is factually incorrect or cannot be verified against known external knowledge sources, even if it is internally consistent. This is distinct from _intrinsic_ hallucinations, which involve contradictions with the provided source material or the model's own previously generated statements. Extrinsic errors are particularly common in open-domain, generative tasks.\n\nAs of 2025, the primary strategies to mitigate extrinsic hallucinations revolve around grounding models in factual, external data. Retrieval-Augmented Generation (RAG) is the foundational method, supplemented by training techniques like contrastive learning and reinforcement learning (RLHF, KFR), and post-hoc editing tools.\n\nA significant trend in research post-2023 has been the development of lightweight, training-free, inference-time methods specifically targeting extrinsic errors. These new decoding strategies and activation engineering techniques aim to detect and suppress factually unsupported tokens during the generation process, representing a strategic shift away from earlier methods that primarily focused on embedding world knowledge into the model's parameters to solve intrinsic inconsistencies. Despite these advances, challenges remain, including retrieval latency, the need for high-quality annotated data, and the inherent tension between relying on external knowledge versus the model's parametric memory.\n\n**2.0 Defining and Differentiating Hallucination Types**\n\nThe term \"hallucination\" in LLMs broadly refers to the generation of content that is nonsensical, factually incorrect, or unfaithful to a provided source. Within this category, researchers have established a critical typology that distinguishes between errors of internal consistency (intrinsic) and errors of external factuality (extrinsic).\n\n**2.1 Intrinsic Hallucination: Contradiction with Source or Self**\n\nIntrinsic hallucination occurs when an LLM's output directly contradicts information presented within the provided source context or contradicts its own statements within the same response (Ji et al., 2023). It is a failure of logical consistency and faithfulness to a given, closed set of information.\n\n**Core Definition:** The generated text contains a direct logical contradiction of either the model’s own earlier claims or the explicit source material (Ji et al., 2023; Gupta).\n\n**Example:** If a source document states, \"The subject was born in 1980,\" an intrinsic hallucination would be a summary that claims, \"The individual was born in 1975\" (Ji et al., 2023).\n\n**Prevalence:** This type of error is more frequently observed in tasks that require adherence to a specific context, such as long-form summarization or multi-step reasoning based on a provided document (Ahadian et al.).\n\n**2.2 Extrinsic Hallucination: Contradiction with Verifiable World Knowledge**\n\nExtrinsic hallucination is a more challenging issue where the model generates information that cannot be verified by or is factually false with respect to the known world (Ji et al., 2023). This information is external to the provided source text and often appears plausible, making it particularly deceptive.\n\n**Core Definition:** The generated content is either demonstrably false according to an external knowledge source or is entirely unverifiable because it introduces new information that is “not supported by and cannot be verified against the provided source or any known external knowledge” (Ahadian et al.). It is a failure of factuality.\n\n**Example:** In an open-ended conversation about legal history, an LLM might invent a plausible-sounding but non-existent law, complete with a fabricated citation number (Ji et al., 2023). Similarly, it could generate content that “cannot be verified against any known source,” such as inventing speculative but unprovable details about a historical figure's private life (Gupta).\n\n**Prevalence:** Extrinsic hallucinations are a hallmark problem in open-domain, open-ended generation tasks like creative writing, chatbot conversations, or any query that requires the model to draw upon its vast, but sometimes flawed, parametric knowledge base (Gupta et al.). While some analyses loosely define this as any output bearing \"no connection to the source\" \\[3\\]\\[6\\]\\[12\\]the academic consensus centers on the concept of being \"false with respect to external knowledge\" (Ji et al., 2023).\n\n**2.3 Key Distinctions and Evolving Terminology**\n\nThe fundamental distinction lies in the frame of reference for verification. **Intrinsic** errors are self-contained and can be identified by comparing the output only to the source text or itself. **Extrinsic** errors require an external fact-checking process against a corpus of world knowledge.\n\nReflecting this distinction, recent surveys have begun to reframe these concepts for user-oriented evaluations. Intrinsic hallucinations are increasingly referred to as **“faithfulness inconsistencies,”** while extrinsic hallucinations are termed **“factuality errors”** (Huang et al.). This shift in terminology maintains the same core definitions but emphasizes the practical impact of each error type: one violates faithfulness to a given context, while the other violates factuality in the open world.\n\n**3.0 Core Mitigation Strategies for Extrinsic Hallucination**\n\nEfforts to combat extrinsic hallucinations primarily focus on preventing the model from inventing facts by grounding its responses in verifiable information. These methods can be broadly categorized into data-centric grounding, training-based refinement, and post-generation correction.\n\n**3.1 Grounding with External Data: Retrieval-Augmented Generation (RAG)**\n\nRetrieval-Augmented Generation (RAG) is the leading strategy for reducing extrinsic hallucinations \\[21\\]\\[26\\]. The RAG framework connects the LLM to one or more external, up-to-date knowledge bases (e.g., corporate documents, real-time news feeds, scientific databases). Before generating a response, the system first retrieves relevant information from these sources and provides it to the LLM as context, compelling the model to base its answer on the retrieved facts rather than its internal, potentially outdated or incorrect parametric knowledge.\n\nTechniques within RAG have become more sophisticated, including iterative retrieval, where the model can perform multiple searches to gather sufficient evidence, and the integration of knowledge graphs to dynamically validate factual relationships during generation \\[21\\]\\[23\\]. However, RAG's effectiveness is contingent on the quality and reliability of the knowledge source; irrelevant or inaccurate retrievals can introduce new noise and lead to different types of hallucinations \\[21\\]. Furthermore, the retrieval step can introduce system latency \\[21\\].\n\n**3.2 Training-Based Factual Enhancement**\n\nThis family of methods aims to improve the model's inherent ability to distinguish between factual and fabricated information through specialized training objectives.\n\n**Contrastive Learning:** Techniques like **MixCL (Mixture of Contrastive Learning)** train a model to differentiate between correct, factual outputs and hallucinated ones \\[26\\]. This is often achieved by creating a dataset of positive (correct) and negative (hallucinated) examples, teaching the model to assign higher probabilities to factual statements.\n\n**Reinforcement Learning:** Reinforcement learning frameworks use feedback to reward the model for generating factually accurate content. In **Knowledge Feedback Reinforcement Learning (KFR)**, an external knowledge base or fact-checker acts as the reward function, penalizing the model for extrinsic hallucinations \\[26\\]. A more common approach is **Reinforcement Learning from Human Feedback (RLHF)**, where human reviewers evaluate and rate model outputs for factuality, and this feedback is used to fine-tune the model to align its responses with verified sources \\[21\\]\\[26\\]\\[58\\].\n\n**3.3 Post-Hoc Correction and Model Editing**\n\nRather than preventing hallucinations during generation, these methods aim to detect and fix them after an initial draft is produced.\n\nThe **\"locate-then-edit\"** approach involves first identifying a likely hallucination in the generated text and then directly editing the model's parameters to correct the faulty knowledge \\[23\\]. Specialized tools like **Woodpecker** automate this process by cross-referencing generated text against external knowledge sources, flagging potential hallucinations, and suggesting corrections, which can then be used to refine the output retroactively \\[35\\].\n\n**4.0 Post-2023 Advances: A Shift Towards Inference-Time Mitigation**\n\nThe research landscape since late 2023 shows a marked pivot, especially for combating _extrinsic_ hallucinations. While earlier methods often involved costly data curation and model retraining \\[44\\]\\[55\\]the most recent innovations are training-free, lightweight methods applied during the inference (i.e., generation) stage. This new wave of techniques directly addresses the challenge of verifying facts that lie outside the model's training data.\n\n**4.1 The Rise of Extrinsic-Focused Decoding Strategies**\n\nDecoding strategies modify the token-selection process at the moment of generation. Several novel techniques emerged in 2024 and 2025 specifically to suppress extrinsic fabrications:\n\n**Self-Introspective Decoding (April 2025):** This training-free method prompts the model to generate a rationale for the factual plausibility of its next token. It then scores tokens based on this self-generated rationale, significantly reducing extrinsic hallucination rates (by 15-20 percentage points on standard benchmarks) without requiring any parameter updates \\[41\\]\\[41\\].\n\n**Contrastive Decoding for Multimodality:** In multimodal models, recent methods use an external signal to steer generation away from hallucinations. **CLIP-Guided Decoding (April 2024)** aligns the language model's token logits with the vision-text similarity scores from a model like CLIP, effectively penalizing text that describes objects not present in an image \\[46\\]. Similarly, methods like **CATCH (November 2024)**, **HALC**, and **EA-H (Feb-Mar 2024)** use contrastive signals from image captions or summaries to down-weight tokens that are unsupported by the visual or textual evidence, directly targeting the invention of extrinsic details \\[43\\]\\[53\\]. For example, HIO combines RAG with visual-language contrastive decoding to amplify the distinction between factual and hallucinated tokens \\[29\\]\\[29\\].\n\n**4.2 Innovations in Activation Engineering**\n\nAnother frontier is \"activation engineering,\" which involves directly manipulating the model's internal activation patterns at inference time. **Temporal-Aware Activation Engineering (TAE) (April 2025)**, applied to Video-LLMs, identifies and suppresses activations associated with tokens that describe facts not present in the input video. This activation-centric approach has been shown to reduce a key video hallucination metric by 25% and highlights a broader 2024-2025 trend of moving from model-centric to activation-centric mitigation for extrinsic errors \\[49\\].\n\n**4.3 The Divergence from Intrinsic-Centric Solutions**\n\nThese recent, inference-stage methods stand in stark contrast to the pre-2024 paradigm for hallucination mitigation. Earlier work often focused on improving the model's _intrinsic_ capacity by compressing more world knowledge into its parameters through extensive fine-tuning or self-alignment objectives \\[44\\]\\[55\\]\\[57\\]. While effective for improving general knowledge recall, these classic methods are largely orthogonal to the problem of grounding a model in a specific, external context or preventing fabrication in open-world settings where the required fact was never in the training data \\[43\\]. The new decoding and activation-based methods are explicitly designed for this latter, more dynamic challenge.\n\n**5.0 Challenges and Methodological Conflicts**\n\nDespite significant progress, mitigating extrinsic hallucinations remains an open research problem with several persistent challenges.\n\n**Inherent Limitations of Current Approaches:** RAG systems are bottlenecked by retrieval speed and their heavy dependence on the quality of the external knowledge base \\[21\\]. Contrastive learning and RLHF methods are powerful but require large, expensive, and difficult-to-create annotated datasets of factual and hallucinated text \\[21\\].\n\n**The Intrinsic vs. Extrinsic Knowledge Dilemma:** A subtle conflict exists between different mitigation philosophies. Strategies that force strict reliance on retrieved external documents (like some RAG implementations) may prevent the model from leveraging its own valuable, correctly-stored parametric knowledge, effectively \"dumbing down\" the model \\[23\\]. Finding the optimal balance between trusting the external source versus the model's internal knowledge is a key area of ongoing research.\n\n**6.0 Conclusion**\n\nAs of July 2025, the understanding of extrinsic hallucination—the generation of content that is verifiably false against external reality—is well-established and distinct from internal, logical contradictions. The field has developed a robust toolkit for mitigation, with Retrieval-Augmented Generation forming the bedrock strategy for grounding models in reality. However, the most significant recent progress is characterized by a paradigm shift. Researchers are increasingly moving away from brute-force retraining and towards sophisticated, training-free, inference-time interventions. These new decoding strategies and activation engineering techniques offer a more agile and computationally efficient means of detecting and suppressing factually unsupported information as it is being generated, marking a critical advancement in the quest for more reliable and trustworthy Large Language Models.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. A Survey on Hallucination in Large Language and Foundation Models](https://www.preprints.org/frontend/manuscript/7b49e7437a3bd104a0b6008f20667fba/download_pub)\n\n[2\\. Retrieval-Augmented Generation and Hallucination in Large Language Models: A Scholarly Overview](https://saspublishers.com/media/articles/SJET_135_328-330.pdf)\n\n[3\\. Investigating the Catastrophic Forgetting in Multimodal Large Language Models](https://zhuanlan.zhihu.com/p/675333375)\n\n[4\\. AutoHall: Automated Hallucination Dataset Generation for Large Language Models](https://openreview.net/pdf?id=PEQWf7Dq1D)\n\n[5\\. A Survey of Large Language Models](http://arxiv.org/pdf/2303.18223)\n\n[6\\. VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models](https://huggingface.co/datasets/leo20000306/svittmp/resolve/main/pprd.docx)\n\n[7\\. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/pdf/2311.05232)\n\n[8\\. Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196v3)\n\n[9\\. Leveraging Large Language Models for Legal Document Understanding and Software System Analysis: Addressing Key Challenges](https://www.rivas.ai/pdfs/quevedo2024llms.pdf)\n\n[10\\. Hallucination is Inevitable: An Innate Limitation of Large Language Models](https://arxiv.org/pdf/2401.11817)\n\n[11\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[12\\. Evaluating Intrinsic and Extrinsic Hallucinations in Large ...](https://paperreading.club/page?id=236057)\n\n[13\\. Hongjian Zhou, Boyang Gu et al. “A Survey of Large Language Models in Medicine: Progress, Application, and Challenge.” ArXiv](https://doi.org/10.48550/arXiv.2311.05112)\n\n[14\\. Awesome Hallucination Papers in MLLMs](https://github.com/shikiw/Awesome-MLLM-Hallucination)\n\n[15\\. The Beginner’s Guide to Hallucinations in Large Language Models](https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models)\n\n[16\\. Hallucinations in AI Models -A Quick Guide](https://bigohtech.com/hallucinations-in-ai-models-a-quick-guide)\n\n[17\\. 8 Challenges Of Building Your Own Large Language Model - Labellerr](https://www.labellerr.com/blog/challenges-in-development-of-llms/#:~:text=Challenges%20in%20LLM%20development,%20such,importance%20of%20addressing%20these%20issues.)\n\n[18\\. HillZhang1999/llm-hallucination-survey](https://github.com/HillZhang1999/llm-hallucination-survey)\n\n[21\\. A Survey on Hallucination in Large Language and Foundation Models](https://www.preprints.org/frontend/manuscript/c5f20698cf44a95d88e568ddde2066e0/download_pub)\n\n[22\\. SELF-INTROSPECTIVE DECODING: ALLEVIATING HALLUCINATIONS FOR LARGE VISION-LANGUAGE MODELS](https://openreview.net/pdf/0f4b0fa510c4886de45bcc5939e1f1cd904c45be.pdf)\n\n[23\\. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/pdf/2311.05232)\n\n[24\\. A Longchain Approach to Reduce Hallucinations in Large Language Models](https://osf.io/dgnbt/download)\n\n[25\\. Large Language Models in Systematic Review Screening](https://www.mdpi.com/2078-2489/16/5/378)\n\n[26\\. Zhibo Yin. “A review of methods for alleviating hallucination issues in large language models.” Applied and Computational Engineering](https://doi.org/10.54254/2755-2721/76/20240608)\n\n[27\\. A Survey of Hallucination Problems Based on Large Language Models](https://www.ewadirect.com/proceedings/ace/article/view/17851/pdf)\n\n[28\\. Hallucination is Inevitable: An Innate Limitation of Large Language Models](https://arxiv.org/pdf/2401.11817)\n\n[29\\. Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization](https://arxiv.org/pdf/2405.15356)\n\n[30\\. MEASURING AND MITIGATING HALLUCINATIONS IN LARGE LANGUAGE MODELS: A MULTIFACETED APPROACH](https://amatria.in/blog/images/Mitigating_Hallucinations.pdf)\n\n[31\\. The secret behind LLM hallucinations and how to tackle them](https://piamedia.com/wp-content/uploads/2024/09/PIAM_Whitepaper_LLM-Halluzinationen_EN.pdf)\n\n[32\\. Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models](https://www.zhouyujia.cn/attaches/ACL2024_MIND.pdf)\n\n[33\\. THaMES: An End-to-End Tool for Hallucination Mitigation and ...](https://www.yiyibooks.cn/__src__/arxiv/2409.11353v1/index.html)\n\n[34\\. 大语言模型 - 联合国创新及产品管理能力建设项目(UNSDGT)](https://www.cposchool.com/tag/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/)\n\n[35\\. Hanchao Liu, Wenyuan Xue et al. “A Survey on Hallucination in Large Vision-Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2402.00253)\n\n[41\\. SELF-INTROSPECTIVE DECODING: ALLEVIATING HALLUCINATIONS FOR LARGE VISION-LANGUAGE MODELS](https://openreview.net/pdf/0f4b0fa510c4886de45bcc5939e1f1cd904c45be.pdf)\n\n[42\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[43\\. showlab/Awesome-MLLM-Hallucination](https://github.com/showlab/Awesome-MLLM-Hallucination)\n\n[44\\. Alleviating Hallucinations of Large Language Models through Induced Hallucinations](https://openreview.net/pdf/90ca529775cf535bed70cca7e2823f1dc596747a.pdf)\n\n[45\\. MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs ...](https://arxiv.org/html/2502.08904v1)\n\n[46\\. Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding](https://arxiv.org/pdf/2402.15300)\n\n[47\\. INSIDE: LLMs’ INTERNAL STATES RETAIN THE POWER OF HALLUCINATION DETECTION](https://openreview.net/pdf?id=Zj12nzlQbz)\n\n[48\\. Hallucination Detection in LLM-enriched Product Listings](https://assets.amazon.science/e7/2d/212a15a847cc9afcdac537a332d6/hallucination-detection-in-llm-enriched-product-listings.pdf)\n\n[49\\. Mitigating Hallucination in VideoLLMs via Temporal-Aware Activation Engineering](https://arxiv.org/pdf/2505.12826)\n\n[50\\. Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics](https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008)\n\n[51\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[52\\. Why do LLMs still hallucinate in 2025?](https://www.rootsignals.ai/post/why-do-llms-still-hallucinate-in-2025)\n\n[53\\. A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models](https://arxiv.org/pdf/2505.01958)\n\n[54\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[55\\. Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation](https://openreview.net/pdf/adca170c36949167e5ddf54f0f91482a678e633e.pdf)\n\n[56\\. Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination](https://openreview.net/pdf/491b7483445977cadc0787dab918717b152ce89f.pdf)\n\n[57\\. INTERVENING ANCHOR TOKEN: DECODING STRATEGY IN ALLEVIATING HALLUCINATIONS FOR MLLMs](https://openreview.net/pdf/f51d9f841f7508a64eea00f2ade4aacb50125d71.pdf)\n\n[58\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)"
  },
  {
    "id": 64,
    "question": "How can organizations effectively implement and scale generative AI according to McKinsey's research, and what key strategies should executives prioritize to maximize value while managing risks?",
    "response": "**Research Report: Implementing and Scaling Generative AI in Organizations - McKinsey's Insights and Strategic Priorities for Executives**\n\n**1\\. Introduction: The Generative AI Imperative**\n\nGenerative AI (GenAI) represents a transformative leap in organizational capabilities, with McKinsey projecting it could add **4.4 trillion annually** to the global economy by 2030 \\[109\\]\\[112\\]. Its applications span industries—from automating 40% of product management tasks to halving coding time—but scaling it effectively requires overcoming technical, operational, and ethical hurdles \\[113\\]. By Q2 2025, McKinsey documented **400+ enterprise GenAI build-outs**, yet only **1% of organizations** report mature deployments \\[110\\]\\[113\\]. This report synthesizes McKinsey's 2025 research on scaling GenAI, prioritizing executive strategies to maximize value while mitigating risks.\n\n**2\\. Foundational Implementation Strategies**\n\n**a) Strategic Prioritization and Use Case Selection**\n\nOrganizations must align GenAI initiatives with high-impact business outcomes. McKinsey emphasizes:\n\n**Value-Driven Selection**: Focus on use cases with measurable ROI, such as marketing (30% of outbound messages GenAI-generated by 2025) or customer service (chatbot market: **$1.25B by 2025**) \\[105\\]\\[164\\].\n\n**Feasibility Assessment**: Evaluate technical complexity, data readiness, and regulatory alignment. High-revenue-impact sectors like high tech (4.8–9.3% revenue growth) and pharma (2.6–4.5%) warrant prioritization \\[161\\].\n\n**b) Talent and Capability Building**\n\nScaling demands specialized skills:\n\n**Upskilling**: Invest in prompt engineering, model fine-tuning, and vector database management \\[8\\].\n\n**Centralized Teams**: Cross-functional units to standardize data access, prompt libraries, and deployment protocols \\[149\\].\n\n_McKinsey Insight_: By 2025, 67% of marketing teams reduced workloads by **~60%** via GenAI, underscoring productivity gains from skill investments \\[171\\].\n\n**c) Governance Frameworks**\n\n**Responsible AI (RAI) Strategy**: Embed ethics into AI lifecycle via four dimensions: strategy, risk management, data/tech, and operating models \\[12\\].\n\n**Policy Enforcement**: Only 1% of firms had mature GenAI policies in 2024; executives must mandate guardrails for data privacy and bias mitigation \\[8\\]\\[132\\].\n\n**3\\. Scaling Infrastructure: MLOps and Architecture**\n\n**a) GenAIOps Evolution**\n\nTraditional MLOps must adapt to GenAI’s unique needs:\n\n**Prompt Management**: Track, version, and optimize prompts via platforms like MLflow or SageMaker \\[24\\]\\[81\\].\n\n**Fine-Tuning Workflows**: Support human input techniques (e.g., RLHF) and resource-efficient model updates \\[25\\]\\[144\\].\n\n_McKinsey Guidance_: Scalability requires **cloud-native infrastructure** (e.g., Kubernetes) to handle computational demands \\[144\\].\n\n**b) Reference Architecture Components**\n\nWhile no single McKinsey diagram exists, core elements include:\n\n**DataOps Layer**: Ensures data versioning, quality, and reproducibility \\[145\\].\n\n**Model Hub**: Central repository for foundation models and APIs \\[8\\].\n\n**Monitoring**: Real-time tracking of performance drift, bias, and security threats \\[96\\].\n\n_Enterprise Blueprint_: Tools like Vertex AI integrate **prompt engineering**, **fine-tuning**, and **edge deployment** for low-latency use cases \\[203\\].\n\n**4\\. Industry-Specific Scaling Benchmarks**\n\nExecutives should contextualize scaling using McKinsey’s 2025 industry data:\n\n|     |     |     |\n| --- | --- | --- |\n| **Industry** | **Key Metric** | **Value** |\n| **Manufacturing** | Adoption by 2027 | 30% of firms \\[163\\] |\n| **Healthcare/Pharma** | Revenue Uplift | 1.8–4.5% \\[161\\] |\n| **Banking** | Economic Impact | $390B \\[52\\] |\n| **Retail/CPG** | Labor Productivity | +3% annually \\[169\\] |\n\n**Critical Insight**: Scaling in pharma accelerates drug design (e.g., novel molecule generation), while manufacturing uses GenAI for **design optimization** (China: 30% adoption leader) \\[105\\]\\[48\\].\n\n**5\\. Risk Management: Technical Controls**\n\n**a) Risk Taxonomy and Mitigation**\n\nMcKinsey categorizes GenAI risks into eight areas: _data, model, output, operational, security, compliance, third-party, and societal_ \\[15\\]. Key controls:\n\n**Data Security**: Private clouds, on-premise servers, and access limitations \\[121\\].\n\n**Bias/Toxicity Filters**: Input/output guardrails for high-risk outputs \\[183\\].\n\n**NIST Alignment**: Map controls to AI Risk Management Framework \\[6\\]\\[188\\].\n\n**b) Implementation Checklist**\n\nMcKinsey’s phased approach:\n\n1.  **Risk Assessment**: Update model criteria and evaluate third-party vendors \\[187\\].\n2.  **Technical Safeguards**: Deploy input/output filters, continuous monitoring \\[121\\]\\[96\\].\n3.  **Compliance Automation**: Tools like Credo AI for policy checks \\[221\\].\n\n_Warning_: **30% of GenAI projects** may fail by 2025 due to poor data quality or inadequate controls \\[135\\].\n\n**6\\. Performance Metrics for Scaling**\n\nExecutives should track:\n\n**Productivity Gains**: e.g., 60% workload reduction in marketing \\[171\\].\n\n**Economic Impact**: Revenue uplift (sector-specific benchmarks).\n\n**Risk KPIs**: Incident rates (hallucinations, bias) and time-to-remediation \\[132\\].\n\n_McKinsey Finding_: Successful scalers use **daily active users** and **user feedback** for real-time refinement \\[166\\].\n\n**7\\. Future Outlook**\n\nBy 2027:\n\n**Converging Technologies**: GenAI integrates with IoT/edge computing \\[105\\].\n\n**Multimodal Systems**: Growth in text, image, and voice agent collaboration \\[21\\].\n\n**Regulatory Shifts**: New EU/U.S. standards for AI safety \\[125\\].\n\n_Critical Gap_: AI security spending lags adoption by **144%**, urging urgent investment \\[129\\].\n\n**8\\. Executive Priorities**\n\nMcKinsey’s 2025 roadmap:\n\n1.  **Industrialize MLOps**: Upgrade pipelines for prompt management and fine-tuning.\n2.  **Target Industry-Scaling**: Align with high-value sectors (e.g., pharma/healthcare).\n3.  **Embed Risk Controls**: Implement NIST-aligned technical safeguards.\n4.  **Track Value/Risk KPIs**: Productivity gains, incident rates, ROI.\n5.  **Upskill Talent**: Centralize teams for prompt engineering and RAI.\n\n**Final Insight**: Organizations that scale GenAI with disciplined risk governance will capture **70% of the $4.4T opportunity** by 2030 \\[109\\]\\[132\\].\n\n\\[AI生成\\]\n\n## References\n\n[1\\. GENERATIVE AI IN BUSINESS: VISUAL ILLUSTRATIONS OF APPLICATIONS AND INSIGHTS FROM Q1 2025](https://philpapers.org/archive/JOSGAI.pdf)\n\n[2\\. Artificial Intelligence: Opportunities, Risks and Regulation](https://www.prometheusnetwork.eu/wp-content/uploads/2023/11/PromethEUs_Joint-Publication_AI.pdf)\n\n[3\\. Decoding Semiconductor Market Risks: An Econometric Approach to Modeling Semiconductor Market Risk Through Volatility and Economic Uncertainty](https://liu.diva-portal.org/smash/get/diva2:1878404/ATTACHMENT01.pdf)\n\n[4\\. Artificial Intelligence Model Risk Management: Observations from a Thematic Review](https://www.mas.gov.sg/-/media/mas-media-library/publications/monographs-or-information-paper/imd/2024/information-paper-on-ai-risk-management-final.pdf)\n\n[5\\. McKinsey: Generative AI to Transform Risk Management in The Next 5 Years](https://fintechnews.sg/94699/ai/mckinsey-generative-ai-risk-management/)\n\n[6\\. Navigating the security landscape of generative AI](https://docs.aws.amazon.com/pdfs/whitepapers/latest/navigating-security-landscape-genai/navigating-security-landscape-genai.pdf)\n\n[7\\. AI law, regulation and policy - highlights from 2024 and what to look forward to in 2025](https://www.burges-salmon.com/articles/102jr1b/ai-law-regulation-and-policy-highlights-from-2024-and-what-to-look-forward-to/)\n\n[8\\. A generative AI reset: Rewiring to turn potential into value in 2024](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/a-generative-ai-reset-rewiring-to-turn-potential-into-value-in-2024)\n\n[9\\. A new future of work: The race to deploy AI and raise skills in Europe and beyond](https://www.mckinsey.de/~/media/mckinsey/locations/europe%20and%20middle%20east/deutschland/news/presse/2024/2024%20-%2005%20-%2023%20mgi%20genai%20future%20of%20work/mgi%20report_a-new-future-of-work-the-race-to-deploy-ai.pdf)\n\n[10\\. Designing a Modular University-Based Online Course Framework for Ethical and Efficient Utilization of Generative Artificial Intelligence](https://scholarspace.manoa.hawaii.edu/server/api/core/bitstreams/7d863060-bedf-446e-9862-154add2711fc/content)\n\n[11\\. Technology Assessment: Artificial Intelligence - Generative AI's Environmental and Human Effects](https://www.gao.gov/assets/gao-25-107172.pdf)\n\n[12\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[13\\. Lareina Yee](https://www.mckinsey.com/our-people/lareina-yee)\n\n[14\\. Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile](https://site.unibo.it/hypermodelex/en/publications/2024-04-01-nist-ai-risk-management-genai.pdf/@@download/file/2024-04-01-NIST-AI-RISK-MANAGEMENT-GENAI.pdf)\n\n[15\\. Implementing generative AI with speed and safety](https://www.mckinsey.com/it/our-insights/implementing-generative-ai-with-speed-and-safety)\n\n[16\\. Safety and security risks of generative artificial intelligence to 2025 (Annex B)](https://www.gov.uk/government/publications/frontier-ai-capabilities-and-risks-discussion-paper/safety-and-security-risks-of-generative-artificial-intelligence-to-2025-annex-b)\n\n[17\\. An Insider’s Guide to Designing and Operationalizing a Responsible AI Governance Framework](https://www.equalai.org/assets/docs/230928_EQUAL_AI_Whitepaper_V4.3.pdf)\n\n[18\\. AI Statistics: Trends, Risks, and Future Predictions (2025 Updated)](https://www.mindinventory.com/blog/ai-statistics/)\n\n[19\\. Generative AI: what it is and what are the business applications of systems like ChatGPT](https://coding180.com/blog/ai/generative-ai-business-applications)\n\n[21\\. AI in 2025: current initiatives and challenges in large enterprises](https://www.wavestone.com/wp-content/uploads/2025/01/ai-action-summit-report.pdf)\n\n[22\\. APPLYING AI IN KEY EUROPEAN INDUSTRIES](https://www.sitra.fi/wp/wp-content/uploads/2025/03/sitra-applying-ai-in-key-european-industries.pdf)\n\n[23\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[24\\. Generative AI Operations for Organizations with MLOps Investments](https://learn.microsoft.com/en-gb/azure/architecture/ai-ml/guide/genaiops-for-mlops)\n\n[25\\. State of generative AI in the enterprise](https://reinvent.awsevents.com/content/dam/reinvent/2024/slides/aim/AIM354_Accelerate-production-for-gen-AI-using-Amazon-SageMaker-MLOps-and-FMOps.pdf)\n\n[26\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[27\\. Generative AI for Supply Chain](https://www.scl.gatech.edu/sites/default/files/presentations/GTSCL-GenAI_20250102.pdf)\n\n[28\\. Top 10 MLOps Platforms for Scalable AI in Summer 2025](https://azumo.com/artificial-intelligence/ai-insights/mlops-platforms)\n\n[29\\. Enhancing generative AI for accessing enterprise knowledge](https://www.theseus.fi/bitstream/10024/865620/1/KhanU_KudryavtsevD_KauttonenJ_Enchancing_generative_AI.pdf)\n\n[30\\. Navigating the LLM Landscape: Operationalizing Generative AI with MLOps](https://www.openagi.codes/AWS-LLMOps/)\n\n[31\\. Integrating DevOps and Large Language Model Operations (LLMOps) for GenAI-Enabled E-commerce Innovations A Pathway to Intelligent Automation](https://wjarr.com/sites/default/files/WJARR-2024-3725.pdf)\n\n[32\\. 麦肯锡2024年全球科技趋势展望](http://www.360doc.com/content/24/0815/20/3066843_1131462679.shtml)\n\n[33\\. Virtual Summit: Generative AI Meets Responsible AI](https://www.fiddler.ai/generative-ai-meets-responsible-ai)\n\n[34\\. AI in Digital Transformation Strategy 2025: 6 Key Trends for Large Companies](https://ttms.com/ai-in-digital-transformation-strategy-6-key-trends-for-large-companies/)\n\n[35\\. Generative AI Services](https://www.nttdata.com/global/en/-/media/nttdataglobal/1_files/media/press-release/2025/me_pr_jan_09_01.pdf?rev=516f913920d744a8aee8712ab9f4de2d)\n\n[36\\. The Magnificent 7: Mastering key MLops strategies for 2025](https://technative.io/the-magnificent-7-mastering-key-mlops-strategies-for-2025/)\n\n[37\\. Generative AI in the Enterprise – Model Customization](https://www.aitimes.kr/news/download.php?subUploadDir=202311/&savefilename=29266_185.pdf&filename=h19825-gen-ai-model%20customization.pdf&idxno=185)\n\n[41\\. Megatrending 2025](https://www.pictet.com/content/dam/www/documents/corporate-publications/Megatrending-2025-final-EN_WEB.pdf.coredownload.pdf)\n\n[42\\. APPLYING AI IN KEY EUROPEAN INDUSTRIES](https://www.sitra.fi/wp/wp-content/uploads/2025/03/sitra-applying-ai-in-key-european-industries.pdf)\n\n[43\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[44\\. Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts](https://hal.science/hal-04908002v1/document)\n\n[45\\. Impact Report 2025: The Agentic AI Industry](https://ahdustechnology.fi/impact-report-2025-the-agentic-ai-industry/)\n\n[46\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[47\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[48\\. AI in the pharmaceutical industry - trends for 2025](https://ttpsc.com/en/blog/ai-trends-in-pharmaceutical-industry-2025/)\n\n[49\\. AI Business Trends 2025](https://cdn.prod.website-files.com/64d4fcb1399fbd4505c91827/67766300c1520f448e25e589_google_cloud_ai_trends.pdf)\n\n[50\\. Navigating the Boom: Confronting Generative AI’s Most Pressing Questions](https://www.williamblair.com/-/media/downloads/eqr/2025/williamblair_navigating-the-boom-confronting-generative-ais-most-pressing-questions.pdf)\n\n[51\\. AI Budget Planning for 2025: Modernize to Compete](https://2515688.fs1.hubspotusercontent-na1.net/hubfs/2515688/2024_budgeting_campaign/ai_budget_planning_for_2025-_modernize_to_compete_8-1-24_v-2%20copy-1.pdf)\n\n[52\\. The Enterprise Edge in an AI-Centric World](https://newsroom.stelia.ai/wp-content/uploads/2025/03/Stelia_The-Enterprise-Edge-in-an-AI_Centric-World.pdf)\n\n[53\\. The Next Generation Intelligent Manufacturing with Generative AI](https://activate.fujitsu/-/media/Project/Fujitsu/Fujitsu-Activate/key-technologies-article/ta-intelligent-manufacturing-generative-ai-20250110/ta-intelligent-manufacturing-generative-ai-20250110-en.pdf?rev=b591726d5e3d419c9a9e97349c7f7748&hash=5267CAA99043DFF2F44B0C60CBDB58A3)\n\n[54\\. AI for CMOs: The Real-World Blueprint for AI-Powered Digital Transformation (2nd Edition)](https://www.marketingaiinstitute.com/hubfs/AI%20for%20CMOs%202023.pdf)\n\n[55\\. 麦肯锡报告：美容行业Gen AI应用加速，这四大场景不容错过](https://www.industrysourcing.cn/article/464664)\n\n[56\\. 人工智能专题报告：生成式人工智能产业全梳理](https://www.ahchanye.com/wp-content/uploads/2023/04/2023041213432920.pdf)\n\n[57\\. AI in 2025: Structured Strategic Insights for Decision-Makers](https://ugc.production.linktr.ee/db4b3471-a59d-492a-ab25-19d4df2c6039_AI-in-2025--Structured-Strategic-Insights-for-Decision-Makers--1-.pdf)\n\n[58\\. 2025: Finding Value in Established Tech](https://www.thinkandinvest.com/p/2025-finding-value-in-established)\n\n[59\\. AI Chatbots: Cut Response Time by 70% & Boost Leads](https://sdh.global/blog/ai-ml/chatbots-2025-how-to-cut-response-time-by-70-and-generate-more-leads-588/)\n\n[61\\. The 2025 Generative AI Implementation Guide](https://www.hkdca.com/wp-content/uploads/2025/04/genai-implementation-guide-koreai.pdf)\n\n[62\\. Generative AI in Business: Visual Illustrations of Applications and Insights from Q1 2025](https://www.preprints.org/frontend/manuscript/a9062d6cee318cbddd886d9fe11c1c18/download_pub)\n\n[63\\. 行政の進化と革新のための生成AIの調達・利活用に係るガイドラインについて（概要）](https://public-comment.e-gov.go.jp/pcm/download?seqNo=0000293331)\n\n[64\\. AI-driven transformation of business models: new opportunities for startups in the global marketplace](https://www.scirj.org/papers-0824/scirj-P0824990.pdf)\n\n[65\\. AI BASELINE GUIDANCE REVIEW](https://www.cmorg.org.uk/sites/default/files/2025-05/CMORG%20-%20AI%20Baseline%20Guidance%20Review%20-%20April%202025%20-%20TLP%20CLEAR.pdf)\n\n[66\\. McKinsey: Generative AI to Transform Risk Management in The Next 5 Years](https://fintechnews.sg/94699/ai/mckinsey-generative-ai-risk-management/)\n\n[67\\. Generative AI Risk Management Framework: A Practical Guide for 2025](https://www.gsdcouncil.org/_ajax/service/downloadAttachmentById/682b03c00a4f5c2a1b4a11a5)\n\n[68\\. iPCMC2024 8th International Project and Construction Management Conference Proceedings Book](https://ipcmc2024.yildiz.edu.tr/wp-content/uploads/2024/07/IPCMC2024_Proceedings-Book_Final.pdf)\n\n[69\\. Decoding Semiconductor Market Risks: An Econometric Approach to Modeling Semiconductor Market Risk Through Volatility and Economic Uncertainty](https://liu.diva-portal.org/smash/get/diva2:1878404/ATTACHMENT01.pdf)\n\n[70\\. Master Thesis “Role of Generative Artificial Intelligence in modern management consulting industry and its impact on the future”](https://zenodo.org/records/13325321/files/Role%20of%20Generative%20Artificial%20Intelligence%20in%20modern%20management%20consulting%20industry%20and%20its%20impact%20on%20the%20future.pdf?download=1)\n\n[71\\. Implementing generative AI with speed and safety](https://www.mckinsey.com/it/our-insights/implementing-generative-ai-with-speed-and-safety)\n\n[72\\. Lareina Yee](https://www.mckinsey.com/our-people/lareina-yee)\n\n[73\\. Future-Proofing Customer Data: Trends and Predictions for AI Risk Management in 2025 and Beyond](https://superagi.com/future-proofing-customer-data-trends-and-predictions-for-ai-risk-management-in-2025-and-beyond/)\n\n[74\\. 5 Quick Steps to Create Generative AI Security Standards](https://www.lasso.security/blog/5-quick-steps-to-create-generative-ai-security-standards-and-free-policy)\n\n[81\\. 10 MLOps Platforms to Streamline Your AI Deployment in 2025](https://www.digitalocean.com/resources/articles/mlops-platforms)\n\n[82\\. MLOps Workflow for Docker-Based AI Model Deployment](https://www.runpod.io/articles/guides/mlops-workflow-docker-ai-deployment)\n\n[83\\. MLOps strategies for scaling enterprise AI initiatives](https://www.sigmoid.com/ebooks-whitepapers/ml-models-poc-to-production/)\n\n[84\\. Introducing MLOps: How to Scale Machine Learning in the Enterprise](https://itsocial.fr/wp-content/uploads/2021/04/Comment-mettre-%C3%A0-l%E2%80%99%C3%A9chelle-le-Machine-Learning-en-entreprise.pdf)\n\n[85\\. ENTERPRISE AI IN THE CLOUD: A Practical Guide to Deploying End-to-End Machine Learning and ChatGPT™ Solutions](https://kc.umn.ac.id/id/eprint/29663/1/ENTERPRISE%20AI%20IN%20THE%20CLOUD%20A%20Practical%20Guide%20to%20Deploying%20End-to-End%20Machine%20Learning%20and%20ChatGPT%E2%84%A2%20Solutions.pdf)\n\n[86\\. Machine Learning at Scale: High-performance, low-cost machine learning to accelerate generative AI development](https://pages.awscloud.com/rs/112-TZM-766/images/GC-400_ml%20at%20scale-ps_eBook_Final_V2.pdf)\n\n[87\\. End-to-End MLOps for Scalable Model Deployment: Engineering Best Practices for Efficient and Reliable Machine Learning Systems](https://ijcttjournal.org/2024/Volume-72%20Issue-11/IJCTT-V72I11P118.pdf)\n\n[88\\. FlashStack for AI: MLOps using Red Hat OpenShift AI](https://www.cisco.com/c/en/us/td/docs/unified_computing/ucs/UCS_CVDs/flashstack_ai_ml_ops.epub)\n\n[89\\. Scaling AI with MLOps and the NVIDIA Partner Ecosystem](https://developer.nvidia.com/blog/scaling-ai-with-mlops-and-the-nvidia-partner-ecosystem/)\n\n[90\\. GitHub - afra16181falakh/vertex-AI-ML-OPS: Google Cloud Platform Vertex AI end-to-end workflows for machine learning operations](https://github.com/afra16181falakh/vertex-AI-ML-OPS)\n\n[91\\. MLOps Productionalization](https://www.hcltech.com/sites/default/files/documents/resources/brochure/files/mlops_productionalization_brochure.pdf)\n\n[92\\. Developing scalable quality assurance pipelines for AI systems: Leveraging LLMs in enterprise applications](https://journalwjarr.com/sites/default/files/fulltext_pdf/WJARR-2025-1268.pdf)\n\n[93\\. MLOps with impact: Drive business outcomes with enterprise adoption of AI](https://www2.deloitte.com/content/dam/Deloitte/us/Documents/consulting/us-mlops-with-impact.pdf)\n\n[94\\. 麦肯锡：2024从LLM(大语言模型)到ROI(投资回报率)：如何在零售业中扩大生成式AI的采用规模](https://www.sgpjbg.com/baogao/173280.html)\n\n[95\\. Operationalizing Generative AI on Vertex AI using MLOps](http://moonknight.cn/MyBlog/assets/file/day5.pdf)\n\n[96\\. Generative AI in the Enterprise with AMD Accelerators](https://media.bitpipe.com/io_31x/io_315126/item_2828616/Generative%20AI%20in%20the%20Enterprise%20With%20AMD%20Accelerators.pdf)\n\n[97\\. Accenture Enterprise AI – Scaling Machine Learning and Deep Learning Models: AWS Whitepaper](https://docs.aws.amazon.com/whitepapers/latest/accenture-ai-scaling-ml-and-deep-learning-models/accenture-ai-scaling-ml-and-deep-learning-models.pdf)\n\n[98\\. Fast-track your generative AI workloads into production](https://pages.awscloud.com/rs/112-TZM-766/images/AWS%20%26%20Loka%20Webinar%20POC%20to%20Production%20Final.pdf?version=0)\n\n[99\\. Moving past gen AI’s honeymoon phase: Seven hard truths for CIOs to get from pilot to scale](https://www.mckinsey.com/uk/our-insights/moving-past-gen-ais-honeymoon-phase-seven-hard-truths-for-cios-to-get-from-pilot-to-scale)\n\n[100\\. Enterprise-Ready MLOps Platform for Scalable AI & ML](https://www.genesiscloud.com/solutions/mlops-platform)\n\n[101\\. Impact Report 2025: The Agentic AI Industry](https://ahdustechnology.fi/impact-report-2025-the-agentic-ai-industry/)\n\n[102\\. Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts](https://hal.science/hal-04908002v1/document)\n\n[103\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[104\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[105\\. 100+ Top Generative AI Statistics for 2024](https://www.mlyearning.org/generative-ai-statistics/)\n\n[106\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[107\\. AI Search Engines Report 2025: Market Trends, User Trust, and Platform Rankings](https://www.allaboutai.com/resources/ai-statistics/ai-search-engines/)\n\n[108\\. Navigating the Boom: Confronting Generative AI’s Most Pressing Questions](https://www.williamblair.com/-/media/downloads/eqr/2025/williamblair_navigating-the-boom-confronting-generative-ais-most-pressing-questions.pdf)\n\n[109\\. Generative AI Landscape: Shaping the Technology of Tomorrow](https://www.kirtanepandit.com/pdf/1737441450.Generative%20AI%20Report.pdf)\n\n[110\\. 2025年人工智能指数报告](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025_chinese_version_061325.pdf)\n\n[111\\. 5 Ways McKinsey Is Using AI \\[Case Studies\\] \\[2025\\]](https://digitaldefynd.com/IQ/ways-mckinsey-is-using-ai/)\n\n[112\\. Generative AI Use Cases Across Industries: A Strategic 2025 Report](https://hatchworks.com/blog/gen-ai/generative-ai-use-cases/)\n\n[113\\. Tech Trends 2025: McKinsey's AI & Quantum Computing Forecast](https://www.accio.com/business/tech_trends_2025_mckinsey)\n\n[114\\. 2025: Finding Value in Established Tech](https://www.thinkandinvest.com/p/2025-finding-value-in-established)\n\n[115\\. 麦肯锡报告：美容行业Gen AI应用加速，这四大场景不容错过](https://www.industrysourcing.cn/article/464664)\n\n[116\\. Generative AI Market Size, Share & Forecast Report, 2024-2030](https://www.psmarketresearch.com/market-analysis/generative-ai-market)\n\n[117\\. AI in Chemical Industry: Top Use Cases You Need To Know](https://smartdev.com/ai-use-cases-in-chemical-industry/)\n\n[118\\. What You Should Know from McKinsey’s New Report on Generative AI](https://verbit.ai/ai-technology/key-points-from-mckinseys-new-report-on-generative-ai/)\n\n[119\\. McKinsey: GenAI to continue to dominate business landscape](https://technologymagazine.com/articles/genai-will-continue-to-dominate-the-2024-business-landscape)\n\n[120\\. AI for CMOs: The Real-World Blueprint for AI-Powered Digital Transformation (2nd Edition)](https://www.marketingaiinstitute.com/hubfs/AI%20for%20CMOs%202023.pdf)\n\n[121\\. Artificial Intelligence Model Risk Management: Observations from a Thematic Review](https://www.mas.gov.sg/-/media/mas-media-library/publications/monographs-or-information-paper/imd/2024/information-paper-on-ai-risk-management-final.pdf)\n\n[122\\. Financial Services in the Era of Generative AI: Facilitating Responsible Adoption](https://www.hkdca.com/wp-content/uploads/2025/05/financial-services-in-era-of-genai-hkimr.pdf)\n\n[123\\. McKinsey: Generative AI to Transform Risk Management in The Next 5 Years](https://fintechnews.sg/94699/ai/mckinsey-generative-ai-risk-management/)\n\n[124\\. Top Audit Committee Priorities for 2025](https://www.mgocpa.com/perspective/audit-committee-priorities-2025/)\n\n[125\\. Model contractual clauses for the public procurement of High-Risk AI ('MCC-AI-High-Risk')](https://public-buyers-community.ec.europa.eu/system/files/2025-05/Model%20Clauses%20High%20Risk.docx)\n\n[126\\. AI-driven transformation of business models: new opportunities for startups in the global marketplace](https://www.scirj.org/papers-0824/scirj-P0824990.pdf)\n\n[127\\. THE EUROFI VIEWS MAGAZINE](https://www.eurofi.net/wp-content/uploads/2025/03/eurofi-views-warsaw-2025-1.pdf)\n\n[128\\. Master Thesis “Role of Generative Artificial Intelligence in modern management consulting industry and its impact on the future”](https://zenodo.org/records/13325321/files/Role%20of%20Generative%20Artificial%20Intelligence%20in%20modern%20management%20consulting%20industry%20and%20its%20impact%20on%20the%20future.pdf?download=1)\n\n[129\\. Future-Proofing Customer Data: Trends and Predictions for AI Risk Management in 2025 and Beyond](https://superagi.com/future-proofing-customer-data-trends-and-predictions-for-ai-risk-management-in-2025-and-beyond/)\n\n[130\\. 2025 Artificial Intelligence Report: Roadmap and Annual Update](https://www.miamidade.gov/technology/library/artificial-intelligence-report-2025.pdf)\n\n[131\\. Decoding Semiconductor Market Risks: An Econometric Approach to Modeling Semiconductor Market Risk Through Volatility and Economic Uncertainty](https://liu.diva-portal.org/smash/get/diva2:1878404/ATTACHMENT01.pdf)\n\n[132\\. Implementing generative AI with speed and safety](https://www.mckinsey.com/it/our-insights/implementing-generative-ai-with-speed-and-safety)\n\n[133\\. International AI Safety Report](http://maruyama-mitsuhiko.cocolog-nifty.com/security/files/international_ai_safety_report_2025_accessible_f.docx)\n\n[134\\. The 2025 Generative AI Implementation Guide](https://www.hkdca.com/wp-content/uploads/2025/04/genai-implementation-guide-koreai.pdf)\n\n[135\\. AI in Banking: 2025 Trends](https://www.devoteam.com/expert-view/ai-in-banking-2025-trends/)\n\n[136\\. Banking on AI: How U.S. Financial Institutions are Transforming with Artificial Intelligence](https://pyramidci.com/wp-content/uploads/2025/04/PCI_032025_Whitepaper_Banking-on-AI-1.pdf)\n\n[137\\. iPCMC2024 8th International Project and Construction Management Conference Proceedings Book](https://ipcmc2024.yildiz.edu.tr/wp-content/uploads/2024/07/IPCMC2024_Proceedings-Book_Final.pdf)\n\n[138\\. Lareina Yee](https://www.mckinsey.com/our-people/lareina-yee)\n\n[139\\. Implementing Generative AI with Speed and Safety](https://www.mckinsey.com/us/our-insights/implementing-generative-ai-with-speed-and-safety)\n\n[141\\. 10 Best MLOps Platforms of 2025](https://www.truefoundry.com/blog/mlops-tools)\n\n[142\\. Reference Architecture for Generative AI Based on Large Language Models (LLMs)](https://lenovopress.lenovo.com/lp1798.pdf)\n\n[143\\. Building an Enterprise-Class AI/ML Infrastructure for MLOps](https://www.ciscolive.com/c/dam/r/ciscolive/global-event/docs/2024/pdf/BRKCOM-2018.pdf)\n\n[144\\. Top 10 MLOps Platforms for Scalable AI in Summer 2025](https://azumo.com/artificial-intelligence/ai-insights/mlops-platforms)\n\n[145\\. Generative AI Operations for Organizations with MLOps Investments](https://learn.microsoft.com/en-in/azure/architecture/ai-ml/guide/genaiops-for-mlops)\n\n[146\\. 企业生成式AI：2024 年企业的 10 多个用例和最佳实践](https://developer.volcengine.com/articles/7395500807572963337)\n\n[147\\. Generative AI in the Enterprise – Model Customization](https://www.aitimes.kr/news/download.php?subUploadDir=202311/&savefilename=29266_185.pdf&filename=h19825-gen-ai-model%20customization.pdf&idxno=185)\n\n[148\\. The Magnificent 7: Mastering key MLops strategies for 2025](https://technative.io/the-magnificent-7-mastering-key-mlops-strategies-for-2025/)\n\n[149\\. 麦肯锡：2024从LLM(大语言模型)到ROI(投资回报率)：如何在零售业中扩大生成式AI的采用规模](https://www.sgpjbg.com/baogao/173280.html)\n\n[150\\. Agents Companion](https://elhacker.info/manuales/Agents_Companion_v2%20%283%29.pdf)\n\n[151\\. Technology’s generational moment with generative AI: A CIO and CTO guide](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/technologys-generational-moment-with-generative-ai-a-cio-and-cto-guide)\n\n[152\\. 麦肯锡2024年全球科技趋势展望](http://www.360doc.com/content/24/0815/20/3066843_1131462679.shtml)\n\n[153\\. The State of Generative AI Adoption and Its Value: A Detailed Analysis](https://vivid-cow-9924242169.media.strapiapp.com/The_State_of_Generative_AI_Adoption_and_Its_Value_A_Detailed_Analysis_2_8b0708e212.pdf)\n\n[154\\. A generative AI reset: Rewiring to turn potential into value in 2024](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/a-generative-ai-reset-rewiring-to-turn-potential-into-value-in-2024?cid=other-eml-dre-mip-mck&hlkid=b2f20b1f7e0142289cbb1097742df4fc&hctky=15132087&hdpid=5c79bd47-bfc6-40ef-b395-330c06542b14)\n\n[155\\. Generative AI in the Enterprise with AMD Accelerators](https://media.bitpipe.com/io_31x/io_315126/item_2828616/Generative%20AI%20in%20the%20Enterprise%20With%20AMD%20Accelerators.pdf)\n\n[156\\. Enterprise AI: Generative AI & Large Language Models for Enterprise Partner Enablement Package](https://cdrdv2-public.intel.com/817880/Enterprise%20AI%20Partner%20Enablement%20Package%20-%20Technical%20Overview.pdf)\n\n[157\\. Generative AI for the enterprise](https://d1.awsstatic.com/events/Summits/nycsummit2023/PRT205-S_Cloudera_GenerativeAI_E2_20230711_HSEdited.pdf)\n\n[161\\. Megatrending 2025](https://www.pictet.com/content/dam/www/documents/corporate-publications/Megatrending-2025-final-EN_WEB.pdf.coredownload.pdf)\n\n[162\\. Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts](https://hal.science/hal-04908002v1/document)\n\n[163\\. 100+ Top Generative AI Statistics for 2024](https://www.mlyearning.org/generative-ai-statistics/)\n\n[164\\. AI and the Impact on the Car Buying, Leasing and Financing Market](https://cioinfluence.com/machine-learning/ai-and-the-impact-on-the-car-buying-leasing-and-financing-market/)\n\n[165\\. Impact Report 2025: The Agentic AI Industry](https://ahdustechnology.fi/impact-report-2025-the-agentic-ai-industry/)\n\n[166\\. Scaling Generative AI for Value: Data Leader Agenda for 2025](https://d1.awsstatic.com/psc-digital/2024/gc-600/cdo-biz-value/CDO-Agenda-2025-ScalingGenerativeAIforValue.pdf)\n\n[167\\. 2025年人工智能指数报告](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025_chinese_version_061325.pdf)\n\n[168\\. The Rise of AI-Generated Content: Expert Insights on the 90% AI-Powered Web by 2025](https://www.makebot.ai/blog-en/the-rise-of-ai-generated-content-expert-insights-on-the-90-ai-powered-web-by-2025)\n\n[169\\. APPLYING AI IN KEY EUROPEAN INDUSTRIES](https://www.sitra.fi/wp/wp-content/uploads/2025/03/sitra-applying-ai-in-key-european-industries.pdf)\n\n[170\\. AI Industry Outlook 2025: Global Trends in the US, Europe, and Asia](https://www.britopian.com/wp-content/uploads/2025/03/AI-Industry-Outlook-2025-Global-Trends.pdf)\n\n[171\\. AI Statistics: Trends, Risks, and Future Predictions (2025 Updated)](https://www.mindinventory.com/blog/ai-statistics/)\n\n[172\\. Generative AI Market Size, Share & Forecast Report, 2024-2030](https://www.psmarketresearch.com/market-analysis/generative-ai-market)\n\n[173\\. 2025: Finding Value in Established Tech](https://www.thinkandinvest.com/p/2025-finding-value-in-established)\n\n[174\\. Generative AI Landscape: Shaping the Technology of Tomorrow](https://www.kirtanepandit.com/pdf/1737441450.Generative%20AI%20Report.pdf)\n\n[175\\. AI for CMOs: The Real-World Blueprint for AI-Powered Digital Transformation (2nd Edition)](https://www.marketingaiinstitute.com/hubfs/AI%20for%20CMOs%202023.pdf)\n\n[176\\. Investing in AI: Everything, everywhere, and all at once](https://assets.ctfassets.net/tl4x668xzide/3gcutAmPXF01g2zGLPeWPd/f6a20a2f47c0b8d6e649bfa8a6e1d370/investing-in-ai-everything-everywhere-and-all-at-once.pdf)\n\n[177\\. Generative AI: What leaders must know and do to win with transformative technology](https://www.kaartech.com/wp-content/uploads/2025/01/Generative-AI-What-leaders-must-know-and-do-to-win-with-transformative-technology-.pdf)\n\n[178\\. 80+ Up-to-Date AI Statistics for 2025 (No Stale Sources)](https://ahrefs.com/blog/ai-statistics/)\n\n[181\\. AI RMF 1.0 Controls Checklist](https://www.aigl.blog/ai-rmf-1-0-controls-checklist/)\n\n[182\\. Top considerations for your business use of GenAI: Five checklists for 2025](https://www.blg.com/-/media/insights/2024/documents/blg---top-considerations-for-your-business-use-of-genai-five-checklists-for-2025.pdf)\n\n[183\\. Artificial Intelligence Model Risk Management: Observations from a Thematic Review](https://www.mas.gov.sg/-/media/mas-media-library/publications/monographs-or-information-paper/imd/2024/information-paper-on-ai-risk-management-final.pdf)\n\n[184\\. Rearchitecting your infrastructure for generative AI](https://info.productiveedge.com/hubfs/Ebooks/Google%20Cloud%20eBook%20-%20Rearchitecting%20Your%20Infrastructure%20for%20Generative%20AI.pdf)\n\n[185\\. Top Audit Committee Priorities for 2025](https://www.mgocpa.com/perspective/audit-committee-priorities-2025/)\n\n[186\\. The Guideline for Japanese Governments' Procurements and Utilizations of Generative AI for the sake of Evolution and Innovation of Public Administration](https://www.digital.go.jp/assets/contents/node/basic_page/field_ref_resources/e2a06143-ed29-4f1d-9c31-0f06fca67afc/6e45a64f/20250527_resources_standard_guidelines_guideline_04.pdf)\n\n[187\\. McKinsey: Generative AI to Transform Risk Management in The Next 5 Years](https://fintechnews.sg/94699/ai/mckinsey-generative-ai-risk-management/)\n\n[188\\. Navigating the security landscape of generative AI](https://docs.aws.amazon.com/pdfs/whitepapers/latest/navigating-security-landscape-genai/navigating-security-landscape-genai.pdf)\n\n[189\\. Six Strategies to Prepare for Trusted Generative AI](https://www.salesforce.com/content/dam/web/en_au/www/documents/research/23191-01_sf_generativeai_midebook_anz_final.pdf)\n\n[190\\. Implementing generative AI with speed and safety](https://www.mckinsey.com/it/our-insights/implementing-generative-ai-with-speed-and-safety)\n\n[191\\. Global AI Regulatory Update - May 2025](https://www.eversheds-sutherland.com/en/global/insights/global-ai-regulatory-update-may-2025)\n\n[192\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[193\\. Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile](https://airc.nist.gov/docs/NIST.AI.600-1.GenAI-Profile.ipd.pdf)\n\n[194\\. AI BASELINE GUIDANCE REVIEW](https://www.cmorg.org.uk/sites/default/files/2025-05/CMORG%20-%20AI%20Baseline%20Guidance%20Review%20-%20April%202025%20-%20TLP%20CLEAR.pdf)\n\n[195\\. Best 9 Compliance Risk Assessment Tools for 2025](https://www.centraleyes.com/best-7-compliance-risk-assessment-tools/)\n\n[196\\. 2025年全球GRC基準調查](https://www.cga.org.tw/f_2_01_news.aspx?news_aid=4799)\n\n[197\\. McKinsey Article Addresses AI Risk, Regulatory Response](https://www.mfdf.org/news-resources/news/2024/01/31/mckinsey-article-addresses-ai-risk-regulatory-response)\n\n[201\\. AI-Powered Construction Document Analysis by Leveraging Computer Vision and Large Language Models](https://aws.amazon.com/blogs/spatial/ai-powered-construction-document-analysis-by-leveraging-computer-vision-and-large-language-models/)\n\n[202\\. A Complete Guide to MLOps Architecture in 2025](https://www.moontechnolabs.com/blog/mlops-architecture/)\n\n[203\\. Operationalizing Generative AI on Vertex AI using MLOps](http://moonknight.cn/MyBlog/assets/file/day5.pdf)\n\n[204\\. Generative AI and Its Transformative Potential](https://mdpi-res.com/bookfiles/book/10996/Generative_AI_and_Its_Transformative_Potential.pdf?v=1749029438)\n\n[205\\. The Machine Learning Solutions Architect Handbook](https://sciendo.com/2/v2/download/chapter/9781805124825/10.0000/9781805124825-001.pdf?Token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VycyI6W3sic3ViIjoyNTY3ODUxNywicHVicmVmIjoiNzY0NDg4IiwibmFtZSI6Ikdvb2dsZSBHb29nbGVib3QgLSBXZWIgQ3Jhd2xlciBTRU8iLCJ0eXBlIjoiaW5zdGl0dXRpb24iLCJsb2dvdXRfbGluayI6Imh0dHBzOi8vY29ubmVjdC5saWJseW54LmNvbS9sb2dvdXQvNjgxNDQzNDBlOGI4ZmY4OTY2ZmZjNWUyNTk3NWU4NTMiLCJhdXRoX21ldGhvZCI6ImlwIiwiaXAiOiI2Ni4yNDkuNzkuMiIsImNvdW50ZXJwYXJ0eV9pZCI6Ijc2NDQ4OCJ9XSwiaWF0IjoxNzQ2MTYxMjIxLCJleHAiOjE3NDczNzA4MjF9.3EgYWpVJfB7rVtMuBptYg975z4pyU_8mBg8xfdy20E0)\n\n[206\\. MLOps for Highly Autonomous Networks](https://www.ngmn.org/wp-content/uploads/MLOPs_for_highly_autonomous_networks_V1.2.pdf)\n\n[207\\. 麦肯锡：2024从LLM(大语言模型)到ROI(投资回报率)：如何在零售业中扩大生成式AI的采用规模](https://www.sgpjbg.com/baogao/173280.html)\n\n[208\\. Operationalize generative AI applications using LLMOps](https://d1.awsstatic.com/events/Summits/reinvent2023/GAM401_Operationalize-generative-AI-applications-using-LLMOps.pdf)\n\n[209\\. Eric Lamarre](https://www.mckinsey.com/our-people/eric-lamarre)\n\n[210\\. D. Budgen, P. Brereton. “Performing systematic literature reviews in software engineering.” Proceedings of the 28th international conference on Software engineering](https://doi.org/10.1145/1134285.1134500)\n\n[211\\. Catalogue de formations 2025](https://www.institut.capgemini.fr/wp-content/uploads/Capgemini-Institut-Catalogue-de-formations-2025-1.pdf)\n\n[212\\. Impact Report 2025: The Agentic AI Industry](https://ahdustechnology.fi/impact-report-2025-the-agentic-ai-industry/)\n\n[213\\. MLOps For Dummies®, Databricks Special Edition](https://www.alvinang.sg/s/MLOps-for-Dummies-Databricks.pdf)\n\n[214\\. Introduction to Machine Learning Systems](https://mlsysbook.ai/Machine-Learning-Systems.pdf)\n\n[215\\. 2025 DATA & AI RADAR: 10 challenges to master your Data & AI transformation in 2025](https://www.wavestone.com/wp-content/uploads/2025/01/2025-data-ai-radar.pdf)\n\n[216\\. An Analysis of MLOps Architectures: A Systematic Mapping Study](https://arxiv.org/html/2406.19847v1)\n\n[217\\. 麦肯锡：生成式人工智能：半导体行业的下一个S曲线？](https://www.sgpjbg.com/baogao/163357.html)\n\n[221\\. 5 Ways McKinsey Is Using AI \\[Case Studies\\] \\[2025\\]](https://digitaldefynd.com/IQ/ways-mckinsey-is-using-ai/)\n\n[222\\. Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts](https://hal.science/hal-04908002v1/document)\n\n[223\\. Artificial Intelligence Index Report 2025](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)\n\n[224\\. 100+ Top Generative AI Statistics for 2024](https://www.mlyearning.org/generative-ai-statistics/)\n\n[225\\. AI Chatbots: Cut Response Time by 70% & Boost Leads](https://sdh.global/blog/ai-ml/chatbots-2025-how-to-cut-response-time-by-70-and-generate-more-leads-588/)\n\n[226\\. Generative AI Market Size, Share & Forecast Report, 2024-2030](https://www.psmarketresearch.com/market-analysis/generative-ai-market)\n\n[227\\. McKinsey’s ecosystem of strategic alliances brings the power of generative AI to clients](https://www.mckinsey.com/about-us/new-at-mckinsey-blog/mckinsey-alliances-bring-the-power-of-generative-ai-to-clients)\n\n[228\\. 2025年人工智能指数报告](https://pdf.dfcfw.com/pdf/H3_AP202506201694220072_1.pdf?1750413587000.pdf)\n\n[229\\. McKinsey: GenAI to continue to dominate business landscape](https://technologymagazine.com/articles/genai-will-continue-to-dominate-the-2024-business-landscape)\n\n[230\\. 2025: Finding Value in Established Tech](https://www.thinkandinvest.com/p/2025-finding-value-in-established)\n\n[231\\. Navigating the Boom: Confronting Generative AI’s Most Pressing Questions](https://www.williamblair.com/-/media/downloads/eqr/2025/williamblair_navigating-the-boom-confronting-generative-ais-most-pressing-questions.pdf)\n\n[232\\. What You Should Know from McKinsey’s New Report on Generative AI](https://verbit.ai/ai-technology/key-points-from-mckinseys-new-report-on-generative-ai/)\n\n[233\\. 麦肯锡报告：美容行业Gen AI应用加速，这四大场景不容错过](https://www.industrysourcing.cn/article/464664)\n\n[234\\. Impact Report 2025: The Agentic AI Industry](https://ahdustechnology.fi/impact-report-2025-the-agentic-ai-industry/)\n\n[235\\. 100 articles on generative AI](https://www.mckinsey.com/featured-insights/themes/100-articles-on-generative-ai)\n\n[236\\. The Guide to Understanding Generative AI for 2024!](https://www.purpleslate.com/generative-ai-guide/)\n\n[237\\. How to Deploy and Scale Generative AI Efficiently and Cost-Effectively](https://hbr.org/sponsored/2024/10/how-to-deploy-and-scale-generative-ai-efficiently-and-cost-effectively)\n\n[238\\. Insights on Artificial Intelligence](https://www.mckinsey.com/capabilities/quantumblack/our-insights)\n\n[239\\. Q2 2024 Investor Presentation](https://innodata.com/wp-content/uploads/2024/08/INOD-Investor-Presentation-August-2024.pdf)\n\n[240\\. The Enterprise Edge in an AI-Centric World](https://newsroom.stelia.ai/wp-content/uploads/2025/03/Stelia_The-Enterprise-Edge-in-an-AI_Centric-World.pdf)"
  },
  {
    "id": 65,
    "question": "How should knowledge graphs evolve in the era of Large Language Models? What are their complementary roles and future directions?",
    "response": "**Research Report: Evolution of Knowledge Graphs in the Era of Large Language Models**\n\n**1\\. Introduction: Convergence of Two Paradigms**\n\nThe advent of large language models (LLMs) has fundamentally reshaped knowledge representation and reasoning. However, LLMs exhibit critical limitations including **factual hallucinations** (up to 70% in specialized domains) \\[29\\], **temporal knowledge stagnation**, and **opaque reasoning processes**. Knowledge graphs (KGs) offer complementary strengths: structured semantic relationships, verifiable facts, and explicit reasoning paths. As of 2025, the integration of KGs and LLMs has evolved beyond simple retrieval augmentation toward **bidirectional neuro-symbolic frameworks** where KGs ground LLMs’ generative capabilities while LLMs enhance KG construction and inference \\[42\\]\\[43\\]. This synthesis is particularly crucial in high-stakes domains like healthcare and finance, where inaccuracies carry significant consequences \\[64\\]\\[131\\].\n\n**2\\. Integration Techniques: Beyond Retrieval-Augmented Generation**\n\nCurrent integration methodologies demonstrate sophisticated interoperability:\n\n**2.1 Advanced Reasoning Architectures**\n\n**Graph-of-Thought (GoT) Prompting**: LLMs traverse KG subgraphs for stepwise reasoning, improving explainability in multi-hop queries by 40% \\[1\\]\\[8\\].\n\n**Logic-Guided Frameworks**: Systems like LARK use LLMs to parse natural language queries into KG-executable logical forms (e.g., SPARQL), reducing semantic parsing errors by 32% \\[4\\]\\[133\\].\n\n**Hybrid Inference Engines**: Ant Group’s KAG framework integrates **neural planning operators** (LLM-based) with **symbolic reasoners** (KG-based) for complex tasks like financial fraud detection \\[81\\].\n\n**2.2 Knowledge Representation Innovations**\n\n**Joint Embedding Models**: KEPLER and DRAGON unify textual embeddings with KG embeddings, enabling cross-modal alignment in biomedical domains \\[4\\]\\[18\\].\n\n**Dynamic Knowledge Adaptation**: LLMs dynamically update KGs using techniques like incremental learning, reducing full retraining overhead by 68% \\[12\\]\\[110\\].\n\n**2.3 Domain-Tailored Techniques**\n\n**Healthcare**: UMLS/FREEBAASE KGs enforce clinical terminology consistency, reducing diagnostic hallucinations by 73% when integrated with BioBERT \\[66\\]\\[131\\].\n\n**Finance**: Temporal KGs track market variable dependencies, improving forecast accuracy by 24% versus standalone LLMs \\[128\\].\n\n**3\\. Quantitative Benefits: Hallucination Reduction & Accuracy Gains**\n\nKGs systematically address LLM hallucinations through verifiable grounding:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n| **Metric** | **Standalone LLMs** | **KG-Augmented LLMs** | **Improvement** |\n| Hallucination Rate (CDS) | 70% | 0%  | 100% reduction \\[29\\] |\n| QA Correctness | Baseline | \\>80% | Absolute gain \\[21\\]\\[21\\] |\n| Factual Accuracy (MedQA) | 47.9% | 71.3% | 49% relative gain \\[126\\]\\[131\\] |\n| Token Efficiency | Baseline | 80% reduction | GraphRAG \\[31\\] |\n\nKey mechanisms enabling these gains:\n\n**Cross-Verification**: KGs provide real-time fact validation during generation \\[23\\]\\[32\\].\n\n**Contextual Enrichment**: Structured domain context improves semantic precision in RAG pipelines \\[21\\].\n\n**Bias Mitigation**: Finance KGs standardize credit scoring variables, reducing demographic bias by 40% \\[64\\].\n\n**4\\. Domain-Specific Tradeoffs: Healthcare vs. Finance**\n\nIntegration complexity and accuracy requirements diverge significantly across domains:\n\n**4.1 Healthcare Applications**\n\n**Accuracy Emphasis**: Patient safety mandates near-zero hallucination tolerance. KGs provide **structured medical ontologies** (e.g., ICD-11 mappings) for rigorous grounding \\[68\\].\n\n**Implementation Challenges**:\n\nHigh computational overhead from fine-tuning with biomolecular KGs (+300% latency) \\[66\\]\\[130\\].\n\nPrivacy constraints limit real-time KG updating \\[68\\].\n\n**Solutions**: Federated KG learning and differential privacy preserve confidentiality while updating clinical knowledge \\[68\\].\n\n**4.2 Financial Applications**\n\n**Latency-Accuracy Balance**: Market volatility demands sub-second inference. Quantized KG-LLMs (e.g., GPTQ) reduce precision loss to <2% versus 8% in AWQ \\[130\\]\\[132\\].\n\n**Regulatory Compliance**: Explainable KG paths audit credit decisions, satisfying FDIC mandates \\[64\\].\n\n**Unique Risks**: Incomplete financial KGs propagate biased correlations (e.g., ZIP-code-based risk models) \\[64\\].\n\n**5\\. Emerging Architectures: GraphRAG & KAG**\n\n**5.1 Microsoft’s GraphRAG**\n\n**Architecture**: Three-stage pipeline:\n\n1.  **Indexing**: Entity-relationship extraction → Community detection → Graph embedding.\n2.  **Retrieval**: Context subgraph matching via hybrid vector/graph queries \\[82\\]\\[90\\].\n3.  **Generation**: LLM synthesis with retrieved subgraph context.\n\n**Innovations**:\n\nDual-level decomposition (LightRAG) optimizes multi-hop retrieval \\[82\\].\n\n**Limitation**: Underperforms on fact-based QA versus benchmarks like HippoRAG \\[93\\].\n\n**Access**: Open-source implementation via microsoft/graphrag GitHub repo \\[158\\].\n\n**5.2 Ant Group’s KAG Framework**\n\n**Architecture Components**:\n\n**SPG (Semantic Programmable Graph)**: Underlying knowledge infrastructure using TuGraph-DB \\[81\\]\\[149\\].\n\n**K-Builder**: Autonomous KG construction from unstructured corpora.\n\n**K-Solver**: Logic-guided inference engine supporting SPARQL-like operations \\[146\\].\n\n**Advantages**:\n\nBidirectional LLM-KG indexing improves precision in e-government Q&A by 33% \\[81\\]\\[95\\].\n\nHybrid inference resolves knowledge conflicts via probabilistic soft logic \\[81\\].\n\n**Access**: Partial open-sourcing via OpenSPG/KAG GitHub \\[146\\]\\[153\\].\n\n**6\\. Performance Metrics: Computational Overhead vs. Accuracy**\n\nQuantifying tradeoffs remains challenging, but key patterns emerge:\n\n**6.1 Hallucination Reduction Metrics**\n\n**Factual Accuracy**: Ground-truth alignment scores (↑86% in MedMCQA) \\[138\\].\n\n**Verification Latency**: EigenScore detectors reduce fact-checking time by 10× versus self-consistency checks \\[115\\].\n\n**Completeness**: Essential information coverage metrics (↓12.5% omissions in KAG) \\[110\\].\n\n**6.2 Computational Overhead**\n\n**Inference Time**: Total KG-LLM response latency = API time + KG reasoning + generation \\[110\\].\n\n**Quantization Impact**: 4-bit quantization (LoRA) maintains 92% of Flan-T5 accuracy in KG link prediction \\[134\\].\n\n**Healthcare Penalty**: Clinical KGs add 350ms latency per query versus 120ms in finance (extrapolated from \\[66\\]\\[130\\].\n\n**7\\. Future Evolution: Six Strategic Directions**\n\nBased on current limitations and 2025 research trajectories:\n\n**7.1 Cognitive Architecture Integration**\n\n**LLMs as Graph Inference Engines**: Using chain-of-thought to dynamically rewrite KG schemas (e.g., temporal relationship induction) \\[3\\].\n\n**Autonomous KG Construction**: End-to-end frameworks like K-Builder eliminating manual curation \\[81\\].\n\n**7.2 Efficiency Optimization**\n\n**Distributed Graph Learning**: Partitioning billion-edge KGs across GPU clusters using techniques like DGL-KE \\[64\\].\n\n**Hardware-Accelerated Reasoning**: Dedicated NPUs for subgraph matching (prototypes show 22× speedup) \\[110\\].\n\n**7.3 Trustworthiness Enhancement**\n\n**Provenance Tracking**: Immutable KG-ledgers recording source attribution for regulatory compliance \\[64\\].\n\n**Uncertainty-Aware KGs**: Embedding confidence scores in edges (e.g., drug-interaction probabilities) \\[131\\].\n\n**7.4 Cross-Modal Unification**\n\n**Multi-Modal KGs**: Integrating image/video embeddings via models like ImageBind \\[54\\].\n\n**Cross-Lingual Alignment**: Wikidata mappings enabling low-resource language QA \\[57\\].\n\n**7.5 Adaptive Learning Systems**\n\n**Continuous Knowledge Integration**: Incremental KG updates via LLM-driven change detection \\[12\\].\n\n**Self-Correcting Frameworks**: RAG-KG-IL systems reducing hallucinations by 73% through iterative feedback \\[166\\].\n\n**7.6 Ethical & Regulatory Alignment**\n\n**Bias Auditing Tools**: KG subgraph analysis detecting demographic skews in training data \\[64\\].\n\n**Compliance-by-Design KGs**: Embedding regulatory rules (e.g., GDPR article trees) into graph structures \\[62\\].\n\n**8\\. Conclusion: Symbiotic Co-Evolution**\n\nKnowledge graphs and LLMs are evolving from loosely coupled systems toward **unified neuro-symbolic architectures** where each compensates for the other’s weaknesses. KGs provide the \"structural skeleton\" for factual consistency, while LLMs enable fluid interaction with complex knowledge spaces. As demonstrated by frameworks like KAG and GraphRAG, this integration reduces hallucinations by up to 100% in critical domains while introducing tractable latency tradeoffs \\[29\\]\\[81\\]. Future advancements will hinge on three imperatives: 1) **scalable graph learning** for web-sized KGs; 2) **formalized reasoning semantics** merging neural and symbolic paradigms; and 3) **regulatory-aware designs** ensuring auditable knowledge provenance. The era of isolated knowledge systems has ended—hybrid KG-LLM architectures now form the foundation of trustworthy AI.\n\n\\[AI生成\\]\n\n## References\n\n[1\\. Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph](https://arxiv.org/pdf/2307.07697v2.pdf)\n\n[2\\. Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities](https://arxiv.org/pdf/2505.20099)\n\n[3\\. GitHub - xionghao132/KG-LLM](https://github.com/xionghao132/KG-LLM)\n\n[4\\. Unifying Large Language Models and Knowledge Graphs: A Roadmap](https://arxiv.org/pdf/2306.08302v1)\n\n[5\\. GitHub - Gavince/KnowledgeGraphCourse: 东南大学《知识图谱》研究生课程](https://github.com/Gavince/KnowledgeGraphCourse)\n\n[6\\. A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods](https://www.arxiv.org/pdf/2501.13947)\n\n[7\\. SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven Retrieval-Augmented Generation](http://arxiv.org/pdf/2412.15272)\n\n[8\\. MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models](https://arxiv.org/pdf/2308.09729v2)\n\n[9\\. Research Papers](https://graphrag.com/appendices/research/)\n\n[10\\. LONG-FORM HALLUCINATION DETECTION WITH SELF-ELICITATION](https://openreview.net/pdf?id=r9mYbs8RTH)\n\n[11\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[12\\. THINK-ON-GRAF 2.0: DEEP AND FAITHFUL LARGE LANGUAGE MODEL REASONING WITH KNOWLEDGE-GUIDED RETRIEVAL AUGMENTED GENERATION](https://openreview.net/pdf/7dd45a0c40609519be9016c9bc8256ce294e3bd0.pdf)\n\n[13\\. Knowledge Graph Combined with Retrieval-Augmented Generation for Enhancing LMs Reasoning: A Survey](https://drpress.org/ojs/index.php/ajst/article/view/29613)\n\n[14\\. GitHub - shiliu-egg/KG-LLM-Papers: \\[Paper List\\] Papers integrating knowledge graphs (KGs) and large language models (LLMs)](https://github.com/shiliu-egg/KG-LLM-Papers)\n\n[15\\. Injecting Knowledge Graphs into Large Language Models](http://www.arxiv.org/pdf/2505.07554)\n\n[16\\. Retrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering](https://openreview.net/pdf?id=Km7sEBdyzh)\n\n[17\\. Application of large language models based on knowledge graphs in question-answering systems: A review](https://www.ewadirect.com/proceedings/ace/article/view/15121/pdf)\n\n[18\\. Combining Knowledge Graphs and Large Language Models](https://arxiv.org/html/2407.06564v1)\n\n[21\\. Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://openreview.net/pdf/b9227f2c252fe5de1df93d17b3348428072eda8a.pdf)\n\n[22\\. GitHub - SunYanCN/KnowledgeGraphCourse: 东南大学《知识图谱》研究生课程](https://github.com/SunYanCN/KnowledgeGraphCourse)\n\n[23\\. An Intro to Building Knowledge Graphs](https://opendatascience.com/an-intro-to-building-knowledge-graphs/)\n\n[24\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[25\\. LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph Question-Answering](http://arxiv.org/html/2406.06621v2)\n\n[26\\. GitHub - xionghao132/KG-LLM](https://github.com/xionghao132/KG-LLM)\n\n[27\\. Know^3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering](https://arxiv.org/pdf/2505.12662)\n\n[28\\. Knowledge graphs for LLM grounding and avoiding hallucination](https://community.sap.com/t5/technology-blog-posts-by-sap/knowledge-graphs-for-llm-grounding-and-avoiding-hallucination/ba-p/13779734)\n\n[29\\. Knowledge Graphs for LLM Grounding and Avoiding Hallucination](https://community.sap.com/t5/technology-blogs-by-sap/knowledge-graphs-for-llm-grounding-and-avoiding-hallucination/ba-p/13779734)\n\n[30\\. GraphRAG API: Optimize your LLM with a knowledge graph.](https://infranodus.com/docs/graph-rag-knowledge-graph)\n\n[31\\. GraphRAG: Leveraging Graph-Based Efficiency to Minimize Hallucinations in LLM-Driven RAG for Finance Data](https://hal.science/hal-04907346v1/document)\n\n[32\\. Knowledge Graphs, Large Language Models, and Hallucinations: An NLP Perspective](https://vbn.aau.dk/files/756595643/JoWS_position.pdf)\n\n[33\\. Experiential Networked Intelligence (ENI); Transformer Architecture for Policy Translation; Knowledge-based Reasoning using the Functionalities of Transformers and Knowledge Graphs to Generate Policies](https://docbox.etsi.org/ISG/ENI/Open/ENI030%20%28Release%204%29/GS%20ENI%20030v4.1.5-Approved%20Baseline.docx)\n\n[34\\. Knowledge Hub - LLM as a Judge: Evaluating LLM Outputs and the Challenge of Hallucinations](https://www.factored.ai/knowledge-hub/llm-as-a-judge-evaluating-llm-outputs-and-the-challenge-of-hallucinations)\n\n[41\\. 蚂蚁KAG等主流框架，融合知识图谱与大语言模型实现智能 ...](https://www.cnblogs.com/ting1/p/18994711)\n\n[42\\. On the Evolution of Knowledge Graphs: A Survey and Perspective](https://arxiv.org/pdf/2310.04835)\n\n[43\\. 人工智能和知识图谱：人工智能中知识图谱的概述](https://36kr.com/p/3314423281117190)\n\n[44\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[45\\. 融合知识图谱的大语言模型研究综述](https://www.arocmag.cn/abs/2024.12.0532)\n\n[46\\. 面向电力领域的知识图谱与大模型融合关键技术及其典型应用](https://epjournal.csee.org.cn/gdyjs/cn/article/pdf/preview/10.13336/j.1003-6520.hve.20241018.pdf)\n\n[47\\. 知识图谱和大语言模型的共存之道](https://jishuzhan.net/article/1788804833902333953)\n\n[48\\. Exploring Large Language Models for Knowledge Graph Completion](https://arxiv.org/pdf/2308.13916)\n\n[49\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[50\\. 知识引导下生成式模型协同合作与联合进化](https://www.engineering.org.cn/engi/CN/10.1016/j.eng.2024.12.008)\n\n[51\\. Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities](http://arxiv.org/pdf/2505.20099)\n\n[52\\. Unifying Large Language Models and Knowledge Graphs for Question Answering: Recent Advances and Opportunities](https://www.openproceedings.org/2025/conf/edbt/paper-T4.pdf)\n\n[53\\. A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models](https://arxiv.org/pdf/2501.13958)\n\n[54\\. 2025知识图谱与多模态最新算法创新点](https://www.bilibili.com/video/BV1TEjBzkEun)\n\n[55\\. 2025最热研究方向：知识图谱+LLM大模型，基于知识图谱搭建医疗问答系统，原理详解+代码精讲，绝对通俗易懂！](https://b23.tv/BV1TeV8zUEWQ?t=214)\n\n[56\\. 2025大模型时代的存储管理与数据分析](https://crad.ict.ac.cn/cn/article/pdf/preview/10.7544/issn1000-1239.20253.pdf)\n\n[57\\. 知识图谱与 AI 大语言模型在日语翻译教学中的应用](http://www.issplc.com/upload/pdf/2025/03/11%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E4%B8%8EAI%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9C%A8%E6%97%A5%E8%AF%AD%E7%BF%BB%E8%AF%91%E6%95%99%E5%AD%A6%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.pdf)\n\n[58\\. 基于知识图谱推理驱动的多模态早期中立评估智能体研究](https://www.chinaxiv.org/user/download.htm?uuid=25f8436d-2d07-4b75-b9f4-60a04a8c8a18&filetype=pdf)\n\n[59\\. 大语言模型和知识图谱协同的跨域异质数据查询框架](https://crad.ict.ac.cn/cn/article/pdf/preview/10.7544/issn1000-1239.202440634.pdf)\n\n[60\\. 用LLM赋能GNN，知识蒸馏又又又升级啦！这类融合思路2025继续发顶会](https://www.bilibili.com/video/av113889110922297)\n\n[61\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[62\\. Knowledge Graphs and Their Reciprocal Relationship with Large Language Models](https://www.mdpi.com/2504-4990/7/2/38)\n\n[63\\. A survey on augmenting knowledge graphs (KGs) with large language models (LLMs): models, evaluation metrics, benchmarks, and challenges](https://d-nb.info/1354098625/34)\n\n[64\\. Detecting and Mitigating Bias in LLMs through Knowledge Graph-Augmented Training](https://openreview.net/pdf?id=pAJ4DyQW3s)\n\n[65\\. Combining Domain and Alignment Vectors to Achieve Better Knowledge-Safety Trade-offs in LLMs](https://openreview.net/pdf/ba754339ac6d40dbaa743c2e5af79bb5e6600c73.pdf)\n\n[66\\. Transactions on Graph Data and Knowledge](https://drops.dagstuhl.de/storage/08tgdk/tgdk-vol001/tgdk-vol001-issue001/TGDK.1.1/TGDK.1.1.pdf)\n\n[67\\. REASONING-ENHANCED HEALTHCARE PREDICTIONS WITH KNOWLEDGE GRAPH COMMUNITY RETRIEVAL](https://openreview.net/pdf/3dc8d743ded22f857bf125eceeb050d58546e18d.pdf)\n\n[68\\. Integrated Application of LLM Model and Knowledge Graph in Medical Text Mining and Knowledge Extraction](https://www.clausiuspress.com/assets/default/article/2024/09/14/article_1726289646.pdf)\n\n[69\\. A Survey on Large Language Models for Critical Societal Domains: Finance, Healthcare, and Law](https://openreview.net/pdf/2a94d2314673972d399c929a8a22e26e872091b4.pdf)\n\n[70\\. Generative Artificial Intelligence in Finance: Risk Considerations](https://www.elibrary.imf.org/downloadpdf/view/journals/063/2023/006/article-A001-en.pdf)\n\n[71\\. 翻译：LLM-augmented KG](https://zhuanlan.zhihu.com/p/687182021)\n\n[72\\. 图模互补:知识图谱与大模型融合综述](http://xblx.whu.edu.cn/rc-pub/front/front-article/download/55755469/lowqualitypdf/KG-LLM-MCom%EF%BC%9A%20A%20Survey%20on%20Integration%20of%20Knowledge%20Graph%20and%20Large%20Language%20Model.pdf)\n\n[73\\. R. Speer, Joshua Chin et al. “ConceptNet 5.5: An Open Multilingual Graph of General Knowledge.” AAAI Conference on Artificial Intelligence](https://doi.org/10.1609/aaai.v31i1.11164)\n\n[74\\. CogCommon: Enhancing Cross-Domain Knowledge Extraction with LLM-Assisted Commonality Discovery](https://openreview.net/pdf?id=fR3einDVSo)\n\n[75\\. Incorporating LLMs for Effective and Efficient Recommendation](https://mp.weixin.qq.com/s?__biz=MzA3OTM1MTAyNw%3D%3D&mid=2649556246&idx=3&sn=50c3dadc342238438c0e1a340f1a0a21&chksm=863677d4f950b47e0208472f0b674f6cf1eb94ddee24af8850fa821c8b6fcf20cfda68eb2575&scene=27)\n\n[81\\. AI-Compass GraphRAG技术生态：集成微软GraphRAG、蚂蚁KAG等主流框架，融合知识图谱与大语言模型实现智能检索生成](https://developer.aliyun.com/article/1672579)\n\n[82\\. HuixiangDou2: A Robustly Optimized GraphRAG Approach](https://www.arxiv.org/pdf/2503.06474)\n\n[83\\. GraphRAG: Enhancing Retrieval with Knowledge Graph Intelligence](http://dotzlaw.com/ai-2/graphrag-enhancing-retrieval-with-knowledge-graph-intelligence/)\n\n[84\\. Comprehensive Comparison: KAG Framework vs. GraphRAG](https://atalupadhyay.wordpress.com/2025/01/28/comprehensive-comparison-kag-framework-vs-graphrag/)\n\n[85\\. Patrick Lewis, Ethan Perez et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” ArXiv](https://arxiv.org/abs/2005.11401)\n\n[86\\. 何时在 RAG 中使用图：图检索增强生成的综合分析](https://www.xueshuxiangzi.com/downloads/2025_6_9/2506.05690.pdf)\n\n[87\\. GraphRAG: Leveraging Graph-Based Efficiency to Minimize Hallucinations in LLM-Driven RAG for Finance Data](https://hal.science/hal-04907346/document)\n\n[88\\. The Rise and Evolution of RAG in 2024: A Year in Review](https://ragflow.io/blog/the-rise-and-evolution-of-rag-in-2024-a-year-in-review)\n\n[89\\. GraphRAG原理](https://www.53ai.com/news/knowledgegraph/2025022281490.html)\n\n[90\\. Retrieval-Augmented Generation with Graphs (GraphRAG)](https://arxiv.org/pdf/2501.00309v1)\n\n[91\\. GraphRAG框架总结：开启智能知识的全新时代](https://cloud.tencent.com/developer/article/2498161)\n\n[92\\. DOMAIN SPECIFIC DATA QUALITY FRAMEWORK](https://www.theseus.fi/bitstream/handle/10024/864054/Raja_Komal.pdf?sequence=2)\n\n[93\\. 蚂蚁自研知识增强大模型服务框架KAG，可显著提升知识推理准确率](https://ioda.lntu.edu.cn/info/1147/3525.htm)\n\n[94\\. GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking](https://arxiv.org/pdf/2502.16514v4)\n\n[95\\. Unifying Large Language Models and Knowledge Graphs for Question Answering: Recent Advances and Opportunities](https://www.openproceedings.org/2025/conf/edbt/paper-T4.pdf)\n\n[96\\. 开箱即用的企业大模型应用平台](https://www.813zy.com/news/knowledgegraph/2025010386435.html)\n\n[97\\. 基于图的 RAG 方法总结](https://www.iaiol.com/ji-yu-tu-de-rag-fang-fa-zong-jie-graphrag-graphreader-lightrag-hipporag-he-kag)\n\n[98\\. Graphing the Unseen: Enhancing RAG Efficacy with Knowledge Graphs](https://arno.uvt.nl/show.cgi?fid=183834)\n\n[99\\. 技术雷达](https://www.thoughtworks.com/content/dam/thoughtworks/documents/radar/2025/04/tr_technology_radar_vol_32_cn.pdf)\n\n[101\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[102\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[103\\. Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://openreview.net/pdf/b9227f2c252fe5de1df93d17b3348428072eda8a.pdf)\n\n[104\\. Reducing LLM Hallucinations with Retrieval Prompt Engineering](https://repository.tudelft.nl/file/File_5972554d-1b6e-4314-87f1-dfe21e23cac9)\n\n[105\\. M. Lewis, Yinhan Liu et al. “BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.” Annual Meeting of the Association for Computational Linguistics](https://doi.org/10.18653/v1/2020.acl-main.703)\n\n[106\\. Hallucinations, Quantisations and Test-Time Computations: POTM August](https://www.graphcore.ai/posts/hallucinations-quantisations-and-test-time-computations-august-potm)\n\n[107\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[108\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[109\\. A survey on augmenting knowledge graphs (KGs) with large language models (LLMs): models, evaluation metrics, benchmarks, and challenges](https://d-nb.info/1354098625/34)\n\n[110\\. RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph Learning Integration](https://arxiv.org/pdf/2503.13514)\n\n[111\\. Knowledge Graph-RAG Approach Based LLM](https://www.infogain.com/blog/knowledge-graph-rag-approach-based-llm/)\n\n[112\\. A Survey on Hallucination in Large Language and Foundation Models](https://www.preprints.org/frontend/manuscript/c5f20698cf44a95d88e568ddde2066e0/download_pub)\n\n[113\\. LLM Evaluation Metrics: The Ultimate LLM Evaluation Guide](https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#:~:text=LLM%20evaluation%20metrics%20such%20as,just%20be%20the%20LLM%20itself.)\n\n[114\\. Combining LoRA to GPT-Neo to Reduce Large Language Model Hallucination](https://assets-eu.researchsquare.com/files/rs-4515250/v1_covered_f4158bc5-ac29-4c45-b782-d2a7767a33a0.pdf)\n\n[115\\. INSIDE: LLMs’ INTERNAL STATES RETAIN THE POWER OF HALLUCINATION DETECTION](https://openreview.net/pdf?id=Zj12nzlQbz)\n\n[116\\. Hallucination is Inevitable: An Innate Limitation of Large Language Models](https://arxiv.org/pdf/2401.11817)\n\n[121\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[122\\. Hugo Touvron, Louis Martin et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” ArXiv](https://arxiv.org/abs/2307.09288)\n\n[123\\. Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking](https://arxiv.org/pdf/2505.23495)\n\n[124\\. Qiao Jin, Bhuwan Dhingra et al. “PubMedQA: A Dataset for Biomedical Research Question Answering.” Conference on Empirical Methods in Natural Language Processing](https://doi.org/10.18653/v1/D19-1259)\n\n[125\\. K. Singhal, Shekoofeh Azizi et al. “Large language models encode clinical knowledge.” Nature](https://doi.org/10.1038/s41586-023-06291-2)\n\n[126\\. Di Jin, Eileen Pan et al. “What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams.” ArXiv](https://doi.org/10.20944/PREPRINTS202105.0498.V1)\n\n[127\\. A survey on augmenting knowledge graphs (KGs) with large language models (LLMs): models, evaluation metrics, benchmarks, and challenges](https://d-nb.info/1354098625/34)\n\n[128\\. Comparative Analysis of LLM-based Market Prediction and Human Expertise with Sentiment Analysis and Machine Learning Integration](https://easychair.org/publications/preprint/mr3d/open)\n\n[129\\. Evaluation of Large Language Models: Review of Metrics, Applications, and Methodologies](https://www.preprints.org/manuscript/202504.0369/v1/download)\n\n[130\\. \"Give Me BF16 or Give Me Death\"? ACCURACY-PERFORMANCE TRADE-OFFS IN LLM QUANTIZATION](https://arxiv.org/pdf/2411.02355v1)\n\n[131\\. Comparative Analysis of GPT-4Vision, GPT-4 and Open Source LLMs in Clinical Diagnostic Accuracy: A Benchmark Against Human Expertise](https://www.medrxiv.org/content/10.1101/2023.11.03.23297957v2.full.pdf)\n\n[132\\. \"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM Quantization](https://arxiv.org/pdf/2411.02355)\n\n[133\\. LLM-KG-Bench 3.0: A Compass for Semantic Technology Capabilities in the Ocean of LLMs](https://arxiv.org/pdf/2505.13098)\n\n[134\\. Knowledge Graph Large Language Model (KG-LLM) for Link Prediction](https://arxiv.org/html/2403.07311v5)\n\n[135\\. Efficient Knowledge Infusion via KG-LLM Alignment](http://arxiv.org/html/2406.03746v1)\n\n[136\\. KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques](http://arxiv.org/html/2403.05881v3)\n\n[137\\. A Survey on Hallucination in Large Language and Foundation Models](https://www.preprints.org/frontend/manuscript/c5f20698cf44a95d88e568ddde2066e0/download_pub)\n\n[138\\. A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics](https://papers-pdfs.assets.alphaxiv.org/2310.05694v3.pdf)\n\n[139\\. Comparison of the accuracy performances of the Gemini Advanced, the GPT-4, the Copilot, and the GPT-3.5 models in medical imaging systems: A Zero-shot prompting analysis](https://dergipark.org.tr/en/download/article-file/3965699)\n\n[141\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[142\\. Thomas Kipf, M. Welling. “Semi-Supervised Classification with Graph Convolutional Networks.” ArXiv](https://arxiv.org/abs/1609.02907)\n\n[143\\. Petar Velickovic, Guillem Cucurull et al. “Graph Attention Networks.” ArXiv](https://doi.org/10.17863/CAM.48429)\n\n[144\\. Keyulu Xu, Weihua Hu et al. “How Powerful are Graph Neural Networks?.” ArXiv](https://arxiv.org/abs/1810.00826)\n\n[145\\. Hanxiao Liu, K. Simonyan et al. “DARTS: Differentiable Architecture Search.” ArXiv](https://arxiv.org/abs/1806.09055)\n\n[146\\. GitHub - RichardZhong/OpenSPG-KAG: KAG is a knowledge-enhanced generation framework based on OpenSPG engine, which is used to build knowledge-enhanced rigorous decision-making and information retrieval knowledge services](https://github.com/RichardZhong/OpenSPG-KAG)\n\n[147\\. AI-Compass GraphRAG技术生态：集成微软GraphRAG、蚂蚁KAG等主流框架，融合知识图谱与大语言模型实现智能检索生成](https://developer.aliyun.com/article/1672579)\n\n[148\\. KAG: A Professional Knowledge Base Q&A Framework for Hybrid Knowledge Graph and Vector Retrieval](https://www.aisharenet.com/en/kag/)\n\n[149\\. Ant Launches Self-Developed Knowledge-Enhanced Large Model Service Framework KAG](https://www.aibase.com/news/11733)\n\n[150\\. OpenSPG](https://github.com/openspg)\n\n[151\\. KAG: 知识增强生成框架](https://raw.githubusercontent.com/OpenSPG/KAG/master/README.md)\n\n[152\\. Knowledge Graphs and GraphRAG with AWS and Neo4j](https://docs.aws.amazon.com/pdfs/architecture-diagrams/latest/knowledge-graphs-and-graphrag-with-neo4j/knowledge-graphs-and-graphrag-with-neo4j.pdf)\n\n[153\\. KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation](https://arxiv.org/pdf/2409.13731v2)\n\n[154\\. The Rise and Evolution of RAG in 2024: A Year in Review](https://ragflow.io/blog/the-rise-and-evolution-of-rag-in-2024-a-year-in-review)\n\n[155\\. 2024 RAG目録、RAG応用戦略 100以上](https://www.aisharenet.com/ja/2024ai-2/)\n\n[156\\. KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG](https://arxiv.org/pdf/2502.09304)\n\n[157\\. GraphRAG框架总结：开启智能知识的全新时代](https://cloud.tencent.com/developer/article/2498161?policyId=1004)\n\n[158\\. GraphRAG：大型语言模型与知识图谱的融合](http://heavi.cn/posts/graphrag/)\n\n[159\\. GitHub - awslabs/graphrag-toolkit](https://github.com/awslabs/graphrag-toolkit/tree/d95590987b0e956c80a1faf9e22488e2883d59af)\n\n[160\\. Graph Retrieval-Augmented Generation: A Survey](https://arxiv.org/pdf/2408.08921v1)\n\n[161\\. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems](https://arxiv.org/abs/1706.03762)\n\n[162\\. Tom B. Brown, Benjamin Mann et al. “Language Models are Few-Shot Learners.” ArXiv](https://arxiv.org/abs/2005.14165)\n\n[163\\. Jacob Devlin, Ming-Wei Chang et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” North American Chapter of the Association for Computational Linguistics](https://doi.org/10.18653/v1/N19-1423)\n\n[164\\. Retrieval-Augmented Generation and Hallucination in Large Language Models: A Scholarly Overview](https://saspublishers.com/media/articles/SJET_135_328-330.pdf)\n\n[165\\. Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://openreview.net/pdf/b9227f2c252fe5de1df93d17b3348428072eda8a.pdf)\n\n[166\\. RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph Learning Integration](https://arxiv.org/pdf/2503.13514)\n\n[167\\. Hallucination Detection in Large Language Models with Metamorphic Relations](https://arxiv.org/pdf/2502.15844)\n\n[168\\. Hugo Touvron, Thibaut Lavril et al. “LLaMA: Open and Efficient Foundation Language Models.” ArXiv](https://doi.org/10.48550/arXiv.2302.13971)\n\n[169\\. Long Ouyang, Jeff Wu et al. “Training language models to follow instructions with human feedback.” ArXiv](https://arxiv.org/abs/2203.02155)\n\n[170\\. Evaluating Retrieval Quality in Retrieval-Augmented Generation](https://dl.acm.org/doi/10.1145/3626772.3657957)\n\n[171\\. Enhancing Textbook Question Answering with Knowledge Graph-Augmented Large Language Models](https://openreview.net/pdf/09c81647b4abc56f1d8be9492d6ffc2546426144.pdf)\n\n[172\\. Knowledge Graphs At EMNLP 2021](https://www.topbots.com/knowledge-graphs-emnlp-2021/)\n\n[173\\. Probabilistic Medical Predictions of Large Language Models](https://pmc.ncbi.nlm.nih.gov/articles/PMC11659327/)"
  }
]